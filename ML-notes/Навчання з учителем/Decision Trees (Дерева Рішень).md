
## Визначення

**Decision Tree** — це алгоритм машинного навчання, який будує модель у вигляді деревоподібної структури для прийняття рішень на основі ознак даних.

**Головна ідея:** На кожному кроці дерево ділить дані на підмножини, обираючи найкращу ознаку для розділення, поки не досягне критерію зупинки.

## Структура дерева
```
        [Root Node]
       /           \
    [Node]       [Node]
    /    \       /    \
[Leaf] [Leaf] [Node] [Leaf]
                /  \
            [Leaf] [Leaf]
```

### Основні компоненти

| Компонент | Опис |
|-----------|------|
| **Root Node** (Корінь) | Перший вузол дерева, представляє всі дані |
| **Internal Node** (Внутрішній вузол) | Вузол з умовою розділення за певною ознакою |
| **Leaf Node** (Листок) | Кінцевий вузол з рішенням (клас або значення) |
| **Branch** (Гілка) | З'єднання між вузлами |
| **Split** (Розділення) | Умова, за якою дані діляться у вузлі |

## Типи дерев рішень

### 1. Classification Trees (Класифікаційні дерева)

Передбачають **категоріальну** змінну (клас).

**Приклад:** Схвалити чи відхилити кредит, діагностувати хворобу

### 2. Regression Trees (Регресійні дерева)

Передбачають **неперервну** змінну (число).

**Приклад:** Передбачити ціну будинку, температуру

## Принцип роботи

### Алгоритм побудови (Recursive Binary Splitting)

1. **Початок:** Всі дані в кореневому вузлі
2. **Вибір ознаки:** Обрати ознаку, яка найкраще розділяє дані
3. **Розділення:** Створити дочірні вузли
4. **Рекурсія:** Повторити для кожного дочірнього вузла
5. **Зупинка:** Коли досягнуто критерій зупинки

### Критерії вибору розділення

#### Для класифікації

**1. Gini Impurity**
$$Gini = 1 - \sum_{i=1}^{n} p_i^2$$

**2. Entropy (Information Gain)**
$$Entropy = -\sum_{i=1}^{n} p_i \log_2(p_i)$$

**3. Classification Error**
$$Error = 1 - \max(p_i)$$

#### Для регресії

**MSE (Mean Squared Error)**
$$MSE = \frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y})^2$$

## Простий приклад: Схвалення кредиту

### Дані
| ID | Дохід | Кредитна історія | Борг | Схвалено |
|----|-------|------------------|------|----------|
| 1  | Високий | Добра | Низький | ✅ Так |
| 2  | Низький | Погана | Високий | ❌ Ні |
| 3  | Середній | Добра | Низький | ✅ Так |
| 4  | Високий | Погана | Високий | ❌ Ні |
| 5  | Низький | Добра | Низький | ✅ Так |

### Побудова дерева
```
                [Кредитна історія?]
                /                 \
           Добра                  Погана
            /                        \
      [Схвалено: ✅]            [Дохід?]
                                /        \
                          Високий      Низький
                             /              \
                    [Борг?]         [Відхилено: ❌]
                    /      \
              Низький    Високий
                 /          \
        [Схвалено: ✅]  [Відхилено: ❌]
```

### Логіка рішення

**Правила:
**
1. Якщо **Кредитна історія = Добра** → **Схвалено ✅**
2. Якщо **Кредитна історія = Погана** AND **Дохід = Високий** AND **Борг = Низький** → **Схвалено ✅**
3. Інакше → **Відхилено ❌**

## Складний приклад: Діагностика хвороби

### Дані пацієнтів

100 пацієнтів з ознаками:

- Температура (>38°C / ≤38°C)
- Кашель (Так / Ні)
- Втома (Так / Ні)
- Вік (<40 / ≥40)

### Крок 1: Корінь дерева

**Початкова ентропія:**

- Хворі: 60
- Здорові: 40

$$H = -\frac{60}{100}\log_2(\frac{60}{100}) - \frac{40}{100}\log_2(\frac{40}{100}) = 0.97$$

### Крок 2: Вибір першого розділення

Розглядаємо всі ознаки:

| Ознака | IG | Обрати? |
|--------|-------|---------|
| Температура >38°C | 0.26 | |
| Кашель | 0.09 | |
| Втома | 0.15 | |
| Вік ≥40 | 0.31 | ✅ Так |

**Обираємо "Вік ≥40"** (найбільший Information Gain)

### Крок 3: Рекурсивне розділення
```
                    [Вік ≥40?]
                   /          \
               Так (45)      Ні (55)
              Хворі: 35    Хворі: 25
              Здорові: 10  Здорові: 30
                 |              |
          [Температура?]  [Температура?]
          /           \    /           \
      >38°C         ≤38°C ...         ...
```

### Результат

**Правила діагностики:**
1. Вік ≥40 AND Температура >38°C → Хворий (точність 90%)
2. Вік ≥40 AND Температура ≤38°C AND Кашель = Так → Хворий
3. Вік <40 AND Втома = Ні → Здоровий
4. ...

## Критерії зупинки

Дерево припиняє рости, коли:

1. **Максимальна глибина** досягнута

```python
   max_depth=5  # Не більше 5 рівнів
```

2. **Мінімальна кількість зразків** у вузлі

```python
   min_samples_split=10  # Мінімум 10 зразків для розділення
```

3. **Мінімальна кількість зразків** у листку

```python
   min_samples_leaf=5  # Мінімум 5 зразків у листку
```

4. **Чистота досягнута**
   - Всі зразки належать одному класу
   - Gini = 0 або Entropy = 0

4. **Мінімальне покращення**

```python
   min_impurity_decrease=0.01  # Розділення має зменшити impurity хоча б на 0.01
```

## Переваги та недоліки

### Переваги ✓

| Перевага | Пояснення |
|----------|-----------|
| **Інтерпретованість** | Легко зрозуміти та пояснити рішення |
| **Візуалізація** | Можна намалювати дерево |
| **Мало підготовки даних** | Не потрібна нормалізація |
| **Працює з різними типами** | Числові та категоріальні ознаки |
| **Нелінійні залежності** | Може моделювати складні взаємодії |
| **Швидкість** | Швидке навчання та передбачення |
| **Автоматичний відбір ознак** | Використовує лише важливі ознаки |

### Недоліки ✗

| Недолік | Пояснення |
|---------|-----------|
| **Перенавчання** | Схильність до overfitting |
| **Нестабільність** | Малі зміни даних → велика зміна дерева |
| **Зміщення** | Перевага ознакам з багатьма значеннями |
| **Неоптимальність** | Жадібний алгоритм (локальний оптимум) |
| **Складні межі** | Важко моделювати діагональні межі |
| **Дисбаланс класів** | Погано працює з незбалансованими даними |

## Приклад коду (scikit-learn)
```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import numpy as np

# Генерація даних
np.random.seed(42)
X = np.random.randn(200, 2)
y = (X[:, 0] + X[:, 1] > 0).astype(int)

# Розділення на train/test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# Навчання моделі
clf = DecisionTreeClassifier(
    criterion='gini',      # або 'entropy'
    max_depth=5,
    min_samples_split=10,
    min_samples_leaf=5,
    random_state=42
)

clf.fit(X_train, y_train)

# Передбачення
y_pred = clf.predict(X_test)

# Оцінка
accuracy = accuracy_score(y_test, y_pred)
print(f"Точність: {accuracy:.4f}")

# Важливість ознак
print(f"Feature importance: {clf.feature_importances_}")
```

## Візуалізація дерева
```python
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

plt.figure(figsize=(20, 10))
plot_tree(
    clf,
    feature_names=['X1', 'X2'],
    class_names=['Class 0', 'Class 1'],
    filled=True,
    rounded=True,
    fontsize=12
)
plt.show()
```

## Гіперпараметри

### Основні параметри

| Параметр | Значення за замовчуванням | Опис |
|----------|---------------------------|------|
| `criterion` | 'gini' | Функція для оцінки якості розділення |
| `max_depth` | None | Максимальна глибина дерева |
| `min_samples_split` | 2 | Мінімум зразків для розділення вузла |
| `min_samples_leaf` | 1 | Мінімум зразків у листку |
| `max_features` | None | Максимум ознак для розділення |
| `random_state` | None | Seed для відтворюваності |
| `max_leaf_nodes` | None | Максимум листків |
| `min_impurity_decrease` | 0.0 | Мінімальне зменшення impurity |

### Поради щодо налаштування

**Для запобігання overfitting:**

```python
clf = DecisionTreeClassifier(
    max_depth=5,              # Обмежити глибину
    min_samples_split=20,     # Більше зразків для розділення
    min_samples_leaf=10,      # Більше зразків у листку
    max_leaf_nodes=20         # Обмежити кількість листків
)
```

**Для складних даних:**

```python
clf = DecisionTreeClassifier(
    criterion='entropy',      # Використати Information Gain
    max_depth=10,
    min_samples_split=5,
    max_features='sqrt'       # Випадковий підбір ознак
)
```

## Порівняння Gini vs Entropy

| Характеристика | Gini | Entropy |
|----------------|------|---------|
| **Формула** | $1 - \sum p_i^2$ | $-\sum p_i \log_2 p_i$ |
| **Швидкість** | Швидше | Повільніше |
| **Чутливість** | Менша | Більша |
| **Діапазон** | [0, 0.5] | [0, 1] |
| **Використання** | CART, sklearn за замовчуванням | ID3, C4.5 |

**На практиці:** Різниця мінімальна, Gini зазвичай швидше

## Варіації алгоритмів

### ID3 (Iterative Dichotomiser 3)

- Використовує **Information Gain** (Entropy)
- Тільки категоріальні ознаки
- Схильність до overfitting

### C4.5

- Покращення ID3
- Використовує **Gain Ratio** (нормалізований IG)
- Працює з неперервними ознаками
- Pruning (обрізання) дерева

### CART (Classification and Regression Trees)

- Використовує **Gini Impurity**
- Бінарні розділення
- Працює з класифікацією та регресією
- **Використовується в scikit-learn**

### CHAID (Chi-squared Automatic Interaction Detection)

- Використовує **χ² тест**
- Множинні розділення (не тільки бінарні)
- Для категоріальних змінних

## Feature Importance (Важливість ознак)

Дерева автоматично оцінюють важливість кожної ознаки:

```python
importances = clf.feature_importances_

# Приклад виводу
# [0.65, 0.25, 0.08, 0.02]
# Ознака 0 найважливіша (65%), ознака 3 майже не використовується (2%)
```

**Розрахунок:**
$$Importance(feature) = \frac{\sum_{nodes} (impurity\ decrease)}{total\ decrease}$$

## Pruning (Обрізання дерева)

### Pre-pruning (Попереднє обрізання)

Зупинка росту дерева під час навчання:
- `max_depth`
- `min_samples_split`
- `min_samples_leaf`

### Post-pruning (Наступне обрізання)

Обрізання дерева після повного росту:
- Cost Complexity Pruning (α-pruning)
- Reduced Error Pruning
```python
# Cost Complexity Pruning в sklearn
path = clf.cost_complexity_pruning_path(X_train, y_train)
ccp_alphas = path.ccp_alphas

clfs = []
for ccp_alpha in ccp_alphas:
    clf = DecisionTreeClassifier(ccp_alpha=ccp_alpha, random_state=42)
    clf.fit(X_train, y_train)
    clfs.append(clf)
```

## Коли використовувати Decision Trees

### Ідеально підходить ✓

- Потрібна **інтерпретованість** моделі
- Дані містять **категоріальні ознаки**
- Необхідно **швидке навчання**
- Є **нелінійні залежності**
- Потрібен **baseline** для складніших моделей

### Краще використати інше ✗

- Потрібна **максимальна точність** → Random Forest, Gradient Boosting
- Дані **лінійно розділимі** → Logistic Regression, SVM
- Малий датасет з **багатьма ознаками** → Regularized models
- Потрібна **стабільність** → Ensemble методи

## Ансамблі дерев

Decision Trees часто використовуються в ансамблях:

### Random Forest

- Багато дерев на різних підвибірках
- Усереднення передбачень
- Зменшує overfitting

### Gradient Boosting

- Послідовне навчання дерев
- Кожне дерево виправляє помилки попереднього
- XGBoost, LightGBM, CatBoost

### AdaBoost

- Адаптивне підвищення ваги складних прикладів
- Комбінація слабких learners

## Практичні поради

1. **Почніть з простого дерева** (`max_depth=3-5`)
2. **Візуалізуйте дерево** для розуміння логіки
3. **Використовуйте cross-validation** для підбору гіперпараметрів
4. **Перевіряйте feature importance** для відбору ознак
5. **Якщо overfitting** → обмежте глибину або використайте pruning
6. **Для production** → розгляньте Random Forest або Gradient Boosting

## Ключові висновки

> Decision Trees — це інтуїтивний та інтерпретований алгоритм, який будує деревоподібну модель рішень на основі ознак даних.

**Основні принципи:**
- Рекурсивне розділення даних за найкращою ознакою
- Використання Gini або Entropy для оцінки якості розділення
- Легко інтерпретувати, але схильні до overfitting
- Основа для потужних ансамблевих методів

**Формула вибору розділення:**
$$Best\ Split = \arg\max_{feature, threshold} (Impurity_{parent} - Weighted\ Impurity_{children})$$