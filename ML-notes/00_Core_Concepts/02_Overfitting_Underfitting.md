# Overfitting & Underfitting (–ü–µ—Ä–µ–Ω–∞–≤—á–∞–Ω–Ω—è —Ç–∞ –ù–µ–¥–æ–Ω–∞–≤—á–∞–Ω–Ω—è)

## –©–æ —Ü–µ?

**Overfitting** ‚Äî –º–æ–¥–µ–ª—å **–∑–∞–Ω–∞–¥—Ç–æ –¥–æ–±—Ä–µ –∑–∞–ø–∞–º'—è—Ç–æ–≤—É—î —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ**, –≤–∫–ª—é—á–Ω–æ –∑ —à—É–º–æ–º —Ç–∞ –≤–∏–ø–∞–¥–∫–æ–≤–∏–º–∏ –≤—ñ–¥—Ö–∏–ª–µ–Ω–Ω—è–º–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç: –≤—ñ–¥–º—ñ–Ω–Ω—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –Ω–∞ train, –ø–æ–≥–∞–Ω—ñ –Ω–∞ test.

**Underfitting** ‚Äî –º–æ–¥–µ–ª—å **–∑–∞–Ω–∞–¥—Ç–æ –ø—Ä–æ—Å—Ç–∞** —ñ –Ω–µ –º–æ–∂–µ –≤–ª–æ–≤–∏—Ç–∏ —Å–ø—Ä–∞–≤–∂–Ω—ñ –∑–∞–∫–æ–Ω–æ–º—ñ—Ä–Ω–æ—Å—Ç—ñ –≤ –¥–∞–Ω–∏—Ö. –†–µ–∑—É–ª—å—Ç–∞—Ç: –ø–æ–≥–∞–Ω—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ —ñ –Ω–∞ train, —ñ –Ω–∞ test.

**–Ü–¥–µ–∞–ª—å–Ω–∞ –º–æ–¥–µ–ª—å** ‚Äî –∑–Ω–∞—Ö–æ–¥–∏—Ç—å –±–∞–ª–∞–Ω—Å –º—ñ–∂ —Å–∫–ª–∞–¥–Ω—ñ—Å—Ç—é —Ç–∞ —É–∑–∞–≥–∞–ª—å–Ω–µ–Ω–Ω—è–º, –¥–æ–±—Ä–µ –ø—Ä–∞—Ü—é—î –Ω–∞ –Ω–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö.

## –ù–∞–≤—ñ—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ?

- üéØ **–î—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –ø—Ä–æ–±–ª–µ–º –º–æ–¥–µ–ª—ñ** ‚Äî —á–æ–º—É –ø–æ–≥–∞–Ω–æ –ø—Ä–∞—Ü—é—î
- üîç **–í–∏–±—ñ—Ä –ø—Ä–∞–≤–∏–ª—å–Ω–æ—ó —Å–∫–ª–∞–¥–Ω–æ—Å—Ç—ñ** ‚Äî –Ω–µ –∑–∞–Ω–∞–¥—Ç–æ –ø—Ä–æ—Å—Ç–æ, –Ω–µ –∑–∞–Ω–∞–¥—Ç–æ —Å–∫–ª–∞–¥–Ω–æ
- üìä **–ü–æ–∫—Ä–∞—â–µ–Ω–Ω—è —É–∑–∞–≥–∞–ª—å–Ω–µ–Ω–Ω—è** ‚Äî –∫—Ä–∞—â–∞ —Ä–æ–±–æ—Ç–∞ –Ω–∞ –Ω–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö
- ‚öôÔ∏è **–ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –≥—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤** ‚Äî —â–æ –∫—Ä—É—Ç–∏—Ç–∏
- üöÄ **Production-ready –º–æ–¥–µ–ª—ñ** ‚Äî —Å—Ç–∞–±—ñ–ª—å–Ω—ñ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
- üí∞ **–ï–∫–æ–Ω–æ–º—ñ—è —Ä–µ—Å—É—Ä—Å—ñ–≤** ‚Äî –Ω–µ –≤–∏—Ç—Ä–∞—á–∞—Ç–∏ —á–∞—Å –Ω–∞ –ø–æ–≥–∞–Ω—ñ –º–æ–¥–µ–ª—ñ

## –ö–æ–ª–∏ –≤–∞–∂–ª–∏–≤–æ —Ä–æ–∑—É–º—ñ—Ç–∏?

**–ü–æ—Ç—Ä—ñ–±–Ω–æ:**

- –ú–æ–¥–µ–ª—å –¥–æ–±—Ä–µ –ø—Ä–∞—Ü—é—î –Ω–∞ train, –∞–ª–µ –ø–æ–≥–∞–Ω–æ –Ω–∞ test
- –í–∏–±–∏—Ä–∞—î—à –º—ñ–∂ –ø—Ä–æ—Å—Ç–æ—é —Ç–∞ —Å–∫–ª–∞–¥–Ω–æ—é –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–æ—é
- **Debugging ML –º–æ–¥–µ–ª—ñ** ‚Äî —â–æ –Ω–µ —Ç–∞–∫?
- –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—ó
- Feature engineering ‚Äî –¥–æ–¥–∞–≤–∞—Ç–∏ —á–∏ –Ω—ñ –æ–∑–Ω–∞–∫–∏

**–ù–µ –ø–æ—Ç—Ä—ñ–±–Ω–æ:**

- –ú–æ–¥–µ–ª—å —ñ–¥–µ –≤ production —ñ –¥–æ–±—Ä–µ –ø—Ä–∞—Ü—é—î
- Train = Test scores (—Ö–æ—Ä–æ—à—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏)

---

## Underfitting (–ù–µ–¥–æ–Ω–∞–≤—á–∞–Ω–Ω—è)

### –©–æ —Ü–µ?

**Underfitting** ‚Äî –º–æ–¥–µ–ª—å **–Ω–µ –º–æ–∂–µ –≤–ª–æ–≤–∏—Ç–∏** –Ω–∞–≤—ñ—Ç—å –æ—Å–Ω–æ–≤–Ω—ñ –ø–∞—Ç—Ç–µ—Ä–Ω–∏ –≤ –¥–∞–Ω–∏—Ö —á–µ—Ä–µ–∑ **–Ω–∞–¥–º—ñ—Ä–Ω—É –ø—Ä–æ—Å—Ç–æ—Ç—É**.

### –í—ñ–∑—É–∞–ª—å–Ω–∞ —ñ–Ω—Ç—É—ó—Ü—ñ—è

```
–°–ø—Ä–∞–≤–∂–Ω—ñ –¥–∞–Ω—ñ (–ø–∞—Ä–∞–±–æ–ª–∞):

     y
     |      ‚Ä¢
     |    ‚Ä¢   ‚Ä¢
     |  ‚Ä¢       ‚Ä¢
     | ‚Ä¢         ‚Ä¢
     |‚Ä¢           ‚Ä¢
     |_______________ x

–õ—ñ–Ω—ñ–π–Ω–∞ –º–æ–¥–µ–ª—å (underfitting):

     y
     |      ‚Ä¢
     |    ‚Ä¢ | ‚Ä¢
     |  ‚Ä¢   |   ‚Ä¢     ‚Üê –ü—Ä—è–º–∞ –Ω–µ –º–æ–∂–µ –≤–ª–æ–≤–∏—Ç–∏ –∫—Ä–∏–≤—É!
     | ‚Ä¢‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚Ä¢
     |‚Ä¢     |     ‚Ä¢
     |______|________ x
     
Train Error: –í–ò–°–û–ö–ò–ô ‚ùå
Test Error: –í–ò–°–û–ö–ò–ô ‚ùå
Gap: –ú–ê–õ–ò–ô
```

### –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏

| –ú–µ—Ç—Ä–∏–∫–∞ | –ó–Ω–∞—á–µ–Ω–Ω—è | –Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü—ñ—è |
|---------|----------|---------------|
| **Train Score** | –ù–∏–∑—å–∫–∏–π (< 0.6) | ‚ùå –ü–æ–≥–∞–Ω–æ –Ω–∞–≤—ñ—Ç—å –Ω–∞ train |
| **Test Score** | –ù–∏–∑—å–∫–∏–π (< 0.6) | ‚ùå –ü–æ–≥–∞–Ω–æ –Ω–∞ test |
| **Gap (Train - Test)** | –ú–∞–ª–∏–π (< 0.05) | Scores –±–ª–∏–∑—å–∫—ñ |
| **–î—ñ–∞–≥–Ω–æ–∑** | –ú–æ–¥–µ–ª—å –∑–∞–Ω–∞–¥—Ç–æ –ø—Ä–æ—Å—Ç–∞ | HIGH BIAS |

### –û–∑–Ω–∞–∫–∏ underfitting

#### 1. –ü–æ–≥–∞–Ω—ñ –º–µ—Ç—Ä–∏–∫–∏ –Ω–∞ train

```python
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

# –ù–µ–ª—ñ–Ω—ñ–π–Ω—ñ –¥–∞–Ω—ñ
X = np.linspace(0, 10, 100).reshape(-1, 1)
y = np.sin(X).ravel() + np.random.normal(0, 0.1, 100)

# –õ—ñ–Ω—ñ–π–Ω–∞ –º–æ–¥–µ–ª—å
lr = LinearRegression()
lr.fit(X, y)

train_score = lr.score(X, y)
print(f"Train R¬≤: {train_score:.3f}")  # –ù–∞–ø—Ä–∏–∫–ª–∞–¥: 0.001 ‚ùå

# –Ø–∫—â–æ train score –ø–æ–≥–∞–Ω–∏–π ‚Üí underfitting!
```

#### 2. Train ‚âà Test (–æ–±–∏–¥–≤–∞ –ø–æ–≥–∞–Ω—ñ)

```python
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

lr.fit(X_train, y_train)

train_score = lr.score(X_train, y_train)
test_score = lr.score(X_test, y_test)

print(f"Train: {train_score:.3f}")  # 0.002
print(f"Test:  {test_score:.3f}")   # 0.001
print(f"Gap:   {train_score - test_score:.3f}")  # 0.001 (–º–∞–ª–∏–π)

# –û–±–∏–¥–≤–∞ –ø–æ–≥–∞–Ω—ñ, gap –º–∞–ª–∏–π ‚Üí underfitting!
```

#### 3. Learning curve –≤–∏—Ö–æ–¥–∏—Ç—å –Ω–∞ "–ø–ª–∞—Ç–æ"

```
R¬≤ Score
    |
0.6 |  Train ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    |  Test  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  ‚Üê –û–±–∏–¥–≤—ñ –∫—Ä–∏–≤—ñ –Ω–∏–∑—å–∫–æ —ñ –ø–ª–æ—Å–∫–æ
0.4 |
    |
0.2 |
    |_____________________________
        Training Set Size
    
–ë—ñ–ª—å—à–µ –¥–∞–Ω–∏—Ö –ù–ï –¥–æ–ø–æ–º–æ–∂–µ!
```

#### 4. –í—ñ–∑—É–∞–ª—å–Ω–∏–π –∞–Ω–∞–ª—ñ–∑

```python
import matplotlib.pyplot as plt

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
plt.figure(figsize=(10, 6))
plt.scatter(X_train, y_train, alpha=0.5, label='Train data')
plt.scatter(X_test, y_test, alpha=0.5, label='Test data')
plt.plot(X, lr.predict(X), 'r-', linewidth=2, label='Model')
plt.plot(X, np.sin(X), 'g--', linewidth=2, label='True function')
plt.legend()
plt.title('Underfitting: Linear model on non-linear data')
plt.show()

# –Ø–∫—â–æ –º–æ–¥–µ–ª—å —è–≤–Ω–æ –Ω–µ –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—î –¥–∞–Ω–∏–º ‚Üí underfitting!
```

### –ü—Ä–∏—á–∏–Ω–∏ underfitting

#### 1. –ú–æ–¥–µ–ª—å –∑–∞–Ω–∞–¥—Ç–æ –ø—Ä–æ—Å—Ç–∞

```python
# ‚ùå –õ—ñ–Ω—ñ–π–Ω–∞ —Ä–µ–≥—Ä–µ—Å—ñ—è –Ω–∞ —Å–∫–ª–∞–¥–Ω–∏—Ö –Ω–µ–ª—ñ–Ω—ñ–π–Ω–∏—Ö –¥–∞–Ω–∏—Ö
lr = LinearRegression()

# ‚ùå Logistic Regression –Ω–∞ –¥–∞–Ω–∏—Ö –∑ —Å–∫–ª–∞–¥–Ω–æ—é boundary
from sklearn.linear_model import LogisticRegression
clf = LogisticRegression()

# ‚ùå Decision Tree –∑ max_depth=1
from sklearn.tree import DecisionTreeRegressor
dt = DecisionTreeRegressor(max_depth=1)  # –ó–∞–Ω–∞–¥—Ç–æ –ø—Ä–æ—Å—Ç–æ!
```

#### 2. –ù–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ –æ–∑–Ω–∞–∫

```python
# ‚ùå –¢—ñ–ª—å–∫–∏ –æ–¥–Ω–∞ –æ–∑–Ω–∞–∫–∞ –¥–ª—è —Å–∫–ª–∞–¥–Ω–æ—ó –∑–∞–¥–∞—á—ñ
X = df[['sqft']]  # –¢—ñ–ª—å–∫–∏ –ø–ª–æ—â–∞
y = df['price']   # –¶—ñ–Ω–∞ –∑–∞–ª–µ–∂–∏—Ç—å –≤—ñ–¥ –±–∞–≥–∞—Ç—å–æ—Ö —Ñ–∞–∫—Ç–æ—Ä—ñ–≤!

# ‚úÖ –ë—ñ–ª—å—à–µ –æ–∑–Ω–∞–∫
X = df[['sqft', 'bedrooms', 'location', 'age', 'condition']]
```

#### 3. –ó–∞–Ω–∞–¥—Ç–æ —Å–∏–ª—å–Ω–∞ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—è

```python
# ‚ùå Ridge –∑ –¥—É–∂–µ –≤–µ–ª–∏–∫–∏–º Œª
from sklearn.linear_model import Ridge
ridge = Ridge(alpha=1000.0)  # –ó–∞–Ω–∞–¥—Ç–æ —Å–∏–ª—å–Ω–æ!

# –ú–æ–¥–µ–ª—å —Å—Ç–∞—î –º–∞–π–∂–µ –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω–æ—é
```

#### 4. –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞ –æ–±—Ä–æ–±–∫–∞ –¥–∞–Ω–∏—Ö

```python
# ‚ùå –í–∏–¥–∞–ª–∏–ª–∏ –≤–∞–∂–ª–∏–≤—ñ –æ–∑–Ω–∞–∫–∏
X = df.drop(['important_feature'], axis=1)

# ‚ùå –ü–æ–≥–∞–Ω–∞ –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è
# –ó–∞–ª–∏—à–∏–ª–∏ –º–∞—Å—à—Ç–∞–±–∏ –Ω–µ—Å—É–º—ñ—Å–Ω–∏–º–∏
```

### –Ø–∫ –≤–∏–ø—Ä–∞–≤–∏—Ç–∏ underfitting?

#### ‚úÖ 1. –ó–±—ñ–ª—å—à–∏—Ç–∏ —Å–∫–ª–∞–¥–Ω—ñ—Å—Ç—å –º–æ–¥–µ–ª—ñ

```python
# Decision Tree: –±—ñ–ª—å—à–∞ –≥–ª–∏–±–∏–Ω–∞
# –ë—É–ª–æ:
dt = DecisionTreeRegressor(max_depth=1)

# –°—Ç–∞–ª–æ:
dt = DecisionTreeRegressor(max_depth=10)

# Neural Network: –±—ñ–ª—å—à–µ —à–∞—Ä—ñ–≤/–Ω–µ–π—Ä–æ–Ω—ñ–≤
# –ë—É–ª–æ:
model = Sequential([
    Dense(10, activation='relu'),
    Dense(1)
])

# –°—Ç–∞–ª–æ:
model = Sequential([
    Dense(128, activation='relu'),
    Dense(64, activation='relu'),
    Dense(32, activation='relu'),
    Dense(1)
])
```

#### ‚úÖ 2. –î–æ–¥–∞—Ç–∏ polynomial features

```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import Pipeline

# –ë—É–ª–æ: y = ax + b
lr = LinearRegression()

# –°—Ç–∞–ª–æ: y = ax¬≤ + bx + c
poly_model = Pipeline([
    ('poly', PolynomialFeatures(degree=2)),
    ('linear', LinearRegression())
])

poly_model.fit(X_train, y_train)
```

#### ‚úÖ 3. –ë—ñ–ª—å—à–µ –æ–∑–Ω–∞–∫ (Feature Engineering)

```python
# –î–æ–¥–∞—Ç–∏ –≤–∑–∞—î–º–æ–¥—ñ—ó
df['price_per_sqft'] = df['price'] / df['sqft']
df['total_rooms'] = df['bedrooms'] + df['bathrooms']

# –î–æ–¥–∞—Ç–∏ –Ω–µ–ª—ñ–Ω—ñ–π–Ω—ñ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ—ó
df['sqft_squared'] = df['sqft'] ** 2
df['log_sqft'] = np.log(df['sqft'])

# Binning
df['age_group'] = pd.cut(df['age'], bins=[0, 5, 10, 20, 50], 
                         labels=['new', 'recent', 'old', 'very_old'])
```

#### ‚úÖ 4. –ó–º–µ–Ω—à–∏—Ç–∏ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—é

```python
# Ridge: –∑–º–µ–Ω—à–∏—Ç–∏ alpha
# –ë—É–ª–æ:
ridge = Ridge(alpha=100.0)

# –°—Ç–∞–ª–æ:
ridge = Ridge(alpha=0.1)

# Lasso: –º–µ–Ω—à–∏–π alpha
lasso = Lasso(alpha=0.01)

# Decision Tree: –º–µ–Ω—à–µ –æ–±–º–µ–∂–µ–Ω—å
dt = DecisionTreeRegressor(
    max_depth=None,           # –ë–µ–∑ –æ–±–º–µ–∂–µ–Ω–Ω—è
    min_samples_split=2,      # –ú—ñ–Ω—ñ–º–∞–ª—å–Ω–µ –∑–Ω–∞—á–µ–Ω–Ω—è
    min_samples_leaf=1        # –ú—ñ–Ω—ñ–º–∞–ª—å–Ω–µ –∑–Ω–∞—á–µ–Ω–Ω—è
)
```

#### ‚úÖ 5. –í–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ —Å–∫–ª–∞–¥–Ω—ñ—à–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º

```python
# –ë—É–ª–æ: Linear Regression
lr = LinearRegression()

# –°—Ç–∞–ª–æ: Random Forest
from sklearn.ensemble import RandomForestRegressor
rf = RandomForestRegressor(n_estimators=100)

# –ê–±–æ: Gradient Boosting
from sklearn.ensemble import GradientBoostingRegressor
gb = GradientBoostingRegressor(n_estimators=100)

# –ê–±–æ: Neural Network
from sklearn.neural_network import MLPRegressor
nn = MLPRegressor(hidden_layer_sizes=(100, 50))
```

---

## Overfitting (–ü–µ—Ä–µ–Ω–∞–≤—á–∞–Ω–Ω—è)

### –©–æ —Ü–µ?

**Overfitting** ‚Äî –º–æ–¥–µ–ª—å **–∑–∞–Ω–∞–¥—Ç–æ –¥–æ–±—Ä–µ –∑–∞–ø–∞–º'—è—Ç–æ–≤—É—î** —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ, –≤–∫–ª—é—á–Ω–æ –∑ **—à—É–º–æ–º —Ç–∞ –≤–∏–ø–∞–¥–∫–æ–≤–æ—Å—Ç—è–º–∏**. –ù–µ —É–∑–∞–≥–∞–ª—å–Ω—é—î –Ω–∞ –Ω–æ–≤—ñ –¥–∞–Ω—ñ.

### –í—ñ–∑—É–∞–ª—å–Ω–∞ —ñ–Ω—Ç—É—ó—Ü—ñ—è

```
–°–ø—Ä–∞–≤–∂–Ω—ñ –¥–∞–Ω—ñ (–ª—ñ–Ω—ñ—è + —à—É–º):

     y
     |    ‚Ä¢
     |  ‚Ä¢   ‚Ä¢
     | ‚Ä¢  ‚Ä¢  ‚Ä¢
     |‚Ä¢  ‚Ä¢  ‚Ä¢
     |  ‚Ä¢  ‚Ä¢
     |_____________ x

–ü–æ–ª—ñ–Ω–æ–º 15-–≥–æ —Å—Ç–µ–ø–µ–Ω—è (overfitting):

     y
     |    ‚ï±‚Ä¢‚ï≤
     |  ‚Ä¢‚ï±   ‚ï≤‚Ä¢      ‚Üê –ü—Ä–æ—Ö–æ–¥–∏—Ç—å —á–µ—Ä–µ–∑ –í–°–Ü —Ç–æ—á–∫–∏!
     | ‚Ä¢‚ï±  ‚Ä¢  ‚ï≤‚Ä¢     ‚Üê –ó–∞–ø–∞–º'—è—Ç–∞–ª–∞ —à—É–º!
     |‚ï±‚Ä¢  ‚Ä¢  ‚Ä¢ ‚ï≤
     |  ‚Ä¢  ‚Ä¢    ‚ï≤
     |_____________ x
     
Train Error: –î–£–ñ–ï –ù–ò–ó–¨–ö–ò–ô ‚úì
Test Error: –í–ò–°–û–ö–ò–ô ‚ùå
Gap: –í–ï–õ–ò–ö–ò–ô ‚ùå
```

### –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏

| –ú–µ—Ç—Ä–∏–∫–∞ | –ó–Ω–∞—á–µ–Ω–Ω—è | –Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü—ñ—è |
|---------|----------|---------------|
| **Train Score** | –î—É–∂–µ –≤–∏—Å–æ–∫–∏–π (> 0.95) | ‚úì –Ü–¥–µ–∞–ª—å–Ω–æ –Ω–∞ train |
| **Test Score** | –ù–∏–∑—å–∫–∏–π (< 0.7) | ‚ùå –ü–æ–≥–∞–Ω–æ –Ω–∞ test |
| **Gap (Train - Test)** | –í–µ–ª–∏–∫–∏–π (> 0.15) | ‚ùå –ü—Ä–æ–±–ª–µ–º–∞! |
| **–î—ñ–∞–≥–Ω–æ–∑** | –ú–æ–¥–µ–ª—å –∑–∞–ø–∞–º'—è—Ç–∞–ª–∞ train set | HIGH VARIANCE |

### –û–∑–Ω–∞–∫–∏ overfitting

#### 1. –í–∏—Å–æ–∫–∏–π train score, –Ω–∏–∑—å–∫–∏–π test score

```python
from sklearn.tree import DecisionTreeRegressor

# –ì–ª–∏–±–æ–∫–µ –¥–µ—Ä–µ–≤–æ (–±–µ–∑ –æ–±–º–µ–∂–µ–Ω—å)
dt = DecisionTreeRegressor(random_state=42)
dt.fit(X_train, y_train)

train_score = dt.score(X_train, y_train)
test_score = dt.score(X_test, y_test)

print(f"Train: {train_score:.3f}")  # 1.000 ‚úì (—ñ–¥–µ–∞–ª—å–Ω–æ!)
print(f"Test:  {test_score:.3f}")   # 0.600 ‚ùå (–ø–æ–≥–∞–Ω–æ)
print(f"Gap:   {train_score - test_score:.3f}")  # 0.400 ‚ùå (–≤–µ–ª–∏–∫–∏–π!)

# –í–µ–ª–∏–∫–∏–π gap ‚Üí overfitting!
```

#### 2. –ú–æ–¥–µ–ª—å –∑–∞–ø–∞–º'—è—Ç–∞–ª–∞ outliers

```python
# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
plt.figure(figsize=(12, 5))

# Subplot 1: Train data
plt.subplot(1, 2, 1)
plt.scatter(X_train, y_train, alpha=0.5)
X_plot = np.linspace(X_train.min(), X_train.max(), 300).reshape(-1, 1)
plt.plot(X_plot, dt.predict(X_plot), 'r-', linewidth=2)
plt.title(f'Train Set (R¬≤ = {train_score:.3f})')
plt.xlabel('X')
plt.ylabel('y')

# Subplot 2: Test data
plt.subplot(1, 2, 2)
plt.scatter(X_test, y_test, alpha=0.5)
plt.plot(X_plot, dt.predict(X_plot), 'r-', linewidth=2)
plt.title(f'Test Set (R¬≤ = {test_score:.3f})')
plt.xlabel('X')
plt.ylabel('y')

plt.tight_layout()
plt.show()

# –Ø–∫—â–æ –Ω–∞ train –º–æ–¥–µ–ª—å "–ø—Ä–æ–±—ñ–≥–∞—î" —á–µ—Ä–µ–∑ –∫–æ–∂–Ω—É —Ç–æ—á–∫—É,
# –∞ –Ω–∞ test "–ø—Ä–æ–º–∞—Ö—É—î—Ç—å—Å—è" ‚Üí overfitting!
```

#### 3. Learning curves —Ä–æ–∑—Ö–æ–¥—è—Ç—å—Å—è

```
R¬≤ Score
    |
1.0 |  Train ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  ‚Üê Train –¥—É–∂–µ –≤–∏—Å–æ–∫–æ
    |           
0.8 |           
    |  Test ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  ‚Üê Test –Ω–∏–∑—å–∫–æ
0.6 |
    |    ‚Üë –í–µ–ª–∏–∫–∏–π gap!
0.4 |
    |_____________________________
        Training Set Size
    
Gap –ù–ï –∑–º–µ–Ω—à—É—î—Ç—å—Å—è –∑—ñ –∑–±—ñ–ª—å—à–µ–Ω–Ω—è–º –¥–∞–Ω–∏—Ö!
(–∞–±–æ –∑–º–µ–Ω—à—É—î—Ç—å—Å—è –¥—É–∂–µ –ø–æ–≤—ñ–ª—å–Ω–æ)
```

#### 4. –î—É–∂–µ —Å–∫–ª–∞–¥–Ω–∞ –º–æ–¥–µ–ª—å

```python
# –î—É–∂–µ –±–∞–≥–∞—Ç–æ –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤
print(f"Number of parameters: {dt.tree_.node_count}")
# –ù–∞–ø—Ä–∏–∫–ª–∞–¥: 5000 nodes –¥–ª—è 1000 train samples ‚Üí overfitting!

# –ü—Ä–∞–≤–∏–ª–æ: —è–∫—â–æ parameters >> samples ‚Üí —Ä–∏–∑–∏–∫ overfitting
```

### –ü—Ä–∏—á–∏–Ω–∏ overfitting

#### 1. –ú–æ–¥–µ–ª—å –∑–∞–Ω–∞–¥—Ç–æ —Å–∫–ª–∞–¥–Ω–∞

```python
# ‚ùå Decision Tree –±–µ–∑ –æ–±–º–µ–∂–µ–Ω—å
dt = DecisionTreeRegressor()  # max_depth=None ‚Üí –º–æ–∂–µ —Ä–æ—Å—Ç–∏ –¥–æ –±–µ–∑–∫—ñ–Ω–µ—á–Ω–æ—Å—Ç—ñ!

# ‚ùå Polynomial –∑ –≤–∏—Å–æ–∫–∏–º degree
poly = PolynomialFeatures(degree=15)  # –ó–∞–Ω–∞–¥—Ç–æ –≥–Ω—É—á–∫–æ!

# ‚ùå KNN –∑ K=1
from sklearn.neighbors import KNeighborsRegressor
knn = KNeighborsRegressor(n_neighbors=1)  # –ó–∞–ø–∞–º'—è—Ç–æ–≤—É—î –∫–æ–∂–Ω—É —Ç–æ—á–∫—É!
```

#### 2. –ú–∞–ª–æ —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö

```python
# ‚ùå 100 samples, 50 features ‚Üí overfitting –Ω–µ–º–∏–Ω—É—á–∏–π!
print(f"Samples: {X_train.shape[0]}")    # 100
print(f"Features: {X_train.shape[1]}")   # 50

# –ü—Ä–∞–≤–∏–ª–æ: –ø–æ—Ç—Ä—ñ–±–Ω–æ —Ö–æ—á–∞ –± 10 samples –Ω–∞ –∫–æ–∂–Ω—É –æ–∑–Ω–∞–∫—É
```

#### 3. –ù–µ–º–∞—î —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—ó

```python
# ‚ùå Linear Regression –±–µ–∑ penalty
lr = LinearRegression()

# ‚ùå Neural Network –±–µ–∑ dropout, –±–µ–∑ regularization
model = Sequential([
    Dense(1000, activation='relu'),
    Dense(1000, activation='relu'),
    Dense(1)
])
# –î—É–∂–µ –±–∞–≥–∞—Ç–æ –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤, –Ω—ñ—á–æ–≥–æ –Ω–µ —Å—Ç—Ä–∏–º—É—î!
```

#### 4. –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –∑–∞–Ω–∞–¥—Ç–æ –¥–æ–≤–≥–æ

```python
# ‚ùå Neural Network —Ç—Ä–µ–Ω—É—î—Ç—å—Å—è 1000 epochs
model.fit(X_train, y_train, epochs=1000, verbose=0)
# –ú–æ–¥–µ–ª—å –ø–æ—á–∏–Ω–∞—î –∑–∞–ø–∞–º'—è—Ç–æ–≤—É–≤–∞—Ç–∏ —à—É–º –ø—ñ—Å–ª—è –ø–µ–≤–Ω–æ—ó –∫—ñ–ª—å–∫–æ—Å—Ç—ñ epochs
```

#### 5. –î—É–±–ª—ñ–∫–∞—Ç–∏ –≤ –¥–∞–Ω–∏—Ö

```python
# ‚ùå Train —ñ test overlap
# –Ø–∫—â–æ –≤–∏–ø–∞–¥–∫–æ–≤–æ —Ç–æ–π —Å–∞–º–∏–π –∑—Ä–∞–∑–æ–∫ –ø–æ—Ç—Ä–∞–ø–∏–≤ —ñ –≤ train, —ñ –≤ test
# ‚Üí —à—Ç—É—á–Ω–æ –∑–∞–≤–∏—â–µ–Ω–∞ test score, –∞–ª–µ –Ω–∞—Å–ø—Ä–∞–≤–¥—ñ overfitting
```

### –Ø–∫ –≤–∏–ø—Ä–∞–≤–∏—Ç–∏ overfitting?

#### ‚úÖ 1. –ë—ñ–ª—å—à–µ —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö

```python
# –ù–∞–π–∫—Ä–∞—â–∏–π —Å–ø–æ—Å—ñ–± (—è–∫—â–æ –º–æ–∂–ª–∏–≤–æ)!

# –ó–±—ñ–ª—å—à–∏—Ç–∏ dataset
# - –ó—ñ–±—Ä–∞—Ç–∏ –±—ñ–ª—å—à–µ –¥–∞–Ω–∏—Ö
# - Data augmentation (–¥–ª—è –∑–æ–±—Ä–∞–∂–µ–Ω—å)
# - Synthetic data generation

# Data augmentation (–ø—Ä–∏–∫–ª–∞–¥ –¥–ª—è –∑–æ–±—Ä–∞–∂–µ–Ω—å)
from keras.preprocessing.image import ImageDataGenerator

datagen = ImageDataGenerator(
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    horizontal_flip=True
)
```

#### ‚úÖ 2. –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—è (Regularization)

**L2 Regularization (Ridge):**

```python
from sklearn.linear_model import Ridge

# –î–æ–¥–∞—î penalty –Ω–∞ –≤–µ–ª–∏–∫—ñ –≤–∞–≥–∏: Œª * ||w||¬≤
ridge = Ridge(alpha=1.0)  # alpha = Œª
ridge.fit(X_train, y_train)

# –ë—ñ–ª—å—à–∏–π alpha ‚Üí —Å–∏–ª—å–Ω—ñ—à–∞ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—è
# alpha = 0 ‚Üí –∑–≤–∏—á–∞–π–Ω–∞ Linear Regression
# alpha = ‚àû ‚Üí –≤—Å—ñ –≤–∞–≥–∏ ‚Üí 0
```

**L1 Regularization (Lasso):**

```python
from sklearn.linear_model import Lasso

# –î–æ–¥–∞—î penalty: Œª * ||w||‚ÇÅ
lasso = Lasso(alpha=0.1)
lasso.fit(X_train, y_train)

# Lasso –º–æ–∂–µ –∑–∞–Ω—É–ª—è—Ç–∏ –≤–∞–≥–∏ ‚Üí feature selection
```

**Elastic Net (L1 + L2):**

```python
from sklearn.linear_model import ElasticNet

elastic = ElasticNet(alpha=1.0, l1_ratio=0.5)
# l1_ratio = 0 ‚Üí Ridge
# l1_ratio = 1 ‚Üí Lasso
# l1_ratio = 0.5 ‚Üí 50/50
```

#### ‚úÖ 3. –ó–º–µ–Ω—à–∏—Ç–∏ —Å–∫–ª–∞–¥–Ω—ñ—Å—Ç—å –º–æ–¥–µ–ª—ñ

**Decision Trees:**

```python
# –ë—É–ª–æ: –Ω–µ–æ–±–º–µ–∂–µ–Ω–µ –¥–µ—Ä–µ–≤–æ
dt = DecisionTreeRegressor()

# –°—Ç–∞–ª–æ: –æ–±–º–µ–∂–µ–Ω–Ω—è
dt = DecisionTreeRegressor(
    max_depth=5,              # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –≥–ª–∏–±–∏–Ω–∞
    min_samples_split=20,     # –ú—ñ–Ω—ñ–º—É–º –∑—Ä–∞–∑–∫—ñ–≤ –¥–ª—è split
    min_samples_leaf=10,      # –ú—ñ–Ω—ñ–º—É–º –∑—Ä–∞–∑–∫—ñ–≤ –≤ –ª–∏—Å—Ç—ñ
    max_features='sqrt'       # –í–∏–ø–∞–¥–∫–æ–≤–∏–π –ø—ñ–¥–º–Ω–æ–∂–∏–Ω–∞ –æ–∑–Ω–∞–∫
)
```

**Neural Networks:**

```python
# –ë—É–ª–æ: –¥—É–∂–µ –≥–ª–∏–±–æ–∫–∞ –º–µ—Ä–µ–∂–∞
model = Sequential([
    Dense(1000, activation='relu'),
    Dense(1000, activation='relu'),
    Dense(1000, activation='relu'),
    Dense(1)
])

# –°—Ç–∞–ª–æ: –º–µ–Ω—à–∞ –º–µ—Ä–µ–∂–∞
model = Sequential([
    Dense(64, activation='relu'),
    Dense(32, activation='relu'),
    Dense(1)
])
```

**Polynomial Regression:**

```python
# –ë—É–ª–æ: degree=15
poly = PolynomialFeatures(degree=15)

# –°—Ç–∞–ª–æ: degree=3
poly = PolynomialFeatures(degree=3)
```

#### ‚úÖ 4. Dropout (–¥–ª—è Neural Networks)

```python
from keras.layers import Dropout

model = Sequential([
    Dense(128, activation='relu'),
    Dropout(0.5),              # –í–∏–ø–∞–¥–∫–æ–≤–æ –≤–∏–º–∏–∫–∞—î 50% –Ω–µ–π—Ä–æ–Ω—ñ–≤
    Dense(64, activation='relu'),
    Dropout(0.3),              # 30%
    Dense(1)
])

# Dropout –ø—Ä–∏–º—É—à—É—î –º–µ—Ä–µ–∂—É –Ω–µ –ø–æ–∫–ª–∞–¥–∞—Ç–∏—Å—è –Ω–∞ –æ–∫—Ä–µ–º—ñ –Ω–µ–π—Ä–æ–Ω–∏
# ‚Üí –∫—Ä–∞—â–µ —É–∑–∞–≥–∞–ª—å–Ω–µ–Ω–Ω—è
```

#### ‚úÖ 5. Early Stopping

```python
from keras.callbacks import EarlyStopping

# –ó—É–ø–∏–Ω–∫–∞, –∫–æ–ª–∏ validation loss –ø–µ—Ä–µ—Å—Ç–∞—î –ø–æ–∫—Ä–∞—â—É–≤–∞—Ç–∏—Å—è
early_stop = EarlyStopping(
    monitor='val_loss',
    patience=10,              # –ß–µ–∫–∞—Ç–∏ 10 epochs
    restore_best_weights=True # –ü–æ–≤–µ—Ä–Ω—É—Ç–∏ –Ω–∞–π–∫—Ä–∞—â—ñ –≤–∞–≥–∏
)

model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=1000,
    callbacks=[early_stop]
)
```

#### ‚úÖ 6. Cross-Validation

```python
from sklearn.model_selection import cross_val_score

# –ó–∞–º—ñ—Å—Ç—å –æ–¥–Ω–æ–≥–æ train/test split ‚Üí 5-fold CV
scores = cross_val_score(model, X, y, cv=5, scoring='r2')

print(f"CV Scores: {scores}")
print(f"Mean: {scores.mean():.3f}")
print(f"Std: {scores.std():.3f}")

# –Ø–∫—â–æ std –≤–∏—Å–æ–∫–∏–π ‚Üí –º–æ–¥–µ–ª—å –Ω–µ—Å—Ç–∞–±—ñ–ª—å–Ω–∞ ‚Üí –º–æ–∂–ª–∏–≤–æ overfitting
```

#### ‚úÖ 7. Ensemble Methods

**Bagging (–∑–º–µ–Ω—à—É—î variance):**

```python
from sklearn.ensemble import RandomForestRegressor

# Random Forest = Bagging of Decision Trees
rf = RandomForestRegressor(
    n_estimators=100,    # 100 –¥–µ—Ä–µ–≤
    max_depth=10,        # –ö–æ–∂–Ω–µ –¥–µ—Ä–µ–≤–æ –æ–±–º–µ–∂–µ–Ω–µ
    max_features='sqrt'  # –í–∏–ø–∞–¥–∫–æ–≤—ñ –æ–∑–Ω–∞–∫–∏
)

# –£—Å–µ—Ä–µ–¥–Ω–µ–Ω–Ω—è –±–∞–≥–∞—Ç—å–æ—Ö –º–æ–¥–µ–ª–µ–π ‚Üí –∑–º–µ–Ω—à—É—î overfitting
```

**Boosting (–∑ regularization):**

```python
from sklearn.ensemble import GradientBoostingRegressor

gb = GradientBoostingRegressor(
    n_estimators=100,
    max_depth=3,           # Shallow trees
    learning_rate=0.1,     # –ü–æ–≤—ñ–ª—å–Ω–µ –Ω–∞–≤—á–∞–Ω–Ω—è
    subsample=0.8,         # 80% –¥–∞–Ω–∏—Ö –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –¥–µ—Ä–µ–≤–∞
    min_samples_leaf=5     # –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—è
)
```

#### ‚úÖ 8. Feature Selection

```python
# –í–∏–¥–∞–ª–∏—Ç–∏ –Ω–µ–ø–æ—Ç—Ä—ñ–±–Ω—ñ/—à—É–º–ª–∏–≤—ñ –æ–∑–Ω–∞–∫–∏
from sklearn.feature_selection import SelectKBest, f_regression

selector = SelectKBest(score_func=f_regression, k=10)
X_selected = selector.fit_transform(X_train, y_train)

# –ú–µ–Ω—à–µ –æ–∑–Ω–∞–∫ ‚Üí –º–µ–Ω—à–∞ —Å–∫–ª–∞–¥–Ω—ñ—Å—Ç—å ‚Üí –º–µ–Ω—à–µ overfitting
```

#### ‚úÖ 9. Data Augmentation (–¥–ª—è –∑–æ–±—Ä–∞–∂–µ–Ω—å)

```python
# –ó–±—ñ–ª—å—à–∏—Ç–∏ train set —á–µ—Ä–µ–∑ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ—ó
from torchvision import transforms

transform = transforms.Compose([
    transforms.RandomRotation(10),
    transforms.RandomHorizontalFlip(),
    transforms.RandomCrop(224),
    transforms.ColorJitter(brightness=0.2, contrast=0.2)
])
```

---

## –û–ø—Ç–∏–º–∞–ª—å–Ω–∞ –º–æ–¥–µ–ª—å (Sweet Spot)

### –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏

| –ú–µ—Ç—Ä–∏–∫–∞ | –ó–Ω–∞—á–µ–Ω–Ω—è |
|---------|----------|
| **Train Score** | –í–∏—Å–æ–∫–∏–π (0.8-0.95) ‚úì |
| **Test Score** | –í–∏—Å–æ–∫–∏–π (0.75-0.9) ‚úì |
| **Gap** | –ú–∞–ª–∏–π (< 0.1) ‚úì |
| **–£–∑–∞–≥–∞–ª—å–Ω–µ–Ω–Ω—è** | –î–æ–±—Ä–µ ‚úì |

### –Ø–∫ –¥–æ—Å—è–≥—Ç–∏?

```python
# 1. –ü–æ—á–∞—Ç–∏ –∑ –ø—Ä–æ—Å—Ç–æ—ó –º–æ–¥–µ–ª—ñ (baseline)
lr = LinearRegression()
lr.fit(X_train, y_train)
baseline_score = lr.score(X_test, y_test)
print(f"Baseline: {baseline_score:.3f}")

# 2. –ü–æ—Å—Ç—É–ø–æ–≤–æ –∑–±—ñ–ª—å—à—É–≤–∞—Ç–∏ —Å–∫–ª–∞–¥–Ω—ñ—Å—Ç—å
from sklearn.model_selection import GridSearchCV

# Decision Tree: tuning max_depth
param_grid = {'max_depth': [1, 2, 3, 5, 7, 10, 15, 20, None]}

grid = GridSearchCV(
    DecisionTreeRegressor(random_state=42),
    param_grid,
    cv=5,
    scoring='r2'
)

grid.fit(X_train, y_train)

print(f"Best max_depth: {grid.best_params_['max_depth']}")
print(f"Best CV score: {grid.best_score_:.3f}")

best_model = grid.best_estimator_
test_score = best_model.score(X_test, y_test)
print(f"Test score: {test_score:.3f}")

# 3. –Ø–∫—â–æ gap –≤–µ–ª–∏–∫–∏–π ‚Üí –¥–æ–¥–∞—Ç–∏ regularization
# 4. –Ø–∫—â–æ train score –Ω–∏–∑—å–∫–∏–π ‚Üí –∑–±—ñ–ª—å—à–∏—Ç–∏ —Å–∫–ª–∞–¥–Ω—ñ—Å—Ç—å
# 5. –ü–æ–≤—Ç–æ—Ä—é–≤–∞—Ç–∏ –¥–æ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –±–∞–ª–∞–Ω—Å—É
```

---

## –ü—Ä–∞–∫—Ç–∏—á–Ω–∏–π workflow –¥–ª—è –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∏

```python
def diagnose_and_fix(model, X_train, X_test, y_train, y_test, model_name="Model"):
    """
    –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞ –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ —Ç–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó
    """
    import numpy as np
    import matplotlib.pyplot as plt
    from sklearn.model_selection import learning_curve
    
    print("="*70)
    print(f"DIAGNOSIS: {model_name}")
    print("="*70)
    
    # 1. –ë–∞–∑–æ–≤—ñ –º–µ—Ç—Ä–∏–∫–∏
    model.fit(X_train, y_train)
    train_score = model.score(X_train, y_train)
    test_score = model.score(X_test, y_test)
    gap = train_score - test_score
    
    print(f"\nüìä Scores:")
    print(f"  Train R¬≤: {train_score:.4f}")
    print(f"  Test R¬≤:  {test_score:.4f}")
    print(f"  Gap:      {gap:.4f}")
    
    # 2. –î—ñ–∞–≥–Ω–æ–∑
    print(f"\nüîç Diagnosis:")
    
    if test_score < 0.6 and gap < 0.1:
        diagnosis = "UNDERFITTING (HIGH BIAS)"
        color = 'üî¥'
        print(f"  {color} {diagnosis}")
        print(f"\nüí° Recommendations:")
        print(f"  1. Increase model complexity:")
        print(f"     - Larger max_depth (trees)")
        print(f"     - Higher polynomial degree")
        print(f"     - More layers/neurons (NN)")
        print(f"  2. Add more features")
        print(f"  3. Reduce regularization (smaller Œª)")
        print(f"  4. Use a more complex algorithm")
        
    elif gap > 0.15:
        diagnosis = "OVERFITTING (HIGH VARIANCE)"
        color = 'üî¥'
        print(f"  {color} {diagnosis}")
        print(f"\nüí° Recommendations:")
        print(f"  1. Get more training data")
        print(f"  2. Add regularization:")
        print(f"     - Ridge/Lasso (larger Œª)")
        print(f"     - Dropout (NN)")
        print(f"  3. Reduce model complexity:")
        print(f"     - Smaller max_depth")
        print(f"     - Pruning")
        print(f"  4. Use ensemble methods (Random Forest)")
        print(f"  5. Feature selection")
        print(f"  6. Early stopping")
        
    elif test_score >= 0.6 and gap <= 0.15:
        diagnosis = "GOOD BALANCE"
        color = '‚úÖ'
        print(f"  {color} {diagnosis}")
        if gap > 0.05:
            print(f"\nüí° Minor recommendations:")
            print(f"  - Slight overfitting detected (gap={gap:.3f})")
            print(f"  - Consider adding light regularization")
        else:
            print(f"\nüéâ Model is well-tuned!")
    
    else:
        diagnosis = "UNUSUAL PATTERN"
        color = '‚ö†Ô∏è'
        print(f"  {color} {diagnosis}")
        print(f"\n‚ö†Ô∏è  Check for:")
        print(f"  - Data leakage")
        print(f"  - Wrong train/test split")
        print(f"  - Bugs in preprocessing")
    
    # 3. Learning Curves
    print(f"\nüìà Generating learning curves...")
    
    train_sizes, train_scores, val_scores = learning_curve(
        model, X_train, y_train,
        train_sizes=np.linspace(0.1, 1.0, 10),
        cv=5,
        scoring='r2',
        n_jobs=-1
    )
    
    train_mean = np.mean(train_scores, axis=1)
    train_std = np.std(train_scores, axis=1)
    val_mean = np.mean(val_scores, axis=1)
    val_std = np.std(val_scores, axis=1)
    
    # –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
    plt.figure(figsize=(12, 5))
    
    # Subplot 1: Learning Curves
    plt.subplot(1, 2, 1)
    plt.plot(train_sizes, train_mean, 'o-', linewidth=2, 
             label='Train Score', color='blue')
    plt.fill_between(train_sizes, 
                     train_mean - train_std, 
                     train_mean + train_std,
                     alpha=0.1, color='blue')
    
    plt.plot(train_sizes, val_mean, 's-', linewidth=2, 
             label='Validation Score', color='red')
    plt.fill_between(train_sizes, 
                     val_mean - val_std, 
                     val_mean + val_std,
                     alpha=0.1, color='red')
    
    plt.xlabel('Training Set Size', fontsize=11)
    plt.ylabel('R¬≤ Score', fontsize=11)
    plt.title(f'Learning Curves: {model_name}', fontsize=12, fontweight='bold')
    plt.legend(fontsize=10)
    plt.grid(True, alpha=0.3)
    
    # Subplot 2: Predictions vs Actual
    plt.subplot(1, 2, 2)
    y_pred_train = model.predict(X_train)
    y_pred_test = model.predict(X_test)
    
    plt.scatter(y_train, y_pred_train, alpha=0.5, s=30, 
               label=f'Train (R¬≤={train_score:.3f})')
    plt.scatter(y_test, y_pred_test, alpha=0.5, s=30, 
               label=f'Test (R¬≤={test_score:.3f})')
    
    # Perfect prediction line
    all_y = np.concatenate([y_train, y_test])
    plt.plot([all_y.min(), all_y.max()], 
             [all_y.min(), all_y.max()], 
             'k--', linewidth=2, label='Perfect Prediction')
    
    plt.xlabel('Actual', fontsize=11)
    plt.ylabel('Predicted', fontsize=11)
    plt.title('Predictions vs Actual', fontsize=12, fontweight='bold')
    plt.legend(fontsize=10)
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    # 4. –ü—ñ–¥—Å—É–º–æ–∫
    print("\n" + "="*70)
    print("SUMMARY")
    print("="*70)
    print(f"Diagnosis: {diagnosis}")
    print(f"Train R¬≤: {train_score:.4f}")
    print(f"Test R¬≤: {test_score:.4f}")
    print(f"Gap: {gap:.4f}")
    
    # –Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü—ñ—è learning curves
    final_gap_lc = train_mean[-1] - val_mean[-1]
    if final_gap_lc > 0.2:
        print(f"\n‚ö†Ô∏è  Learning curves show high variance")
        print(f"   More data would likely help!")
    elif val_mean[-1] < 0.6 and final_gap_lc < 0.1:
        print(f"\n‚ö†Ô∏è  Learning curves plateau early")
        print(f"   More data won't help - need more complex model!")
    else:
        print(f"\n‚úì Learning curves look reasonable")
    
    print("="*70)
    
    return {
        'train_score': train_score,
        'test_score': test_score,
        'gap': gap,
        'diagnosis': diagnosis
    }


# –ü—Ä–∏–∫–ª–∞–¥ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è
from sklearn.tree import DecisionTreeRegressor
from sklearn.datasets import make_regression

# –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –¥–∞–Ω–∏—Ö
X, y = make_regression(n_samples=500, n_features=10, noise=10, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# –¢–µ—Å—Ç —Ä—ñ–∑–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π
print("\n### TEST 1: Shallow Tree (likely UNDERFITTING) ###")
dt_shallow = DecisionTreeRegressor(max_depth=2, random_state=42)
diagnose_and_fix(dt_shallow, X_train, X_test, y_train, y_test, 
                "Shallow Tree (max_depth=2)")

print("\n### TEST 2: Deep Tree (likely OVERFITTING) ###")
dt_deep = DecisionTreeRegressor(random_state=42)
diagnose_and_fix(dt_deep, X_train, X_test, y_train, y_test, 
                "Deep Tree (no limits)")

print("\n### TEST 3: Optimal Tree ###")
dt_optimal = DecisionTreeRegressor(max_depth=7, min_samples_leaf=10, random_state=42)
diagnose_and_fix(dt_optimal, X_train, X_test, y_train, y_test, 
                "Optimal Tree (max_depth=7)")
```

---

## –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è Underfitting vs Overfitting

| –ê—Å–ø–µ–∫—Ç | Underfitting | Overfitting | Optimal |
|--------|--------------|-------------|---------|
| **Train Error** | –í–∏—Å–æ–∫–∏–π ‚ùå | –î—É–∂–µ –Ω–∏–∑—å–∫–∏–π ‚úì | –ù–∏–∑—å–∫–∏–π ‚úì |
| **Test Error** | –í–∏—Å–æ–∫–∏–π ‚ùå | –í–∏—Å–æ–∫–∏–π ‚ùå | –ù–∏–∑—å–∫–∏–π ‚úì |
| **Gap** | –ú–∞–ª–∏–π | –í–µ–ª–∏–∫–∏–π | –ú–∞–ª–∏–π |
| **–ü—Ä–æ–±–ª–µ–º–∞** | –ó–∞–Ω–∞–¥—Ç–æ –ø—Ä–æ—Å—Ç–æ | –ó–∞–Ω–∞–¥—Ç–æ —Å–∫–ª–∞–¥–Ω–æ | –ë–∞–ª–∞–Ω—Å |
| **Bias** | –í–∏—Å–æ–∫–∏–π ‚ùå | –ù–∏–∑—å–∫–∏–π ‚úì | –ü–æ–º—ñ—Ä–Ω–∏–π ‚úì |
| **Variance** | –ù–∏–∑—å–∫–∞ ‚úì | –í–∏—Å–æ–∫–∞ ‚ùå | –ü–æ–º—ñ—Ä–Ω–∞ ‚úì |
| **–£–∑–∞–≥–∞–ª—å–Ω–µ–Ω–Ω—è** | –ü–æ–≥–∞–Ω–µ | –ü–æ–≥–∞–Ω–µ | –î–æ–±—Ä–µ ‚úì |
| **Learning Curve** | –ü–ª–∞—Ç–æ —Ä–∞–Ω–æ | –í–µ–ª–∏–∫–∏–π gap | –ó–±—ñ–∂–Ω—ñ—Å—Ç—å |
| **–í—ñ–∑—É–∞–ª—å–Ω–æ** | –ù–µ –≤–ª–æ–≤–ª—é—î –ø–∞—Ç—Ç–µ—Ä–Ω | –ü—Ä–æ—Ö–æ–¥–∏—Ç—å —á–µ—Ä–µ–∑ –≤—Å—ñ —Ç–æ—á–∫–∏ | –í–ª–æ–≤–ª—é—î —Ç—Ä–µ–Ω–¥ |

### –†—ñ—à–µ–Ω–Ω—è

**Underfitting ‚Üí** –ó–±—ñ–ª—å—à–∏—Ç–∏ —Å–∫–ª–∞–¥–Ω—ñ—Å—Ç—å:
- ‚Üë max_depth
- ‚Üë polynomial degree
- ‚Üë features
- ‚Üì regularization

**Overfitting ‚Üí** –ó–º–µ–Ω—à–∏—Ç–∏ variance:
- ‚Üë training data
- ‚Üë regularization
- ‚Üì model complexity
- Use ensemble methods
- Feature selection

---

## –ü—Ä–∏–∫–ª–∞–¥–∏ –Ω–∞ —Ä—ñ–∑–Ω–∏—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–∞—Ö

### Linear Regression

```python
from sklearn.datasets import make_regression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import Pipeline

# –ù–µ–ª—ñ–Ω—ñ–π–Ω—ñ –¥–∞–Ω—ñ
np.random.seed(42)
X = np.linspace(0, 10, 100).reshape(-1, 1)
y = 2 * np.sin(X).ravel() + np.random.normal(0, 0.3, 100)

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# –¢–µ—Å—Ç—É—î–º–æ —Ä—ñ–∑–Ω—ñ —Å—Ç–µ–ø–µ–Ω—ñ
degrees = [1, 3, 15]
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

for idx, degree in enumerate(degrees):
    # –ú–æ–¥–µ–ª—å
    poly_model = Pipeline([
        ('poly', PolynomialFeatures(degree=degree)),
        ('linear', LinearRegression())
    ])
    
    poly_model.fit(X_train, y_train)
    
    train_score = poly_model.score(X_train, y_train)
    test_score = poly_model.score(X_test, y_test)
    gap = train_score - test_score
    
    # –î—ñ–∞–≥–Ω–æ–∑
    if degree == 1:
        diagnosis = "UNDERFITTING"
        color = 'red'
    elif degree == 3:
        diagnosis = "OPTIMAL"
        color = 'green'
    else:
        diagnosis = "OVERFITTING"
        color = 'red'
    
    # –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
    X_plot = np.linspace(0, 10, 300).reshape(-1, 1)
    y_plot = poly_model.predict(X_plot)
    
    axes[idx].scatter(X_train, y_train, alpha=0.6, s=40, label='Train')
    axes[idx].scatter(X_test, y_test, alpha=0.6, s=40, label='Test')
    axes[idx].plot(X_plot, y_plot, 'r-', linewidth=2, label='Model')
    axes[idx].plot(X_plot, 2 * np.sin(X_plot), 'g--', 
                   linewidth=2, alpha=0.7, label='True function')
    
    axes[idx].set_title(
        f'Degree = {degree}\n'
        f'Train R¬≤ = {train_score:.3f}, Test R¬≤ = {test_score:.3f}\n'
        f'Gap = {gap:.3f}\n'
        f'{diagnosis}',
        fontsize=11, fontweight='bold', color=color
    )
    axes[idx].set_xlabel('X')
    axes[idx].set_ylabel('y')
    axes[idx].legend(fontsize=9)
    axes[idx].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### Decision Trees

```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import make_moons

# –ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è: –¥–≤–∞ "–º—ñ—Å—è—Ü—ñ"
X, y = make_moons(n_samples=200, noise=0.2, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# –¢–µ—Å—Ç—É—î–º–æ —Ä—ñ–∑–Ω—ñ max_depth
depths = [1, 5, None]
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

for idx, depth in enumerate(depths):
    # –ú–æ–¥–µ–ª—å
    dt = DecisionTreeClassifier(max_depth=depth, random_state=42)
    dt.fit(X_train, y_train)
    
    train_score = dt.score(X_train, y_train)
    test_score = dt.score(X_test, y_test)
    gap = train_score - test_score
    
    # –î—ñ–∞–≥–Ω–æ–∑
    if depth == 1:
        diagnosis = "UNDERFITTING"
        color = 'red'
    elif depth == 5:
        diagnosis = "OPTIMAL"
        color = 'green'
    else:
        diagnosis = "OVERFITTING"
        color = 'red'
    
    # –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è decision boundary
    from matplotlib.colors import ListedColormap
    
    h = 0.02  # step size
    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5
    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))
    
    Z = dt.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    
    cmap_light = ListedColormap(['#FFAAAA', '#AAAAFF'])
    cmap_bold = ['red', 'blue']
    
    axes[idx].contourf(xx, yy, Z, alpha=0.3, cmap=cmap_light)
    
    # Train points
    axes[idx].scatter(X_train[:, 0], X_train[:, 1], 
                     c=y_train, cmap='coolwarm', 
                     edgecolor='k', s=50, alpha=0.7, label='Train')
    
    # Test points
    axes[idx].scatter(X_test[:, 0], X_test[:, 1], 
                     c=y_test, cmap='coolwarm', 
                     edgecolor='k', s=50, marker='s', alpha=0.7, label='Test')
    
    axes[idx].set_title(
        f'max_depth = {depth}\n'
        f'Train Acc = {train_score:.3f}, Test Acc = {test_score:.3f}\n'
        f'Gap = {gap:.3f}\n'
        f'{diagnosis}',
        fontsize=11, fontweight='bold', color=color
    )
    axes[idx].set_xlabel('X‚ÇÅ')
    axes[idx].set_ylabel('X‚ÇÇ')
    axes[idx].legend(fontsize=9)

plt.tight_layout()
plt.show()
```

### Neural Networks

```python
from sklearn.neural_network import MLPRegressor

# –†—ñ–∑–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –Ω–µ–π—Ä–æ–Ω—ñ–≤
architectures = [
    (10,),           # –ü—Ä–æ—Å—Ç–∞
    (100, 50),       # –°–µ—Ä–µ–¥–Ω—è
    (200, 200, 200)  # –°–∫–ª–∞–¥–Ω–∞
]

fig, axes = plt.subplots(1, 3, figsize=(18, 5))

for idx, hidden_layers in enumerate(architectures):
    # –ú–æ–¥–µ–ª—å
    nn = MLPRegressor(
        hidden_layer_sizes=hidden_layers,
        max_iter=1000,
        random_state=42,
        early_stopping=False  # –í–∏–º–∫–Ω—É—Ç–∏ –¥–ª—è —á–∏—Å—Ç–æ–≥–æ —Ç–µ—Å—Ç—É
    )
    
    nn.fit(X_train, y_train)
    
    train_score = nn.score(X_train, y_train)
    test_score = nn.score(X_test, y_test)
    gap = train_score - test_score
    
    # –î—ñ–∞–≥–Ω–æ–∑
    if idx == 0:
        diagnosis = "UNDERFITTING"
        color = 'red'
    elif idx == 1:
        diagnosis = "OPTIMAL"
        color = 'green'
    else:
        diagnosis = "OVERFITTING"
        color = 'red'
    
    # –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
    X_plot = np.linspace(0, 10, 300).reshape(-1, 1)
    y_plot = nn.predict(X_plot)
    
    axes[idx].scatter(X_train, y_train, alpha=0.6, s=40, label='Train')
    axes[idx].scatter(X_test, y_test, alpha=0.6, s=40, label='Test')
    axes[idx].plot(X_plot, y_plot, 'r-', linewidth=2, label='Model')
    axes[idx].plot(X_plot, 2 * np.sin(X_plot), 'g--', 
                   linewidth=2, alpha=0.7, label='True function')
    
    axes[idx].set_title(
        f'Architecture: {hidden_layers}\n'
        f'Train R¬≤ = {train_score:.3f}, Test R¬≤ = {test_score:.3f}\n'
        f'Gap = {gap:.3f}\n'
        f'{diagnosis}',
        fontsize=11, fontweight='bold', color=color
    )
    axes[idx].set_xlabel('X')
    axes[idx].set_ylabel('y')
    axes[idx].legend(fontsize=9)
    axes[idx].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

---

## –ü–æ—à–∏—Ä–µ–Ω—ñ –ø–æ–º–∏–ª–∫–∏ ‚ùå

### 1. –û—Ü—ñ–Ω—é–≤–∞—Ç–∏ —Ç—ñ–ª—å–∫–∏ train score

```python
# ‚ùå –ü–û–ì–ê–ù–û
model.fit(X_train, y_train)
print(f"Score: {model.score(X_train, y_train)}")
# –ú–æ–∂–µ –±—É—Ç–∏ 1.0, –∞–ª–µ —Ü–µ –Ω—ñ—á–æ–≥–æ –Ω–µ –æ–∑–Ω–∞—á–∞—î!

# ‚úÖ –î–û–ë–†–ï
print(f"Train: {model.score(X_train, y_train)}")
print(f"Test:  {model.score(X_test, y_test)}")
```

### 2. –ù–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ validation set

```python
# ‚ùå Tuning –Ω–∞ test set
for depth in [1, 5, 10, 20]:
    dt = DecisionTreeRegressor(max_depth=depth)
    dt.fit(X_train, y_train)
    score = dt.score(X_test, y_test)  # –í–∏—Ç—ñ–∫ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó!

# ‚úÖ –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π validation –∞–±–æ CV
from sklearn.model_selection import GridSearchCV
grid = GridSearchCV(DecisionTreeRegressor(), 
                    {'max_depth': [1, 5, 10, 20]}, 
                    cv=5)
grid.fit(X_train, y_train)
```

### 3. –ó–±—ñ–ª—å—à—É–≤–∞—Ç–∏ —Å–∫–ª–∞–¥–Ω—ñ—Å—Ç—å –±–µ–∑ –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∏

```python
# ‚ùå "–ú–æ–¥–µ–ª—å –ø–æ–≥–∞–Ω–∞, –¥–æ–¥–∞–º–æ –±—ñ–ª—å—à–µ features!"
# –Ø–∫—â–æ –≤–∂–µ —î overfitting, —Ü–µ —Ç—ñ–ª—å–∫–∏ –ø–æ–≥—ñ—Ä—à–∏—Ç—å!

# ‚úÖ –°–ø–æ—á–∞—Ç–∫—É –¥—ñ–∞–≥–Ω–æ—Å—Ç—É–π
diagnose_and_fix(model, X_train, X_test, y_train, y_test)
# –ü–æ—Ç—ñ–º –¥—ñ–π –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–æ –¥–æ –¥—ñ–∞–≥–Ω–æ–∑—É
```

### 4. –Ü–≥–Ω–æ—Ä—É–≤–∞—Ç–∏ early stopping signs

```python
# ‚ùå –¢—Ä–µ–Ω—É–≤–∞—Ç–∏ –¥–æ –∫—ñ–Ω—Ü—è epochs –Ω–µ–∑–∞–ª–µ–∂–Ω–æ –≤—ñ–¥ validation
model.fit(X, y, epochs=1000)

# ‚úÖ Early stopping
from keras.callbacks import EarlyStopping
early_stop = EarlyStopping(monitor='val_loss', patience=10)
model.fit(X, y, validation_split=0.2, callbacks=[early_stop])
```

### 5. –ù–µ –≤—ñ–∑—É–∞–ª—ñ–∑—É–≤–∞—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏

```python
# ‚ùå –î–∏–≤–∏—Ç–∏—Å—è —Ç—ñ–ª—å–∫–∏ –Ω–∞ —á–∏—Å–ª–∞
print(f"R¬≤ = {score}")

# ‚úÖ –ë—É–¥—É–≤–∞—Ç–∏ –≥—Ä–∞—Ñ—ñ–∫–∏
plt.scatter(y_test, model.predict(X_test))
plt.xlabel('Actual')
plt.ylabel('Predicted')
plt.show()
```

---

## –ü—Ä–∞–∫—Ç–∏—á–Ω—ñ –ø–æ—Ä–∞–¥–∏ üí°

### 1. –ó–∞–≤–∂–¥–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π Train/Val/Test split

```python
# 60% train, 20% validation, 20% test
from sklearn.model_selection import train_test_split

X_temp, X_test, y_temp, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

X_train, X_val, y_train, y_val = train_test_split(
    X_temp, y_temp, test_size=0.25, random_state=42
)

# Train ‚Üí –Ω–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ
# Validation ‚Üí –ø—ñ–¥–±—ñ—Ä –≥—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤
# Test ‚Üí —Ñ—ñ–Ω–∞–ª—å–Ω–∞ –æ—Ü—ñ–Ω–∫–∞ (—Ç–æ—Ä–∫–∞—î–º–æ—Å—è –û–î–ò–ù —Ä–∞–∑!)
```

### 2. Baseline —Å–ø–æ—á–∞—Ç–∫—É

```python
# –ó–∞–≤–∂–¥–∏ –ø–æ—á–∏–Ω–∞–π –∑ –ø—Ä–æ—Å—Ç–æ—ó –º–æ–¥–µ–ª—ñ
from sklearn.dummy import DummyRegressor

dummy = DummyRegressor(strategy='mean')
dummy.fit(X_train, y_train)
baseline = dummy.score(X_test, y_test)

print(f"Baseline R¬≤: {baseline:.3f}")
# –ë—É–¥—å-—è–∫–∞ –º–æ–¥–µ–ª—å –º–∞—î –±—É—Ç–∏ –∫—Ä–∞—â–µ –∑–∞ baseline!
```

### 3. –†–µ–≥—É–ª—è—Ä–Ω–∞ –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∞

```python
# –ü—ñ—Å–ª—è –∫–æ–∂–Ω–æ—ó –∑–º—ñ–Ω–∏ –º–æ–¥–µ–ª—ñ
def quick_check(model, X_tr, X_te, y_tr, y_te):
    model.fit(X_tr, y_tr)
    train = model.score(X_tr, y_tr)
    test = model.score(X_te, y_te)
    print(f"Train: {train:.3f} | Test: {test:.3f} | Gap: {train-test:.3f}")

quick_check(model, X_train, X_test, y_train, y_test)
```

### 4. –î–æ–∫—É–º–µ–Ω—Ç—É–π –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏

```python
import pandas as pd

results = []

for max_depth in [1, 3, 5, 7, 10, None]:
    dt = DecisionTreeRegressor(max_depth=max_depth, random_state=42)
    dt.fit(X_train, y_train)
    
    results.append({
        'max_depth': max_depth,
        'train_r2': dt.score(X_train, y_train),
        'val_r2': dt.score(X_val, y_val),
        'gap': dt.score(X_train, y_train) - dt.score(X_val, y_val)
    })

df = pd.DataFrame(results)
print(df.sort_values('val_r2', ascending=False))
```

### 5. Cross-Validation –¥–ª—è –Ω–∞–¥—ñ–π–Ω–æ—Å—Ç—ñ

```python
from sklearn.model_selection import cross_val_score

scores = cross_val_score(model, X, y, cv=5, scoring='r2')
print(f"CV R¬≤: {scores.mean():.3f} (+/- {scores.std()*2:.3f})")

# –Ø–∫—â–æ std –≤–µ–ª–∏–∫–∏–π ‚Üí –Ω–µ—Å—Ç–∞–±—ñ–ª—å–Ω–∞ –º–æ–¥–µ–ª—å
```

---

## –†–µ–∞–ª—å–Ω–∏–π –ø—Ä–∏–∫–ª–∞–¥: –ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è

```python
from sklearn.datasets import load_breast_cancer
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, learning_curve
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö
cancer = load_breast_cancer()
X = cancer.data
y = cancer.target

print("="*70)
print("OVERFITTING vs UNDERFITTING: Breast Cancer Classification")
print("="*70)
print(f"Samples: {X.shape[0]}")
print(f"Features: {X.shape[1]}")
print(f"Classes: {np.unique(y)}")

# –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è
X_temp, X_test, y_temp, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)
X_train, X_val, y_train, y_val = train_test_split(
    X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp
)

print(f"\nTrain: {X_train.shape[0]} samples")
print(f"Validation: {X_val.shape[0]} samples")
print(f"Test: {X_test.shape[0]} samples")

# –ú–æ–¥–µ–ª—ñ –¥–ª—è —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è
models = {
    'Shallow Tree (depth=1)': DecisionTreeClassifier(max_depth=1, random_state=42),
    'Medium Tree (depth=5)': DecisionTreeClassifier(max_depth=5, random_state=42),
    'Deep Tree (no limit)': DecisionTreeClassifier(random_state=42),
    'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)
}

results = []

for name, model in models.items():
    print(f"\n{'='*70}")
    print(f"Model: {name}")
    print(f"{'='*70}")
    
    # –ù–∞–≤—á–∞–Ω–Ω—è
    model.fit(X_train, y_train)
    
    # –û—Ü—ñ–Ω–∫–∞
    train_acc = model.score(X_train, y_train)
    val_acc = model.score(X_val, y_val)
    test_acc = model.score(X_test, y_test)
    gap = train_acc - val_acc
    
    print(f"Train Accuracy: {train_acc:.4f}")
    print(f"Val Accuracy:   {val_acc:.4f}")
    print(f"Test Accuracy:  {test_acc:.4f}")
    print(f"Gap:            {gap:.4f}")
    
    # –î—ñ–∞–≥–Ω–æ–∑
    if val_acc < 0.85 and gap < 0.05:
        diagnosis = "üî¥ UNDERFITTING"
    elif gap > 0.1:
        diagnosis = "üî¥ OVERFITTING"
    else:
        diagnosis = "‚úÖ GOOD BALANCE"
    
    print(f"Diagnosis: {diagnosis}")
    
    results.append({
        'Model': name,
        'Train Acc': train_acc,
        'Val Acc': val_acc,
        'Test Acc': test_acc,
        'Gap': gap,
        'Diagnosis': diagnosis
    })

# –ü—ñ–¥—Å—É–º–∫–æ–≤–∞ —Ç–∞–±–ª–∏—Ü—è
print("\n" + "="*70)
print("SUMMARY")
print("="*70)
df_results = pd.DataFrame(results)
print(df_results.to_string(index=False))

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
fig, axes = plt.subplots(2, 2, figsize=(16, 12))

# 1. Accuracy comparison
model_names = df_results['Model'].values
train_accs = df_results['Train Acc'].values
val_accs = df_results['Val Acc'].values
test_accs = df_results['Test Acc'].values

x = np.arange(len(model_names))
width = 0.25

axes[0, 0].bar(x - width, train_accs, width, label='Train', alpha=0.8)
axes[0, 0].bar(x, val_accs, width, label='Validation', alpha=0.8)
axes[0, 0].bar(x + width, test_accs, width, label='Test', alpha=0.8)
axes[0, 0].set_ylabel('Accuracy', fontsize=11)
axes[0, 0].set_title('Model Comparison', fontsize=13, fontweight='bold')
axes[0, 0].set_xticks(x)
axes[0, 0].set_xticklabels(model_names, rotation=45, ha='right')
axes[0, 0].legend()
axes[0, 0].grid(True, alpha=0.3, axis='y')

# 2. Gap analysis
gaps = df_results['Gap'].values
colors = ['red' if g > 0.1 else 'green' if g < 0.05 else 'orange' for g in gaps]

axes[0, 1].barh(model_names, gaps, color=colors, alpha=0.7)
axes[0, 1].axvline(x=0.05, color='green', linestyle='--', label='Good (<0.05)')
axes[0, 1].axvline(x=0.1, color='orange', linestyle='--', label='Warning (>0.1)')
axes[0, 1].set_xlabel('Gap (Train - Val)', fontsize=11)
axes[0, 1].set_title('Overfitting Analysis', fontsize=13, fontweight='bold')
axes[0, 1].legend()
axes[0, 1].grid(True, alpha=0.3, axis='x')

# 3. Learning curves –¥–ª—è Random Forest
print("\nGenerating learning curves for Random Forest...")
train_sizes, train_scores, val_scores = learning_curve(
    RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42),
    X_train, y_train,
    train_sizes=np.linspace(0.1, 1.0, 10),
    cv=5,
    n_jobs=-1
)

train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)
val_mean = np.mean(val_scores, axis=1)
val_std = np.std(val_scores, axis=1)

axes[1, 0].plot(train_sizes, train_mean, 'o-', linewidth=2, label='Train')
axes[1, 0].fill_between(train_sizes, train_mean - train_std,
                        train_mean + train_std, alpha=0.1)
axes[1, 0].plot(train_sizes, val_mean, 's-', linewidth=2, label='Validation')
axes[1, 0].fill_between(train_sizes, val_mean - val_std,
                        val_mean + val_std, alpha=0.1)
axes[1, 0].set_xlabel('Training Set Size', fontsize=11)
axes[1, 0].set_ylabel('Accuracy', fontsize=11)
axes[1, 0].set_title('Learning Curves: Random Forest', fontsize=13, fontweight='bold')
axes[1, 0].legend()
axes[1, 0].grid(True, alpha=0.3)

# 4. Bias-Variance map
axes[1, 1].scatter(gaps, val_accs, s=200, alpha=0.6, c=range(len(gaps)), cmap='viridis')

for i, name in enumerate(model_names):
    axes[1, 1].annotate(name.split('(')[0].strip(), 
                       (gaps[i], val_accs[i]), 
                       fontsize=8, ha='center')

axes[1, 1].axvline(x=0.05, color='green', linestyle='--', alpha=0.5)
axes[1, 1].axhline(y=0.9, color='blue', linestyle='--', alpha=0.5)

axes[1, 1].text(0.02, 0.92, 'Excellent', fontsize=9, 
               bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.5))
axes[1, 1].text(0.15, 0.88, 'High Variance', fontsize=9,
               bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.5))

axes[1, 1].set_xlabel('Gap (Train - Val)', fontsize=11)
axes[1, 1].set_ylabel('Validation Accuracy', fontsize=11)
axes[1, 1].set_title('Bias-Variance Map', fontsize=13, fontweight='bold')
axes[1, 1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("\n" + "="*70)
```

---

## –ü–æ–≤'—è–∑–∞–Ω—ñ —Ç–µ–º–∏

- [[01_Bias_Variance_Tradeoff]] ‚Äî —Ç–µ–æ—Ä–µ—Ç–∏—á–Ω–∞ –æ—Å–Ω–æ–≤–∞
- [[03_Train_Test_Split]] ‚Äî –ø—Ä–∞–≤–∏–ª—å–Ω–µ —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö
- [[04_Cross_Validation]] ‚Äî –Ω–∞–¥—ñ–π–Ω–∞ –≤–∞–ª—ñ–¥–∞—Ü—ñ—è
- [[03_Regularization]] ‚Äî –º–µ—Ç–æ–¥–∏ –±–æ—Ä–æ—Ç—å–±–∏ –∑ overfitting
- [[02_Random_Forest]] ‚Äî –∑–º–µ–Ω—à–µ–Ω–Ω—è variance
- [[Learning_Curves]] ‚Äî –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –ø—Ä–æ–±–ª–µ–º

## –†–µ—Å—É—Ä—Å–∏

- [Machine Learning Mastery: Overfitting and Underfitting](https://machinelearningmastery.com/overfitting-and-underfitting-with-machine-learning-algorithms/)
- [Towards Data Science: Understanding Overfitting](https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229)
- [Andrew Ng: Advice for Applying ML](https://www.coursera.org/learn/machine-learning)

---

## –ö–ª—é—á–æ–≤—ñ –≤–∏—Å–Ω–æ–≤–∫–∏

> **Underfitting** ‚Äî –º–æ–¥–µ–ª—å –∑–∞–Ω–∞–¥—Ç–æ –ø—Ä–æ—Å—Ç–∞, –Ω–µ –≤–ª–æ–≤–ª—é—î –ø–∞—Ç—Ç–µ—Ä–Ω–∏. **Overfitting** ‚Äî –º–æ–¥–µ–ª—å –∑–∞–Ω–∞–¥—Ç–æ —Å–∫–ª–∞–¥–Ω–∞, –∑–∞–ø–∞–º'—è—Ç–æ–≤—É—î —à—É–º. **–û–ø—Ç–∏–º–∞–ª—å–Ω–∞ –º–æ–¥–µ–ª—å** ‚Äî –∑–Ω–∞—Ö–æ–¥–∏—Ç—å –±–∞–ª–∞–Ω—Å.

**–î—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∞:**
| –ü—Ä–æ–±–ª–µ–º–∞ | Train | Test | Gap | –†—ñ—à–µ–Ω–Ω—è |
|----------|-------|------|-----|---------|
| Underfitting | –ù–∏–∑—å–∫–∏–π | –ù–∏–∑—å–∫–∏–π | –ú–∞–ª–∏–π | ‚Üë –°–∫–ª–∞–¥–Ω—ñ—Å—Ç—å |
| Overfitting | –í–∏—Å–æ–∫–∏–π | –ù–∏–∑—å–∫–∏–π | –í–µ–ª–∏–∫–∏–π | ‚Üë –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—è |
| Optimal | –í–∏—Å–æ–∫–∏–π | –í–∏—Å–æ–∫–∏–π | –ú–∞–ª–∏–π | ‚úÖ |

**–ö–ª—é—á–æ–≤—ñ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∏:**
- Train/Val/Test split ‚Äî –ø—Ä–∞–≤–∏–ª—å–Ω–∞ –≤–∞–ª—ñ–¥–∞—Ü—ñ—è
- Learning Curves ‚Äî –≤—ñ–∑—É–∞–ª—å–Ω–∞ –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∞
- Cross-Validation ‚Äî –Ω–∞–¥—ñ–π–Ω–∞ –æ—Ü—ñ–Ω–∫–∞
- Regularization ‚Äî –∫–æ–Ω—Ç—Ä–æ–ª—å —Å–∫–ª–∞–¥–Ω–æ—Å—Ç—ñ

**–ù–∞–π–≤–∞–∂–ª–∏–≤—ñ—à–µ:**
- –ó–∞–≤–∂–¥–∏ –ø–µ—Ä–µ–≤—ñ—Ä—è–π train **–Ü** test scores
- –í—ñ–∑—É–∞–ª—ñ–∑—É–π learning curves
- –ü–æ—á–∏–Ω–∞–π –ø—Ä–æ—Å—Ç–æ (baseline)
- –î–æ–¥–∞–≤–∞–π —Å–∫–ª–∞–¥–Ω—ñ—Å—Ç—å –ø–æ—Å—Ç—É–ø–æ–≤–æ
- –†–µ–≥—É–ª—è—Ä–Ω–æ –¥—ñ–∞–≥–Ω–æ—Å—Ç—É–π
- –î–æ–∫—É–º–µ–Ω—Ç—É–π –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏

---

#ml #core-concepts #overfitting #underfitting #model-selection #diagnostics #regularization
