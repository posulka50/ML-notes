# Bias-Variance Tradeoff (–ö–æ–º–ø—Ä–æ–º—ñ—Å –º—ñ–∂ –∑–º—ñ—â–µ–Ω–Ω—è–º —Ç–∞ –¥–∏—Å–ø–µ—Ä—Å—ñ—î—é)

## –©–æ —Ü–µ?

**Bias-Variance Tradeoff** ‚Äî —Ü–µ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–∞ –∫–æ–Ω—Ü–µ–ø—Ü—ñ—è –≤ –º–∞—à–∏–Ω–Ω–æ–º—É –Ω–∞–≤—á–∞–Ω–Ω—ñ, —è–∫–∞ –æ–ø–∏—Å—É—î **–∫–æ–º–ø—Ä–æ–º—ñ—Å –º—ñ–∂ –ø–æ–º–∏–ª–∫–æ—é —á–µ—Ä–µ–∑ –Ω–µ–¥–æ—Å—Ç–∞—Ç–Ω—é —Å–∫–ª–∞–¥–Ω—ñ—Å—Ç—å –º–æ–¥–µ–ª—ñ (bias) —Ç–∞ –ø–æ–º–∏–ª–∫–æ—é —á–µ—Ä–µ–∑ –Ω–∞–¥–º—ñ—Ä–Ω—É —á—É—Ç–ª–∏–≤—ñ—Å—Ç—å –¥–æ —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö (variance)**.

**–ì–æ–ª–æ–≤–Ω–∞ —ñ–¥–µ—è:** –±—É–¥—å-—è–∫–∞ –º–æ–¥–µ–ª—å –º–∞—î –∑–Ω–∞–π—Ç–∏ –±–∞–ª–∞–Ω—Å –º—ñ–∂ —Ç–∏–º, —â–æ–± –±—É—Ç–∏ –¥–æ—Å—Ç–∞—Ç–Ω—å–æ —Å–∫–ª–∞–¥–Ω–æ—é –¥–ª—è –≤–ª–æ–≤–ª—é–≤–∞–Ω–Ω—è –ø–∞—Ç—Ç–µ—Ä–Ω—ñ–≤ (–Ω–∏–∑—å–∫–∏–π bias) —Ç–∞ –¥–æ—Å—Ç–∞—Ç–Ω—å–æ –ø—Ä–æ—Å—Ç–æ—é, —â–æ–± –Ω–µ –ø–µ—Ä–µ–Ω–∞–≤—á–∞—Ç–∏—Å—è –Ω–∞ —à—É–º—ñ (–Ω–∏–∑—å–∫–∏–π variance).

## –ù–∞–≤—ñ—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ?

- üéØ **–†–æ–∑—É–º—ñ–Ω–Ω—è –ø–æ–º–∏–ª–æ–∫ –º–æ–¥–µ–ª—ñ** ‚Äî —á–æ–º—É –º–æ–¥–µ–ª—å –ø–æ–º–∏–ª—è—î—Ç—å—Å—è
- üìä **–í–∏–±—ñ—Ä —Å–∫–ª–∞–¥–Ω–æ—Å—Ç—ñ –º–æ–¥–µ–ª—ñ** ‚Äî –ø—Ä–æ—Å—Ç–∞ —á–∏ —Å–∫–ª–∞–¥–Ω–∞ –º–æ–¥–µ–ª—å
- üîß **–î—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –ø—Ä–æ–±–ª–µ–º** ‚Äî underfitting vs overfitting
- üí° **–í–∏–±—ñ—Ä –∞–ª–≥–æ—Ä–∏—Ç–º—É** ‚Äî —è–∫–∏–π –º–µ—Ç–æ–¥ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏
- üéöÔ∏è **Tuning –≥—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤** ‚Äî —è–∫ –Ω–∞–ª–∞—à—Ç–æ–≤—É–≤–∞—Ç–∏ –º–æ–¥–µ–ª—å
- üîç **–ü–æ–∫—Ä–∞—â–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ** ‚Äî –¥–µ —Ñ–æ–∫—É—Å—É–≤–∞—Ç–∏ –∑—É—Å–∏–ª–ª—è

## –ö–æ–ª–∏ –≤–∞–∂–ª–∏–≤–æ?

**–ü–æ—Ç—Ä—ñ–±–Ω–æ —Ä–æ–∑—É–º—ñ—Ç–∏:**

- –ú–æ–¥–µ–ª—å –ø–æ–≥–∞–Ω–æ –ø—Ä–∞—Ü—é—î —ñ —Ç—Ä–µ–±–∞ –∑—Ä–æ–∑—É–º—ñ—Ç–∏ —á–æ–º—É
- –í–∏–±–∏—Ä–∞—î—à –º—ñ–∂ –ø—Ä–æ—Å—Ç–æ—é —Ç–∞ —Å–∫–ª–∞–¥–Ω–æ—é –º–æ–¥–µ–ª–ª—é
- **Debugging** ‚Äî train score vs test score –¥—É–∂–µ —Ä—ñ–∑–Ω—ñ
- –í–∏–±—ñ—Ä –º—ñ–∂ —Ä—ñ–∑–Ω–∏–º–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–∞–º–∏
- **Feature engineering** ‚Äî –¥–æ–¥–∞–≤–∞—Ç–∏ —á–∏ –Ω—ñ –Ω–æ–≤—ñ –æ–∑–Ω–∞–∫–∏

---

## –†–æ–∑–∫–ª–∞–¥–∞–Ω–Ω—è –ø–æ–º–∏–ª–∫–∏ (Error Decomposition)

### –ó–∞–≥–∞–ª—å–Ω–∞ —Ñ–æ—Ä–º—É–ª–∞

–î–ª—è –±—É–¥—å-—è–∫–æ—ó –º–æ–¥–µ–ª—ñ, –æ—á—ñ–∫—É–≤–∞–Ω–∞ –ø–æ–º–∏–ª–∫–∞ –Ω–∞ –Ω–æ–≤–æ–º—É –∑—Ä–∞–∑–∫—É —Å–∫–ª–∞–¥–∞—î—Ç—å—Å—è –∑ —Ç—Ä—å–æ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ñ–≤:

$$\text{Expected Error} = \text{Bias}^2 + \text{Variance} + \text{Irreducible Error}$$

–¥–µ:
- **Bias¬≤** ‚Äî –ø–æ–º–∏–ª–∫–∞ —á–µ—Ä–µ–∑ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ñ –ø—Ä–∏–ø—É—â–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ
- **Variance** ‚Äî –ø–æ–º–∏–ª–∫–∞ —á–µ—Ä–µ–∑ —á—É—Ç–ª–∏–≤—ñ—Å—Ç—å –¥–æ —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö
- **Irreducible Error** ‚Äî —à—É–º —É –¥–∞–Ω–∏—Ö (–Ω–µ–ø–µ—Ä–µ–¥–±–∞—á—É–≤–∞–Ω–∞ —á–∞—Å—Ç–∏–Ω–∞)

### –î–µ—Ç–∞–ª—å–Ω–∞ —Ñ–æ—Ä–º—É–ª–∞

–î–ª—è —Ä–µ–≥—Ä–µ—Å—ñ—ó (MSE):

$$\mathbb{E}[(y - \hat{f}(x))^2] = \text{Bias}[\hat{f}(x)]^2 + \text{Var}[\hat{f}(x)] + \sigma^2$$

–¥–µ:
- $y$ ‚Äî —Å–ø—Ä–∞–≤–∂–Ω—î –∑–Ω–∞—á–µ–Ω–Ω—è
- $\hat{f}(x)$ ‚Äî –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ
- $\sigma^2$ ‚Äî irreducible error (variance —à—É–º—É)

---

## Bias (–ó–º—ñ—â–µ–Ω–Ω—è)

### –©–æ —Ü–µ?

**Bias** ‚Äî —Ü–µ –ø–æ–º–∏–ª–∫–∞ —á–µ—Ä–µ–∑ **–Ω–∞–¥—Ç–æ —Å–ø—Ä–æ—â–µ–Ω—ñ –ø—Ä–∏–ø—É—â–µ–Ω–Ω—è** –º–æ–¥–µ–ª—ñ –ø—Ä–æ –∑–∞–ª–µ–∂–Ω—ñ—Å—Ç—å –º—ñ–∂ $X$ —Ç–∞ $y$.

### –§–æ—Ä–º—É–ª–∞

$$\text{Bias}[\hat{f}(x)] = \mathbb{E}[\hat{f}(x)] - f(x)$$

–¥–µ:
- $\mathbb{E}[\hat{f}(x)]$ ‚Äî –æ—á—ñ–∫—É–≤–∞–Ω–µ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ (—É—Å–µ—Ä–µ–¥–Ω–µ–Ω–µ –ø–æ –≤—Å—ñ—Ö –º–æ–∂–ª–∏–≤–∏—Ö —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∏—Ö –Ω–∞–±–æ—Ä–∞—Ö)
- $f(x)$ ‚Äî —Å–ø—Ä–∞–≤–∂–Ω—è —Ñ—É–Ω–∫—Ü—ñ—è

### –Ü–Ω—Ç—É—ó—Ü—ñ—è

**–í–∏—Å–æ–∫–∏–π bias** –æ–∑–Ω–∞—á–∞—î, —â–æ –º–æ–¥–µ–ª—å:

- ‚úó –†–æ–±–∏—Ç—å **—Å–∏–ª—å–Ω—ñ —Å–ø—Ä–æ—â–µ–Ω–Ω—è**
- ‚úó **–ù–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ –≥–Ω—É—á–∫–∞** –¥–ª—è –≤–ª–æ–≤–ª—é–≤–∞–Ω–Ω—è –ø–∞—Ç—Ç–µ—Ä–Ω—ñ–≤
- ‚úó –î–∞—î **—Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–Ω–æ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ñ** –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
- ‚úó **Underfits** (–Ω–µ–¥–æ–Ω–∞–≤—á–∞—î—Ç—å—Å—è)

**–ü—Ä–∏–∫–ª–∞–¥ –≤–∏—Å–æ–∫–æ–≥–æ bias:**

```
–°–ø—Ä–∞–≤–∂–Ω—è –∑–∞–ª–µ–∂–Ω—ñ—Å—Ç—å: y = x¬≤ (–ø–∞—Ä–∞–±–æ–ª–∞)
–ú–æ–¥–µ–ª—å: y = ax + b (–ø—Ä—è–º–∞ –ª—ñ–Ω—ñ—è)

     y
     |     ‚Ä¢
     |   ‚Ä¢   ‚Ä¢
     | ‚Ä¢       ‚Ä¢    ‚Üê –°–ø—Ä–∞–≤–∂–Ω—ñ –¥–∞–Ω—ñ (–ø–∞—Ä–∞–±–æ–ª–∞)
     |‚Ä¢‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Ä¢
     |           ‚Ä¢
     |____________ x
          ‚Üë
      –õ—ñ–Ω—ñ–π–Ω–∞ –º–æ–¥–µ–ª—å –Ω–µ –º–æ–∂–µ –≤–ª–æ–≤–∏—Ç–∏ –∫—Ä–∏–≤—É!
```

### –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –≤–∏—Å–æ–∫–æ–≥–æ bias

| –û–∑–Ω–∞–∫–∞ | –û–ø–∏—Å |
|--------|------|
| **Train Error** | –í–∏—Å–æ–∫–∏–π ‚ùå |
| **Test Error** | –í–∏—Å–æ–∫–∏–π ‚ùå |
| **Gap** | –ú–∞–ª–∏–π (train ‚âà test) |
| **–ü—Ä–æ–±–ª–µ–º–∞** | Underfitting |
| **–ú–æ–¥–µ–ª—å** | –ó–∞–Ω–∞–¥—Ç–æ –ø—Ä–æ—Å—Ç–∞ |

### –ú–æ–¥–µ–ª—ñ –∑ –≤–∏—Å–æ–∫–∏–º bias

- Linear Regression (–¥–ª—è –Ω–µ–ª—ñ–Ω—ñ–π–Ω–∏—Ö –¥–∞–Ω–∏—Ö)
- Logistic Regression (–¥–ª—è —Å–∫–ª–∞–¥–Ω–∏—Ö –≥—Ä–∞–Ω–∏—Ü—å)
- Decision Tree –∑ `max_depth=1` (decision stump)
- Naive Bayes (—Å–∏–ª—å–Ω—ñ –ø—Ä–∏–ø—É—â–µ–Ω–Ω—è –ø—Ä–æ –Ω–µ–∑–∞–ª–µ–∂–Ω—ñ—Å—Ç—å)

### –Ø–∫ –∑–º–µ–Ω—à–∏—Ç–∏ bias?

‚úÖ –ó–±—ñ–ª—å—à–∏—Ç–∏ —Å–∫–ª–∞–¥–Ω—ñ—Å—Ç—å –º–æ–¥–µ–ª—ñ:

- Polynomial features
- –ë—ñ–ª—å—à–∞ –≥–ª–∏–±–∏–Ω–∞ –¥–µ—Ä–µ–≤ (`max_depth`)
- –ë—ñ–ª—å—à–µ layers —É –Ω–µ–π—Ä–æ–Ω–Ω–∏—Ö –º–µ—Ä–µ–∂–∞—Ö
- –ë—ñ–ª—å—à–µ –æ–∑–Ω–∞–∫ (feature engineering)

‚úÖ –í–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ —Å–∫–ª–∞–¥–Ω—ñ—à–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º:

- Linear ‚Üí Polynomial ‚Üí Neural Network
- Decision Stump ‚Üí Deep Tree

‚úÖ –ó–º–µ–Ω—à–∏—Ç–∏ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—é:

- –ú–µ–Ω—à–∏–π $\lambda$ (Ridge/Lasso)
- –ú–µ–Ω—à–∏–π `min_samples_leaf`

---

## Variance (–î–∏—Å–ø–µ—Ä—Å—ñ—è)

### –©–æ —Ü–µ?

**Variance** ‚Äî —Ü–µ –ø–æ–º–∏–ª–∫–∞ —á–µ—Ä–µ–∑ **–Ω–∞–¥–º—ñ—Ä–Ω—É —á—É—Ç–ª–∏–≤—ñ—Å—Ç—å** –º–æ–¥–µ–ª—ñ –¥–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–æ–≥–æ –Ω–∞–±–æ—Ä—É.

### –§–æ—Ä–º—É–ª–∞

$$\text{Variance}[\hat{f}(x)] = \mathbb{E}[(\hat{f}(x) - \mathbb{E}[\hat{f}(x)])^2]$$

### –Ü–Ω—Ç—É—ó—Ü—ñ—è

**–í–∏—Å–æ–∫–∞ variance** –æ–∑–Ω–∞—á–∞—î, —â–æ –º–æ–¥–µ–ª—å:
- ‚úó –ó–∞–Ω–∞–¥—Ç–æ **–≥–Ω—É—á–∫–∞**
- ‚úó **–ó–∞–ø–∞–º'—è—Ç–æ–≤—É—î —à—É–º** —É —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö
- ‚úó –î—É–∂–µ **—Ä—ñ–∑–Ω—ñ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è** –Ω–∞ —Ä—ñ–∑–Ω–∏—Ö train sets
- ‚úó **Overfits** (–ø–µ—Ä–µ–Ω–∞–≤—á–∞—î—Ç—å—Å—è)

**–ü—Ä–∏–∫–ª–∞–¥ –≤–∏—Å–æ–∫–æ—ó variance:**
```
–°–ø—Ä–∞–≤–∂–Ω—è –∑–∞–ª–µ–∂–Ω—ñ—Å—Ç—å: –ø—Ä—è–º–∞ –ª—ñ–Ω—ñ—è –∑ —à—É–º–æ–º
–ú–æ–¥–µ–ª—å: –ø–æ–ª—ñ–Ω–æ–º 10-–≥–æ —Å—Ç–µ–ø–µ–Ω—è

     y
     |    ‚ï±‚ï≤‚ï±‚ï≤
     |   ‚ï±    ‚ï≤‚ï±‚ï≤    ‚Üê –ú–æ–¥–µ–ª—å (–ø—Ä–æ—Ö–æ–¥–∏—Ç—å —á–µ—Ä–µ–∑ –∫–æ–∂–Ω—É —Ç–æ—á–∫—É!)
     | ‚Ä¢‚ï±   ‚Ä¢    ‚ï≤‚Ä¢
     |‚ï±   ‚Ä¢    ‚Ä¢   ‚ï≤
     |  ‚Ä¢    ‚Ä¢   ‚Ä¢  ‚ï≤
     |________________ x
     
–ú–æ–¥–µ–ª—å –∑–∞–ø–∞–º'—è—Ç–∞–ª–∞ —à—É–º!
```

### –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –≤–∏—Å–æ–∫–æ—ó variance

| –û–∑–Ω–∞–∫–∞ | –û–ø–∏—Å |
|--------|------|
| **Train Error** | –î—É–∂–µ –Ω–∏–∑—å–∫–∏–π ‚úì |
| **Test Error** | –í–∏—Å–æ–∫–∏–π ‚ùå |
| **Gap** | –í–µ–ª–∏–∫–∏–π (train << test) |
| **–ü—Ä–æ–±–ª–µ–º–∞** | Overfitting |
| **–ú–æ–¥–µ–ª—å** | –ó–∞–Ω–∞–¥—Ç–æ —Å–∫–ª–∞–¥–Ω–∞ |

### –ú–æ–¥–µ–ª—ñ –∑ –≤–∏—Å–æ–∫–æ—é variance

- Polynomial Regression (–≤–∏—Å–æ–∫—ñ —Å—Ç–µ–ø–µ–Ω—ñ)
- Decision Tree (–±–µ–∑ –æ–±–º–µ–∂–µ–Ω—å –≥–ª–∏–±–∏–Ω–∏)
- KNN –∑ –º–∞–ª–∏–º K (K=1)
- Neural Networks (–±–µ–∑ regularization)

### –Ø–∫ –∑–º–µ–Ω—à–∏—Ç–∏ variance?

‚úÖ –ó–º–µ–Ω—à–∏—Ç–∏ —Å–∫–ª–∞–¥–Ω—ñ—Å—Ç—å –º–æ–¥–µ–ª—ñ:
- –ú–µ–Ω—à–∞ –≥–ª–∏–±–∏–Ω–∞ –¥–µ—Ä–µ–≤ (`max_depth`)
- –ú–µ–Ω—à–∏–π degree —É –ø–æ–ª—ñ–Ω–æ–º–∞—Ö
- –ú–µ–Ω—à–µ parameters —É NN

‚úÖ –î–æ–¥–∞—Ç–∏ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—é:
- Ridge/Lasso (–±—ñ–ª—å—à–∏–π $\lambda$)
- Dropout —É –Ω–µ–π—Ä–æ–Ω–Ω–∏—Ö –º–µ—Ä–µ–∂–∞—Ö
- Early stopping

‚úÖ –ë—ñ–ª—å—à–µ —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö:
- –ó–±—ñ–ª—å—à–∏—Ç–∏ —Ä–æ–∑–º—ñ—Ä dataset
- Data augmentation

‚úÖ –í–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ –∞–Ω—Å–∞–º–±–ª—ñ:
- Random Forest (–∑–º–µ–Ω—à—É—î variance —á–µ—Ä–µ–∑ bagging)
- Gradient Boosting (–∑ regularization)

---

## Irreducible Error (–ù–µ–ø–µ—Ä–µ–¥–±–∞—á—É–≤–∞–Ω–∞ –ø–æ–º–∏–ª–∫–∞)

### –©–æ —Ü–µ?

**Irreducible Error** ‚Äî —Ü–µ –ø–æ–º–∏–ª–∫–∞ —á–µ—Ä–µ–∑ **—à—É–º —É –¥–∞–Ω–∏—Ö**, —è–∫—É **–Ω–µ–º–æ–∂–ª–∏–≤–æ —É—Å—É–Ω—É—Ç–∏** –∂–æ–¥–Ω–æ—é –º–æ–¥–µ–ª–ª—é.

### –§–æ—Ä–º—É–ª–∞

$$\sigma^2 = \text{Var}[\epsilon]$$

–¥–µ $\epsilon$ ‚Äî –≤–∏–ø–∞–¥–∫–æ–≤–∏–π —à—É–º —É –¥–∞–Ω–∏—Ö: $y = f(x) + \epsilon$

### –Ü–Ω—Ç—É—ó—Ü—ñ—è

**–î–∂–µ—Ä–µ–ª–∞ irreducible error:**

- üìè –ü–æ–º–∏–ª–∫–∏ –≤–∏–º—ñ—Ä—é–≤–∞–Ω—å
- üé≤ –°–ø—Ä–∞–≤–∂–Ω—è –≤–∏–ø–∞–¥–∫–æ–≤—ñ—Å—Ç—å —É –ø—Ä–æ—Ü–µ—Å—ñ
- üîç –í—ñ–¥—Å—É—Ç–Ω—ñ –≤–∞–∂–ª–∏–≤—ñ –æ–∑–Ω–∞–∫–∏
- üåä Noise —É –¥–∞–Ω–∏—Ö

**–ü—Ä–∏–∫–ª–∞–¥:**

```
–ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è —Ü—ñ–Ω–∏ –∞–∫—Ü—ñ—ó —á–µ—Ä–µ–∑ —Ä—ñ–∫:
- –ú–æ–¥–µ–ª—å –º–æ–∂–µ –≤–ª–æ–≤–∏—Ç–∏ —Ç—Ä–µ–Ω–¥–∏ ‚úì
- –ê–ª–µ —Ä–∞–ø—Ç–æ–≤—ñ –ø–æ–¥—ñ—ó (–≤—ñ–π–Ω–∞, –ø–∞–Ω–¥–µ–º—ñ—è) –Ω–µ–ø–µ—Ä–µ–¥–±–∞—á—É–≤–∞–Ω—ñ ‚úó
- –¶–µ irreducible error
```

### –í–∞–∂–ª–∏–≤–æ!

- ‚ùå **–ù–µ–º–æ–∂–ª–∏–≤–æ –∑–º–µ–Ω—à–∏—Ç–∏** –∂–æ–¥–Ω–æ—é –º–æ–¥–µ–ª–ª—é
- ‚úÖ –ú–æ–∂–Ω–∞ —Ç—ñ–ª—å–∫–∏ **–æ—Ü—ñ–Ω–∏—Ç–∏** (–Ω–∏–∂–Ω—è –º–µ–∂–∞ –ø–æ–º–∏–ª–∫–∏)
- üéØ –ú–µ—Ç–∞ ML: –º—ñ–Ω—ñ–º—ñ–∑—É–≤–∞—Ç–∏ Bias¬≤ + Variance, –ø—Ä–∏–π–º–∞—é—á–∏ $\sigma^2$

---

## Tradeoff (–ö–æ–º–ø—Ä–æ–º—ñ—Å)

### –í—ñ–∑—É–∞–ª—å–Ω–∞ —ñ–Ω—Ç—É—ó—Ü—ñ—è

```
Total Error
    |
    |    ‚ï±‚Äæ‚Äæ‚Äæ‚ï≤        ‚Üê Total Error = Bias¬≤ + Variance + œÉ¬≤
    |   ‚ï±     ‚ï≤
    |  ‚ï±   ‚ï±‚Äæ‚Äæ‚Äæ‚Äæ‚ï≤     ‚Üê Variance
    | ‚ï±___‚ï±      ‚ï≤
    |‚ï±             ‚ï≤__ ‚Üê Bias¬≤
    |
    |‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ï≤__ ‚Üê Irreducible Error (œÉ¬≤)
    |_________________ Model Complexity
    Simple          Complex
    
    High Bias       Optimal        High Variance
    Low Variance    Balance        Low Bias
```

### –¢–∞–±–ª–∏—Ü—è –∫–æ–º–ø—Ä–æ–º—ñ—Å—É

| –°–∫–ª–∞–¥–Ω—ñ—Å—Ç—å –º–æ–¥–µ–ª—ñ | Bias | Variance | Train Error | Test Error | –ü—Ä–æ–±–ª–µ–º–∞ |
|-------------------|------|----------|-------------|------------|----------|
| **–î—É–∂–µ –ø—Ä–æ—Å—Ç–∞** | ‚¨ÜÔ∏è –í–∏—Å–æ–∫–∏–π | ‚¨áÔ∏è –ù–∏–∑—å–∫–∞ | –í–∏—Å–æ–∫–∏–π | –í–∏—Å–æ–∫–∏–π | Underfitting |
| **–û–ø—Ç–∏–º–∞–ª—å–Ω–∞** | ‚¨áÔ∏è –ü–æ–º—ñ—Ä–Ω–∏–π | ‚¨áÔ∏è –ü–æ–º—ñ—Ä–Ω–∞ | –ù–∏–∑—å–∫–∏–π | –ù–∏–∑—å–∫–∏–π | **–Ü–¥–µ–∞–ª—å–Ω–æ** ‚úì |
| **–î—É–∂–µ —Å–∫–ª–∞–¥–Ω–∞** | ‚¨áÔ∏è –ù–∏–∑—å–∫–∏–π | ‚¨ÜÔ∏è –í–∏—Å–æ–∫–∞ | –î—É–∂–µ –Ω–∏–∑—å–∫–∏–π | –í–∏—Å–æ–∫–∏–π | Overfitting |

### –ö–ª—é—á–æ–≤–∞ —ñ–¥–µ—è

> **–ù–µ –º–æ–∂–Ω–∞ –æ–¥–Ω–æ—á–∞—Å–Ω–æ –º—ñ–Ω—ñ–º—ñ–∑—É–≤–∞—Ç–∏ —ñ bias, —ñ variance!**

- –ó–º–µ–Ω—à–µ–Ω–Ω—è bias ‚Üí –∑–±—ñ–ª—å—à–µ–Ω–Ω—è variance
- –ó–º–µ–Ω—à–µ–Ω–Ω—è variance ‚Üí –∑–±—ñ–ª—å—à–µ–Ω–Ω—è bias
- **–ú–µ—Ç–∞:** –∑–Ω–∞–π—Ç–∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω–∏–π –±–∞–ª–∞–Ω—Å

---

## –ü—Ä–∏–∫–ª–∞–¥–∏ –Ω–∞ —Ä—ñ–∑–Ω–∏—Ö –º–æ–¥–µ–ª—è—Ö

### Linear Regression

```python
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
import numpy as np
import matplotlib.pyplot as plt

# –°–ø—Ä–∞–≤–∂–Ω—è —Ñ—É–Ω–∫—Ü—ñ—è: y = sin(x) + —à—É–º
np.random.seed(42)
X = np.linspace(0, 10, 100).reshape(-1, 1)
y_true = np.sin(X).ravel()
y = y_true + np.random.normal(0, 0.1, 100)

# Train/Test split
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# –†—ñ–∑–Ω—ñ —Å—Ç–µ–ø–µ–Ω—ñ –ø–æ–ª—ñ–Ω–æ–º—ñ–≤
degrees = [1, 3, 15]
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

for idx, degree in enumerate(degrees):
    # Polynomial features
    poly = PolynomialFeatures(degree=degree)
    X_train_poly = poly.fit_transform(X_train)
    X_test_poly = poly.transform(X_test)
    
    # –ú–æ–¥–µ–ª—å
    model = LinearRegression()
    model.fit(X_train_poly, y_train)
    
    # Scores
    train_score = model.score(X_train_poly, y_train)
    test_score = model.score(X_test_poly, y_test)
    
    # –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –¥–ª—è –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó
    X_plot = np.linspace(0, 10, 300).reshape(-1, 1)
    X_plot_poly = poly.transform(X_plot)
    y_plot = model.predict(X_plot_poly)
    
    # –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
    axes[idx].scatter(X_train, y_train, alpha=0.5, s=30, label='Train')
    axes[idx].scatter(X_test, y_test, alpha=0.5, s=30, label='Test')
    axes[idx].plot(X_plot, y_plot, 'r-', linewidth=2, label='Model')
    axes[idx].plot(X_plot, np.sin(X_plot), 'g--', linewidth=2, 
                   label='True function', alpha=0.7)
    
    # –î—ñ–∞–≥–Ω–æ–∑
    if degree == 1:
        diagnosis = "HIGH BIAS\nUnderfitting"
        color = 'red'
    elif degree == 3:
        diagnosis = "OPTIMAL\nGood Balance"
        color = 'green'
    else:
        diagnosis = "HIGH VARIANCE\nOverfitting"
        color = 'red'
    
    axes[idx].set_title(
        f'Degree={degree}\n'
        f'Train R¬≤={train_score:.3f}, Test R¬≤={test_score:.3f}\n'
        f'{diagnosis}',
        fontsize=11, fontweight='bold', color=color
    )
    axes[idx].set_xlabel('X')
    axes[idx].set_ylabel('y')
    axes[idx].legend(fontsize=9)
    axes[idx].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

**–û—á—ñ–∫—É–≤–∞–Ω–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:**
- **Degree=1:** Train R¬≤=0.4, Test R¬≤=0.4 (HIGH BIAS)
- **Degree=3:** Train R¬≤=0.9, Test R¬≤=0.88 (OPTIMAL)
- **Degree=15:** Train R¬≤=0.99, Test R¬≤=0.2 (HIGH VARIANCE)

### Decision Trees

```python
from sklearn.tree import DecisionTreeRegressor

# –†—ñ–∑–Ω—ñ max_depth
depths = [1, 5, None]
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

for idx, depth in enumerate(depths):
    # –ú–æ–¥–µ–ª—å
    dt = DecisionTreeRegressor(max_depth=depth, random_state=42)
    dt.fit(X_train, y_train)
    
    # Scores
    train_score = dt.score(X_train, y_train)
    test_score = dt.score(X_test, y_test)
    
    # –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
    X_plot = np.linspace(0, 10, 300).reshape(-1, 1)
    y_plot = dt.predict(X_plot)
    
    # –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
    axes[idx].scatter(X_train, y_train, alpha=0.5, s=30, label='Train')
    axes[idx].scatter(X_test, y_test, alpha=0.5, s=30, label='Test')
    axes[idx].plot(X_plot, y_plot, 'r-', linewidth=2, label='Model')
    axes[idx].plot(X_plot, np.sin(X_plot), 'g--', linewidth=2, 
                   label='True function', alpha=0.7)
    
    # –î—ñ–∞–≥–Ω–æ–∑
    if depth == 1:
        diagnosis = "HIGH BIAS"
    elif depth == 5:
        diagnosis = "OPTIMAL"
    else:
        diagnosis = "HIGH VARIANCE"
    
    axes[idx].set_title(
        f'max_depth={depth}\n'
        f'Train R¬≤={train_score:.3f}, Test R¬≤={test_score:.3f}\n'
        f'{diagnosis}',
        fontsize=11, fontweight='bold'
    )
    axes[idx].set_xlabel('X')
    axes[idx].set_ylabel('y')
    axes[idx].legend(fontsize=9)
    axes[idx].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

---

## Learning Curves (–ö—Ä–∏–≤—ñ –Ω–∞–≤—á–∞–Ω–Ω—è)

### –©–æ —Ü–µ?

**Learning Curves** –ø–æ–∫–∞–∑—É—é—Ç—å, —è–∫ train —Ç–∞ test error –∑–º—ñ–Ω—é—é—Ç—å—Å—è –∑—ñ –∑–º—ñ–Ω–æ—é:

- –ö—ñ–ª—å–∫–æ—Å—Ç—ñ —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∏—Ö –∑—Ä–∞–∑–∫—ñ–≤
- –°–∫–ª–∞–¥–Ω–æ—Å—Ç—ñ –º–æ–¥–µ–ª—ñ

### High Bias (Underfitting)

```
Error
    |
    |  Test Error ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    |                      ‚Üê Gap –º–∞–ª–∏–π
    |  Train Error ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    |
    |__________________ Training Set Size
    
–û–∑–Ω–∞–∫–∏:
- Train error –≤–∏—Å–æ–∫–∏–π —ñ —Å—Ç–∞–±—ñ–ª—å–Ω–∏–π
- Test error –≤–∏—Å–æ–∫–∏–π —ñ —Å—Ç–∞–±—ñ–ª—å–Ω–∏–π
- Gap –º—ñ–∂ –Ω–∏–º–∏ –º–∞–ª–∏–π
- –ë—ñ–ª—å—à–µ –¥–∞–Ω–∏—Ö –ù–ï –¥–æ–ø–æ–º–æ–∂–µ!
```

### High Variance (Overfitting)

```
Error
    |
    |  Test Error ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    |                      ‚Üê Gap –≤–µ–ª–∏–∫–∏–π
    |            
    |  Train Error ‚ï≤
    |               ‚ï≤_____ ‚Üê –î—É–∂–µ –Ω–∏–∑—å–∫–∏–π
    |__________________ Training Set Size
    
–û–∑–Ω–∞–∫–∏:
- Train error –¥—É–∂–µ –Ω–∏–∑—å–∫–∏–π
- Test error –≤–∏—Å–æ–∫–∏–π
- –í–µ–ª–∏–∫–∏–π gap
- –ë—ñ–ª—å—à–µ –¥–∞–Ω–∏—Ö –î–û–ü–û–ú–û–ñ–ï!
```

### Optimal Model

```
Error
    |
    |  Test Error ‚ï≤
    |              ‚ï≤______
    |               ‚ï≤      ‚Üê Gap –º–∞–ª–∏–π
    |  Train Error  ‚ï≤_____
    |__________________ Training Set Size
    
–û–∑–Ω–∞–∫–∏:
- Train error –ø–æ–º—ñ—Ä–Ω–∏–π
- Test error –ø–æ–º—ñ—Ä–Ω–∏–π
- Gap –º–∞–ª–∏–π
- –û–±–∏–¥–≤—ñ –∫—Ä–∏–≤—ñ –∑–±—ñ–≥–∞—é—Ç—å—Å—è
```

### –ö–æ–¥

```python
from sklearn.model_selection import learning_curve

def plot_learning_curves(estimator, X, y, title):
    """–ü–æ–±—É–¥–æ–≤–∞ learning curves"""
    train_sizes, train_scores, test_scores = learning_curve(
        estimator, X, y,
        train_sizes=np.linspace(0.1, 1.0, 10),
        cv=5,
        n_jobs=-1,
        scoring='r2'
    )
    
    # –£—Å–µ—Ä–µ–¥–Ω–µ–Ω–Ω—è –ø–æ folds
    train_scores_mean = np.mean(train_scores, axis=1)
    train_scores_std = np.std(train_scores, axis=1)
    test_scores_mean = np.mean(test_scores, axis=1)
    test_scores_std = np.std(test_scores, axis=1)
    
    # –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
    plt.figure(figsize=(10, 6))
    
    # Train scores
    plt.plot(train_sizes, train_scores_mean, 'o-', linewidth=2,
             label='Train Score', color='blue')
    plt.fill_between(train_sizes,
                     train_scores_mean - train_scores_std,
                     train_scores_mean + train_scores_std,
                     alpha=0.1, color='blue')
    
    # Test scores
    plt.plot(train_sizes, test_scores_mean, 's-', linewidth=2,
             label='Validation Score', color='red')
    plt.fill_between(train_sizes,
                     test_scores_mean - test_scores_std,
                     test_scores_mean + test_scores_std,
                     alpha=0.1, color='red')
    
    plt.xlabel('Training Set Size', fontsize=12)
    plt.ylabel('R¬≤ Score', fontsize=12)
    plt.title(f'Learning Curves: {title}', fontsize=14, fontweight='bold')
    plt.legend(fontsize=11)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()
    
    # –î—ñ–∞–≥–Ω–æ–∑
    final_gap = train_scores_mean[-1] - test_scores_mean[-1]
    final_test_score = test_scores_mean[-1]
    
    print(f"\n=== Diagnosis for {title} ===")
    print(f"Final Train Score: {train_scores_mean[-1]:.3f}")
    print(f"Final Test Score: {test_scores_mean[-1]:.3f}")
    print(f"Gap (Train - Test): {final_gap:.3f}")
    
    if final_test_score < 0.6 and final_gap < 0.1:
        print("‚ö†Ô∏è HIGH BIAS (Underfitting)")
        print("   ‚Üí Use more complex model or add features")
    elif final_gap > 0.2:
        print("‚ö†Ô∏è HIGH VARIANCE (Overfitting)")
        print("   ‚Üí Add regularization or more data")
    else:
        print("‚úì Good balance!")

# –ü—Ä–∏–∫–ª–∞–¥–∏
# High Bias
poly_1 = Pipeline([
    ('poly', PolynomialFeatures(degree=1)),
    ('linear', LinearRegression())
])
plot_learning_curves(poly_1, X, y, "Linear (High Bias)")

# Optimal
poly_3 = Pipeline([
    ('poly', PolynomialFeatures(degree=3)),
    ('linear', LinearRegression())
])
plot_learning_curves(poly_3, X, y, "Degree 3 (Optimal)")

# High Variance
poly_15 = Pipeline([
    ('poly', PolynomialFeatures(degree=15)),
    ('linear', LinearRegression())
])
plot_learning_curves(poly_15, X, y, "Degree 15 (High Variance)")
```

---

## Validation Curves

### –©–æ —Ü–µ?

**Validation Curves** –ø–æ–∫–∞–∑—É—é—Ç—å, —è–∫ train —Ç–∞ test error –∑–∞–ª–µ–∂–∞—Ç—å –≤—ñ–¥ **–≥—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞** –º–æ–¥–µ–ª—ñ.

### –ö–æ–¥

```python
from sklearn.model_selection import validation_curve

# –î–ª—è Decision Tree: max_depth
param_range = range(1, 21)

train_scores, test_scores = validation_curve(
    DecisionTreeRegressor(random_state=42),
    X, y,
    param_name='max_depth',
    param_range=param_range,
    cv=5,
    scoring='r2'
)

# –£—Å–µ—Ä–µ–¥–Ω–µ–Ω–Ω—è
train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)
test_mean = np.mean(test_scores, axis=1)
test_std = np.std(test_scores, axis=1)

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
plt.figure(figsize=(10, 6))

plt.plot(param_range, train_mean, 'o-', linewidth=2,
         label='Train Score', color='blue')
plt.fill_between(param_range, train_mean - train_std,
                 train_mean + train_std, alpha=0.1, color='blue')

plt.plot(param_range, test_mean, 's-', linewidth=2,
         label='Validation Score', color='red')
plt.fill_between(param_range, test_mean - test_std,
                 test_mean + test_std, alpha=0.1, color='red')

# –û–ø—Ç–∏–º–∞–ª—å–Ω–µ –∑–Ω–∞—á–µ–Ω–Ω—è
optimal_depth = param_range[np.argmax(test_mean)]
plt.axvline(x=optimal_depth, color='green', linestyle='--',
            linewidth=2, label=f'Optimal (depth={optimal_depth})')

plt.xlabel('max_depth', fontsize=12)
plt.ylabel('R¬≤ Score', fontsize=12)
plt.title('Validation Curve: Decision Tree', fontsize=14, fontweight='bold')
plt.legend(fontsize=11)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

print(f"\nOptimal max_depth: {optimal_depth}")
print(f"Best Validation Score: {test_mean[optimal_depth-1]:.3f}")
```

**–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü—ñ—è:**
```
max_depth=1-3:   High Bias (–æ–±–∏–¥–≤—ñ –∫—Ä–∏–≤—ñ –Ω–∏–∑—å–∫–æ)
max_depth=5-8:   Optimal (–Ω–∞–π–∫—Ä–∞—â–∞ test score)
max_depth=15+:   High Variance (train –≤–∏—Å–æ–∫–æ, test –Ω–∏–∑—å–∫–æ)
```

---

## –î—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∞: Bias vs Variance

### –¢–∞–±–ª–∏—Ü—è –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∏

| –ü–æ–∫–∞–∑–Ω–∏–∫ | High Bias | Optimal | High Variance |
|----------|-----------|---------|---------------|
| **Train Error** | –í–∏—Å–æ–∫–∏–π ‚ùå | –ù–∏–∑—å–∫–∏–π ‚úì | –î—É–∂–µ –Ω–∏–∑—å–∫–∏–π ‚úì |
| **Test Error** | –í–∏—Å–æ–∫–∏–π ‚ùå | –ù–∏–∑—å–∫–∏–π ‚úì | –í–∏—Å–æ–∫–∏–π ‚ùå |
| **Gap (Train - Test)** | –ú–∞–ª–∏–π (~0) | –ú–∞–ª–∏–π (~0-0.05) | –í–µ–ª–∏–∫–∏–π (>0.1) |
| **Learning Curve** | –ü–ª–∞—Ç–æ —Ä–∞–Ω–æ | –ó–±—ñ–∂–Ω—ñ—Å—Ç—å | Gap –Ω–µ –∑–º–µ–Ω—à—É—î—Ç—å—Å—è |
| **–ü—Ä–æ–±–ª–µ–º–∞** | Underfitting | - | Overfitting |
| **–ú–æ–¥–µ–ª—å** | –ó–∞–Ω–∞–¥—Ç–æ –ø—Ä–æ—Å—Ç–∞ | –Ü–¥–µ–∞–ª—å–Ω–∞ | –ó–∞–Ω–∞–¥—Ç–æ —Å–∫–ª–∞–¥–Ω–∞ |

### –®–≤–∏–¥–∫–∞ –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ (–∫–æ–¥)

```python
def diagnose_model(model, X_train, X_test, y_train, y_test):
    """–®–≤–∏–¥–∫–∞ –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ bias vs variance"""
    
    # Scores
    train_score = model.score(X_train, y_train)
    test_score = model.score(X_test, y_test)
    gap = train_score - test_score
    
    print("="*60)
    print("MODEL DIAGNOSIS")
    print("="*60)
    print(f"Train Score: {train_score:.4f}")
    print(f"Test Score:  {test_score:.4f}")
    print(f"Gap:         {gap:.4f}")
    print()
    
    # –î—ñ–∞–≥–Ω–æ–∑
    if test_score < 0.7 and gap < 0.1:
        print("üî¥ HIGH BIAS (Underfitting)")
        print("   Problem: Model is too simple")
        print("   Solutions:")
        print("   ‚Üí Increase model complexity")
        print("   ‚Üí Add more features (polynomial, interactions)")
        print("   ‚Üí Use more complex algorithm")
        print("   ‚Üí Reduce regularization (smaller Œª)")
        
    elif gap > 0.15:
        print("üî¥ HIGH VARIANCE (Overfitting)")
        print("   Problem: Model is too complex")
        print("   Solutions:")
        print("   ‚Üí Add more training data")
        print("   ‚Üí Add regularization (Ridge, Lasso)")
        print("   ‚Üí Reduce model complexity")
        print("   ‚Üí Use ensemble methods (Random Forest)")
        print("   ‚Üí Feature selection (remove irrelevant features)")
        
    elif test_score >= 0.7 and gap <= 0.15:
        print("‚úÖ GOOD BALANCE")
        print("   Model appears to be well-tuned!")
        if gap > 0.05:
            print("   Minor overfitting - consider slight regularization")
    
    else:
        print("‚ö†Ô∏è UNUSUAL PATTERN")
        print("   Check for data leakage or other issues")
    
    print("="*60)
    
    return {
        'train_score': train_score,
        'test_score': test_score,
        'gap': gap
    }

# –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è
from sklearn.linear_model import Ridge
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor

print("\n### Linear Model (likely HIGH BIAS) ###")
lr = LinearRegression()
lr.fit(X_train, y_train)
diagnose_model(lr, X_train, X_test, y_train, y_test)

print("\n### Deep Tree (likely HIGH VARIANCE) ###")
dt_deep = DecisionTreeRegressor(max_depth=20, random_state=42)
dt_deep.fit(X_train, y_train)
diagnose_model(dt_deep, X_train, X_test, y_train, y_test)

print("\n### Random Forest (likely OPTIMAL) ###")
rf = RandomForestRegressor(n_estimators=100, max_depth=5, random_state=42)
rf.fit(X_train, y_train)
diagnose_model(rf, X_train, X_test, y_train, y_test)
```

---

## –°—Ç—Ä–∞—Ç–µ–≥—ñ—ó –≤–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è

### –Ø–∫—â–æ HIGH BIAS (Underfitting)

```python
# ‚ùå –ü—Ä–æ–±–ª–µ–º–∞: –º–æ–¥–µ–ª—å –∑–∞–Ω–∞–¥—Ç–æ –ø—Ä–æ—Å—Ç–∞

# ‚úÖ –†—ñ—à–µ–Ω–Ω—è 1: –ë—ñ–ª—å—à–∞ —Å–∫–ª–∞–¥–Ω—ñ—Å—Ç—å
# –ë—É–ª–æ:
model = DecisionTreeRegressor(max_depth=1)

# –°—Ç–∞–ª–æ:
model = DecisionTreeRegressor(max_depth=10)

# ‚úÖ –†—ñ—à–µ–Ω–Ω—è 2: Polynomial features
from sklearn.preprocessing import PolynomialFeatures
poly = PolynomialFeatures(degree=3)
X_poly = poly.fit_transform(X)

# ‚úÖ –†—ñ—à–µ–Ω–Ω—è 3: –ë—ñ–ª—å—à–µ –æ–∑–Ω–∞–∫
# Feature engineering: —Å—Ç–≤–æ—Ä–∏—Ç–∏ –≤–∑–∞—î–º–æ–¥—ñ—ó, –Ω–µ–ª—ñ–Ω—ñ–π–Ω–æ—Å—Ç—ñ

# ‚úÖ –†—ñ—à–µ–Ω–Ω—è 4: –ó–º–µ–Ω—à–∏—Ç–∏ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—é
# –ë—É–ª–æ:
model = Ridge(alpha=10.0)

# –°—Ç–∞–ª–æ:
model = Ridge(alpha=0.1)

# ‚úÖ –†—ñ—à–µ–Ω–Ω—è 5: –°–∫–ª–∞–¥–Ω—ñ—à–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º
# –ë—É–ª–æ: Linear Regression
# –°—Ç–∞–ª–æ: Random Forest –∞–±–æ Neural Network
```

### –Ø–∫—â–æ HIGH VARIANCE (Overfitting)

```python
# ‚ùå –ü—Ä–æ–±–ª–µ–º–∞: –º–æ–¥–µ–ª—å –∑–∞–Ω–∞–¥—Ç–æ —Å–∫–ª–∞–¥–Ω–∞

# ‚úÖ –†—ñ—à–µ–Ω–Ω—è 1: –ë—ñ–ª—å—à–µ –¥–∞–Ω–∏—Ö
# - –ó–±—ñ–ª—å—à–∏—Ç–∏ train set
# - Data augmentation (–¥–ª—è –∑–æ–±—Ä–∞–∂–µ–Ω—å)
# - Synthetic data generation

# ‚úÖ –†—ñ—à–µ–Ω–Ω—è 2: Regularization
# Ridge (L2)
model = Ridge(alpha=1.0)

# Lasso (L1)
model = Lasso(alpha=0.1)

# ‚úÖ –†—ñ—à–µ–Ω–Ω—è 3: –ó–º–µ–Ω—à–∏—Ç–∏ —Å–∫–ª–∞–¥–Ω—ñ—Å—Ç—å
# –ë—É–ª–æ:
model = DecisionTreeRegressor(max_depth=None)

# –°—Ç–∞–ª–æ:
model = DecisionTreeRegressor(max_depth=5, min_samples_leaf=10)

# ‚úÖ –†—ñ—à–µ–Ω–Ω—è 4: Early Stopping
# –î–ª—è Gradient Boosting
model = GradientBoostingRegressor(
    n_estimators=1000,
    validation_fraction=0.1,
    n_iter_no_change=50
)

# ‚úÖ –†—ñ—à–µ–Ω–Ω—è 5: Dropout (Neural Networks)
# Keras/TensorFlow
model.add(Dropout(0.5))

# ‚úÖ –†—ñ—à–µ–Ω–Ω—è 6: Ensemble –º–µ—Ç–æ–¥–∏
from sklearn.ensemble import RandomForestRegressor
rf = RandomForestRegressor()  # Bagging –∑–º–µ–Ω—à—É—î variance

# ‚úÖ –†—ñ—à–µ–Ω–Ω—è 7: Feature Selection
from sklearn.feature_selection import SelectKBest
selector = SelectKBest(k=10)
X_selected = selector.fit_transform(X, y)
```

---

## –ó–≤'—è–∑–æ–∫ –∑ —ñ–Ω—à–∏–º–∏ –∫–æ–Ω—Ü–µ–ø—Ü—ñ—è–º–∏

### Bias-Variance —ñ –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—è

```python
# Ridge Regression: –∫–æ–Ω—Ç—Ä–æ–ª—å variance —á–µ—Ä–µ–∑ Œª
from sklearn.linear_model import Ridge

lambdas = [0.01, 0.1, 1.0, 10.0, 100.0]
train_scores = []
test_scores = []

for lam in lambdas:
    ridge = Ridge(alpha=lam)
    ridge.fit(X_train_poly, y_train)
    
    train_scores.append(ridge.score(X_train_poly, y_train))
    test_scores.append(ridge.score(X_test_poly, y_test))

plt.figure(figsize=(10, 6))
plt.plot(lambdas, train_scores, 'o-', label='Train', linewidth=2)
plt.plot(lambdas, test_scores, 's-', label='Test', linewidth=2)
plt.xlabel('Œª (Regularization Strength)', fontsize=12)
plt.ylabel('R¬≤ Score', fontsize=12)
plt.title('Regularization Effect on Bias-Variance', 
          fontsize=14, fontweight='bold')
plt.xscale('log')
plt.legend(fontsize=11)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# –°–ø–æ—Å—Ç–µ—Ä–µ–∂–µ–Ω–Ω—è:
# Œª –º–∞–ª–∏–π ‚Üí high variance (overfitting)
# Œª –≤–µ–ª–∏–∫–∏–π ‚Üí high bias (underfitting)
```

### Bias-Variance —ñ Ensemble Methods

**Bagging (Random Forest):**
- ‚úÖ –ó–º–µ–Ω—à—É—î **Variance** (—á–µ—Ä–µ–∑ —É—Å–µ—Ä–µ–¥–Ω–µ–Ω–Ω—è)
- ‚ùå –ú–∞–π–∂–µ –Ω–µ –≤–ø–ª–∏–≤–∞—î –Ω–∞ **Bias**
- **–í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è:** high-variance –º–æ–¥–µ–ª—ñ (deep trees)

**Boosting (Gradient Boosting):**
- ‚úÖ –ó–º–µ–Ω—à—É—î **Bias** (–ø–æ—Å–ª—ñ–¥–æ–≤–Ω–µ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è)
- ‚úÖ –ú–æ–∂–µ –∑–º–µ–Ω—à–∏—Ç–∏ **Variance** (–∑ regularization)
- **–í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è:** high-bias –º–æ–¥–µ–ª—ñ (shallow trees)

```python
# –ë–∞–ª–∞–Ω—Å —á–µ—Ä–µ–∑ –∞–Ω—Å–∞–º–±–ª—ñ
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor

# High Variance ‚Üí Bagging
rf = RandomForestRegressor(n_estimators=100, max_depth=None)
# Variance –∑–º–µ–Ω—à—É—î—Ç—å—Å—è —á–µ—Ä–µ–∑ —É—Å–µ—Ä–µ–¥–Ω–µ–Ω–Ω—è –¥–µ—Ä–µ–≤

# High Bias ‚Üí Boosting
gb = GradientBoostingRegressor(n_estimators=100, max_depth=3)
# Bias –∑–º–µ–Ω—à—É—î—Ç—å—Å—è —á–µ—Ä–µ–∑ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–µ –Ω–∞–≤—á–∞–Ω–Ω—è
```

---

## –ü—Ä–∞–∫—Ç–∏—á–Ω—ñ –ø–æ—Ä–∞–¥–∏ üí°

### 1. –ó–∞–≤–∂–¥–∏ –±—É–¥—É–π Learning Curves

```python
# –í—ñ–∑—É–∞–ª—ñ–∑—É–π –Ω–∞–≤—á–∞–Ω–Ω—è –ø–µ—Ä–µ–¥ production
plot_learning_curves(model, X, y, "My Model")
```

### 2. –†–æ–∑–¥—ñ–ª—è–π –¥–∞–Ω—ñ –ø—Ä–∞–≤–∏–ª—å–Ω–æ

```python
# –û–±–æ–≤'—è–∑–∫–æ–≤–æ: Train / Validation / Test
from sklearn.model_selection import train_test_split

# 60% train, 20% validation, 20% test
X_temp, X_test, y_temp, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)
X_train, X_val, y_train, y_val = train_test_split(
    X_temp, y_temp, test_size=0.25, random_state=42  # 0.25 * 0.8 = 0.2
)
```

### 3. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π Cross-Validation

```python
# –ë—ñ–ª—å—à –Ω–∞–¥—ñ–π–Ω–∞ –æ—Ü—ñ–Ω–∫–∞
from sklearn.model_selection import cross_val_score

scores = cross_val_score(model, X, y, cv=5)
print(f"CV Score: {scores.mean():.3f} (+/- {scores.std()*2:.3f})")
```

### 4. –ü–æ—á–Ω–∏ –ø—Ä–æ—Å—Ç–æ, —É—Å–∫–ª–∞–¥–Ω—é–π –ø–æ—Å—Ç—É–ø–æ–≤–æ

```python
# –ö—Ä–æ–∫ 1: Baseline (–ø—Ä–æ—Å—Ç–∞ –º–æ–¥–µ–ª—å)
baseline = LinearRegression()

# –ö—Ä–æ–∫ 2: –î–æ–¥–∞–π —Å–∫–ª–∞–¥–Ω—ñ—Å—Ç—å
poly_model = Pipeline([
    ('poly', PolynomialFeatures(degree=2)),
    ('linear', LinearRegression())
])

# –ö—Ä–æ–∫ 3: –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—è –ø—Ä–∏ –ø–æ—Ç—Ä–µ–±—ñ
ridge_model = Pipeline([
    ('poly', PolynomialFeatures(degree=2)),
    ('ridge', Ridge(alpha=1.0))
])
```

### 5. –ú–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥ –ø—ñ–¥ —á–∞—Å –Ω–∞–≤—á–∞–Ω–Ω—è

```python
# –î–ª—è –º–æ–¥–µ–ª–µ–π –∑ —ñ—Ç–µ—Ä–∞—Ç–∏–≤–Ω–∏–º –Ω–∞–≤—á–∞–Ω–Ω—è–º
import xgboost as xgb

model = xgb.XGBRegressor(n_estimators=1000)
model.fit(
    X_train, y_train,
    eval_set=[(X_train, y_train), (X_val, y_val)],
    early_stopping_rounds=50,
    verbose=True
)

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
results = model.evals_result()
plt.plot(results['validation_0']['rmse'], label='Train')
plt.plot(results['validation_1']['rmse'], label='Validation')
plt.legend()
plt.show()
```

### 6. –î–æ–∫—É–º–µ–Ω—Ç—É–π –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏

```python
# –ó–±–µ—Ä—ñ–≥–∞–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
experiments = []

for model_name, model in models.items():
    model.fit(X_train, y_train)
    
    experiments.append({
        'model': model_name,
        'train_score': model.score(X_train, y_train),
        'val_score': model.score(X_val, y_val),
        'gap': model.score(X_train, y_train) - model.score(X_val, y_val)
    })

import pandas as pd
df_results = pd.DataFrame(experiments)
print(df_results.sort_values('val_score', ascending=False))
```

### 7. –†–æ–∑—É–º—ñ–π —Å–≤–æ—ó –¥–∞–Ω—ñ

```python
# EDA –ø–µ—Ä–µ–¥ –º–æ–¥–µ–ª—é–≤–∞–Ω–Ω—è–º
import seaborn as sns

# –†–æ–∑–ø–æ–¥—ñ–ª target
sns.histplot(y)

# –ö–æ—Ä–µ–ª—è—Ü—ñ—ó
sns.heatmap(df.corr(), annot=True)

# Scatter plots
sns.pairplot(df)
```

### 8. Feature Engineering –≤—ñ–¥–ø–æ–≤—ñ–¥–∞–ª—å–Ω–æ

```python
# –ù–µ –¥–æ–¥–∞–≤–∞–π features —Å–ª—ñ–ø–æ!
# –ö–æ–∂–Ω–∞ –Ω–æ–≤–∞ –æ–∑–Ω–∞–∫–∞ ‚Üí –±—ñ–ª—å—à–∞ —Å–∫–ª–∞–¥–Ω—ñ—Å—Ç—å ‚Üí —Ä–∏–∑–∏–∫ variance

# ‚úÖ –î–æ–±—Ä–µ: –æ–±“ë—Ä—É–Ω—Ç–æ–≤–∞–Ω—ñ –æ–∑–Ω–∞–∫–∏
df['price_per_sqft'] = df['price'] / df['sqft']

# ‚ùå –ü–æ–≥–∞–Ω–æ: –±–µ–∑–≥–ª—É–∑–¥—ñ –∫–æ–º–±—ñ–Ω–∞—Ü—ñ—ó
df['random_feature'] = df['feature1'] * df['feature2'] * df['feature3']
```

### 9. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π –∞–Ω—Å–∞–º–±–ª—ñ —Ä–æ–∑—É–º–Ω–æ

```python
# Random Forest: –∫–æ–ª–∏ –º–∞—î—à high variance
rf = RandomForestRegressor()

# Gradient Boosting: –∫–æ–ª–∏ –º–∞—î—à high bias
gb = GradientBoostingRegressor()

# Stacking: –∫–æ–ª–∏ –ø–æ—Ç—Ä—ñ–±–Ω–∞ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å
from sklearn.ensemble import StackingRegressor
stack = StackingRegressor(
    estimators=[('rf', rf), ('gb', gb)],
    final_estimator=Ridge()
)
```

### 10. –ù–µ –∑–∞–±—É–≤–∞–π –ø—Ä–æ domain knowledge

```python
# ML metrics –≤–∞–∂–ª–∏–≤—ñ, –∞–ª–µ –Ω–µ –≤—Å–µ!
# –ü–µ—Ä–µ–≤—ñ—Ä—è–π, —á–∏ –º–∞—î —Å–µ–Ω—Å –º–æ–¥–µ–ª—å –∑ —Ç–æ—á–∫–∏ –∑–æ—Ä—É –±—ñ–∑–Ω–µ—Å—É/–Ω–∞—É–∫–∏

# –ü—Ä–∏–∫–ª–∞–¥: —è–∫—â–æ –º–æ–¥–µ–ª—å –∫–∞–∂–µ, —â–æ –≤—ñ–∫ = -5 —Ä–æ–∫—ñ–≤ ‚Üí –ø—Ä–æ–±–ª–µ–º–∞!
```

---

## –ü–æ—à–∏—Ä–µ–Ω—ñ –ø–æ–º–∏–ª–∫–∏ ‚ùå

### 1. –û—Ü—ñ–Ω—é–≤–∞—Ç–∏ —Ç—ñ–ª—å–∫–∏ –Ω–∞ train set

```python
# ‚ùå –ü–û–ì–ê–ù–û
model.fit(X_train, y_train)
print(f"Accuracy: {model.score(X_train, y_train)}")  # –ú–æ–∂–µ –±—É—Ç–∏ overfitting!

# ‚úÖ –î–û–ë–†–ï
print(f"Train: {model.score(X_train, y_train)}")
print(f"Test: {model.score(X_test, y_test)}")
print(f"Gap: {model.score(X_train, y_train) - model.score(X_test, y_test)}")
```

### 2. –ù–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ validation set

```python
# ‚ùå Tuning –Ω–∞ test set ‚Üí data leakage
for alpha in [0.1, 1, 10]:
    model = Ridge(alpha=alpha)
    model.fit(X_train, y_train)
    score = model.score(X_test, y_test)  # ‚ùå –í–∏—Ç—ñ–∫ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó!

# ‚úÖ –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π validation –∞–±–æ cross-validation
from sklearn.model_selection import GridSearchCV
grid = GridSearchCV(Ridge(), {'alpha': [0.1, 1, 10]}, cv=5)
grid.fit(X_train, y_train)
best_model = grid.best_estimator_
final_score = best_model.score(X_test, y_test)  # ‚úÖ –ß–µ—Å–Ω–∞ –æ—Ü—ñ–Ω–∫–∞
```

### 3. –î–æ–¥–∞–≤–∞—Ç–∏ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—é –¥–æ –ø—Ä–æ—Å—Ç–æ—ó –º–æ–¥–µ–ª—ñ

```python
# ‚ùå –Ø–∫—â–æ –≤–∂–µ —î high bias, —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—è –ø–æ–≥—ñ—Ä—à–∏—Ç—å!
# Linear model –Ω–∞ –Ω–µ–ª—ñ–Ω—ñ–π–Ω–∏—Ö –¥–∞–Ω–∏—Ö + Ridge = —â–µ –≥—ñ—Ä—à–µ

# ‚úÖ –°–ø–æ—á–∞—Ç–∫—É –∑–±—ñ–ª—å—à —Å–∫–ª–∞–¥–Ω—ñ—Å—Ç—å, –ø–æ—Ç—ñ–º –¥–æ–¥–∞–π regularization
```

### 4. –Ü–≥–Ω–æ—Ä—É–≤–∞—Ç–∏ —Ä–æ–∑–ø–æ–¥—ñ–ª –¥–∞–Ω–∏—Ö

```python
# ‚ùå –Ø–∫—â–æ train/test –∑ —Ä—ñ–∑–Ω–∏—Ö —Ä–æ–∑–ø–æ–¥—ñ–ª—ñ–≤
# –ú–æ–¥–µ–ª—å –º–æ–∂–µ –∑–¥–∞–≤–∞—Ç–∏—Å—è —Ö–æ—Ä–æ—à–æ—é –Ω–∞ train, –∞–ª–µ –ø–æ–≥–∞–Ω–æ –Ω–∞ test

# ‚úÖ –ü–µ—Ä–µ–∫–æ–Ω–∞–π—Å—è, —â–æ —Ä–æ–∑–ø–æ–¥—ñ–ª–∏ —Å—Ö–æ–∂—ñ
import matplotlib.pyplot as plt
plt.hist(y_train, alpha=0.5, label='Train')
plt.hist(y_test, alpha=0.5, label='Test')
plt.legend()
plt.show()
```

### 5. –ó–∞–Ω–∞–¥—Ç–æ —Ä–∞–Ω–æ –∑—É–ø–∏–Ω—è—Ç–∏—Å—è

```python
# ‚ùå "Train accuracy = 70%, –ø–æ–≥–∞–Ω–æ, –≤—ñ–¥–º–æ–≤–ª—è—é—Å—å –≤—ñ–¥ –º–æ–¥–µ–ª—ñ"
# –ú–æ–∂–ª–∏–≤–æ, —Ü–µ optimal –¥–ª—è —Ü–∏—Ö –¥–∞–Ω–∏—Ö —á–µ—Ä–µ–∑ irreducible error!

# ‚úÖ –ü–æ—Ä—ñ–≤–Ω—è–π –∑ baseline —Ç–∞ —ñ–Ω—à–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏
```

---

## –†–µ–∞–ª—å–Ω–∏–π –ø—Ä–∏–∫–ª–∞–¥: –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è —Ü—ñ–Ω–∏ –±—É–¥–∏–Ω–∫—ñ–≤

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split, learning_curve
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score

# –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö
housing = fetch_california_housing()
X = housing.data
y = housing.target

# –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è: 60% train, 20% val, 20% test
X_temp, X_test, y_temp, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)
X_train, X_val, y_train, y_val = train_test_split(
    X_temp, y_temp, test_size=0.25, random_state=42
)

print("="*70)
print("BIAS-VARIANCE ANALYSIS: California Housing")
print("="*70)
print(f"Train set: {X_train.shape[0]} samples")
print(f"Validation set: {X_val.shape[0]} samples")
print(f"Test set: {X_test.shape[0]} samples")

# –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

# –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
def evaluate_model(name, model, X_tr, X_v, X_te, y_tr, y_v, y_te):
    """–ù–∞–≤—á–∏—Ç–∏ —Ç–∞ –æ—Ü—ñ–Ω–∏—Ç–∏ –º–æ–¥–µ–ª—å"""
    model.fit(X_tr, y_tr)
    
    train_score = model.score(X_tr, y_tr)
    val_score = model.score(X_v, y_v)
    test_score = model.score(X_te, y_te)
    gap = train_score - val_score
    
    # –î—ñ–∞–≥–Ω–æ–∑
    if val_score < 0.6 and gap < 0.1:
        diagnosis = "HIGH BIAS"
        color = "üî¥"
    elif gap > 0.15:
        diagnosis = "HIGH VARIANCE"
        color = "üî¥"
    else:
        diagnosis = "BALANCED"
        color = "‚úÖ"
    
    return {
        'Model': name,
        'Train R¬≤': f"{train_score:.3f}",
        'Val R¬≤': f"{val_score:.3f}",
        'Test R¬≤': f"{test_score:.3f}",
        'Gap': f"{gap:.3f}",
        'Diagnosis': f"{color} {diagnosis}"
    }

# –¢–µ—Å—Ç—É—î–º–æ —Ä—ñ–∑–Ω—ñ –º–æ–¥–µ–ª—ñ
results = []

# 1. Linear Regression (likely high bias –Ω–∞ —Å–∫–ª–∞–¥–Ω–∏—Ö –¥–∞–Ω–∏—Ö)
print("\n1. Linear Regression...")
lr = LinearRegression()
results.append(evaluate_model(
    "Linear Regression", lr,
    X_train_scaled, X_val_scaled, X_test_scaled,
    y_train, y_val, y_test
))

# 2. Polynomial Features degree=2 (–∑–±—ñ–ª—å—à–µ–Ω–Ω—è —Å–∫–ª–∞–¥–Ω–æ—Å—Ç—ñ)
print("2. Polynomial Regression (degree=2)...")
poly_features = PolynomialFeatures(degree=2, include_bias=False)
X_train_poly = poly_features.fit_transform(X_train_scaled)
X_val_poly = poly_features.transform(X_val_scaled)
X_test_poly = poly_features.transform(X_test_scaled)

lr_poly = LinearRegression()
results.append(evaluate_model(
    "Polynomial (deg=2)", lr_poly,
    X_train_poly, X_val_poly, X_test_poly,
    y_train, y_val, y_test
))

# 3. Ridge (polynomial + regularization)
print("3. Ridge Regression...")
ridge = Ridge(alpha=1.0)
results.append(evaluate_model(
    "Ridge (Œ±=1.0)", ridge,
    X_train_poly, X_val_poly, X_test_poly,
    y_train, y_val, y_test
))

# 4. Decision Tree –±–µ–∑ –æ–±–º–µ–∂–µ–Ω—å (likely high variance)
print("4. Deep Decision Tree...")
dt_deep = DecisionTreeRegressor(random_state=42)
results.append(evaluate_model(
    "Deep Tree (no limit)", dt_deep,
    X_train_scaled, X_val_scaled, X_test_scaled,
    y_train, y_val, y_test
))

# 5. Decision Tree –∑ –æ–±–º–µ–∂–µ–Ω–Ω—è–º
print("5. Shallow Decision Tree...")
dt_shallow = DecisionTreeRegressor(max_depth=5, min_samples_leaf=10, random_state=42)
results.append(evaluate_model(
    "Shallow Tree (depth=5)", dt_shallow,
    X_train_scaled, X_val_scaled, X_test_scaled,
    y_train, y_val, y_test
))

# 6. Random Forest (–∑–º–µ–Ω—à—É—î variance)
print("6. Random Forest...")
rf = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)
results.append(evaluate_model(
    "Random Forest", rf,
    X_train_scaled, X_val_scaled, X_test_scaled,
    y_train, y_val, y_test
))

# 7. Gradient Boosting (–∑–º–µ–Ω—à—É—î bias)
print("7. Gradient Boosting...")
gb = GradientBoostingRegressor(
    n_estimators=100, max_depth=3, learning_rate=0.1, random_state=42
)
results.append(evaluate_model(
    "Gradient Boosting", gb,
    X_train_scaled, X_val_scaled, X_test_scaled,
    y_train, y_val, y_test
))

# –†–µ–∑—É–ª—å—Ç–∞—Ç–∏
print("\n" + "="*70)
print("RESULTS SUMMARY")
print("="*70)
df_results = pd.DataFrame(results)
print(df_results.to_string(index=False))

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
fig, axes = plt.subplots(2, 2, figsize=(16, 12))

# 1. Comparison of scores
model_names = df_results['Model'].values
train_scores = [float(x) for x in df_results['Train R¬≤'].values]
val_scores = [float(x) for x in df_results['Val R¬≤'].values]
test_scores = [float(x) for x in df_results['Test R¬≤'].values]

x = np.arange(len(model_names))
width = 0.25

axes[0, 0].bar(x - width, train_scores, width, label='Train', alpha=0.8)
axes[0, 0].bar(x, val_scores, width, label='Validation', alpha=0.8)
axes[0, 0].bar(x + width, test_scores, width, label='Test', alpha=0.8)
axes[0, 0].set_ylabel('R¬≤ Score', fontsize=11)
axes[0, 0].set_title('Model Comparison', fontsize=13, fontweight='bold')
axes[0, 0].set_xticks(x)
axes[0, 0].set_xticklabels(model_names, rotation=45, ha='right')
axes[0, 0].legend(fontsize=10)
axes[0, 0].grid(True, alpha=0.3, axis='y')

# 2. Gap visualization
gaps = [float(x) for x in df_results['Gap'].values]
colors = ['red' if g > 0.15 else 'green' if g < 0.1 else 'orange' for g in gaps]

axes[0, 1].barh(model_names, gaps, color=colors, alpha=0.7)
axes[0, 1].axvline(x=0.1, color='green', linestyle='--', 
                   linewidth=2, label='Good (<0.1)')
axes[0, 1].axvline(x=0.15, color='orange', linestyle='--', 
                   linewidth=2, label='Warning (>0.15)')
axes[0, 1].set_xlabel('Gap (Train - Val)', fontsize=11)
axes[0, 1].set_title('Overfitting Analysis', fontsize=13, fontweight='bold')
axes[0, 1].legend(fontsize=10)
axes[0, 1].grid(True, alpha=0.3, axis='x')

# 3. Learning Curves –¥–ª—è –Ω–∞–π–∫—Ä–∞—â–æ—ó –º–æ–¥–µ–ª—ñ
best_model_idx = np.argmax(val_scores)
best_model_name = model_names[best_model_idx]

print(f"\nGenerating learning curves for best model: {best_model_name}")

# –î–ª—è –ø—Ä–∏–∫–ª–∞–¥—É –≤—ñ–∑—å–º–µ–º–æ Gradient Boosting
train_sizes, train_scores_lc, val_scores_lc = learning_curve(
    GradientBoostingRegressor(n_estimators=100, max_depth=3, random_state=42),
    X_train_scaled, y_train,
    train_sizes=np.linspace(0.1, 1.0, 10),
    cv=5,
    scoring='r2',
    n_jobs=-1
)

train_mean = np.mean(train_scores_lc, axis=1)
train_std = np.std(train_scores_lc, axis=1)
val_mean = np.mean(val_scores_lc, axis=1)
val_std = np.std(val_scores_lc, axis=1)

axes[1, 0].plot(train_sizes, train_mean, 'o-', linewidth=2, label='Train')
axes[1, 0].fill_between(train_sizes, train_mean - train_std,
                        train_mean + train_std, alpha=0.1)
axes[1, 0].plot(train_sizes, val_mean, 's-', linewidth=2, label='Validation')
axes[1, 0].fill_between(train_sizes, val_mean - val_std,
                        val_mean + val_std, alpha=0.1)
axes[1, 0].set_xlabel('Training Set Size', fontsize=11)
axes[1, 0].set_ylabel('R¬≤ Score', fontsize=11)
axes[1, 0].set_title(f'Learning Curves: {best_model_name}', 
                     fontsize=13, fontweight='bold')
axes[1, 0].legend(fontsize=10)
axes[1, 0].grid(True, alpha=0.3)

# 4. Bias-Variance visualization
axes[1, 1].scatter(gaps, val_scores, s=200, alpha=0.6, c=range(len(gaps)), 
                   cmap='viridis')

for i, name in enumerate(model_names):
    axes[1, 1].annotate(name, (gaps[i], val_scores[i]), 
                       fontsize=8, ha='center')

# –ó–æ–Ω–∏
axes[1, 1].axvline(x=0.1, color='green', linestyle='--', alpha=0.5)
axes[1, 1].axhline(y=0.7, color='blue', linestyle='--', alpha=0.5)

axes[1, 1].text(0.05, 0.75, 'Good Balance\n(Low Bias, Low Variance)', 
               fontsize=9, bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.5))
axes[1, 1].text(0.25, 0.65, 'High Variance\n(Overfitting)', 
               fontsize=9, bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.5))
axes[1, 1].text(0.05, 0.55, 'High Bias\n(Underfitting)', 
               fontsize=9, bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.5))

axes[1, 1].set_xlabel('Gap (Train - Val)', fontsize=11)
axes[1, 1].set_ylabel('Validation R¬≤', fontsize=11)
axes[1, 1].set_title('Bias-Variance Map', fontsize=13, fontweight='bold')
axes[1, 1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("\n" + "="*70)
print("RECOMMENDATIONS")
print("="*70)

# –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó
for i, row in df_results.iterrows():
    gap = float(row['Gap'])
    val = float(row['Val R¬≤'])
    
    if 'HIGH BIAS' in row['Diagnosis']:
        print(f"\n{row['Model']}:")
        print("  ‚Üí Increase model complexity")
        print("  ‚Üí Add more features or polynomial terms")
    elif 'HIGH VARIANCE' in row['Diagnosis']:
        print(f"\n{row['Model']}:")
        print("  ‚Üí Add regularization")
        print("  ‚Üí Get more training data")
        print("  ‚Üí Reduce model complexity")
    else:
        print(f"\n{row['Model']}: ‚úÖ Well balanced!")

print("\n" + "="*70)
```

---

## –ü–æ–≤'—è–∑–∞–Ω—ñ —Ç–µ–º–∏

- [[02_Overfitting_Underfitting]] ‚Äî –ø—Ä–∞–∫—Ç–∏—á–Ω—ñ –ø—Ä–æ—è–≤–∏ bias-variance
- [[03_Train_Test_Split]] ‚Äî –ø—Ä–∞–≤–∏–ª—å–Ω–µ —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö
- [[04_Cross_Validation]] ‚Äî –Ω–∞–¥—ñ–π–Ω–∞ –æ—Ü—ñ–Ω–∫–∞ –º–æ–¥–µ–ª—ñ
- [[03_Regularization]] ‚Äî –∫–æ–Ω—Ç—Ä–æ–ª—å variance
- [[02_Random_Forest]] ‚Äî –∑–º–µ–Ω—à–µ–Ω–Ω—è variance —á–µ—Ä–µ–∑ bagging
- [[03_Gradient_Boosting]] ‚Äî –∑–º–µ–Ω—à–µ–Ω–Ω—è bias —á–µ—Ä–µ–∑ boosting

## –†–µ—Å—É—Ä—Å–∏

- [Understanding the Bias-Variance Tradeoff](http://scott.fortmann-roe.com/docs/BiasVariance.html)
- [Elements of Statistical Learning (ESL)](https://hastie.su.domains/ElemStatLearn/)
- [Andrew Ng: Machine Learning Course](https://www.coursera.org/learn/machine-learning)
- [StatQuest: Bias and Variance](https://www.youtube.com/watch?v=EuBBz3bI-aA)

---

## –ö–ª—é—á–æ–≤—ñ –≤–∏—Å–Ω–æ–≤–∫–∏

> Bias-Variance Tradeoff ‚Äî —Ü–µ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–∏–π –∫–æ–º–ø—Ä–æ–º—ñ—Å –≤ ML –º—ñ–∂ –Ω–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ—é —Å–∫–ª–∞–¥–Ω—ñ—Å—Ç—é –º–æ–¥–µ–ª—ñ (high bias) —Ç–∞ –Ω–∞–¥–º—ñ—Ä–Ω–æ—é —á—É—Ç–ª–∏–≤—ñ—Å—Ç—é –¥–æ —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö (high variance).

**–§–æ—Ä–º—É–ª–∞ –ø–æ–º–∏–ª–∫–∏:**
$$\text{Expected Error} = \text{Bias}^2 + \text{Variance} + \text{Irreducible Error}$$

**–ö–ª—é—á–æ–≤—ñ –ø—Ä–∏–Ω—Ü–∏–ø–∏:**
- **High Bias** ‚Üí –º–æ–¥–µ–ª—å –∑–∞–Ω–∞–¥—Ç–æ –ø—Ä–æ—Å—Ç–∞ ‚Üí underfitting
- **High Variance** ‚Üí –º–æ–¥–µ–ª—å –∑–∞–Ω–∞–¥—Ç–æ —Å–∫–ª–∞–¥–Ω–∞ ‚Üí overfitting
- **Optimal** ‚Üí –±–∞–ª–∞–Ω—Å –º—ñ–∂ bias —Ç–∞ variance

**–î—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∞:**
| –ü—Ä–æ–±–ª–µ–º–∞ | Train Error | Test Error | Gap | –†—ñ—à–µ–Ω–Ω—è |
|----------|-------------|------------|-----|---------|
| High Bias | –í–∏—Å–æ–∫–∏–π | –í–∏—Å–æ–∫–∏–π | –ú–∞–ª–∏–π | ‚Üë –°–∫–ª–∞–¥–Ω—ñ—Å—Ç—å |
| High Variance | –ù–∏–∑—å–∫–∏–π | –í–∏—Å–æ–∫–∏–π | –í–µ–ª–∏–∫–∏–π | ‚Üë Regularization / –î–∞–Ω—ñ |
| Optimal | –ù–∏–∑—å–∫–∏–π | –ù–∏–∑—å–∫–∏–π | –ú–∞–ª–∏–π | ‚úÖ |

**–ü—Ä–∞–∫—Ç–∏—á–Ω—ñ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∏:**
- Learning Curves ‚Äî –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –Ω–∞–≤—á–∞–Ω–Ω—è
- Validation Curves ‚Äî –ø—ñ–¥–±—ñ—Ä –≥—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤
- Cross-Validation ‚Äî –Ω–∞–¥—ñ–π–Ω–∞ –æ—Ü—ñ–Ω–∫–∞
- Train/Val/Test split ‚Äî –ø—Ä–∞–≤–∏–ª—å–Ω–∞ –≤–∞–ª—ñ–¥–∞—Ü—ñ—è

**–ù–∞–π–≤–∞–∂–ª–∏–≤—ñ—à–µ:**
- –ó–∞–≤–∂–¥–∏ –ø–æ—Ä—ñ–≤–Ω—é–π train —Ç–∞ test scores
- –ë—É–¥—É–π learning curves
- –ü–æ—á–∏–Ω–∞–π –ø—Ä–æ—Å—Ç–æ, —É—Å–∫–ª–∞–¥–Ω—é–π –ø–æ—Å—Ç—É–ø–æ–≤–æ
- –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π regularization —Ä–æ–∑—É–º–Ω–æ
- –ë–∞–ª–∞–Ω—Å > –Ω–∞–¥–º—ñ—Ä–Ω–∞ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è

---

#ml #core-concepts #bias-variance #tradeoff #underfitting #overfitting #model-complexity #diagnostics
