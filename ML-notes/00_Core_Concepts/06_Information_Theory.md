# Information Theory (–¢–µ–æ—Ä—ñ—è —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó)

## –©–æ —Ü–µ?

**Information Theory** ‚Äî —Ü–µ –º–∞—Ç–µ–º–∞—Ç–∏—á–Ω–∞ —Ç–µ–æ—Ä—ñ—è, —è–∫–∞ **–≤–∏–º—ñ—Ä—é—î –∫—ñ–ª—å–∫—ñ—Å—Ç—å —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó** —Ç–∞ **–Ω–µ–≤–∏–∑–Ω–∞—á–µ–Ω—ñ—Å—Ç—å** —É –¥–∞–Ω–∏—Ö. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –≤ ML, –æ—Å–æ–±–ª–∏–≤–æ –≤ Decision Trees, –¥–ª—è –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è **—è–∫–æ—Å—Ç—ñ —Ä–æ–∑–¥—ñ–ª–µ–Ω—å** —Ç–∞ **–≤–∏–±–æ—Ä—É –æ–∑–Ω–∞–∫**.

**–ì–æ–ª–æ–≤–Ω–∞ —ñ–¥–µ—è:** —è–∫—â–æ –ø–æ–¥—ñ—è –¥—É–∂–µ –ø–µ—Ä–µ–¥–±–∞—á—É–≤–∞–Ω–∞ (–º–∞–ª–æ –Ω–µ–≤–∏–∑–Ω–∞—á–µ–Ω–æ—Å—Ç—ñ) ‚Üí –º–∞–ª–æ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó. –Ø–∫—â–æ –ø–æ–¥—ñ—è –Ω–µ—Å–ø–æ–¥—ñ–≤–∞–Ω–∞ (–≤–∏—Å–æ–∫–∞ –Ω–µ–≤–∏–∑–Ω–∞—á–µ–Ω—ñ—Å—Ç—å) ‚Üí –±–∞–≥–∞—Ç–æ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó.

## –ù–∞–≤—ñ—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ?

- üå≥ **Decision Trees** ‚Äî –≤–∏–±—ñ—Ä –Ω–∞–π–∫—Ä–∞—â–æ–≥–æ split
- üéØ **Feature Selection** ‚Äî —è–∫—ñ –æ–∑–Ω–∞–∫–∏ –Ω–∞–π—ñ–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ñ—à—ñ
- üìä **–í–∏–º—ñ—Ä —á–∏—Å—Ç–æ—Ç–∏** ‚Äî –Ω–∞—Å–∫—ñ–ª—å–∫–∏ –æ–¥–Ω–æ—Ä—ñ–¥–Ω–∏–π –Ω–∞–±—ñ—Ä –¥–∞–Ω–∏—Ö
- üîç **–û—Ü—ñ–Ω–∫–∞ —è–∫–æ—Å—Ç—ñ —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è** ‚Äî —á–∏ –¥–æ–±—Ä–µ —Ä–æ–∑–¥—ñ–ª–∏–ª–∏ –¥–∞–Ω—ñ
- üí° **Compression** ‚Äî —Å—Ç–∏—Å–Ω–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö
- üé≤ **–ï–Ω—Ç—Ä–æ–ø—ñ—è** ‚Äî –º—ñ—Ä–∞ –Ω–µ–≤–∏–∑–Ω–∞—á–µ–Ω–æ—Å—Ç—ñ

## –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏?

**–ü–æ—Ç—Ä—ñ–±–Ω–æ:**
- –ü–æ–±—É–¥–æ–≤–∞ Decision Trees (ID3, C4.5, CART)
- Feature importance –≤ Random Forest
- Feature selection
- –í–∏–º—ñ—Ä —è–∫–æ—Å—Ç—ñ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—ó

**–ù–µ –ø–æ—Ç—Ä—ñ–±–Ω–æ:**
- Linear models (–≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—Ç—å —ñ–Ω—à—ñ –∫—Ä–∏—Ç–µ—Ä—ñ—ó)
- Neural Networks (–≥—Ä–∞–¥—ñ—î–Ω—Ç–Ω—ñ –º–µ—Ç–æ–¥–∏)

---

## –ö–ª—é—á–æ–≤—ñ –∫–æ–Ω—Ü–µ–ø—Ü—ñ—ó

```
Information Theory –¥–ª—è ML
‚îÇ
‚îú‚îÄ‚îÄ Entropy (–ï–Ω—Ç—Ä–æ–ø—ñ—è)
‚îÇ   ‚îî‚îÄ‚îÄ –ú—ñ—Ä–∞ –Ω–µ–≤–∏–∑–Ω–∞—á–µ–Ω–æ—Å—Ç—ñ/–±–µ–∑–ª–∞–¥—É
‚îÇ
‚îú‚îÄ‚îÄ Information Gain (–ü—Ä–∏—Ä—ñ—Å—Ç —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó)
‚îÇ   ‚îî‚îÄ‚îÄ –°–∫—ñ–ª—å–∫–∏ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó –¥–∞—î —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è
‚îÇ
‚îú‚îÄ‚îÄ Gini Impurity (–Ü–Ω–¥–µ–∫—Å –î–∂–∏–Ω—ñ)
‚îÇ   ‚îî‚îÄ‚îÄ –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ –µ–Ω—Ç—Ä–æ–ø—ñ—ó
‚îÇ
‚îî‚îÄ‚îÄ Gain Ratio
    ‚îî‚îÄ‚îÄ –ù–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω–∏–π Information Gain
```

---

# 1. Entropy (–ï–Ω—Ç—Ä–æ–ø—ñ—è)

## –©–æ —Ü–µ?

**Entropy** ‚Äî —Ü–µ **–º—ñ—Ä–∞ –Ω–µ–≤–∏–∑–Ω–∞—á–µ–Ω–æ—Å—Ç—ñ** –∞–±–æ **–±–µ–∑–ª–∞–¥—É** –≤ –Ω–∞–±–æ—Ä—ñ –¥–∞–Ω–∏—Ö. –ß–∏–º –≤–∏—â–µ entropy, —Ç–∏–º –±—ñ–ª—å—à–µ —Ö–∞–æ—Å—É.

### –§–æ—Ä–º—É–ª–∞ (Shannon Entropy)

$$H(S) = -\sum_{i=1}^{c} p_i \log_2(p_i)$$

–¥–µ:
- $H(S)$ ‚Äî –µ–Ω—Ç—Ä–æ–ø—ñ—è –º–Ω–æ–∂–∏–Ω–∏ $S$
- $c$ ‚Äî –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∫–ª–∞—Å—ñ–≤
- $p_i$ ‚Äî –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å –∫–ª–∞—Å—É $i$
- $\log_2$ ‚Äî –ª–æ–≥–∞—Ä–∏—Ñ–º –ø–æ –æ—Å–Ω–æ–≤—ñ 2 (–±—ñ—Ç–∏ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó)

### –Ü–Ω—Ç—É—ó—Ü—ñ—è

```
–ü—Ä–∏–∫–ª–∞–¥: –ú–æ–Ω–µ—Ç–∞

–ß–µ—Å–Ω–∞ –º–æ–Ω–µ—Ç–∞ (50/50):
H = -(0.5 * log‚ÇÇ(0.5) + 0.5 * log‚ÇÇ(0.5))
  = -(0.5 * (-1) + 0.5 * (-1))
  = -(-0.5 - 0.5)
  = 1.0 –±—ñ—Ç

–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –Ω–µ–≤–∏–∑–Ω–∞—á–µ–Ω—ñ—Å—Ç—å! –ù–µ –∑–Ω–∞—î–º–æ, —â–æ –≤–∏–ø–∞–¥–µ.

–ù–µ—á–µ—Å–Ω–∞ –º–æ–Ω–µ—Ç–∞ (100% –æ—Ä–µ–ª):
H = -(1.0 * log‚ÇÇ(1.0) + 0.0 * log‚ÇÇ(0.0))
  = -(0 + 0)  [–≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ lim x‚Üí0 (x log x) = 0]
  = 0.0 –±—ñ—Ç

–ñ–æ–¥–Ω–æ—ó –Ω–µ–≤–∏–∑–Ω–∞—á–µ–Ω–æ—Å—Ç—ñ! –ó–∞–≤–∂–¥–∏ –æ—Ä–µ–ª.
```

### –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è Entropy

```python
import numpy as np
import matplotlib.pyplot as plt

def entropy(p):
    """–ë—ñ–Ω–∞—Ä–Ω–∞ –µ–Ω—Ç—Ä–æ–ø—ñ—è"""
    if p == 0 or p == 1:
        return 0
    return -(p * np.log2(p) + (1-p) * np.log2(1-p))

# –ô–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ
p_values = np.linspace(0.001, 0.999, 100)
entropy_values = [entropy(p) for p in p_values]

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
plt.figure(figsize=(10, 6))
plt.plot(p_values, entropy_values, linewidth=3, color='blue')
plt.xlabel('p (–ô–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å –∫–ª–∞—Å—É 1)', fontsize=12)
plt.ylabel('Entropy H(p)', fontsize=12)
plt.title('Binary Entropy Function', fontsize=14, fontweight='bold')
plt.grid(True, alpha=0.3)
plt.axhline(y=1.0, color='red', linestyle='--', alpha=0.5, label='Max entropy = 1.0')
plt.axvline(x=0.5, color='green', linestyle='--', alpha=0.5, label='p = 0.5')
plt.legend(fontsize=11)

# –ê–Ω–æ—Ç–∞—Ü—ñ—ó
plt.annotate('–ú–∞–∫—Å–∏–º—É–º\nH = 1.0\n(p = 0.5)', 
            xy=(0.5, 1.0), xytext=(0.3, 0.85),
            fontsize=10, ha='center',
            bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.7),
            arrowprops=dict(arrowstyle='->', color='green', lw=2))

plt.annotate('–ß–∏—Å—Ç–∞ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è\nH = 0\n(p = 0 –∞–±–æ p = 1)', 
            xy=(0.1, 0.05), xytext=(0.25, 0.3),
            fontsize=10, ha='center',
            bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.7),
            arrowprops=dict(arrowstyle='->', color='red', lw=2))

plt.tight_layout()
plt.show()
```

**–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü—ñ—è –≥—Ä–∞—Ñ—ñ–∫–∞:**
- –ú–∞–∫—Å–∏–º—É–º –ø—Ä–∏ $p = 0.5$ (50/50) ‚Äî –Ω–∞–π–±—ñ–ª—å—à–∞ –Ω–µ–≤–∏–∑–Ω–∞—á–µ–Ω—ñ—Å—Ç—å
- –ú—ñ–Ω—ñ–º—É–º –ø—Ä–∏ $p = 0$ –∞–±–æ $p = 1$ ‚Äî –ø–æ–≤–Ω–∞ –≤–∏–∑–Ω–∞—á–µ–Ω—ñ—Å—Ç—å
- –°–∏–º–µ—Ç—Ä–∏—á–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è

### –ü—Ä–∏–∫–ª–∞–¥ –æ–±—á–∏—Å–ª–µ–Ω–Ω—è

```python
import numpy as np

def calculate_entropy(labels):
    """
    –û–±—á–∏—Å–ª–∏—Ç–∏ –µ–Ω—Ç—Ä–æ–ø—ñ—é –¥–ª—è —Å–ø–∏—Å–∫—É –º—ñ—Ç–æ–∫ –∫–ª–∞—Å—ñ–≤
    
    Parameters:
    -----------
    labels : array-like
        –ú—ñ—Ç–∫–∏ –∫–ª–∞—Å—ñ–≤
        
    Returns:
    --------
    float : –µ–Ω—Ç—Ä–æ–ø—ñ—è (–≤ –±—ñ—Ç–∞—Ö)
    """
    # –£–Ω—ñ–∫–∞–ª—å–Ω—ñ –∫–ª–∞—Å–∏ —Ç–∞ —ó—Ö —á–∞—Å—Ç–æ—Ç–∏
    unique, counts = np.unique(labels, return_counts=True)
    
    # –ô–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ
    probabilities = counts / len(labels)
    
    # –ï–Ω—Ç—Ä–æ–ø—ñ—è
    entropy = 0
    for p in probabilities:
        if p > 0:  # –£–Ω–∏–∫–∞—î–º–æ log(0)
            entropy -= p * np.log2(p)
    
    return entropy

# –ü—Ä–∏–∫–ª–∞–¥ 1: –Ü–¥–µ–∞–ª—å–Ω–æ –∑–±–∞–ª–∞–Ω—Å–æ–≤–∞–Ω—ñ –∫–ª–∞—Å–∏ (50/50)
labels_balanced = ['A'] * 50 + ['B'] * 50
H_balanced = calculate_entropy(labels_balanced)
print(f"–ü—Ä–∏–∫–ª–∞–¥ 1 (50/50):")
print(f"  Labels: 50 A, 50 B")
print(f"  Entropy: {H_balanced:.4f} –±—ñ—Ç")

# –ü—Ä–∏–∫–ª–∞–¥ 2: –ù–µ—Å–±–∞–ª–∞–Ω—Å–æ–≤–∞–Ω—ñ –∫–ª–∞—Å–∏ (80/20)
labels_imbalanced = ['A'] * 80 + ['B'] * 20
H_imbalanced = calculate_entropy(labels_imbalanced)
print(f"\n–ü—Ä–∏–∫–ª–∞–¥ 2 (80/20):")
print(f"  Labels: 80 A, 20 B")
print(f"  Entropy: {H_imbalanced:.4f} –±—ñ—Ç")

# –ü—Ä–∏–∫–ª–∞–¥ 3: –ß–∏—Å—Ç–∏–π –Ω–∞–±—ñ—Ä (100/0)
labels_pure = ['A'] * 100
H_pure = calculate_entropy(labels_pure)
print(f"\n–ü—Ä–∏–∫–ª–∞–¥ 3 (100/0):")
print(f"  Labels: 100 A, 0 B")
print(f"  Entropy: {H_pure:.4f} –±—ñ—Ç")

# –ü—Ä–∏–∫–ª–∞–¥ 4: –¢—Ä–∏ –∫–ª–∞—Å–∏ (40/30/30)
labels_multiclass = ['A'] * 40 + ['B'] * 30 + ['C'] * 30
H_multiclass = calculate_entropy(labels_multiclass)
print(f"\n–ü—Ä–∏–∫–ª–∞–¥ 4 (3 –∫–ª–∞—Å–∏: 40/30/30):")
print(f"  Labels: 40 A, 30 B, 30 C")
print(f"  Entropy: {H_multiclass:.4f} –±—ñ—Ç")
```

**–í–∏–≤—ñ–¥:**
```
–ü—Ä–∏–∫–ª–∞–¥ 1 (50/50):
  Labels: 50 A, 50 B
  Entropy: 1.0000 –±—ñ—Ç  ‚Üê –ú–∞–∫—Å–∏–º—É–º –¥–ª—è 2 –∫–ª–∞—Å—ñ–≤

–ü—Ä–∏–∫–ª–∞–¥ 2 (80/20):
  Labels: 80 A, 20 B
  Entropy: 0.7219 –±—ñ—Ç  ‚Üê –ú–µ–Ω—à–µ –Ω–µ–≤–∏–∑–Ω–∞—á–µ–Ω–æ—Å—Ç—ñ

–ü—Ä–∏–∫–ª–∞–¥ 3 (100/0):
  Labels: 100 A, 0 B
  Entropy: 0.0000 –±—ñ—Ç  ‚Üê –ñ–æ–¥–Ω–æ—ó –Ω–µ–≤–∏–∑–Ω–∞—á–µ–Ω–æ—Å—Ç—ñ

–ü—Ä–∏–∫–ª–∞–¥ 4 (3 –∫–ª–∞—Å–∏: 40/30/30):
  Labels: 40 A, 30 B, 30 C
  Entropy: 1.5710 –±—ñ—Ç  ‚Üê –ë—ñ–ª—å—à–µ –¥–ª—è 3 –∫–ª–∞—Å—ñ–≤
```

### –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –µ–Ω—Ç—Ä–æ–ø—ñ—è

–î–ª—è $c$ –∫–ª–∞—Å—ñ–≤, –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –µ–Ω—Ç—Ä–æ–ø—ñ—è:

$$H_{\max} = \log_2(c)$$

```
2 –∫–ª–∞—Å–∏:   H_max = log‚ÇÇ(2) = 1.0 –±—ñ—Ç
3 –∫–ª–∞—Å–∏:   H_max = log‚ÇÇ(3) ‚âà 1.585 –±—ñ—Ç
4 –∫–ª–∞—Å–∏:   H_max = log‚ÇÇ(4) = 2.0 –±—ñ—Ç
10 –∫–ª–∞—Å—ñ–≤: H_max = log‚ÇÇ(10) ‚âà 3.322 –±—ñ—Ç
```

### –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è —Ä–æ–∑–ø–æ–¥—ñ–ª—ñ–≤

```python
import matplotlib.pyplot as plt
import numpy as np

# –†—ñ–∑–Ω—ñ —Ä–æ–∑–ø–æ–¥—ñ–ª–∏
distributions = [
    {'name': 'Pure (100/0)', 'values': [100, 0], 'entropy': 0.0},
    {'name': 'Very skewed (90/10)', 'values': [90, 10], 'entropy': 0.469},
    {'name': 'Skewed (80/20)', 'values': [80, 20], 'entropy': 0.722},
    {'name': 'Imbalanced (70/30)', 'values': [70, 30], 'entropy': 0.881},
    {'name': 'Balanced (50/50)', 'values': [50, 50], 'entropy': 1.0}
]

fig, axes = plt.subplots(1, 5, figsize=(18, 4))

for idx, dist in enumerate(distributions):
    axes[idx].bar(['Class A', 'Class B'], dist['values'], 
                  color=['blue', 'orange'], alpha=0.7)
    axes[idx].set_title(f"{dist['name']}\nH = {dist['entropy']:.3f}", 
                       fontsize=11, fontweight='bold')
    axes[idx].set_ylim([0, 105])
    axes[idx].set_ylabel('Count' if idx == 0 else '')
    axes[idx].grid(True, alpha=0.3, axis='y')

plt.suptitle('Entropy for Different Class Distributions', 
            fontsize=14, fontweight='bold', y=1.02)
plt.tight_layout()
plt.show()
```

---

## 2. Information Gain (–ü—Ä–∏—Ä—ñ—Å—Ç —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó)

### –©–æ —Ü–µ?

**Information Gain** ‚Äî —Ü–µ **–∑–º–µ–Ω—à–µ–Ω–Ω—è –µ–Ω—Ç—Ä–æ–ø—ñ—ó** –ø—ñ—Å–ª—è —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö –∑–∞ –ø–µ–≤–Ω–æ—é –æ–∑–Ω–∞–∫–æ—é. –ü–æ–∫–∞–∑—É—î, **—Å–∫—ñ–ª—å–∫–∏ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó** –º–∏ –æ—Ç—Ä–∏–º–∞–ª–∏ –≤—ñ–¥ —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è.

### –§–æ—Ä–º—É–ª–∞

$$\text{IG}(S, A) = H(S) - \sum_{v \in \text{Values}(A)} \frac{|S_v|}{|S|} H(S_v)$$

–¥–µ:
- $H(S)$ ‚Äî –µ–Ω—Ç—Ä–æ–ø—ñ—è –¥–æ —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è
- $A$ ‚Äî –æ–∑–Ω–∞–∫–∞, –∑–∞ —è–∫–æ—é —Ä–æ–∑–¥—ñ–ª—è—î–º–æ
- $S_v$ ‚Äî –ø—ñ–¥–º–Ω–æ–∂–∏–Ω–∞ –¥–∞–Ω–∏—Ö, –¥–µ $A = v$
- $\frac{|S_v|}{|S|}$ ‚Äî —á–∞—Å—Ç–∫–∞ –∑—Ä–∞–∑–∫—ñ–≤ —É –ø—ñ–¥–º–Ω–æ–∂–∏–Ω—ñ

**–Ü–Ω—Ç—É—ó—Ü—ñ—è:**
```
IG = Entropy(–¥–æ) - Weighted_Average(Entropy(–ø—ñ—Å–ª—è))

IG > 0 ‚Üí –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –∑–º–µ–Ω—à–∏–ª–æ –Ω–µ–≤–∏–∑–Ω–∞—á–µ–Ω—ñ—Å—Ç—å ‚úì
IG = 0 ‚Üí –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –Ω–µ –¥–∞–ª–æ –Ω—ñ—á–æ–≥–æ
IG < 0 ‚Üí –ù–µ–º–æ–∂–ª–∏–≤–æ (–µ–Ω—Ç—Ä–æ–ø—ñ—è –Ω–µ –º–æ–∂–µ –∑—Ä–æ—Å—Ç–∏)
```

### –ü—Ä–∏–∫–ª–∞–¥: Play Tennis

```
Dataset: –ì—Ä–∞—Ç–∏ –≤ —Ç–µ–Ω—ñ—Å —á–∏ –Ω—ñ?

Outlook | Temperature | Humidity | Windy | Play?
--------|-------------|----------|-------|------
Sunny   | Hot         | High     | False | No
Sunny   | Hot         | High     | True  | No
Overcast| Hot         | High     | False | Yes
Rain    | Mild        | High     | False | Yes
Rain    | Cool        | Normal   | False | Yes
Rain    | Cool        | Normal   | True  | No
Overcast| Cool        | Normal   | True  | Yes
Sunny   | Mild        | High     | False | No
Sunny   | Cool        | Normal   | False | Yes
Rain    | Mild        | Normal   | False | Yes
Sunny   | Mild        | Normal   | True  | Yes
Overcast| Mild        | High     | True  | Yes
Overcast| Hot         | Normal   | False | Yes
Rain    | Mild        | High     | True  | No

–í—Å—å–æ–≥–æ: 14 –∑—Ä–∞–∑–∫—ñ–≤
Yes: 9 (–≥—Ä–∞—Ç–∏)
No: 5 (–Ω–µ –≥—Ä–∞—Ç–∏)
```

### –ö—Ä–æ–∫ 1: –ï–Ω—Ç—Ä–æ–ø—ñ—è –¥–æ —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è

```python
import numpy as np

# –î–∞–Ω—ñ
total = 14
yes = 9
no = 5

# –ô–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ
p_yes = yes / total
p_no = no / total

# –ï–Ω—Ç—Ä–æ–ø—ñ—è
H_before = -(p_yes * np.log2(p_yes) + p_no * np.log2(p_no))

print(f"–î–æ —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è:")
print(f"  Yes: {yes}, No: {no}")
print(f"  p(Yes): {p_yes:.3f}, p(No): {p_no:.3f}")
print(f"  H(S) = {H_before:.4f} –±—ñ—Ç")

# –í–∏–≤—ñ–¥:
# –î–æ —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è:
#   Yes: 9, No: 5
#   p(Yes): 0.643, p(No): 0.357
#   H(S) = 0.9403 –±—ñ—Ç
```

### –ö—Ä–æ–∫ 2: –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –∑–∞ Outlook

```python
# –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –∑–∞ Outlook
outlook_splits = {
    'Sunny': {'yes': 2, 'no': 3},      # 5 –∑—Ä–∞–∑–∫—ñ–≤
    'Overcast': {'yes': 4, 'no': 0},   # 4 –∑—Ä–∞–∑–∫–∏
    'Rain': {'yes': 3, 'no': 2}        # 5 –∑—Ä–∞–∑–∫—ñ–≤
}

def calculate_split_entropy(splits, total):
    """–û–±—á–∏—Å–ª–∏—Ç–∏ –∑–≤–∞–∂–µ–Ω—É –µ–Ω—Ç—Ä–æ–ø—ñ—é –ø—ñ—Å–ª—è —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è"""
    weighted_entropy = 0
    
    for value, counts in splits.items():
        n = counts['yes'] + counts['no']
        weight = n / total
        
        if n == 0:
            continue
        
        p_yes = counts['yes'] / n
        p_no = counts['no'] / n
        
        # –ï–Ω—Ç—Ä–æ–ø—ñ—è —Ü—ñ—î—ó –ø—ñ–¥–º–Ω–æ–∂–∏–Ω–∏
        H = 0
        if p_yes > 0:
            H -= p_yes * np.log2(p_yes)
        if p_no > 0:
            H -= p_no * np.log2(p_no)
        
        weighted_entropy += weight * H
        
        print(f"  {value:10s}: {counts['yes']} Yes, {counts['no']} No ‚Üí "
              f"H = {H:.4f}, weight = {weight:.3f}")
    
    return weighted_entropy

print(f"\n–†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –∑–∞ Outlook:")
H_after_outlook = calculate_split_entropy(outlook_splits, total)
print(f"  Weighted H(S|Outlook) = {H_after_outlook:.4f}")

# Information Gain
IG_outlook = H_before - H_after_outlook
print(f"  IG(S, Outlook) = {H_before:.4f} - {H_after_outlook:.4f} = {IG_outlook:.4f}")

# –í–∏–≤—ñ–¥:
# –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –∑–∞ Outlook:
#   Sunny     : 2 Yes, 3 No ‚Üí H = 0.9710, weight = 0.357
#   Overcast  : 4 Yes, 0 No ‚Üí H = 0.0000, weight = 0.286
#   Rain      : 3 Yes, 2 No ‚Üí H = 0.9710, weight = 0.357
#   Weighted H(S|Outlook) = 0.6935
#   IG(S, Outlook) = 0.9403 - 0.6935 = 0.2467
```

### –ö—Ä–æ–∫ 3: –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –∑ —ñ–Ω—à–∏–º–∏ –æ–∑–Ω–∞–∫–∞–º–∏

```python
# –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –∑–∞ Temperature
temp_splits = {
    'Hot': {'yes': 2, 'no': 2},
    'Mild': {'yes': 4, 'no': 2},
    'Cool': {'yes': 3, 'no': 1}
}

print(f"\n–†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –∑–∞ Temperature:")
H_after_temp = calculate_split_entropy(temp_splits, total)
IG_temp = H_before - H_after_temp
print(f"  IG(S, Temperature) = {IG_temp:.4f}")

# –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –∑–∞ Humidity
humidity_splits = {
    'High': {'yes': 3, 'no': 4},
    'Normal': {'yes': 6, 'no': 1}
}

print(f"\n–†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –∑–∞ Humidity:")
H_after_humidity = calculate_split_entropy(humidity_splits, total)
IG_humidity = H_before - H_after_humidity
print(f"  IG(S, Humidity) = {IG_humidity:.4f}")

# –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –∑–∞ Windy
windy_splits = {
    'False': {'yes': 6, 'no': 2},
    'True': {'yes': 3, 'no': 3}
}

print(f"\n–†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –∑–∞ Windy:")
H_after_windy = calculate_split_entropy(windy_splits, total)
IG_windy = H_before - H_after_windy
print(f"  IG(S, Windy) = {IG_windy:.4f}")

# –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è
print(f"\n{'='*50}")
print("–ü–û–†–Ü–í–ù–Ø–ù–ù–Ø INFORMATION GAIN:")
print(f"{'='*50}")
gains = {
    'Outlook': IG_outlook,
    'Humidity': IG_humidity,
    'Windy': IG_windy,
    'Temperature': IG_temp
}

for feature, gain in sorted(gains.items(), key=lambda x: x[1], reverse=True):
    print(f"  {feature:15s}: {gain:.4f}")

print(f"\n‚úÖ –ù–∞–π–∫—Ä–∞—â–∞ –æ–∑–Ω–∞–∫–∞ –¥–ª—è —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è: Outlook (IG = {IG_outlook:.4f})")
```

### –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è Information Gain

```python
import matplotlib.pyplot as plt

features = list(gains.keys())
ig_values = list(gains.values())

plt.figure(figsize=(10, 6))
bars = plt.bar(features, ig_values, color=['green', 'orange', 'blue', 'purple'], alpha=0.7)

# –í–∏–¥—ñ–ª–∏—Ç–∏ –Ω–∞–π–∫—Ä–∞—â—É
max_idx = np.argmax(ig_values)
bars[max_idx].set_color('red')
bars[max_idx].set_alpha(1.0)

plt.ylabel('Information Gain', fontsize=12)
plt.title('Information Gain for Each Feature', fontsize=14, fontweight='bold')
plt.grid(True, alpha=0.3, axis='y')

# –ê–Ω–æ—Ç–∞—Ü—ñ—ó
for i, (feature, value) in enumerate(zip(features, ig_values)):
    plt.text(i, value + 0.01, f'{value:.4f}', 
            ha='center', va='bottom', fontsize=11, fontweight='bold')

plt.axhline(y=0, color='black', linestyle='-', linewidth=0.5)
plt.tight_layout()
plt.show()
```

### Decision Tree: –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è IG

```python
from sklearn.tree import DecisionTreeClassifier, plot_tree
import pandas as pd

# –°—Ç–≤–æ—Ä–∏—Ç–∏ DataFrame
data = {
    'Outlook': ['Sunny', 'Sunny', 'Overcast', 'Rain', 'Rain', 'Rain', 
                'Overcast', 'Sunny', 'Sunny', 'Rain', 'Sunny', 
                'Overcast', 'Overcast', 'Rain'],
    'Temperature': ['Hot', 'Hot', 'Hot', 'Mild', 'Cool', 'Cool', 'Cool',
                   'Mild', 'Cool', 'Mild', 'Mild', 'Mild', 'Hot', 'Mild'],
    'Humidity': ['High', 'High', 'High', 'High', 'Normal', 'Normal', 'Normal',
                'High', 'Normal', 'Normal', 'Normal', 'High', 'Normal', 'High'],
    'Windy': ['False', 'True', 'False', 'False', 'False', 'True', 'True',
             'False', 'False', 'False', 'True', 'True', 'False', 'True'],
    'Play': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes',
            'Yes', 'Yes', 'Yes', 'Yes', 'No']
}

df = pd.DataFrame(data)

# Encode categorical variables
from sklearn.preprocessing import LabelEncoder

le_dict = {}
for col in df.columns:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    le_dict[col] = le

X = df.drop('Play', axis=1)
y = df['Play']

# Decision Tree –∑ criterion='entropy' (Information Gain)
dt_entropy = DecisionTreeClassifier(criterion='entropy', random_state=42)
dt_entropy.fit(X, y)

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –¥–µ—Ä–µ–≤–∞
plt.figure(figsize=(20, 10))
plot_tree(dt_entropy, 
         feature_names=['Outlook', 'Temperature', 'Humidity', 'Windy'],
         class_names=['No', 'Yes'],
         filled=True,
         rounded=True,
         fontsize=10)
plt.title('Decision Tree (criterion=entropy, Information Gain)', 
         fontsize=16, fontweight='bold')
plt.tight_layout()
plt.show()

# Feature importances
importances = dt_entropy.feature_importances_
feature_names = ['Outlook', 'Temperature', 'Humidity', 'Windy']

print("\nFeature Importances (based on Information Gain):")
for name, importance in zip(feature_names, importances):
    print(f"  {name:15s}: {importance:.4f}")
```

---

## 3. Gini Impurity (–Ü–Ω–¥–µ–∫—Å –î–∂–∏–Ω—ñ)

### –©–æ —Ü–µ?

**Gini Impurity** ‚Äî —Ü–µ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∞ –º–µ—Ç—Ä–∏–∫–∞ **—á–∏—Å—Ç–æ—Ç–∏** –Ω–∞–±–æ—Ä—É –¥–∞–Ω–∏—Ö. –í–∏–º—ñ—Ä—é—î –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å **–Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ—ó –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó** –≤–∏–ø–∞–¥–∫–æ–≤–æ–≥–æ –∑—Ä–∞–∑–∫–∞.

### –§–æ—Ä–º—É–ª–∞

$$\text{Gini}(S) = 1 - \sum_{i=1}^{c} p_i^2$$

–¥–µ:
- $p_i$ ‚Äî –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å –∫–ª–∞—Å—É $i$
- $c$ ‚Äî –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∫–ª–∞—Å—ñ–≤

### –Ü–Ω—Ç—É—ó—Ü—ñ—è

```
Gini Impurity = –ô–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å –ø–æ–º–∏–ª–∫–æ–≤–æ—ó –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó

–ü—Ä–∏–∫–ª–∞–¥: –ú–æ–Ω–µ—Ç–∞ (50/50)

Gini = 1 - (0.5¬≤ + 0.5¬≤)
     = 1 - (0.25 + 0.25)
     = 1 - 0.5
     = 0.5

–Ø–∫—â–æ –≤–∏–ø–∞–¥–∫–æ–≤–æ –≤–∑—è—Ç–∏ –∑—Ä–∞–∑–æ–∫ —ñ –≤–∏–ø–∞–¥–∫–æ–≤–æ –ø—Ä–∏—Å–≤–æ—ó—Ç–∏ –º—ñ—Ç–∫—É,
–π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å –ø–æ–º–∏–ª–∫–∏ = 0.5 (50%)

–ß–∏—Å—Ç–∏–π –Ω–∞–±—ñ—Ä (100/0):
Gini = 1 - (1.0¬≤ + 0.0¬≤)
     = 1 - 1.0
     = 0.0

–ô–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å –ø–æ–º–∏–ª–∫–∏ = 0 (–∂–æ–¥–Ω–∏—Ö –ø–æ–º–∏–ª–æ–∫)
```

### –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è Entropy vs Gini

```python
import numpy as np
import matplotlib.pyplot as plt

def gini_impurity(p):
    """–ë—ñ–Ω–∞—Ä–Ω–∞ Gini impurity"""
    return 1 - (p**2 + (1-p)**2)

def entropy(p):
    """–ë—ñ–Ω–∞—Ä–Ω–∞ –µ–Ω—Ç—Ä–æ–ø—ñ—è"""
    if p == 0 or p == 1:
        return 0
    return -(p * np.log2(p) + (1-p) * np.log2(1-p))

# –ô–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ
p_values = np.linspace(0.001, 0.999, 100)
gini_values = [gini_impurity(p) for p in p_values]
entropy_values = [entropy(p) for p in p_values]

# Normalize entropy to [0, 1] –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è
entropy_normalized = [e / max(entropy_values) for e in entropy_values]

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
plt.figure(figsize=(12, 6))

plt.plot(p_values, gini_values, linewidth=3, label='Gini Impurity', color='blue')
plt.plot(p_values, entropy_normalized, linewidth=3, label='Entropy (normalized)', 
        color='red', linestyle='--')

plt.xlabel('p (–ô–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å –∫–ª–∞—Å—É 1)', fontsize=12)
plt.ylabel('Impurity', fontsize=12)
plt.title('Gini Impurity vs Entropy', fontsize=14, fontweight='bold')
plt.legend(fontsize=12)
plt.grid(True, alpha=0.3)

# –ê–Ω–æ—Ç–∞—Ü—ñ—ó
plt.axvline(x=0.5, color='green', linestyle=':', alpha=0.5, linewidth=2)
plt.text(0.5, 0.05, 'p = 0.5\n(–º–∞–∫—Å–∏–º—É–º)', ha='center', fontsize=10,
        bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.7))

plt.tight_layout()
plt.show()

# –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –∑–Ω–∞—á–µ–Ω—å
print("–ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è Entropy vs Gini:")
print("="*50)
for p in [0.0, 0.1, 0.25, 0.5, 0.75, 0.9, 1.0]:
    e = entropy(p)
    g = gini_impurity(p)
    print(f"p = {p:.2f}: Entropy = {e:.4f}, Gini = {g:.4f}")
```

### –û–±—á–∏—Å–ª–µ–Ω–Ω—è Gini

```python
def calculate_gini(labels):
    """
    –û–±—á–∏—Å–ª–∏—Ç–∏ Gini impurity –¥–ª—è —Å–ø–∏—Å–∫—É –º—ñ—Ç–æ–∫
    
    Parameters:
    -----------
    labels : array-like
        –ú—ñ—Ç–∫–∏ –∫–ª–∞—Å—ñ–≤
        
    Returns:
    --------
    float : Gini impurity
    """
    # –£–Ω—ñ–∫–∞–ª—å–Ω—ñ –∫–ª–∞—Å–∏ —Ç–∞ —ó—Ö —á–∞—Å—Ç–æ—Ç–∏
    unique, counts = np.unique(labels, return_counts=True)
    
    # –ô–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ
    probabilities = counts / len(labels)
    
    # Gini impurity
    gini = 1 - np.sum(probabilities ** 2)
    
    return gini

# –ü—Ä–∏–∫–ª–∞–¥–∏
labels_50_50 = ['A'] * 50 + ['B'] * 50
labels_80_20 = ['A'] * 80 + ['B'] * 20
labels_pure = ['A'] * 100

print("Gini Impurity:")
print(f"  50/50:  {calculate_gini(labels_50_50):.4f}")
print(f"  80/20:  {calculate_gini(labels_80_20):.4f}")
print(f"  100/0:  {calculate_gini(labels_pure):.4f}")

# –í–∏–≤—ñ–¥:
# Gini Impurity:
#   50/50:  0.5000  ‚Üê –ú–∞–∫—Å–∏–º—É–º –¥–ª—è 2 –∫–ª–∞—Å—ñ–≤
#   80/20:  0.3200
#   100/0:  0.0000  ‚Üê –ß–∏—Å—Ç–∏–π –Ω–∞–±—ñ—Ä
```

### Gini Gain (–∞–Ω–∞–ª–æ–≥ Information Gain)

```python
def calculate_gini_gain(labels_before, splits):
    """
    –û–±—á–∏—Å–ª–∏—Ç–∏ Gini Gain –ø—ñ—Å–ª—è —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è
    
    Parameters:
    -----------
    labels_before : array-like
        –ú—ñ—Ç–∫–∏ –¥–æ —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è
    splits : dict
        –°–ª–æ–≤–Ω–∏–∫ {–∑–Ω–∞—á–µ–Ω–Ω—è: —Å–ø–∏—Å–æ–∫_–º—ñ—Ç–æ–∫}
        
    Returns:
    --------
    float : Gini gain
    """
    # Gini –¥–æ —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è
    gini_before = calculate_gini(labels_before)
    
    # –ó–≤–∞–∂–µ–Ω–∞ Gini –ø—ñ—Å–ª—è —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è
    n_total = len(labels_before)
    weighted_gini_after = 0
    
    for value, labels_subset in splits.items():
        weight = len(labels_subset) / n_total
        gini_subset = calculate_gini(labels_subset)
        weighted_gini_after += weight * gini_subset
    
    # Gini Gain
    gini_gain = gini_before - weighted_gini_after
    
    return gini_gain, gini_before, weighted_gini_after

# –ü—Ä–∏–∫–ª–∞–¥: Tennis dataset, —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –∑–∞ Outlook
labels_all = ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 
             'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No']

outlook_splits = {
    'Sunny': ['No', 'No', 'No', 'Yes', 'Yes'],
    'Overcast': ['Yes', 'Yes', 'Yes', 'Yes'],
    'Rain': ['Yes', 'Yes', 'No', 'Yes', 'No']
}

gini_gain, gini_before, gini_after = calculate_gini_gain(labels_all, outlook_splits)

print(f"Gini Gain –¥–ª—è Outlook:")
print(f"  Gini –¥–æ:    {gini_before:.4f}")
print(f"  Gini –ø—ñ—Å–ª—è: {gini_after:.4f}")
print(f"  Gini Gain:  {gini_gain:.4f}")
```

### Decision Tree –∑ Gini

```python
from sklearn.tree import DecisionTreeClassifier

# –¢–æ–π —Å–∞–º–∏–π Tennis dataset
# dt_gini –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î Gini impurity (default)
dt_gini = DecisionTreeClassifier(criterion='gini', random_state=42)
dt_gini.fit(X, y)

# –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è Entropy vs Gini
dt_entropy = DecisionTreeClassifier(criterion='entropy', random_state=42)
dt_entropy.fit(X, y)

print("Feature Importances:")
print("="*50)
print(f"{'Feature':<15} {'Gini':>10} {'Entropy':>10}")
print("="*50)

for i, name in enumerate(['Outlook', 'Temperature', 'Humidity', 'Windy']):
    print(f"{name:<15} {dt_gini.feature_importances_[i]:>10.4f} "
          f"{dt_entropy.feature_importances_[i]:>10.4f}")
```

---

## 4. Gain Ratio (–ù–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω–∏–π Information Gain)

### –ü—Ä–æ–±–ª–µ–º–∞ Information Gain

**–ü—Ä–æ–±–ª–µ–º–∞:** Information Gain **—Å—Ö–∏–ª—å–Ω–∏–π –¥–æ –æ–∑–Ω–∞–∫ –∑ –±–∞–≥–∞—Ç—å–º–∞ –∑–Ω–∞—á–µ–Ω–Ω—è–º–∏**.

```python
# –ü—Ä–∏–∫–ª–∞–¥: –û–∑–Ω–∞–∫–∞ "ID" (—É–Ω—ñ–∫–∞–ª—å–Ω–∞ –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –∑—Ä–∞–∑–∫–∞)

# Dataset: 14 –∑—Ä–∞–∑–∫—ñ–≤, 9 Yes, 5 No
# ID: 1, 2, 3, ..., 14 (—É–Ω—ñ–∫–∞–ª—å–Ω—ñ)

# –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –∑–∞ ID:
# –ö–æ–∂–µ–Ω split –º–∞—î 1 –∑—Ä–∞–∑–æ–∫ ‚Üí Entropy = 0 –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ

# IG(S, ID) = H(S) - 0 = H(S) = –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∏–π!
# –ê–ª–µ ID –Ω–µ –º–∞—î –ø—Ä–µ–¥–∏–∫—Ç–∏–≤–Ω–æ—ó —Å–∏–ª–∏! ‚ùå
```

### –†—ñ—à–µ–Ω–Ω—è: Gain Ratio

$$\text{GainRatio}(S, A) = \frac{\text{IG}(S, A)}{\text{SplitInfo}(S, A)}$$

–¥–µ:

$$\text{SplitInfo}(S, A) = -\sum_{v \in \text{Values}(A)} \frac{|S_v|}{|S|} \log_2 \frac{|S_v|}{|S|}$$

**SplitInfo** ‚Äî —Ü–µ –µ–Ω—Ç—Ä–æ–ø—ñ—è —Ä–æ–∑–ø–æ–¥—ñ–ª—É –∑—Ä–∞–∑–∫—ñ–≤ –ø–æ –∑–Ω–∞—á–µ–Ω–Ω—è—Ö –æ–∑–Ω–∞–∫–∏ (–±–µ–∑ –≤—Ä–∞—Ö—É–≤–∞–Ω–Ω—è –∫–ª–∞—Å—ñ–≤).

### –Ü–Ω—Ç—É—ó—Ü—ñ—è

```
SplitInfo –∫–∞—Ä–∞—î –æ–∑–Ω–∞–∫–∏ –∑ –±–∞–≥–∞—Ç—å–º–∞ –∑–Ω–∞—á–µ–Ω–Ω—è–º–∏:

–û–∑–Ω–∞–∫–∞ –∑ 2 –∑–Ω–∞—á–µ–Ω–Ω—è–º–∏ (50/50):
  SplitInfo = -(0.5 log‚ÇÇ(0.5) + 0.5 log‚ÇÇ(0.5)) = 1.0

–û–∑–Ω–∞–∫–∞ –∑ 14 –∑–Ω–∞—á–µ–Ω–Ω—è–º–∏ (–ø–æ 1 –∑—Ä–∞–∑–∫—É):
  SplitInfo = -14 * (1/14 log‚ÇÇ(1/14)) ‚âà 3.807

Gain Ratio = IG / SplitInfo
–î–ª—è ID: GainRatio –±—É–¥–µ –∑–Ω–∞—á–Ω–æ –º–µ–Ω—à–µ!
```

### –û–±—á–∏—Å–ª–µ–Ω–Ω—è

```python
def calculate_gain_ratio(labels_all, splits):
    """
    –û–±—á–∏—Å–ª–∏—Ç–∏ Gain Ratio
    
    Parameters:
    -----------
    labels_all : array-like
        –í—Å—ñ –º—ñ—Ç–∫–∏
    splits : dict
        –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è {–∑–Ω–∞—á–µ–Ω–Ω—è: —Å–ø–∏—Å–æ–∫_–º—ñ—Ç–æ–∫}
        
    Returns:
    --------
    float : Gain Ratio
    """
    # Information Gain
    H_before = calculate_entropy(labels_all)
    
    n_total = len(labels_all)
    weighted_H_after = 0
    
    # SplitInfo
    split_info = 0
    
    for value, labels_subset in splits.items():
        n_subset = len(labels_subset)
        weight = n_subset / n_total
        
        # –î–ª—è IG
        H_subset = calculate_entropy(labels_subset)
        weighted_H_after += weight * H_subset
        
        # –î–ª—è SplitInfo
        if weight > 0:
            split_info -= weight * np.log2(weight)
    
    IG = H_before - weighted_H_after
    
    # Gain Ratio
    if split_info == 0:
        return 0
    
    gain_ratio = IG / split_info
    
    return gain_ratio, IG, split_info

# –ü—Ä–∏–∫–ª–∞–¥ 1: Outlook (3 –∑–Ω–∞—á–µ–Ω–Ω—è)
outlook_splits = {
    'Sunny': ['No', 'No', 'No', 'Yes', 'Yes'],
    'Overcast': ['Yes', 'Yes', 'Yes', 'Yes'],
    'Rain': ['Yes', 'Yes', 'No', 'Yes', 'No']
}

labels_all = ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 
             'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No']

gr_outlook, ig_outlook, si_outlook = calculate_gain_ratio(labels_all, outlook_splits)

print("Outlook:")
print(f"  IG:         {ig_outlook:.4f}")
print(f"  SplitInfo:  {si_outlook:.4f}")
print(f"  GainRatio:  {gr_outlook:.4f}")

# –ü—Ä–∏–∫–ª–∞–¥ 2: ID (14 —É–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω—å)
id_splits = {f'ID_{i}': [labels_all[i]] for i in range(len(labels_all))}

gr_id, ig_id, si_id = calculate_gain_ratio(labels_all, id_splits)

print("\nID (14 unique values):")
print(f"  IG:         {ig_id:.4f}")  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∏–π!
print(f"  SplitInfo:  {si_id:.4f}")  # –î—É–∂–µ –≤–µ–ª–∏–∫–∏–π
print(f"  GainRatio:  {gr_id:.4f}")  # –ù–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω–∏–π

# –í–∏–≤—ñ–¥:
# Outlook:
#   IG:         0.2467
#   SplitInfo:  1.5774
#   GainRatio:  0.1564

# ID:
#   IG:         0.9403  ‚Üê –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∏–π!
#   SplitInfo:  3.8074  ‚Üê –î—É–∂–µ –≤–µ–ª–∏–∫–∏–π
#   GainRatio:  0.2469  ‚Üê –ú–µ–Ω—à–µ –∑–∞ Outlook!
```

---

## –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –º–µ—Ç—Ä–∏–∫

### –¢–∞–±–ª–∏—Ü—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è

| –ú–µ—Ç—Ä–∏–∫–∞ | –§–æ—Ä–º—É–ª–∞ | Range | –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ |
|---------|---------|-------|---------------------|
| **Entropy** | $-\sum p_i \log_2 p_i$ | [0, $\log_2(c)$] | ID3, C4.5 algorithms |
| **Gini** | $1 - \sum p_i^2$ | [0, $1-\frac{1}{c}$] | CART algorithm (sklearn default) |
| **Information Gain** | $H(S) - H(S\|A)$ | [0, $H(S)$] | Feature selection |
| **Gain Ratio** | $\frac{IG}{SplitInfo}$ | [0, 1] | C4.5 (—É–Ω–∏–∫–∞—î bias –¥–æ –±–∞–≥–∞—Ç—å–æ—Ö –∑–Ω–∞—á–µ–Ω—å) |

### –í—ñ–∑—É–∞–ª—å–Ω–µ –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è

```python
import matplotlib.pyplot as plt
import numpy as np

# –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –¥–∞–Ω–∏—Ö
p_values = np.linspace(0.001, 0.999, 100)

entropy_values = [entropy(p) for p in p_values]
gini_values = [gini_impurity(p) for p in p_values]

# Normalize –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è
entropy_norm = np.array(entropy_values) / max(entropy_values)
gini_norm = np.array(gini_values) / max(gini_values)

# –†—ñ–∑–Ω–∏—Ü—è
diff = entropy_norm - gini_norm

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
fig, axes = plt.subplots(1, 2, figsize=(15, 5))

# Subplot 1: –û–±–∏–¥–≤—ñ –∫—Ä–∏–≤—ñ
axes[0].plot(p_values, entropy_norm, linewidth=2, label='Entropy (normalized)', color='blue')
axes[0].plot(p_values, gini_norm, linewidth=2, label='Gini (normalized)', color='red')
axes[0].set_xlabel('p', fontsize=12)
axes[0].set_ylabel('Impurity (normalized)', fontsize=12)
axes[0].set_title('Entropy vs Gini Impurity', fontsize=14, fontweight='bold')
axes[0].legend(fontsize=11)
axes[0].grid(True, alpha=0.3)

# Subplot 2: –†—ñ–∑–Ω–∏—Ü—è
axes[1].plot(p_values, diff, linewidth=2, color='green')
axes[1].axhline(y=0, color='black', linestyle='--', linewidth=1)
axes[1].fill_between(p_values, 0, diff, where=(diff > 0), alpha=0.3, color='green', label='Entropy > Gini')
axes[1].fill_between(p_values, 0, diff, where=(diff < 0), alpha=0.3, color='red', label='Gini > Entropy')
axes[1].set_xlabel('p', fontsize=12)
axes[1].set_ylabel('Difference (Entropy - Gini)', fontsize=12)
axes[1].set_title('Difference between Metrics', fontsize=14, fontweight='bold')
axes[1].legend(fontsize=11)
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# –°–ø–æ—Å—Ç–µ—Ä–µ–∂–µ–Ω–Ω—è
print("–°–ø–æ—Å—Ç–µ—Ä–µ–∂–µ–Ω–Ω—è:")
print("1. –û–±–∏–¥–≤—ñ –º–µ—Ç—Ä–∏–∫–∏ –º–∞—é—Ç—å –º–∞–∫—Å–∏–º—É–º –ø—Ä–∏ p=0.5")
print("2. –û–±–∏–¥–≤—ñ –º—ñ–Ω—ñ–º–∞–ª—å–Ω—ñ (0) –ø—Ä–∏ p=0 –∞–±–æ p=1")
print("3. Entropy —Ç—Ä–æ—Ö–∏ –±—ñ–ª—å—à–∞ –∑–∞ Gini –≤ —Å–µ—Ä–µ–¥–Ω—å–æ–º—É –¥—ñ–∞–ø–∞–∑–æ–Ω—ñ")
print("4. –ù–∞ –ø—Ä–∞–∫—Ç–∏—Ü—ñ –æ–±–∏–¥–≤—ñ –¥–∞—é—Ç—å —Å—Ö–æ–∂—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏")
```

### –®–≤–∏–¥–∫—ñ—Å—Ç—å –æ–±—á–∏—Å–ª–µ–Ω–Ω—è

```python
import time

# –í–µ–ª–∏–∫–∏–π dataset
np.random.seed(42)
n = 100000
labels = np.random.choice(['A', 'B', 'C'], size=n)

# Benchmark Entropy
start = time.time()
for _ in range(100):
    H = calculate_entropy(labels)
time_entropy = time.time() - start

# Benchmark Gini
start = time.time()
for _ in range(100):
    G = calculate_gini(labels)
time_gini = time.time() - start

print("–ß–∞—Å –æ–±—á–∏—Å–ª–µ–Ω–Ω—è (100 —ñ—Ç–µ—Ä–∞—Ü—ñ–π):")
print(f"  Entropy: {time_entropy:.4f} —Å–µ–∫")
print(f"  Gini:    {time_gini:.4f} —Å–µ–∫")
print(f"\nGini —à–≤–∏–¥—à–∞ –Ω–∞ {(time_entropy/time_gini - 1)*100:.1f}%")

# –ó–∞–∑–≤–∏—á–∞–π Gini —Ç—Ä–æ—Ö–∏ —à–≤–∏–¥—à–∞ (–±–µ–∑ –ª–æ–≥–∞—Ä–∏—Ñ–º—ñ–≤)
```

---

## –ü—Ä–∞–∫—Ç–∏—á–Ω–µ –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è

### 1. –ü–æ–±—É–¥–æ–≤–∞ Decision Tree –≤—Ä—É—á–Ω—É

```python
import numpy as np
import pandas as pd

class SimpleDecisionTree:
    """–°–ø—Ä–æ—â–µ–Ω–µ –¥–µ—Ä–µ–≤–æ —Ä—ñ—à–µ–Ω—å –∑ Information Gain"""
    
    def __init__(self, max_depth=3):
        self.max_depth = max_depth
        self.tree = None
    
    def entropy(self, labels):
        """–û–±—á–∏—Å–ª–∏—Ç–∏ –µ–Ω—Ç—Ä–æ–ø—ñ—é"""
        unique, counts = np.unique(labels, return_counts=True)
        probabilities = counts / len(labels)
        return -np.sum([p * np.log2(p) for p in probabilities if p > 0])
    
    def information_gain(self, X, y, feature):
        """–û–±—á–∏—Å–ª–∏—Ç–∏ Information Gain –¥–ª—è –æ–∑–Ω–∞–∫–∏"""
        # –ï–Ω—Ç—Ä–æ–ø—ñ—è –¥–æ —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è
        H_before = self.entropy(y)
        
        # –£–Ω—ñ–∫–∞–ª—å–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è –æ–∑–Ω–∞–∫–∏
        values = np.unique(X[:, feature])
        
        # –ó–≤–∞–∂–µ–Ω–∞ –µ–Ω—Ç—Ä–æ–ø—ñ—è –ø—ñ—Å–ª—è —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è
        weighted_H_after = 0
        for value in values:
            mask = X[:, feature] == value
            subset_y = y[mask]
            weight = len(subset_y) / len(y)
            weighted_H_after += weight * self.entropy(subset_y)
        
        # Information Gain
        return H_before - weighted_H_after
    
    def find_best_feature(self, X, y):
        """–ó–Ω–∞–π—Ç–∏ –Ω–∞–π–∫—Ä–∞—â—É –æ–∑–Ω–∞–∫—É –¥–ª—è —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è"""
        n_features = X.shape[1]
        
        gains = []
        for feature in range(n_features):
            gain = self.information_gain(X, y, feature)
            gains.append(gain)
        
        return np.argmax(gains), max(gains)
    
    def build_tree(self, X, y, depth=0):
        """–†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –ø–æ–±—É–¥—É–≤–∞—Ç–∏ –¥–µ—Ä–µ–≤–æ"""
        # Base cases
        if depth >= self.max_depth or len(np.unique(y)) == 1:
            # Leaf node: –ø–æ–≤–µ—Ä–Ω—É—Ç–∏ –Ω–∞–π—á–∞—Å—Ç—ñ—à–∏–π –∫–ª–∞—Å
            unique, counts = np.unique(y, return_counts=True)
            return unique[np.argmax(counts)]
        
        # –ó–Ω–∞–π—Ç–∏ –Ω–∞–π–∫—Ä–∞—â—É –æ–∑–Ω–∞–∫—É
        best_feature, best_gain = self.find_best_feature(X, y)
        
        if best_gain == 0:
            # –ù–µ–º–∞—î –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è
            unique, counts = np.unique(y, return_counts=True)
            return unique[np.argmax(counts)]
        
        # –°—Ç–≤–æ—Ä–∏—Ç–∏ –≤—É–∑–æ–ª
        tree = {
            'feature': best_feature,
            'gain': best_gain,
            'children': {}
        }
        
        # –†–æ–∑–¥—ñ–ª–∏—Ç–∏ —Ç–∞ —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ –ø–æ–±—É–¥—É–≤–∞—Ç–∏ –ø—ñ–¥–¥–µ—Ä–µ–≤–∞
        values = np.unique(X[:, best_feature])
        for value in values:
            mask = X[:, best_feature] == value
            subset_X = X[mask]
            subset_y = y[mask]
            
            tree['children'][value] = self.build_tree(subset_X, subset_y, depth + 1)
        
        return tree
    
    def fit(self, X, y):
        """–ù–∞–≤—á–∏—Ç–∏ –¥–µ—Ä–µ–≤–æ"""
        self.tree = self.build_tree(X, y)
        return self
    
    def predict_single(self, x, tree):
        """–ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –¥–ª—è –æ–¥–Ω–æ–≥–æ –∑—Ä–∞–∑–∫–∞"""
        if not isinstance(tree, dict):
            # Leaf node
            return tree
        
        feature_value = x[tree['feature']]
        
        if feature_value in tree['children']:
            return self.predict_single(x, tree['children'][feature_value])
        else:
            # –ó–Ω–∞—á–µ–Ω–Ω—è –Ω–µ –±–∞—á–∏–ª–∏ –ø—ñ–¥ —á–∞—Å –Ω–∞–≤—á–∞–Ω–Ω—è
            # –ü–æ–≤–µ—Ä–Ω—É—Ç–∏ –Ω–∞–π—á–∞—Å—Ç—ñ—à–∏–π –∫–ª–∞—Å —Å–µ—Ä–µ–¥ children
            leaves = []
            for child in tree['children'].values():
                if not isinstance(child, dict):
                    leaves.append(child)
            return max(set(leaves), key=leaves.count) if leaves else 0
    
    def predict(self, X):
        """–ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –¥–ª—è –±–∞–≥–∞—Ç—å–æ—Ö –∑—Ä–∞–∑–∫—ñ–≤"""
        return np.array([self.predict_single(x, self.tree) for x in X])

# –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# –î–∞–Ω—ñ
iris = load_iris()
X_train, X_test, y_train, y_test = train_test_split(
    iris.data, iris.target, test_size=0.3, random_state=42
)

# –î–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü—ñ—è (–¥–ª—è —Å–ø—Ä–æ—â–µ–Ω–Ω—è)
# –†–æ–∑–±–∏—Ç–∏ –Ω–∞ 3 bins
for i in range(X_train.shape[1]):
    X_train[:, i] = np.digitize(X_train[:, i], 
                                bins=np.percentile(X_train[:, i], [33, 66]))
    X_test[:, i] = np.digitize(X_test[:, i], 
                               bins=np.percentile(X_train[:, i], [33, 66]))

# –ù–∞–≤—á–∏—Ç–∏ –¥–µ—Ä–µ–≤–æ
tree = SimpleDecisionTree(max_depth=3)
tree.fit(X_train, y_train)

# –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
y_pred = tree.predict(X_test)

# –û—Ü—ñ–Ω–∫–∞
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.4f}")

# –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–µ—Ä–µ–≤–∞
print("\nTree structure:")
print(tree.tree)
```

### 2. Feature Selection –∑ Information Gain

```python
from sklearn.datasets import load_breast_cancer
import pandas as pd

# –î–∞–Ω—ñ
cancer = load_breast_cancer()
X = pd.DataFrame(cancer.data, columns=cancer.feature_names)
y = cancer.target

# –û–±—á–∏—Å–ª–∏—Ç–∏ IG –¥–ª—è –≤—Å—ñ—Ö –æ–∑–Ω–∞–∫
def calculate_feature_ig(X, y, n_bins=5):
    """–û–±—á–∏—Å–ª–∏—Ç–∏ Information Gain –¥–ª—è –≤—Å—ñ—Ö –æ–∑–Ω–∞–∫"""
    igs = {}
    
    for feature in X.columns:
        # –î–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü—ñ—è continuous –æ–∑–Ω–∞–∫–∏
        X_binned = pd.cut(X[feature], bins=n_bins, labels=False)
        
        # Information Gain
        H_before = calculate_entropy(y)
        
        weighted_H_after = 0
        for bin_value in range(n_bins):
            mask = X_binned == bin_value
            if mask.sum() == 0:
                continue
            
            subset_y = y[mask]
            weight = len(subset_y) / len(y)
            weighted_H_after += weight * calculate_entropy(subset_y)
        
        ig = H_before - weighted_H_after
        igs[feature] = ig
    
    return igs

# –û–±—á–∏—Å–ª–∏—Ç–∏ IG
igs = calculate_feature_ig(X, y)

# –í—ñ–¥—Å–æ—Ä—Ç—É–≤–∞—Ç–∏
igs_sorted = sorted(igs.items(), key=lambda x: x[1], reverse=True)

# –¢–æ–ø-10
print("Top 10 Features by Information Gain:")
print("="*60)
for i, (feature, ig) in enumerate(igs_sorted[:10], 1):
    print(f"{i:2d}. {feature:30s}: {ig:.6f}")

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
import matplotlib.pyplot as plt

features = [f for f, _ in igs_sorted[:15]]
values = [ig for _, ig in igs_sorted[:15]]

plt.figure(figsize=(12, 6))
plt.barh(features, values, color='steelblue', alpha=0.7)
plt.xlabel('Information Gain', fontsize=12)
plt.title('Top 15 Features by Information Gain', fontsize=14, fontweight='bold')
plt.gca().invert_yaxis()
plt.grid(True, alpha=0.3, axis='x')
plt.tight_layout()
plt.show()
```

---

## –ü–æ—à–∏—Ä–µ–Ω—ñ –ø–æ–º–∏–ª–∫–∏ ‚ùå

### 1. –ó–∞–±—É–≤–∞—Ç–∏ –ø—Ä–æ log(0)

```python
# ‚ùå –ü–û–ì–ê–ù–û
def entropy_bad(labels):
    unique, counts = np.unique(labels, return_counts=True)
    probabilities = counts / len(labels)
    return -np.sum(probabilities * np.log2(probabilities))  # log(0) = -inf!

# ‚úÖ –î–û–ë–†–ï
def entropy_good(labels):
    unique, counts = np.unique(labels, return_counts=True)
    probabilities = counts / len(labels)
    return -np.sum([p * np.log2(p) for p in probabilities if p > 0])
```

### 2. –ü–ª—É—Ç–∞—Ç–∏ Entropy –∑ Gini

```python
# Entropy: -Œ£ p log(p)
# Gini:    1 - Œ£ p¬≤

# –ù–ï –ø–ª—É—Ç–∞—Ç–∏!
```

### 3. –ù–µ –Ω–æ—Ä–º–∞–ª—ñ–∑—É–≤–∞—Ç–∏ Gain Ratio

```python
# Information Gain —Å—Ö–∏–ª—å–Ω–∏–π –¥–æ –±–∞–≥–∞—Ç—å–æ—Ö –∑–Ω–∞—á–µ–Ω—å
# –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π Gain Ratio –¥–ª—è –æ–∑–Ω–∞–∫ –∑ –≤–µ–ª–∏–∫–æ—é –∫—ñ–ª—å–∫—ñ—Å—Ç—é —É–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω—å
```

---

## –ü—Ä–∞–∫—Ç–∏—á–Ω—ñ –ø–æ—Ä–∞–¥–∏ üí°

### 1. Entropy vs Gini: —â–æ –≤–∏–±—Ä–∞—Ç–∏?

```python
# –ù–∞ –ø—Ä–∞–∫—Ç–∏—Ü—ñ –æ–±–∏–¥–≤—ñ –¥–∞—é—Ç—å —Å—Ö–æ–∂—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏

# Entropy:
# ‚úÖ –¢–µ–æ—Ä–µ—Ç–∏—á–Ω–æ –±—ñ–ª—å—à –æ–±“ë—Ä—É–Ω—Ç–æ–≤–∞–Ω–∞
# ‚ùå –ü–æ–≤—ñ–ª—å–Ω—ñ—à–∞ (–ª–æ–≥–∞—Ä–∏—Ñ–º–∏)

# Gini:
# ‚úÖ –®–≤–∏–¥—à–∞ (–±–µ–∑ –ª–æ–≥–∞—Ä–∏—Ñ–º—ñ–≤)
# ‚úÖ Default —É sklearn
# ‚úÖ –ü—Ä–æ—Å—Ç–∞ —ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü—ñ—è

# –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—è: –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π Gini (sklearn default)
```

### 2. Information Gain –¥–ª—è Feature Selection

```python
from sklearn.feature_selection import mutual_info_classif

# mutual_info_classif = Information Gain –¥–ª—è continuous features
mi_scores = mutual_info_classif(X, y, random_state=42)

# –¢–æ–ø –æ–∑–Ω–∞–∫–∏
top_features = np.argsort(mi_scores)[::-1][:10]
print("Top 10 features by Mutual Information:")
for idx in top_features:
    print(f"  {X.columns[idx]}: {mi_scores[idx]:.4f}")
```

### 3. –í—ñ–∑—É–∞–ª—ñ–∑—É–π —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è

```python
import matplotlib.pyplot as plt

def visualize_split(X, y, feature, threshold):
    """–í—ñ–∑—É–∞–ª—ñ–∑—É–≤–∞—Ç–∏ —è–∫—ñ—Å—Ç—å —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è"""
    
    # –†–æ–∑–¥—ñ–ª–∏—Ç–∏
    left_mask = X[:, feature] <= threshold
    right_mask = ~left_mask
    
    # –û–±—á–∏—Å–ª–∏—Ç–∏ –º–µ—Ç—Ä–∏–∫–∏
    H_before = calculate_entropy(y)
    H_left = calculate_entropy(y[left_mask])
    H_right = calculate_entropy(y[right_mask])
    
    weight_left = left_mask.sum() / len(y)
    weight_right = right_mask.sum() / len(y)
    
    H_after = weight_left * H_left + weight_right * H_right
    IG = H_before - H_after
    
    # –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
    fig, axes = plt.subplots(1, 3, figsize=(15, 4))
    
    # Subplot 1: –†–æ–∑–ø–æ–¥—ñ–ª –¥–æ
    axes[0].hist(y, bins=np.unique(y), alpha=0.7, color='gray')
    axes[0].set_title(f'Before Split\nEntropy = {H_before:.4f}', fontweight='bold')
    axes[0].set_xlabel('Class')
    axes[0].set_ylabel('Count')
    
    # Subplot 2: –õ—ñ–≤–∞ –≥—ñ–ª–∫–∞
    axes[1].hist(y[left_mask], bins=np.unique(y), alpha=0.7, color='blue')
    axes[1].set_title(f'Left ({left_mask.sum()} samples)\nEntropy = {H_left:.4f}', 
                     fontweight='bold')
    axes[1].set_xlabel('Class')
    
    # Subplot 3: –ü—Ä–∞–≤–∞ –≥—ñ–ª–∫–∞
    axes[2].hist(y[right_mask], bins=np.unique(y), alpha=0.7, color='red')
    axes[2].set_title(f'Right ({right_mask.sum()} samples)\nEntropy = {H_right:.4f}', 
                     fontweight='bold')
    axes[2].set_xlabel('Class')
    
    plt.suptitle(f'Information Gain = {IG:.4f}', fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.show()

# –ü—Ä–∏–∫–ª–∞–¥
from sklearn.datasets import load_iris
iris = load_iris()
visualize_split(iris.data, iris.target, feature=2, threshold=2.5)
```

---

## –†–µ–∞–ª—å–Ω–∏–π –ø—Ä–∏–∫–ª–∞–¥: Comprehensive Analysis

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_wine
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import accuracy_score

print("="*70)
print("INFORMATION THEORY: COMPREHENSIVE ANALYSIS")
print("="*70)

# –î–∞–Ω—ñ
wine = load_wine()
X = pd.DataFrame(wine.data, columns=wine.feature_names)
y = wine.target

print(f"\nDataset: {len(y)} samples, {X.shape[1]} features, {len(np.unique(y))} classes")
print(f"Class distribution: {np.bincount(y)}")

# Train/Test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

# ============================================
# 1. ENTROPY ANALYSIS
# ============================================
print("\n" + "="*70)
print("1. ENTROPY ANALYSIS")
print("="*70)

H_train = calculate_entropy(y_train)
H_test = calculate_entropy(y_test)

print(f"\nEntropy:")
print(f"  Train set: {H_train:.4f} bits")
print(f"  Test set:  {H_test:.4f} bits")
print(f"  Max entropy (3 classes): {np.log2(3):.4f} bits")

# ============================================
# 2. INFORMATION GAIN FOR EACH FEATURE
# ============================================
print("\n" + "="*70)
print("2. INFORMATION GAIN PER FEATURE")
print("="*70)

# –û–±—á–∏—Å–ª–∏—Ç–∏ IG –¥–ª—è –∫–æ–∂–Ω–æ—ó –æ–∑–Ω–∞–∫–∏ (–∑ –¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü—ñ—î—é)
igs = {}
ginis = {}

for feature in X.columns:
    # –î–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü—ñ—è
    X_binned = pd.cut(X_train[feature], bins=5, labels=False, duplicates='drop')
    
    # Information Gain
    weighted_H = 0
    for bin_val in range(5):
        mask = X_binned == bin_val
        if mask.sum() == 0:
            continue
        subset_y = y_train[mask]
        weight = len(subset_y) / len(y_train)
        weighted_H += weight * calculate_entropy(subset_y)
    
    ig = H_train - weighted_H
    igs[feature] = ig

# –¢–æ–ø-10
igs_sorted = sorted(igs.items(), key=lambda x: x[1], reverse=True)

print("\nTop 10 Features by Information Gain:")
for i, (feature, ig) in enumerate(igs_sorted[:10], 1):
    print(f"  {i:2d}. {feature:30s}: {ig:.6f}")

# ============================================
# 3. DECISION TREES: ENTROPY VS GINI
# ============================================
print("\n" + "="*70)
print("3. DECISION TREES: ENTROPY VS GINI")
print("="*70)

# Tree –∑ Entropy
dt_entropy = DecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=42)
dt_entropy.fit(X_train, y_train)
acc_entropy_train = dt_entropy.score(X_train, y_train)
acc_entropy_test = dt_entropy.score(X_test, y_test)

# Tree –∑ Gini
dt_gini = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=42)
dt_gini.fit(X_train, y_train)
acc_gini_train = dt_gini.score(X_train, y_train)
acc_gini_test = dt_gini.score(X_test, y_test)

print("\nDecision Tree Performance:")
print(f"  Entropy - Train: {acc_entropy_train:.4f}, Test: {acc_entropy_test:.4f}")
print(f"  Gini    - Train: {acc_gini_train:.4f}, Test: {acc_gini_test:.4f}")

# Feature importances
print("\nFeature Importances (Entropy):")
for feature, importance in sorted(zip(X.columns, dt_entropy.feature_importances_), 
                                  key=lambda x: x[1], reverse=True)[:5]:
    print(f"  {feature:30s}: {importance:.4f}")

# ============================================
# 4. VISUALIZATIONS
# ============================================
fig, axes = plt.subplots(2, 2, figsize=(16, 12))

# Subplot 1: Information Gain bar plot
features_top10 = [f for f, _ in igs_sorted[:10]]
ig_top10 = [ig for _, ig in igs_sorted[:10]]

axes[0, 0].barh(features_top10, ig_top10, color='steelblue', alpha=0.7)
axes[0, 0].set_xlabel('Information Gain', fontsize=11)
axes[0, 0].set_title('Top 10 Features by Information Gain', 
                    fontsize=13, fontweight='bold')
axes[0, 0].invert_yaxis()
axes[0, 0].grid(True, alpha=0.3, axis='x')

# Subplot 2: Feature Importances comparison
features_imp = X.columns
imp_entropy = dt_entropy.feature_importances_
imp_gini = dt_gini.feature_importances_

# –¢–æ–ø-10
top_features_idx = np.argsort(imp_entropy)[-10:][::-1]

x_pos = np.arange(len(top_features_idx))
width = 0.35

axes[0, 1].barh(x_pos - width/2, imp_entropy[top_features_idx], 
               width, label='Entropy', alpha=0.8)
axes[0, 1].barh(x_pos + width/2, imp_gini[top_features_idx], 
               width, label='Gini', alpha=0.8)
axes[0, 1].set_yticks(x_pos)
axes[0, 1].set_yticklabels([features_imp[i] for i in top_features_idx])
axes[0, 1].set_xlabel('Feature Importance', fontsize=11)
axes[0, 1].set_title('Feature Importances: Entropy vs Gini', 
                    fontsize=13, fontweight='bold')
axes[0, 1].legend()
axes[0, 1].invert_yaxis()
axes[0, 1].grid(True, alpha=0.3, axis='x')

# Subplot 3: Entropy vs Gini curve
p_vals = np.linspace(0.001, 0.999, 100)
entropy_vals = [entropy(p) for p in p_vals]
gini_vals = [gini_impurity(p) for p in p_vals]

axes[1, 0].plot(p_vals, entropy_vals, linewidth=2, label='Entropy', color='blue')
axes[1, 0].plot(p_vals, [g * np.log2(np.e) for g in gini_vals], linewidth=2, 
               label='Gini (scaled)', color='red', linestyle='--')
axes[1, 0].set_xlabel('p (Probability of class 1)', fontsize=11)
axes[1, 0].set_ylabel('Impurity', fontsize=11)
axes[1, 0].set_title('Entropy vs Gini Impurity', fontsize=13, fontweight='bold')
axes[1, 0].legend(fontsize=10)
axes[1, 0].grid(True, alpha=0.3)

# Subplot 4: Decision Tree (Entropy)
from sklearn.tree import plot_tree

plot_tree(dt_entropy, 
         feature_names=X.columns,
         class_names=wine.target_names,
         filled=True,
         rounded=True,
         fontsize=8,
         ax=axes[1, 1])
axes[1, 1].set_title('Decision Tree (Entropy, max_depth=3)', 
                    fontsize=13, fontweight='bold')

plt.tight_layout()
plt.show()

print("\n" + "="*70)
print("SUMMARY")
print("="*70)
print(f"‚úÖ Dataset entropy: {H_train:.4f} bits")
print(f"‚úÖ Most informative feature: {igs_sorted[0][0]} (IG={igs_sorted[0][1]:.4f})")
print(f"‚úÖ Entropy-based tree accuracy: {acc_entropy_test:.4f}")
print(f"‚úÖ Gini-based tree accuracy: {acc_gini_test:.4f}")
print("="*70)
```

---

## –ü—ñ–¥—Å—É–º–∫–æ–≤–∞ —Ç–∞–±–ª–∏—Ü—è

| –ö–æ–Ω—Ü–µ–ø—Ü—ñ—è | –§–æ—Ä–º—É–ª–∞ | –î—ñ–∞–ø–∞–∑–æ–Ω | –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è |
|-----------|---------|----------|--------------|
| **Entropy** | $-\sum p_i \log_2 p_i$ | [0, $\log_2(c)$] | –ú—ñ—Ä–∞ –Ω–µ–≤–∏–∑–Ω–∞—á–µ–Ω–æ—Å—Ç—ñ |
| **Gini** | $1 - \sum p_i^2$ | [0, $1-\frac{1}{c}$] | –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ –µ–Ω—Ç—Ä–æ–ø—ñ—ó |
| **IG** | $H(S) - H(S\|A)$ | [0, $H(S)$] | –í–∏–±—ñ—Ä –æ–∑–Ω–∞–∫–∏ –¥–ª—è split |
| **Gain Ratio** | $\frac{IG}{SplitInfo}$ | [0, 1] | –ù–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω–∏–π IG |

---

## –ü–æ–≤'—è–∑–∞–Ω—ñ —Ç–µ–º–∏

- [[Decision_Trees]] ‚Äî –æ—Å–Ω–æ–≤–Ω–µ –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è Information Theory
- [[Random_Forest]] ‚Äî Feature importance —á–µ—Ä–µ–∑ IG
- [[Feature_Selection]] ‚Äî –≤–∏–±—ñ—Ä –æ–∑–Ω–∞–∫ –∑–∞ IG
- [[CART]] ‚Äî –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è Gini impurity
- [[ID3_C45]] ‚Äî –∞–ª–≥–æ—Ä–∏—Ç–º–∏ –Ω–∞ –æ—Å–Ω–æ–≤—ñ Entropy

## –†–µ—Å—É—Ä—Å–∏

- [Shannon Entropy](https://en.wikipedia.org/wiki/Entropy_(information_theory))
- [Information Gain](https://en.wikipedia.org/wiki/Information_gain_in_decision_trees)
- [Elements of Information Theory](https://www.wiley.com/en-us/Elements+of+Information+Theory%2C+2nd+Edition-p-9780471241959)

---

## –ö–ª—é—á–æ–≤—ñ –≤–∏—Å–Ω–æ–≤–∫–∏

> Information Theory –Ω–∞–¥–∞—î –º–∞—Ç–µ–º–∞—Ç–∏—á–Ω–∏–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≤–∏–º—ñ—Ä—é–≤–∞–Ω–Ω—è –Ω–µ–≤–∏–∑–Ω–∞—á–µ–Ω–æ—Å—Ç—ñ —Ç–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó –≤ –¥–∞–Ω–∏—Ö. Entropy, Information Gain, —Ç–∞ Gini Impurity ‚Äî –∫–ª—é—á–æ–≤—ñ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –ø–æ–±—É–¥–æ–≤–∏ Decision Trees.

**–ö–ª—é—á–æ–≤—ñ –∫–æ–Ω—Ü–µ–ø—Ü—ñ—ó:**
- **Entropy** ‚Äî –º—ñ—Ä–∞ –Ω–µ–≤–∏–∑–Ω–∞—á–µ–Ω–æ—Å—Ç—ñ (–±—ñ–ª—å—à–µ —Ö–∞–æ—Å—É = –±—ñ–ª—å—à–µ –µ–Ω—Ç—Ä–æ–ø—ñ—ó)
- **Information Gain** ‚Äî —Å–∫—ñ–ª—å–∫–∏ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó –¥–∞—î —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è
- **Gini Impurity** ‚Äî –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ –µ–Ω—Ç—Ä–æ–ø—ñ—ó (—à–≤–∏–¥—à–∞, —Å—Ö–æ–∂—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏)
- **Gain Ratio** ‚Äî –Ω–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω–∏–π IG (—É–Ω–∏–∫–∞—î bias –¥–æ –±–∞–≥–∞—Ç—å–æ—Ö –∑–Ω–∞—á–µ–Ω—å)

**–ü—Ä–∞–∫—Ç–∏—á–Ω–µ –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è:**
- **Decision Trees** ‚Äî –≤–∏–±—ñ—Ä –Ω–∞–π–∫—Ä–∞—â–æ–≥–æ split
- **Feature Selection** ‚Äî –Ω–∞–π—ñ–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ñ—à—ñ –æ–∑–Ω–∞–∫–∏
- **Feature Importance** ‚Äî —Ä–∞–Ω–∂—É–≤–∞–Ω–Ω—è –æ–∑–Ω–∞–∫

**Entropy vs Gini:**
- –û–±–∏–¥–≤—ñ –¥–∞—é—Ç—å —Å—Ö–æ–∂—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
- Gini —à–≤–∏–¥—à–∞ (–±–µ–∑ –ª–æ–≥–∞—Ä–∏—Ñ–º—ñ–≤)
- Entropy —Ç–µ–æ—Ä–µ—Ç–∏—á–Ω–æ –±—ñ–ª—å—à –æ–±“ë—Ä—É–Ω—Ç–æ–≤–∞–Ω–∞
- **–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—è:** –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π Gini (sklearn default)

**–§–æ—Ä–º—É–ª–∏ –¥–ª—è –∑–∞–ø–∞–º'—è—Ç–æ–≤—É–≤–∞–Ω–Ω—è:**
```
Entropy:  H = -Œ£ p·µ¢ log‚ÇÇ(p·µ¢)
Gini:     G = 1 - Œ£ p·µ¢¬≤
IG:       IG = H(before) - H(after)
```

---

#ml #information-theory #entropy #information-gain #gini-impurity #decision-trees #feature-selection
