# Distance Metrics (–ú–µ—Ç—Ä–∏–∫–∏ –≤—ñ–¥—Å—Ç–∞–Ω—ñ)

## –©–æ —Ü–µ?

**Distance Metrics** ‚Äî —Ü–µ —Ñ—É–Ω–∫—Ü—ñ—ó, —è–∫—ñ **–≤–∏–º—ñ—Ä—é—é—Ç—å –≤—ñ–¥—Å—Ç–∞–Ω—å** (–Ω–µ—Å—Ö–æ–∂—ñ—Å—Ç—å) –º—ñ–∂ –¥–≤–æ–º–∞ —Ç–æ—á–∫–∞–º–∏ –≤ –ø—Ä–æ—Å—Ç–æ—Ä—ñ. –ß–∏–º –º–µ–Ω—à–∞ –≤—ñ–¥—Å—Ç–∞–Ω—å, —Ç–∏–º –±—ñ–ª—å—à —Å—Ö–æ–∂—ñ –æ–±'—î–∫—Ç–∏.

**–ì–æ–ª–æ–≤–Ω–∞ —ñ–¥–µ—è:** —Ä—ñ–∑–Ω—ñ –º–µ—Ç—Ä–∏–∫–∏ –ø—ñ–¥—Ö–æ–¥—è—Ç—å –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö —Ç–∏–ø—ñ–≤ –¥–∞–Ω–∏—Ö —Ç–∞ –∑–∞–¥–∞—á. –í–∏–±—ñ—Ä –ø—Ä–∞–≤–∏–ª—å–Ω–æ—ó –º–µ—Ç—Ä–∏–∫–∏ –∫—Ä–∏—Ç–∏—á–Ω–æ –≤–∞–∂–ª–∏–≤–∏–π –¥–ª—è —É—Å–ø—ñ—Ö—É –∞–ª–≥–æ—Ä–∏—Ç–º—ñ–≤, —â–æ –±–∞–∑—É—é—Ç—å—Å—è –Ω–∞ –≤—ñ–¥—Å—Ç–∞–Ω—ñ.

## –ù–∞–≤—ñ—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ?

- üéØ **K-Nearest Neighbors (KNN)** ‚Äî –ø–æ—à—É–∫ –Ω–∞–π–±–ª–∏–∂—á–∏—Ö —Å—É—Å—ñ–¥—ñ–≤
- üîç **K-Means Clustering** ‚Äî –≥—Ä—É–ø—É–≤–∞–Ω–Ω—è —Å—Ö–æ–∂–∏—Ö –æ–±'—î–∫—Ç—ñ–≤
- üìä **Anomaly Detection** ‚Äî –≤–∏—è–≤–ª–µ–Ω–Ω—è –≤–∏–∫–∏–¥—ñ–≤
- üó∫Ô∏è **Dimensionality Reduction** ‚Äî MDS, t-SNE
- üîé **Information Retrieval** ‚Äî –ø–æ—à—É–∫ —Å—Ö–æ–∂–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤
- üì∏ **Image Recognition** ‚Äî –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –∑–æ–±—Ä–∞–∂–µ–Ω—å

## –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏?

**–ü–æ—Ç—Ä—ñ–±–Ω–æ:**
- –ë—É–¥—å-—è–∫—ñ distance-based –∞–ª–≥–æ—Ä–∏—Ç–º–∏ (KNN, K-Means)
- –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –≤–µ–∫—Ç–æ—Ä—ñ–≤/–æ–±'—î–∫—Ç—ñ–≤
- –ü–æ—à—É–∫ –Ω–∞–π–±–ª–∏–∂—á–∏—Ö —Å—É—Å—ñ–¥—ñ–≤
- –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—è

**–ù–µ –ø–æ—Ç—Ä—ñ–±–Ω–æ:**
- Linear models (–≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—Ç—å —ñ–Ω—à—ñ –ø—ñ–¥—Ö–æ–¥–∏)
- Tree-based models (–Ω–µ –±–∞–∑—É—é—Ç—å—Å—è –Ω–∞ –≤—ñ–¥—Å—Ç–∞–Ω—ñ)

---

## –ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è –º–µ—Ç—Ä–∏–∫

```
Distance Metrics
‚îÇ
‚îú‚îÄ‚îÄ Minkowski Distance Family
‚îÇ   ‚îú‚îÄ‚îÄ Euclidean Distance (p=2)
‚îÇ   ‚îú‚îÄ‚îÄ Manhattan Distance (p=1)
‚îÇ   ‚îî‚îÄ‚îÄ Chebyshev Distance (p=‚àû)
‚îÇ
‚îú‚îÄ‚îÄ Specialized Metrics
‚îÇ   ‚îú‚îÄ‚îÄ Cosine Distance
‚îÇ   ‚îú‚îÄ‚îÄ Hamming Distance
‚îÇ   ‚îî‚îÄ‚îÄ Mahalanobis Distance
‚îÇ
‚îî‚îÄ‚îÄ Other Metrics
    ‚îú‚îÄ‚îÄ Jaccard Distance
    ‚îú‚îÄ‚îÄ Haversine Distance
    ‚îî‚îÄ‚îÄ Edit Distance (Levenshtein)
```

---

# 1. Euclidean Distance (–ï–≤–∫–ª—ñ–¥–æ–≤–∞ –≤—ñ–¥—Å—Ç–∞–Ω—å)

## –§–æ—Ä–º—É–ª–∞

$$d_{\text{Euclidean}}(\mathbf{x}, \mathbf{y}) = \sqrt{\sum_{i=1}^{n} (x_i - y_i)^2}$$

–ê–±–æ —É –≤–µ–∫—Ç–æ—Ä–Ω—ñ–π —Ñ–æ—Ä–º—ñ:

$$d(\mathbf{x}, \mathbf{y}) = \|\mathbf{x} - \mathbf{y}\|_2$$

## –Ü–Ω—Ç—É—ó—Ü—ñ—è

**Euclidean Distance** ‚Äî —Ü–µ **–ø—Ä—è–º–∞ –ª—ñ–Ω—ñ—è** –º—ñ–∂ –¥–≤–æ–º–∞ —Ç–æ—á–∫–∞–º–∏. –ù–∞–π—ñ–Ω—Ç—É—ó—Ç–∏–≤–Ω—ñ—à–∞ –º–µ—Ç—Ä–∏–∫–∞ –≤—ñ–¥—Å—Ç–∞–Ω—ñ.

```
2D –ø—Ä–∏–∫–ª–∞–¥:

    y
    |
  4 |     B(3,4)
    |      *
  3 |     /
    |    /
  2 |   /
    |  /
  1 | * A(1,1)
    |
    +------------- x
      1 2 3 4

A = (1, 1)
B = (3, 4)

d = ‚àö[(3-1)¬≤ + (4-1)¬≤]
  = ‚àö[4 + 9]
  = ‚àö13
  ‚âà 3.606
```

## –ö–æ–¥

```python
import numpy as np

def euclidean_distance(x, y):
    """
    –û–±—á–∏—Å–ª–∏—Ç–∏ Euclidean distance –º—ñ–∂ –¥–≤–æ–º–∞ –≤–µ–∫—Ç–æ—Ä–∞–º–∏
    
    Parameters:
    -----------
    x, y : array-like
        –í–µ–∫—Ç–æ—Ä–∏
        
    Returns:
    --------
    float : –≤—ñ–¥—Å—Ç–∞–Ω—å
    """
    return np.sqrt(np.sum((x - y) ** 2))

# –ü—Ä–∏–∫–ª–∞–¥
x = np.array([1, 1])
y = np.array([3, 4])

dist = euclidean_distance(x, y)
print(f"Euclidean distance: {dist:.4f}")

# –ß–µ—Ä–µ–∑ scipy
from scipy.spatial.distance import euclidean
dist_scipy = euclidean(x, y)
print(f"Scipy distance: {dist_scipy:.4f}")

# –ß–µ—Ä–µ–∑ sklearn
from sklearn.metrics.pairwise import euclidean_distances
dist_sklearn = euclidean_distances([x], [y])[0, 0]
print(f"Sklearn distance: {dist_sklearn:.4f}")
```

## –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è

```python
import matplotlib.pyplot as plt
import numpy as np

# –¢–æ—á–∫–∏
A = np.array([1, 1])
B = np.array([3, 4])

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
plt.figure(figsize=(8, 8))

# –¢–æ—á–∫–∏
plt.scatter(*A, s=200, c='blue', marker='o', label='A(1,1)', zorder=3)
plt.scatter(*B, s=200, c='red', marker='o', label='B(3,4)', zorder=3)

# –ü—Ä—è–º–∞ –ª—ñ–Ω—ñ—è (Euclidean)
plt.plot([A[0], B[0]], [A[1], B[1]], 'g-', linewidth=3, 
         label=f'Euclidean = {euclidean_distance(A, B):.2f}')

# –°—ñ—Ç–∫–∞
plt.grid(True, alpha=0.3)
plt.axis('equal')
plt.xlim(0, 5)
plt.ylim(0, 5)
plt.xlabel('x', fontsize=12)
plt.ylabel('y', fontsize=12)
plt.title('Euclidean Distance', fontsize=14, fontweight='bold')
plt.legend(fontsize=11)

# –ê–Ω–æ—Ç–∞—Ü—ñ—ó
mid_x = (A[0] + B[0]) / 2
mid_y = (A[1] + B[1]) / 2
plt.annotate(f'd = {euclidean_distance(A, B):.2f}', 
            xy=(mid_x, mid_y), xytext=(mid_x + 0.5, mid_y + 0.5),
            fontsize=12, fontweight='bold',
            bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.7),
            arrowprops=dict(arrowstyle='->', color='green', lw=2))

plt.tight_layout()
plt.show()
```

## –ü–µ—Ä–µ–≤–∞–≥–∏ —Ç–∞ –Ω–µ–¥–æ–ª—ñ–∫–∏

| –ü–µ—Ä–µ–≤–∞–≥–∏ | –ù–µ–¥–æ–ª—ñ–∫–∏ |
|----------|----------|
| ‚úÖ –Ü–Ω—Ç—É—ó—Ç–∏–≤–Ω–æ –∑—Ä–æ–∑—É–º—ñ–ª–∞ | ‚ùå –ß—É—Ç–ª–∏–≤–∞ –¥–æ –º–∞—Å—à—Ç–∞–±—É –æ–∑–Ω–∞–∫ |
| ‚úÖ –ì–µ–æ–º–µ—Ç—Ä–∏—á–Ω–∞ —ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü—ñ—è | ‚ùå Curse of dimensionality |
| ‚úÖ –ü—ñ–¥—Ö–æ–¥–∏—Ç—å –¥–ª—è continuous features | ‚ùå –ù–µ –ø—ñ–¥—Ö–æ–¥–∏—Ç—å –¥–ª—è categorical |
| ‚úÖ –°—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è –±–∞–≥–∞—Ç—å–æ—Ö –∑–∞–¥–∞—á | ‚ùå –ü—Ä–∏–ø—É—Å–∫–∞—î —ñ–∑–æ—Ç—Ä–æ–ø–Ω—É —Å—Ç—Ä—É–∫—Ç—É—Ä—É |

## –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏?

- ‚úÖ Geographical data (–∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∏)
- ‚úÖ Computer vision (pixel values)
- ‚úÖ Continuous numeric features
- ‚úÖ –ö–æ–ª–∏ –æ–∑–Ω–∞–∫–∏ –º–∞—é—Ç—å –æ–¥–Ω–∞–∫–æ–≤–∏–π –º–∞—Å—à—Ç–∞–±
- ‚ùå Text data (–≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π cosine)
- ‚ùå High-dimensional sparse data

---

# 2. Manhattan Distance (–ú–∞–Ω—Ö–µ—Ç—Ç–µ–Ω—Å—å–∫–∞ –≤—ñ–¥—Å—Ç–∞–Ω—å)

## –§–æ—Ä–º—É–ª–∞

$$d_{\text{Manhattan}}(\mathbf{x}, \mathbf{y}) = \sum_{i=1}^{n} |x_i - y_i|$$

–ê–±–æ:

$$d(\mathbf{x}, \mathbf{y}) = \|\mathbf{x} - \mathbf{y}\|_1$$

## –Ü–Ω—Ç—É—ó—Ü—ñ—è

**Manhattan Distance** (—Ç–∞–∫–æ–∂ **L1 distance**, **Taxicab distance**) ‚Äî —Ü–µ —Å—É–º–∞ **–∞–±—Å–æ–ª—é—Ç–Ω–∏—Ö —Ä—ñ–∑–Ω–∏—Ü—å**. –ù–∞–∑–≤–∞ –≤—ñ–¥ –≤—É–ª–∏—Ü—å –ú–∞–Ω—Ö–µ—Ç—Ç–µ–Ω–∞ (–º–æ–∂–Ω–∞ —Ä—É—Ö–∞—Ç–∏—Å—è —Ç—ñ–ª—å–∫–∏ –≤–∑–¥–æ–≤–∂ –≤—É–ª–∏—Ü—å).

```
2D –ø—Ä–∏–∫–ª–∞–¥:

    y
    |
  4 |     B(3,4)
    |     *
  3 |     ‚Üë
    |     ‚Üë
  2 |     ‚Üë
    |     ‚Üë
  1 | *‚Üí‚Üí‚Üí* A(1,1)
    |
    +------------- x
      1 2 3 4

A = (1, 1)
B = (3, 4)

d = |3-1| + |4-1|
  = 2 + 3
  = 5

–ù–µ –º–æ–∂–Ω–∞ –π—Ç–∏ –ø–æ –¥—ñ–∞–≥–æ–Ω–∞–ª—ñ ‚Äî —Ç—ñ–ª—å–∫–∏ –≤–∑–¥–æ–≤–∂ –æ—Å–µ–π!
```

## –ö–æ–¥

```python
import numpy as np

def manhattan_distance(x, y):
    """–û–±—á–∏—Å–ª–∏—Ç–∏ Manhattan distance"""
    return np.sum(np.abs(x - y))

# –ü—Ä–∏–∫–ª–∞–¥
x = np.array([1, 1])
y = np.array([3, 4])

dist = manhattan_distance(x, y)
print(f"Manhattan distance: {dist:.4f}")

# –ß–µ—Ä–µ–∑ scipy
from scipy.spatial.distance import cityblock
dist_scipy = cityblock(x, y)
print(f"Scipy distance: {dist_scipy:.4f}")

# –ß–µ—Ä–µ–∑ sklearn
from sklearn.metrics.pairwise import manhattan_distances
dist_sklearn = manhattan_distances([x], [y])[0, 0]
print(f"Sklearn distance: {dist_sklearn:.4f}")
```

## –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è Euclidean vs Manhattan

```python
import matplotlib.pyplot as plt
import numpy as np

# –¢–æ—á–∫–∏
A = np.array([1, 1])
B = np.array([3, 4])

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
plt.figure(figsize=(10, 8))

# –¢–æ—á–∫–∏
plt.scatter(*A, s=200, c='blue', marker='o', label='A(1,1)', zorder=3)
plt.scatter(*B, s=200, c='red', marker='o', label='B(3,4)', zorder=3)

# Euclidean (–ø—Ä—è–º–∞ –ª—ñ–Ω—ñ—è)
plt.plot([A[0], B[0]], [A[1], B[1]], 'g-', linewidth=3, alpha=0.7,
         label=f'Euclidean = {euclidean_distance(A, B):.2f}')

# Manhattan (–ª–∞–º–∞–Ω–∞ –ª—ñ–Ω—ñ—è)
plt.plot([A[0], B[0], B[0]], [A[1], A[1], B[1]], 'r-', linewidth=3, alpha=0.7,
         label=f'Manhattan = {manhattan_distance(A, B):.2f}')

# –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∏–π Manhattan —à–ª—è—Ö
plt.plot([A[0], A[0], B[0]], [A[1], B[1], B[1]], 'orange', linewidth=2, 
         alpha=0.5, linestyle='--', label='Alternative path')

plt.grid(True, alpha=0.3)
plt.axis('equal')
plt.xlim(0, 5)
plt.ylim(0, 5)
plt.xlabel('x', fontsize=12)
plt.ylabel('y', fontsize=12)
plt.title('Euclidean vs Manhattan Distance', fontsize=14, fontweight='bold')
plt.legend(fontsize=11)
plt.tight_layout()
plt.show()

# –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –≤ —Ä—ñ–∑–Ω–∏—Ö –≤–∏–º—ñ—Ä–∞—Ö
print("–ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –≤—ñ–¥—Å—Ç–∞–Ω–µ–π:")
print("="*50)
dimensions = [2, 5, 10, 50, 100]

for dim in dimensions:
    x = np.random.randn(dim)
    y = np.random.randn(dim)
    
    euc = euclidean_distance(x, y)
    man = manhattan_distance(x, y)
    ratio = man / euc
    
    print(f"Dim={dim:3d}: Euclidean={euc:6.2f}, Manhattan={man:6.2f}, "
          f"Ratio={ratio:.2f}")
```

## –ü–µ—Ä–µ–≤–∞–≥–∏ —Ç–∞ –Ω–µ–¥–æ–ª—ñ–∫–∏

| –ü–µ—Ä–µ–≤–∞–≥–∏ | –ù–µ–¥–æ–ª—ñ–∫–∏ |
|----------|----------|
| ‚úÖ –ú–µ–Ω—à —á—É—Ç–ª–∏–≤–∞ –¥–æ –≤–∏–∫–∏–¥—ñ–≤ | ‚ùå –ù–µ –º–∞—î –≥–µ–æ–º–µ—Ç—Ä–∏—á–Ω–æ—ó –ø—Ä—è–º–æ—ó |
| ‚úÖ –ü—ñ–¥—Ö–æ–¥–∏—Ç—å –¥–ª—è grid-based –ø—Ä–æ–±–ª–µ–º | ‚ùå –ú–æ–∂–µ –¥–∞–≤–∞—Ç–∏ –±–∞–≥–∞—Ç–æ –æ–¥–Ω–∞–∫–æ–≤–∏—Ö –≤—ñ–¥—Å—Ç–∞–Ω–µ–π |
| ‚úÖ –®–≤–∏–¥—à–∞ –≤ –æ–±—á–∏—Å–ª–µ–Ω–Ω—ñ (–±–µ–∑ sqrt) | ‚ùå –ú–µ–Ω—à —ñ–Ω—Ç—É—ó—Ç–∏–≤–Ω–∞ |
| ‚úÖ –ü—Ä–∞—Ü—é—î –≤ high dimensions | |

## –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏?

- ‚úÖ Grid-based problems (—à–∞—Ö–∏, –ª–∞–±—ñ—Ä–∏–Ω—Ç–∏)
- ‚úÖ –ö–æ–ª–∏ —Ä—É—Ö –º–æ–∂–ª–∏–≤–∏–π —Ç—ñ–ª—å–∫–∏ –≤–∑–¥–æ–≤–∂ –æ—Å–µ–π
- ‚úÖ High-dimensional data
- ‚úÖ –ö–æ–ª–∏ —î –≤–∏–∫–∏–¥–∏

---

# 3. Minkowski Distance (–£–∑–∞–≥–∞–ª—å–Ω–µ–Ω–Ω—è)

## –§–æ—Ä–º—É–ª–∞

$$d_{\text{Minkowski}}(\mathbf{x}, \mathbf{y}) = \left(\sum_{i=1}^{n} |x_i - y_i|^p\right)^{1/p}$$

–ê–±–æ:

$$d(\mathbf{x}, \mathbf{y}) = \|\mathbf{x} - \mathbf{y}\|_p$$

## –Ü–Ω—Ç—É—ó—Ü—ñ—è

**Minkowski Distance** ‚Äî —Ü–µ **—É–∑–∞–≥–∞–ª—å–Ω–µ–Ω–Ω—è** —á–µ—Ä–µ–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä $p$:

```
p = 1  ‚Üí Manhattan Distance
p = 2  ‚Üí Euclidean Distance
p = ‚àû  ‚Üí Chebyshev Distance
```

## –ö–æ–¥

```python
def minkowski_distance(x, y, p):
    """–û–±—á–∏—Å–ª–∏—Ç–∏ Minkowski distance"""
    return np.sum(np.abs(x - y) ** p) ** (1/p)

# –ü—Ä–∏–∫–ª–∞–¥–∏
x = np.array([1, 1])
y = np.array([3, 4])

for p in [1, 2, 3, 5, 10]:
    dist = minkowski_distance(x, y, p)
    print(f"p={p:2d}: {dist:.4f}")

# –ß–µ—Ä–µ–∑ scipy
from scipy.spatial.distance import minkowski
for p in [1, 2, 3]:
    dist = minkowski(x, y, p)
    print(f"Scipy p={p}: {dist:.4f}")
```

## –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è Unit Circles

```python
import matplotlib.pyplot as plt
import numpy as np

# Unit circles –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö p
theta = np.linspace(0, 2*np.pi, 1000)

fig, axes = plt.subplots(2, 3, figsize=(15, 10))
axes = axes.ravel()

p_values = [0.5, 1, 1.5, 2, 3, 10]

for idx, p in enumerate(p_values):
    # –û–±—á–∏—Å–ª–∏—Ç–∏ unit circle –¥–ª—è Minkowski –∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–º p
    if p == np.inf:
        # Chebyshev
        x = np.sign(np.cos(theta))
        y = np.sign(np.sin(theta))
    else:
        # –î–ª—è —ñ–Ω—à–∏—Ö p
        t = np.linspace(0, 2*np.pi, 1000)
        x = np.sign(np.cos(t)) * np.abs(np.cos(t)) ** (2/p)
        y = np.sign(np.sin(t)) * np.abs(np.sin(t)) ** (2/p)
    
    axes[idx].plot(x, y, linewidth=2)
    axes[idx].set_xlim(-1.5, 1.5)
    axes[idx].set_ylim(-1.5, 1.5)
    axes[idx].set_aspect('equal')
    axes[idx].grid(True, alpha=0.3)
    axes[idx].set_title(f'p = {p}', fontsize=12, fontweight='bold')
    axes[idx].axhline(y=0, color='k', linewidth=0.5)
    axes[idx].axvline(x=0, color='k', linewidth=0.5)

plt.suptitle('Unit Circles for Minkowski Distance (Different p)', 
            fontsize=14, fontweight='bold')
plt.tight_layout()
plt.show()
```

---

# 4. Chebyshev Distance

## –§–æ—Ä–º—É–ª–∞

$$d_{\text{Chebyshev}}(\mathbf{x}, \mathbf{y}) = \max_{i} |x_i - y_i|$$

–¶–µ Minkowski distance –∑ $p = \infty$:

$$d(\mathbf{x}, \mathbf{y}) = \|\mathbf{x} - \mathbf{y}\|_\infty$$

## –Ü–Ω—Ç—É—ó—Ü—ñ—è

**Chebyshev Distance** ‚Äî —Ü–µ **–º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ —Ä—ñ–∑–Ω–∏—Ü—è** —Å–µ—Ä–µ–¥ –≤—Å—ñ—Ö –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç.

```
A = (1, 1)
B = (3, 4)

–†—ñ–∑–Ω–∏—Ü—ñ:
  x: |3 - 1| = 2
  y: |4 - 1| = 3

d = max(2, 3) = 3

–í—ñ–∑—É–∞–ª—å–Ω–æ ‚Äî —Ü–µ –Ω–∞–π–±—ñ–ª—å—à–∏–π "–∫—Ä–æ–∫" –≤–∑–¥–æ–≤–∂ –æ–¥–Ω—ñ—î—ó –æ—Å—ñ.
```

## –ö–æ–¥

```python
def chebyshev_distance(x, y):
    """–û–±—á–∏—Å–ª–∏—Ç–∏ Chebyshev distance"""
    return np.max(np.abs(x - y))

# –ü—Ä–∏–∫–ª–∞–¥
x = np.array([1, 1])
y = np.array([3, 4])

dist = chebyshev_distance(x, y)
print(f"Chebyshev distance: {dist:.4f}")

# –ß–µ—Ä–µ–∑ scipy
from scipy.spatial.distance import chebyshev
dist_scipy = chebyshev(x, y)
print(f"Scipy distance: {dist_scipy:.4f}")
```

## –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –≤—Å—ñ—Ö Minkowski –º–µ—Ç—Ä–∏–∫

```python
import matplotlib.pyplot as plt
import numpy as np

# –¢–æ—á–∫–∏
A = np.array([1, 1])
B = np.array([3, 4])

# –û–±—á–∏—Å–ª–∏—Ç–∏ –≤—ñ–¥—Å—Ç–∞–Ω—ñ
distances = {
    'Manhattan (p=1)': manhattan_distance(A, B),
    'Euclidean (p=2)': euclidean_distance(A, B),
    'Minkowski (p=3)': minkowski_distance(A, B, 3),
    'Chebyshev (p=‚àû)': chebyshev_distance(A, B)
}

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
plt.figure(figsize=(12, 6))

names = list(distances.keys())
values = list(distances.values())
colors = ['red', 'green', 'blue', 'purple']

bars = plt.bar(names, values, color=colors, alpha=0.7)

for i, (name, value) in enumerate(zip(names, values)):
    plt.text(i, value + 0.1, f'{value:.2f}', 
            ha='center', fontsize=11, fontweight='bold')

plt.ylabel('Distance', fontsize=12)
plt.title('Comparison of Minkowski Family Distances', fontsize=14, fontweight='bold')
plt.grid(True, alpha=0.3, axis='y')
plt.ylim(0, max(values) * 1.2)
plt.xticks(rotation=15, ha='right')
plt.tight_layout()
plt.show()

print("Distance Comparison:")
print("="*50)
for name, value in distances.items():
    print(f"{name:20s}: {value:.4f}")
```

## –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏?

- ‚úÖ Chess/checkers (–∫–æ—Ä–æ–ª—å —Ä—É—Ö–∞—î—Ç—å—Å—è)
- ‚úÖ –ö–æ–ª–∏ –≤–∞–∂–ª–∏–≤–∞ —Ç—ñ–ª—å–∫–∏ –Ω–∞–π–±—ñ–ª—å—à–∞ —Ä—ñ–∑–Ω–∏—Ü—è
- ‚úÖ Warehouse logistics
- ‚ùå –ë—ñ–ª—å—à—ñ—Å—Ç—å ML –∑–∞–¥–∞—á (—Ä—ñ–¥–∫–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è)

---

# 5. Cosine Distance / Similarity

## –§–æ—Ä–º—É–ª–∞

**Cosine Similarity:**

$$\text{cos}(\mathbf{x}, \mathbf{y}) = \frac{\mathbf{x} \cdot \mathbf{y}}{\|\mathbf{x}\| \|\mathbf{y}\|} = \frac{\sum_{i=1}^{n} x_i y_i}{\sqrt{\sum_{i=1}^{n} x_i^2} \sqrt{\sum_{i=1}^{n} y_i^2}}$$

**Cosine Distance:**

$$d_{\text{cosine}}(\mathbf{x}, \mathbf{y}) = 1 - \text{cos}(\mathbf{x}, \mathbf{y})$$

## –Ü–Ω—Ç—É—ó—Ü—ñ—è

**Cosine Similarity** –≤–∏–º—ñ—Ä—é—î **–∫—É—Ç –º—ñ–∂ –≤–µ–∫—Ç–æ—Ä–∞–º–∏**, –Ω–µ –≤—Ä–∞—Ö–æ–≤—É—é—á–∏ —ó—Ö –¥–æ–≤–∂–∏–Ω—É.

```
2D –ø—Ä–∏–∫–ª–∞–¥:

    y
    |
    |    B(2,4)
    |     *
    |    /
    |   / Œ∏
    |  /
    | *-----> A(4,2)
    |
    +------------- x

cos(Œ∏) = A¬∑B / (|A| |B|)

–Ø–∫—â–æ Œ∏ = 0¬∞   ‚Üí cos = 1  (—ñ–¥–µ–Ω—Ç–∏—á–Ω–∏–π –Ω–∞–ø—Ä—è–º–æ–∫)
–Ø–∫—â–æ Œ∏ = 90¬∞  ‚Üí cos = 0  (–æ—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω—ñ)
–Ø–∫—â–æ Œ∏ = 180¬∞ ‚Üí cos = -1 (–ø—Ä–æ—Ç–∏–ª–µ–∂–Ω–∏–π –Ω–∞–ø—Ä—è–º–æ–∫)
```

## –ö–æ–¥

```python
def cosine_similarity(x, y):
    """–û–±—á–∏—Å–ª–∏—Ç–∏ Cosine Similarity"""
    dot_product = np.dot(x, y)
    norm_x = np.linalg.norm(x)
    norm_y = np.linalg.norm(y)
    return dot_product / (norm_x * norm_y)

def cosine_distance(x, y):
    """–û–±—á–∏—Å–ª–∏—Ç–∏ Cosine Distance"""
    return 1 - cosine_similarity(x, y)

# –ü—Ä–∏–∫–ª–∞–¥
x = np.array([1, 2, 3])
y = np.array([2, 4, 6])  # –£ 2 —Ä–∞–∑–∏ –±—ñ–ª—å—à–µ ‚Äî —Ç–æ–π —Å–∞–º–∏–π –Ω–∞–ø—Ä—è–º–æ–∫!

sim = cosine_similarity(x, y)
dist = cosine_distance(x, y)

print(f"Cosine Similarity: {sim:.4f}")
print(f"Cosine Distance: {dist:.4f}")

# –ß–µ—Ä–µ–∑ sklearn
from sklearn.metrics.pairwise import cosine_similarity as sklearn_cosine
sim_sklearn = sklearn_cosine([x], [y])[0, 0]
print(f"Sklearn Similarity: {sim_sklearn:.4f}")

# –ß–µ—Ä–µ–∑ scipy
from scipy.spatial.distance import cosine
dist_scipy = cosine(x, y)
print(f"Scipy Distance: {dist_scipy:.4f}")
```

## –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è

```python
import matplotlib.pyplot as plt
import numpy as np

# –í–µ–∫—Ç–æ—Ä–∏
A = np.array([4, 2])
B = np.array([2, 4])
C = np.array([8, 4])  # 2*A (—Ç–æ–π —Å–∞–º–∏–π –Ω–∞–ø—Ä—è–º–æ–∫)

origin = np.array([0, 0])

# –û–±—á–∏—Å–ª–∏—Ç–∏ –∫—É—Ç–∏
def angle_between(v1, v2):
    cos_angle = cosine_similarity(v1, v2)
    angle = np.arccos(np.clip(cos_angle, -1, 1))
    return np.degrees(angle)

angle_AB = angle_between(A, B)
angle_AC = angle_between(A, C)

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# Subplot 1: –†—ñ–∑–Ω—ñ –Ω–∞–ø—Ä—è–º–∫–∏
axes[0].quiver(*origin, *A, angles='xy', scale_units='xy', scale=1, 
              color='blue', width=0.015, label='A(4,2)')
axes[0].quiver(*origin, *B, angles='xy', scale_units='xy', scale=1, 
              color='red', width=0.015, label='B(2,4)')

# –ö—É—Ç
from matplotlib.patches import Arc
arc = Arc((0, 0), 2, 2, angle=0, theta1=0, theta2=angle_AB, 
         color='green', linewidth=2)
axes[0].add_patch(arc)
axes[0].text(1, 0.5, f'Œ∏={angle_AB:.1f}¬∞', fontsize=11, fontweight='bold',
            bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.7))

axes[0].set_xlim(-1, 5)
axes[0].set_ylim(-1, 5)
axes[0].set_aspect('equal')
axes[0].grid(True, alpha=0.3)
axes[0].legend(fontsize=11)
axes[0].set_title(f'Different Directions\ncos(Œ∏)={cosine_similarity(A, B):.3f}', 
                 fontsize=12, fontweight='bold')

# Subplot 2: –¢–æ–π —Å–∞–º–∏–π –Ω–∞–ø—Ä—è–º–æ–∫
axes[1].quiver(*origin, *A, angles='xy', scale_units='xy', scale=1, 
              color='blue', width=0.015, label='A(4,2)')
axes[1].quiver(*origin, *C, angles='xy', scale_units='xy', scale=1, 
              color='purple', width=0.015, label='C(8,4) = 2*A')

axes[1].text(6, 3, f'Œ∏={angle_AC:.1f}¬∞', fontsize=11, fontweight='bold',
            bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.7))

axes[1].set_xlim(-1, 9)
axes[1].set_ylim(-1, 5)
axes[1].set_aspect('equal')
axes[1].grid(True, alpha=0.3)
axes[1].legend(fontsize=11)
axes[1].set_title(f'Same Direction\ncos(Œ∏)={cosine_similarity(A, C):.3f}', 
                 fontsize=12, fontweight='bold')

plt.suptitle('Cosine Similarity: Measures Angle, Not Magnitude', 
            fontsize=14, fontweight='bold')
plt.tight_layout()
plt.show()
```

## –ö–ª—é—á–æ–≤–∞ –≤—ñ–¥–º—ñ–Ω–Ω—ñ—Å—Ç—å –≤—ñ–¥ Euclidean

```python
# –ü—Ä–∏–∫–ª–∞–¥
doc1 = np.array([1, 1, 0])
doc2 = np.array([10, 10, 0])  # –¢–æ–π —Å–∞–º–∏–π –Ω–∞–ø—Ä—è–º–æ–∫, –∞–ª–µ —É 10 —Ä–∞–∑—ñ–≤ –±—ñ–ª—å—à–µ
doc3 = np.array([0, 1, 1])    # –Ü–Ω—à–∏–π –Ω–∞–ø—Ä—è–º–æ–∫

print("–ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –º–µ—Ç—Ä–∏–∫:")
print("="*60)

# Euclidean
euc_12 = euclidean_distance(doc1, doc2)
euc_13 = euclidean_distance(doc1, doc3)

print(f"Euclidean:")
print(f"  doc1 vs doc2: {euc_12:.4f}")
print(f"  doc1 vs doc3: {euc_13:.4f}")
print(f"  ‚Üí doc3 –±–ª–∏–∂—á–µ –∑–∞ Euclidean!")

# Cosine
cos_12 = cosine_similarity(doc1, doc2)
cos_13 = cosine_similarity(doc1, doc3)

print(f"\nCosine Similarity:")
print(f"  doc1 vs doc2: {cos_12:.4f}")
print(f"  doc1 vs doc3: {cos_13:.4f}")
print(f"  ‚Üí doc2 –±—ñ–ª—å—à —Å—Ö–æ–∂–∏–π –∑–∞ Cosine!")

print("\n–í–∏—Å–Ω–æ–≤–æ–∫:")
print("Euclidean —á—É—Ç–ª–∏–≤–∞ –¥–æ magnitude (–¥–æ–≤–∂–∏–Ω–∏)")
print("Cosine —á—É—Ç–ª–∏–≤–∞ —Ç—ñ–ª—å–∫–∏ –¥–æ direction (–Ω–∞–ø—Ä—è–º–∫—É)")
```

## –ü–µ—Ä–µ–≤–∞–≥–∏ —Ç–∞ –Ω–µ–¥–æ–ª—ñ–∫–∏

| –ü–µ—Ä–µ–≤–∞–≥–∏ | –ù–µ–¥–æ–ª—ñ–∫–∏ |
|----------|----------|
| ‚úÖ –ù–µ –∑–∞–ª–µ–∂–∏—Ç—å –≤—ñ–¥ magnitude | ‚ùå –í—Ç—Ä–∞—á–∞—î —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ magnitude |
| ‚úÖ –ü—ñ–¥—Ö–æ–¥–∏—Ç—å –¥–ª—è text/sparse data | ‚ùå –ù–µ –º–µ—Ç—Ä–∏–∫–∞ (–Ω–µ –∑–∞–¥–æ–≤–æ–ª—å–Ω—è—î triangle inequality) |
| ‚úÖ –ù–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω–∞ [-1, 1] | ‚ùå –ù–µ –ø—ñ–¥—Ö–æ–¥–∏—Ç—å –¥–ª—è negative values —ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü—ñ—ó |
| ‚úÖ High-dimensional data | |

## –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏?

- ‚úÖ **Text mining** (TF-IDF vectors)
- ‚úÖ **Recommender systems** (user preferences)
- ‚úÖ **Document similarity**
- ‚úÖ **Image retrieval** (feature vectors)
- ‚úÖ –ö–æ–ª–∏ –≤–∞–∂–ª–∏–≤–∏–π –Ω–∞–ø—Ä—è–º–æ–∫, –Ω–µ magnitude
- ‚ùå Geographical data (–≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π Euclidean)

---

# 6. Hamming Distance

## –§–æ—Ä–º—É–ª–∞

$$d_{\text{Hamming}}(\mathbf{x}, \mathbf{y}) = \sum_{i=1}^{n} \mathbb{1}(x_i \neq y_i)$$

–¥–µ $\mathbb{1}$ ‚Äî —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è (1 —è–∫—â–æ true, 0 —è–∫—â–æ false).

## –Ü–Ω—Ç—É—ó—Ü—ñ—è

**Hamming Distance** ‚Äî —Ü–µ **–∫—ñ–ª—å–∫—ñ—Å—Ç—å –ø–æ–∑–∏—Ü—ñ–π**, –¥–µ —Å–∏–º–≤–æ–ª–∏ —Ä—ñ–∑–Ω—ñ. –î–ª—è **binary** –∞–±–æ **categorical** –¥–∞–Ω–∏—Ö.

```
–ü—Ä–∏–∫–ª–∞–¥:

x = [1, 0, 1, 1, 0]
y = [1, 1, 1, 0, 0]
     ‚úì  ‚úó  ‚úì  ‚úó  ‚úì

Hamming distance = 2 (—Ä—ñ–∑–Ω—ñ –≤ 2 –ø–æ–∑–∏—Ü—ñ—è—Ö)
```

## –ö–æ–¥

```python
def hamming_distance(x, y):
    """–û–±—á–∏—Å–ª–∏—Ç–∏ Hamming distance"""
    return np.sum(x != y)

# –ü—Ä–∏–∫–ª–∞–¥ 1: Binary
x = np.array([1, 0, 1, 1, 0])
y = np.array([1, 1, 1, 0, 0])

dist = hamming_distance(x, y)
print(f"Hamming distance: {dist}")

# –ß–µ—Ä–µ–∑ scipy
from scipy.spatial.distance import hamming
# Scipy –ø–æ–≤–µ—Ä—Ç–∞—î normalized (—á–∞—Å—Ç–∫—É —Ä—ñ–∑–Ω–∏—Ö)
dist_scipy = hamming(x, y)
print(f"Scipy (normalized): {dist_scipy:.4f}")
print(f"Scipy (count): {int(dist_scipy * len(x))}")

# –ü—Ä–∏–∫–ª–∞–¥ 2: Strings
s1 = "karolin"
s2 = "kathrin"

# –ü–µ—Ä–µ—Ç–≤–æ—Ä–∏—Ç–∏ –≤ —á–∏—Å–ª–∞
x = np.array([ord(c) for c in s1])
y = np.array([ord(c) for c in s2])

dist = hamming_distance(x, y)
print(f"\nHamming distance ('{s1}' vs '{s2}'): {dist}")
```

## –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è

```python
import matplotlib.pyplot as plt
import numpy as np

# Binary vectors
x = np.array([1, 0, 1, 1, 0, 1, 0, 0])
y = np.array([1, 1, 1, 0, 0, 1, 1, 0])

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
fig, axes = plt.subplots(3, 1, figsize=(12, 8))

# Vector x
axes[0].imshow([x], cmap='gray_r', aspect='auto')
axes[0].set_yticks([0])
axes[0].set_yticklabels(['x'])
axes[0].set_xticks(range(len(x)))
axes[0].set_title('Vector x', fontsize=12, fontweight='bold')

# –î–æ–¥–∞—Ç–∏ –∑–Ω–∞—á–µ–Ω–Ω—è
for i, val in enumerate(x):
    axes[0].text(i, 0, str(val), ha='center', va='center', 
                fontsize=14, fontweight='bold', color='red' if val == 1 else 'blue')

# Vector y
axes[1].imshow([y], cmap='gray_r', aspect='auto')
axes[1].set_yticks([0])
axes[1].set_yticklabels(['y'])
axes[1].set_xticks(range(len(y)))
axes[1].set_title('Vector y', fontsize=12, fontweight='bold')

for i, val in enumerate(y):
    axes[1].text(i, 0, str(val), ha='center', va='center', 
                fontsize=14, fontweight='bold', color='red' if val == 1 else 'blue')

# Differences
diff = x != y
axes[2].imshow([diff], cmap='RdYlGn_r', aspect='auto')
axes[2].set_yticks([0])
axes[2].set_yticklabels(['Diff'])
axes[2].set_xticks(range(len(diff)))
axes[2].set_title(f'Differences (Hamming Distance = {diff.sum()})', 
                 fontsize=12, fontweight='bold')

for i, d in enumerate(diff):
    symbol = '‚úó' if d else '‚úì'
    color = 'red' if d else 'green'
    axes[2].text(i, 0, symbol, ha='center', va='center', 
                fontsize=16, fontweight='bold', color=color)

plt.tight_layout()
plt.show()
```

## –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏?

- ‚úÖ **Binary data** (0/1 features)
- ‚úÖ **Error detection** (–∫–æ–¥–∏ –•–µ–º–º—ñ–Ω–≥–∞)
- ‚úÖ **Categorical variables**
- ‚úÖ **DNA sequences** (ATCG)
- ‚úÖ **Image hashing**
- ‚ùå Continuous data (–≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π Euclidean)

---

# 7. Mahalanobis Distance

## –§–æ—Ä–º—É–ª–∞

$$d_{\text{Mahalanobis}}(\mathbf{x}, \mathbf{y}) = \sqrt{(\mathbf{x} - \mathbf{y})^T \mathbf{S}^{-1} (\mathbf{x} - \mathbf{y})}$$

–¥–µ $\mathbf{S}$ ‚Äî covariance matrix.

## –Ü–Ω—Ç—É—ó—Ü—ñ—è

**Mahalanobis Distance** –≤—Ä–∞—Ö–æ–≤—É—î **–∫–æ—Ä–µ–ª—è—Ü—ñ—ó –º—ñ–∂ –æ–∑–Ω–∞–∫–∞–º–∏** —Ç–∞ **—Ä—ñ–∑–Ω—ñ –º–∞—Å—à—Ç–∞–±–∏**. –¶–µ Euclidean distance —É —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–æ–≤–∞–Ω–æ–º—É –ø—Ä–æ—Å—Ç–æ—Ä—ñ.

```
–Ø–∫—â–æ –æ–∑–Ω–∞–∫–∏ –Ω–µ–∫–æ—Ä–µ–ª—å–æ–≤–∞–Ω—ñ —Ç–∞ –º–∞—é—Ç—å –æ–¥–Ω–∞–∫–æ–≤–∏–π –º–∞—Å—à—Ç–∞–±:
  Mahalanobis = Euclidean

–Ø–∫—â–æ —î –∫–æ—Ä–µ–ª—è—Ü—ñ—ó –∞–±–æ —Ä—ñ–∑–Ω—ñ –º–∞—Å—à—Ç–∞–±–∏:
  Mahalanobis –≤—Ä–∞—Ö–æ–≤—É—î —Ü–µ!
```

## –ö–æ–¥

```python
def mahalanobis_distance(x, y, cov_matrix):
    """
    –û–±—á–∏—Å–ª–∏—Ç–∏ Mahalanobis distance
    
    Parameters:
    -----------
    x, y : array-like
        –í–µ–∫—Ç–æ—Ä–∏
    cov_matrix : array-like
        Covariance matrix
        
    Returns:
    --------
    float : –≤—ñ–¥—Å—Ç–∞–Ω—å
    """
    diff = x - y
    inv_cov = np.linalg.inv(cov_matrix)
    return np.sqrt(diff @ inv_cov @ diff)

# –ü—Ä–∏–∫–ª–∞–¥
np.random.seed(42)

# –ó–≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ –¥–∞–Ω—ñ –∑ –∫–æ—Ä–µ–ª—è—Ü—ñ—è–º–∏
mean = [0, 0]
cov = [[2, 1],   # –ö–æ—Ä–µ–ª—è—Ü—ñ—è –º—ñ–∂ –æ–∑–Ω–∞–∫–∞–º–∏
       [1, 2]]

data = np.random.multivariate_normal(mean, cov, 1000)

# –î–≤—ñ —Ç–æ—á–∫–∏
x = np.array([2, 2])
y = np.array([0, 0])

# Covariance matrix
cov_matrix = np.cov(data.T)

# –í—ñ–¥—Å—Ç–∞–Ω—ñ
euclidean_dist = euclidean_distance(x, y)
mahal_dist = mahalanobis_distance(x, y, cov_matrix)

print(f"Euclidean distance: {euclidean_dist:.4f}")
print(f"Mahalanobis distance: {mahal_dist:.4f}")

# –ß–µ—Ä–µ–∑ scipy
from scipy.spatial.distance import mahalanobis as scipy_mahal
mahal_scipy = scipy_mahal(x, y, np.linalg.inv(cov_matrix))
print(f"Scipy Mahalanobis: {mahal_scipy:.4f}")
```

## –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è

```python
import matplotlib.pyplot as plt
from matplotlib.patches import Ellipse
import numpy as np

# –î–∞–Ω—ñ
np.random.seed(42)
mean = [0, 0]
cov = [[2, 1.5],
       [1.5, 2]]

data = np.random.multivariate_normal(mean, cov, 1000)

# –¢–æ—á–∫–∏ –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è
origin = np.array([0, 0])
point1 = np.array([2, 0])
point2 = np.array([1.5, 1.5])

# –í—ñ–¥—Å—Ç–∞–Ω—ñ
euc1 = euclidean_distance(origin, point1)
euc2 = euclidean_distance(origin, point2)
mahal1 = mahalanobis_distance(origin, point1, cov)
mahal2 = mahalanobis_distance(origin, point2, cov)

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# Subplot 1: Euclidean
axes[0].scatter(data[:, 0], data[:, 1], alpha=0.3, s=10)
axes[0].scatter(*origin, c='red', s=200, marker='*', zorder=5, label='Origin')
axes[0].scatter(*point1, c='blue', s=200, marker='o', zorder=5, 
               label=f'P1: Euc={euc1:.2f}')
axes[0].scatter(*point2, c='green', s=200, marker='s', zorder=5, 
               label=f'P2: Euc={euc2:.2f}')

# Euclidean circles
circle1 = plt.Circle(origin, euc1, fill=False, color='blue', linewidth=2, linestyle='--')
circle2 = plt.Circle(origin, euc2, fill=False, color='green', linewidth=2, linestyle='--')
axes[0].add_patch(circle1)
axes[0].add_patch(circle2)

axes[0].set_xlim(-4, 4)
axes[0].set_ylim(-4, 4)
axes[0].set_aspect('equal')
axes[0].grid(True, alpha=0.3)
axes[0].legend(fontsize=10)
axes[0].set_title('Euclidean Distance\n(Circles)', fontsize=12, fontweight='bold')

# Subplot 2: Mahalanobis
axes[1].scatter(data[:, 0], data[:, 1], alpha=0.3, s=10)
axes[1].scatter(*origin, c='red', s=200, marker='*', zorder=5, label='Origin')
axes[1].scatter(*point1, c='blue', s=200, marker='o', zorder=5, 
               label=f'P1: Mahal={mahal1:.2f}')
axes[1].scatter(*point2, c='green', s=200, marker='s', zorder=5, 
               label=f'P2: Mahal={mahal2:.2f}')

# Mahalanobis ellipses
eigenvalues, eigenvectors = np.linalg.eig(cov)

for distance in [mahal1, mahal2]:
    width = 2 * distance * np.sqrt(eigenvalues[0])
    height = 2 * distance * np.sqrt(eigenvalues[1])
    angle = np.degrees(np.arctan2(eigenvectors[1, 0], eigenvectors[0, 0]))
    
    ellipse = Ellipse(origin, width, height, angle=angle, 
                     fill=False, linewidth=2, linestyle='--',
                     color='blue' if distance == mahal1 else 'green')
    axes[1].add_patch(ellipse)

axes[1].set_xlim(-4, 4)
axes[1].set_ylim(-4, 4)
axes[1].set_aspect('equal')
axes[1].grid(True, alpha=0.3)
axes[1].legend(fontsize=10)
axes[1].set_title('Mahalanobis Distance\n(Ellipses - accounts for correlation)', 
                 fontsize=12, fontweight='bold')

plt.tight_layout()
plt.show()

print(f"\nPoint 1: Euclidean={euc1:.2f}, Mahalanobis={mahal1:.2f}")
print(f"Point 2: Euclidean={euc2:.2f}, Mahalanobis={mahal2:.2f}")
print(f"\n–ó–∞ Euclidean: —Ç–æ—á–∫–∏ –Ω–∞ –æ–¥–Ω–∞–∫–æ–≤—ñ–π –≤—ñ–¥—Å—Ç–∞–Ω—ñ")
print(f"–ó–∞ Mahalanobis: –≤—Ä–∞—Ö–æ–≤—É—î—Ç—å—Å—è –∫–æ—Ä–µ–ª—è—Ü—ñ—è –¥–∞–Ω–∏—Ö")
```

## –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏?

- ‚úÖ **Anomaly detection** (–≤–∏–∫–∏–¥–∏)
- ‚úÖ –ö–æ–ª–∏ –æ–∑–Ω–∞–∫–∏ –∫–æ—Ä–µ–ª—å–æ–≤–∞–Ω—ñ
- ‚úÖ –ö–æ–ª–∏ –æ–∑–Ω–∞–∫–∏ –º–∞—é—Ç—å —Ä—ñ–∑–Ω—ñ –º–∞—Å—à—Ç–∞–±–∏
- ‚úÖ **Multivariate statistics**
- ‚ùå –ú–∞–ª—ñ –¥–∞—Ç–∞—Å–µ—Ç–∏ (covariance –Ω–µ—Å—Ç–∞–±—ñ–ª—å–Ω–∞)
- ‚ùå High dimensions (invertible covariance)

---

# –Ü–Ω—à—ñ –º–µ—Ç—Ä–∏–∫–∏

## Jaccard Distance

–î–ª—è **–º–Ω–æ–∂–∏–Ω**.

$$J(\mathbf{A}, \mathbf{B}) = \frac{|\mathbf{A} \cap \mathbf{B}|}{|\mathbf{A} \cup \mathbf{B}|}$$

$$d_{\text{Jaccard}} = 1 - J$$

```python
from scipy.spatial.distance import jaccard

# Binary vectors
x = np.array([1, 1, 0, 0, 1])
y = np.array([1, 0, 0, 1, 1])

dist = jaccard(x, y)
print(f"Jaccard distance: {dist:.4f}")

# –î–ª—è –º–Ω–æ–∂–∏–Ω
set1 = {1, 2, 3, 4}
set2 = {3, 4, 5, 6}

intersection = len(set1 & set2)
union = len(set1 | set2)
jaccard_sim = intersection / union
jaccard_dist = 1 - jaccard_sim

print(f"Jaccard similarity: {jaccard_sim:.4f}")
print(f"Jaccard distance: {jaccard_dist:.4f}")
```

## Haversine Distance

–î–ª—è **–≥–µ–æ–≥—Ä–∞—Ñ—ñ—á–Ω–∏—Ö –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç** (lat/lon).

```python
from math import radians, sin, cos, sqrt, atan2

def haversine_distance(lat1, lon1, lat2, lon2):
    """
    –û–±—á–∏—Å–ª–∏—Ç–∏ –≤—ñ–¥—Å—Ç–∞–Ω—å –º—ñ–∂ –¥–≤–æ–º–∞ —Ç–æ—á–∫–∞–º–∏ –Ω–∞ –ó–µ–º–ª—ñ
    
    Returns: –≤—ñ–¥—Å—Ç–∞–Ω—å –≤ –∫—ñ–ª–æ–º–µ—Ç—Ä–∞—Ö
    """
    R = 6371  # –†–∞–¥—ñ—É—Å –ó–µ–º–ª—ñ –≤ –∫–º
    
    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])
    
    dlat = lat2 - lat1
    dlon = lon2 - lon1
    
    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2
    c = 2 * atan2(sqrt(a), sqrt(1-a))
    
    return R * c

# –ü—Ä–∏–∫–ª–∞–¥: –ö–∏—ó–≤ ‚Üí –õ—å–≤—ñ–≤
kiev = (50.4501, 30.5234)
lviv = (49.8397, 24.0297)

dist = haversine_distance(*kiev, *lviv)
print(f"–í—ñ–¥—Å—Ç–∞–Ω—å –ö–∏—ó–≤-–õ—å–≤—ñ–≤: {dist:.2f} –∫–º")
```

## Edit Distance (Levenshtein)

–î–ª—è **—Ä—è–¥–∫—ñ–≤** (strings).

```python
def levenshtein_distance(s1, s2):
    """–û–±—á–∏—Å–ª–∏—Ç–∏ Edit Distance –º—ñ–∂ –¥–≤–æ–º–∞ —Ä—è–¥–∫–∞–º–∏"""
    m, n = len(s1), len(s2)
    
    # DP table
    dp = [[0] * (n + 1) for _ in range(m + 1)]
    
    # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è
    for i in range(m + 1):
        dp[i][0] = i
    for j in range(n + 1):
        dp[0][j] = j
    
    # –ó–∞–ø–æ–≤–Ω–∏—Ç–∏ —Ç–∞–±–ª–∏—Ü—é
    for i in range(1, m + 1):
        for j in range(1, n + 1):
            if s1[i-1] == s2[j-1]:
                dp[i][j] = dp[i-1][j-1]
            else:
                dp[i][j] = 1 + min(
                    dp[i-1][j],     # Deletion
                    dp[i][j-1],     # Insertion
                    dp[i-1][j-1]    # Substitution
                )
    
    return dp[m][n]

# –ü—Ä–∏–∫–ª–∞–¥
s1 = "kitten"
s2 = "sitting"

dist = levenshtein_distance(s1, s2)
print(f"Edit distance ('{s1}' ‚Üí '{s2}'): {dist}")

# –ß–µ—Ä–µ–∑ python-Levenshtein
try:
    import Levenshtein
    dist_lib = Levenshtein.distance(s1, s2)
    print(f"Library distance: {dist_lib}")
except ImportError:
    print("Install: pip install python-Levenshtein")
```

---

## –ü–æ—Ä—ñ–≤–Ω—è–ª—å–Ω–∞ —Ç–∞–±–ª–∏—Ü—è

| –ú–µ—Ç—Ä–∏–∫–∞ | –§–æ—Ä–º—É–ª–∞ | –î–∞–Ω—ñ | Complexity | –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ |
|---------|---------|------|------------|---------------------|
| **Euclidean** | $\|\|\mathbf{x}-\mathbf{y}\|\|_2$ | Continuous | O(n) | Geographical, general |
| **Manhattan** | $\|\|\mathbf{x}-\mathbf{y}\|\|_1$ | Continuous | O(n) | Grid problems, high-dim |
| **Cosine** | $1 - \frac{\mathbf{x} \cdot \mathbf{y}}{\|\|\mathbf{x}\|\| \|\|\mathbf{y}\|\|}$ | Continuous | O(n) | Text, sparse data |
| **Hamming** | $\sum \mathbb{1}(x_i \neq y_i)$ | Binary/Categorical | O(n) | Binary data, DNA |
| **Mahalanobis** | $\sqrt{(\mathbf{x}-\mathbf{y})^T\mathbf{S}^{-1}(\mathbf{x}-\mathbf{y})}$ | Continuous | O(n¬≥) | Correlated features |
| **Jaccard** | $1 - \frac{\|A \cap B\|}{\|A \cup B\|}$ | Sets | O(n) | Sets, recommenders |
| **Levenshtein** | Edit operations | Strings | O(mn) | Text similarity |

---

## –ü—Ä–∞–∫—Ç–∏—á–Ω—ñ –ø–æ—Ä–∞–¥–∏ üí°

### 1. –ó–∞–≤–∂–¥–∏ –Ω–æ—Ä–º–∞–ª—ñ–∑—É–π –¥–∞–Ω—ñ

```python
from sklearn.preprocessing import StandardScaler

# ‚ùå –ü–û–ì–ê–ù–û: —Ä—ñ–∑–Ω—ñ –º–∞—Å—à—Ç–∞–±–∏
X = np.array([[1, 1000],
              [2, 2000],
              [3, 1500]])

# Euclidean –±—É–¥–µ dominated –¥—Ä—É–≥–æ—é –æ–∑–Ω–∞–∫–æ—é!
dist = euclidean_distance(X[0], X[1])
print(f"Without scaling: {dist:.2f}")  # ~1000 (dominated by 2nd feature)

# ‚úÖ –î–û–ë–†–ï: –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

dist_scaled = euclidean_distance(X_scaled[0], X_scaled[1])
print(f"With scaling: {dist_scaled:.2f}")  # –ó–±–∞–ª–∞–Ω—Å–æ–≤–∞–Ω–æ
```

### 2. –í–∏–±—ñ—Ä –º–µ—Ç—Ä–∏–∫–∏ –∑–∞–ª–µ–∂–∏—Ç—å –≤—ñ–¥ –¥–∞–Ω–∏—Ö

```python
# Continuous numeric ‚Üí Euclidean –∞–±–æ Manhattan
# Text/Sparse ‚Üí Cosine
# Binary ‚Üí Hamming
# Sets ‚Üí Jaccard
# Geographic ‚Üí Haversine
# Strings ‚Üí Levenshtein
```

### 3. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π scipy/sklearn

```python
from scipy.spatial.distance import pdist, squareform

# –ú–∞—Ç—Ä–∏—Ü—è –≤—ñ–¥—Å—Ç–∞–Ω–µ–π –¥–ª—è –±–∞–≥–∞—Ç—å–æ—Ö —Ç–æ—á–æ–∫
X = np.random.randn(5, 3)

# Euclidean
dist_matrix = squareform(pdist(X, metric='euclidean'))
print("Distance matrix:")
print(dist_matrix)

# –ê–±–æ —á–µ—Ä–µ–∑ sklearn
from sklearn.metrics.pairwise import pairwise_distances

dist_matrix_sklearn = pairwise_distances(X, metric='euclidean')
```

### 4. KNN: –≤–∏–±—ñ—Ä –º–µ—Ç—Ä–∏–∫–∏

```python
from sklearn.neighbors import KNeighborsClassifier
from sklearn.datasets import make_classification
from sklearn.model_selection import cross_val_score

X, y = make_classification(n_samples=1000, n_features=20, random_state=42)

# –¢–µ—Å—Ç—É—î–º–æ —Ä—ñ–∑–Ω—ñ –º–µ—Ç—Ä–∏–∫–∏
metrics = ['euclidean', 'manhattan', 'chebyshev', 'cosine']

for metric in metrics:
    knn = KNeighborsClassifier(n_neighbors=5, metric=metric)
    scores = cross_val_score(knn, X, y, cv=5)
    print(f"{metric:12s}: {scores.mean():.4f} (+/- {scores.std():.4f})")
```

---

## –†–µ–∞–ª—å–Ω–∏–π –ø—Ä–∏–∫–ª–∞–¥: KNN –∑ —Ä—ñ–∑–Ω–∏–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report

print("="*70)
print("KNN WITH DIFFERENT DISTANCE METRICS")
print("="*70)

# –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –¥–∞–Ω–∏—Ö
X, y = make_classification(n_samples=500, n_features=2, n_informative=2,
                          n_redundant=0, n_clusters_per_class=1,
                          random_state=42)

# Train/Test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# Scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# –†—ñ–∑–Ω—ñ –º–µ—Ç—Ä–∏–∫–∏
metrics = {
    'Euclidean': 'euclidean',
    'Manhattan': 'manhattan',
    'Chebyshev': 'chebyshev',
    'Cosine': 'cosine'
}

results = []

for name, metric in metrics.items():
    knn = KNeighborsClassifier(n_neighbors=5, metric=metric)
    knn.fit(X_train_scaled, y_train)
    
    y_pred = knn.predict(X_test_scaled)
    accuracy = accuracy_score(y_test, y_pred)
    
    results.append({
        'Metric': name,
        'Accuracy': accuracy
    })
    
    print(f"\n{name}:")
    print(f"  Accuracy: {accuracy:.4f}")

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
fig, axes = plt.subplots(2, 2, figsize=(14, 12))
axes = axes.ravel()

for idx, (name, metric) in enumerate(metrics.items()):
    # –ú–æ–¥–µ–ª—å
    knn = KNeighborsClassifier(n_neighbors=5, metric=metric)
    knn.fit(X_train_scaled, y_train)
    
    # –°—Ç–≤–æ—Ä–∏—Ç–∏ mesh
    x_min, x_max = X_train_scaled[:, 0].min() - 1, X_train_scaled[:, 0].max() + 1
    y_min, y_max = X_train_scaled[:, 1].min() - 1, X_train_scaled[:, 1].max() + 1
    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),
                         np.linspace(y_min, y_max, 100))
    
    # –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    
    # –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è decision boundary
    axes[idx].contourf(xx, yy, Z, alpha=0.3, cmap='RdYlBu')
    axes[idx].scatter(X_train_scaled[:, 0], X_train_scaled[:, 1], 
                     c=y_train, cmap='RdYlBu', edgecolors='k', s=50, alpha=0.7)
    axes[idx].set_title(f'{name}\nAccuracy: {results[idx]["Accuracy"]:.4f}', 
                       fontsize=12, fontweight='bold')
    axes[idx].set_xlabel('Feature 1')
    axes[idx].set_ylabel('Feature 2')

plt.tight_layout()
plt.show()

# –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è
print("\n" + "="*70)
print("SUMMARY")
print("="*70)

import pandas as pd
df_results = pd.DataFrame(results)
df_results = df_results.sort_values('Accuracy', ascending=False)
print(df_results.to_string(index=False))

print("="*70)
```

---

## –ü–æ–≤'—è–∑–∞–Ω—ñ —Ç–µ–º–∏

- [[08_Similarity_Measures]] ‚Äî –º–µ—Ç—Ä–∏–∫–∏ —Å—Ö–æ–∂–æ—Å—Ç—ñ (complement)
- [[KNN]] ‚Äî –æ—Å–Ω–æ–≤–Ω–µ –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è distance metrics
- [[K_Means]] ‚Äî –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è Euclidean distance
- [[Anomaly_Detection]] ‚Äî Mahalanobis distance
- [[Text_Mining]] ‚Äî Cosine distance –¥–ª—è TF-IDF

## –†–µ—Å—É—Ä—Å–∏

- [Scipy Distance Metrics](https://docs.scipy.org/doc/scipy/reference/spatial.distance.html)
- [Sklearn Metrics](https://scikit-learn.org/stable/modules/metrics.html)
- [Distance Metrics Tutorial](https://machinelearningmastery.com/distance-measures-for-machine-learning/)

---

## –ö–ª—é—á–æ–≤—ñ –≤–∏—Å–Ω–æ–≤–∫–∏

> Distance Metrics –≤–∏–º—ñ—Ä—é—é—Ç—å –Ω–µ—Å—Ö–æ–∂—ñ—Å—Ç—å –º—ñ–∂ –æ–±'—î–∫—Ç–∞–º–∏. –í–∏–±—ñ—Ä –ø—Ä–∞–≤–∏–ª—å–Ω–æ—ó –º–µ—Ç—Ä–∏–∫–∏ –∑–∞–ª–µ–∂–∏—Ç—å –≤—ñ–¥ —Ç–∏–ø—É –¥–∞–Ω–∏—Ö —Ç–∞ –∑–∞–¥–∞—á—ñ.

**–û—Å–Ω–æ–≤–Ω—ñ –º–µ—Ç—Ä–∏–∫–∏:**
- **Euclidean** ‚Äî –ø—Ä—è–º–∞ –ª—ñ–Ω—ñ—è (default –¥–ª—è continuous)
- **Manhattan** ‚Äî —Ä—É—Ö –≤–∑–¥–æ–≤–∂ –æ—Å–µ–π (grid problems)
- **Cosine** ‚Äî –∫—É—Ç –º—ñ–∂ –≤–µ–∫—Ç–æ—Ä–∞–º–∏ (text, direction –≤–∞–∂–ª–∏–≤—ñ—à–∏–π –∑–∞ magnitude)
- **Hamming** ‚Äî –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ä—ñ–∑–Ω–∏—Ö –ø–æ–∑–∏—Ü—ñ–π (binary/categorical)
- **Mahalanobis** ‚Äî –≤—Ä–∞—Ö–æ–≤—É—î –∫–æ—Ä–µ–ª—è—Ü—ñ—ó (anomaly detection)

**–í–∏–±—ñ—Ä –º–µ—Ç—Ä–∏–∫–∏:**
```
Continuous numeric    ‚Üí Euclidean, Manhattan
Text / Sparse data    ‚Üí Cosine
Binary data           ‚Üí Hamming, Jaccard
Geographic coords     ‚Üí Haversine
Strings               ‚Üí Levenshtein
Correlated features   ‚Üí Mahalanobis
```

**–í–∞–∂–ª–∏–≤–æ:**
- –ó–∞–≤–∂–¥–∏ **–Ω–æ—Ä–º–∞–ª—ñ–∑—É–π** –¥–∞–Ω—ñ –ø–µ—Ä–µ–¥ –æ–±—á–∏—Å–ª–µ–Ω–Ω—è–º –≤—ñ–¥—Å—Ç–∞–Ω—ñ
- –†—ñ–∑–Ω—ñ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö **—Ç–∏–ø—ñ–≤ –¥–∞–Ω–∏—Ö**
- Euclidean —á—É—Ç–ª–∏–≤–∞ –¥–æ **–º–∞—Å—à—Ç–∞–±—É**
- Cosine **–Ω–µ –∑–∞–ª–µ–∂–∏—Ç—å** –≤—ñ–¥ magnitude

**–§–æ—Ä–º—É–ª–∏ –¥–ª—è –∑–∞–ø–∞–º'—è—Ç–æ–≤—É–≤–∞–Ω–Ω—è:**
```
Euclidean:  ‚àöŒ£(x·µ¢ - y·µ¢)¬≤
Manhattan:  Œ£|x·µ¢ - y·µ¢|
Cosine:     1 - (x¬∑y)/(||x|| ||y||)
Hamming:    Œ£ ùüô(x·µ¢ ‚â† y·µ¢)
```

---

#ml #distance-metrics #euclidean #manhattan #cosine #hamming #mahalanobis #knn #clustering
