# Cross-Validation (–ö—Ä–æ—Å-–≤–∞–ª—ñ–¥–∞—Ü—ñ—è)

## –©–æ —Ü–µ?

**Cross-Validation (CV)** ‚Äî —Ü–µ —Ç–µ—Ö–Ω—ñ–∫–∞ **–æ—Ü—ñ–Ω–∫–∏ –º–æ–¥–µ–ª—ñ**, —è–∫–∞ —Ä–æ–∑–¥—ñ–ª—è—î –¥–∞—Ç–∞—Å–µ—Ç –Ω–∞ **–∫—ñ–ª—å–∫–∞ —á–∞—Å—Ç–∏–Ω** (folds), –Ω–∞–≤—á–∞—î –º–æ–¥–µ–ª—å –Ω–∞ –æ–¥–Ω–∏—Ö —á–∞—Å—Ç–∏–Ω–∞—Ö —Ç–∞ —Ç–µ—Å—Ç—É—î –Ω–∞ —ñ–Ω—à–∏—Ö, **–ø–æ–≤—Ç–æ—Ä—é—é—á–∏ –ø—Ä–æ—Ü–µ—Å** –∫—ñ–ª—å–∫–∞ —Ä–∞–∑—ñ–≤.

**–ì–æ–ª–æ–≤–Ω–∞ —ñ–¥–µ—è:** –∑–∞–º—ñ—Å—Ç—å –æ–¥–Ω–æ–≥–æ —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –Ω–∞ train/test, —Ä–æ–±–∏–º–æ **–±–∞–≥–∞—Ç–æ —Ä–æ–∑–¥—ñ–ª–µ–Ω—å** —ñ —É—Å–µ—Ä–µ–¥–Ω—é—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏. –¶–µ –¥–∞—î **–±—ñ–ª—å—à –Ω–∞–¥—ñ–π–Ω—É –æ—Ü—ñ–Ω–∫—É** –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ –º–æ–¥–µ–ª—ñ.

## –ù–∞–≤—ñ—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ?

- üéØ **–ù–∞–¥—ñ–π–Ω—ñ—à–∞ –æ—Ü—ñ–Ω–∫–∞** ‚Äî –º–µ–Ω—à–∞ –∑–∞–ª–µ–∂–Ω—ñ—Å—Ç—å –≤—ñ–¥ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è
- üìä **–û—Ü—ñ–Ω–∫–∞ variance** ‚Äî —è–∫ —Å—Ç–∞–±—ñ–ª—å–Ω–∞ –º–æ–¥–µ–ª—å
- üîç **–í–∏—è–≤–ª–µ–Ω–Ω—è overfitting** ‚Äî –∫—Ä–∞—â–µ, –Ω—ñ–∂ –æ–¥–∏–Ω train/test split
- ‚öôÔ∏è **Hyperparameter tuning** ‚Äî GridSearchCV, RandomizedSearchCV
- üí° **–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –¥–∞–Ω–∏—Ö** ‚Äî –≤—Å—ñ –∑—Ä–∞–∑–∫–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—Ç—å—Å—è –¥–ª—è train —ñ test
- üìâ **–ó–º–µ–Ω—à–µ–Ω–Ω—è bias –æ—Ü—ñ–Ω–∫–∏** ‚Äî –æ—Å–æ–±–ª–∏–≤–æ –Ω–∞ –º–∞–ª–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö

## –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏?

**–ü–æ—Ç—Ä—ñ–±–Ω–æ:**

- –ú–∞–ª—ñ —Ç–∞ —Å–µ—Ä–µ–¥–Ω—ñ –¥–∞—Ç–∞—Å–µ—Ç–∏ (< 100k –∑—Ä–∞–∑–∫—ñ–≤)
- Hyperparameter tuning
- –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –º–æ–¥–µ–ª–µ–π
- –ö–æ–ª–∏ –ø–æ—Ç—Ä—ñ–±–Ω–∞ **–Ω–∞–¥—ñ–π–Ω–∞ –æ—Ü—ñ–Ω–∫–∞** –ø–µ—Ä–µ–¥ production

**–ù–µ –ø–æ—Ç—Ä—ñ–±–Ω–æ:**

- –î—É–∂–µ –≤–µ–ª–∏–∫—ñ –¥–∞—Ç–∞—Å–µ—Ç–∏ (> 1M) ‚Äî –∑–∞–Ω–∞–¥—Ç–æ –ø–æ–≤—ñ–ª—å–Ω–æ
- Time series (–≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π TimeSeriesSplit)
- –ö–æ–ª–∏ —î –æ–∫—Ä–µ–º–∏–π –≤–µ–ª–∏–∫–∏–π test set

---

## K-Fold Cross-Validation

### –Ø–∫ –ø—Ä–∞—Ü—é—î?

```
Dataset —Ä–æ–∑–¥—ñ–ª—è—î—Ç—å—Å—è –Ω–∞ K —á–∞—Å—Ç–∏–Ω (folds):

‚ïî‚ïê‚ïê‚ïê‚ïê‚ï¶‚ïê‚ïê‚ïê‚ïê‚ï¶‚ïê‚ïê‚ïê‚ïê‚ï¶‚ïê‚ïê‚ïê‚ïê‚ï¶‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë F1 ‚ïë F2 ‚ïë F3 ‚ïë F4 ‚ïë F5 ‚ïë  K = 5 folds
‚ïö‚ïê‚ïê‚ïê‚ïê‚ï©‚ïê‚ïê‚ïê‚ïê‚ï©‚ïê‚ïê‚ïê‚ïê‚ï©‚ïê‚ïê‚ïê‚ïê‚ï©‚ïê‚ïê‚ïê‚ïê‚ïù

Fold 1: [TEST][TRAIN][TRAIN][TRAIN][TRAIN]
Fold 2: [TRAIN][TEST][TRAIN][TRAIN][TRAIN]
Fold 3: [TRAIN][TRAIN][TEST][TRAIN][TRAIN]
Fold 4: [TRAIN][TRAIN][TRAIN][TEST][TRAIN]
Fold 5: [TRAIN][TRAIN][TRAIN][TRAIN][TEST]

–ö–æ–∂–µ–Ω fold –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è —è–∫ test —Ä—ñ–≤–Ω–æ 1 —Ä–∞–∑
–í—Å—ñ —ñ–Ω—à—ñ folds ‚Äî train set

Final Score = Average(Fold1, Fold2, Fold3, Fold4, Fold5)
```

### –ë–∞–∑–æ–≤–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è

```python
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import load_breast_cancer
import numpy as np

# –î–∞–Ω—ñ
cancer = load_breast_cancer()
X = cancer.data
y = cancer.target

# –ú–æ–¥–µ–ª—å
model = LogisticRegression(max_iter=10000, random_state=42)

# 5-Fold Cross-Validation
scores = cross_val_score(
    model,           # –ú–æ–¥–µ–ª—å
    X, y,           # –î–∞–Ω—ñ
    cv=5,           # –ö—ñ–ª—å–∫—ñ—Å—Ç—å folds
    scoring='accuracy'  # –ú–µ—Ç—Ä–∏–∫–∞
)

print(f"CV Scores: {scores}")
print(f"Mean: {scores.mean():.4f}")
print(f"Std: {scores.std():.4f}")
print(f"95% CI: [{scores.mean() - 2*scores.std():.4f}, "
      f"{scores.mean() + 2*scores.std():.4f}]")

# –ü—Ä–∏–∫–ª–∞–¥ –≤–∏–≤–æ–¥—É:
# CV Scores: [0.956 0.965 0.973 0.964 0.973]
# Mean: 0.9662
# Std: 0.0065
# 95% CI: [0.9532, 0.9792]
```

### Manual K-Fold

```python
from sklearn.model_selection import KFold

# K-Fold splitter
kf = KFold(n_splits=5, shuffle=True, random_state=42)

scores = []

for fold, (train_idx, test_idx) in enumerate(kf.split(X), 1):
    # –†–æ–∑–¥—ñ–ª–∏—Ç–∏ –¥–∞–Ω—ñ
    X_train, X_test = X[train_idx], X[test_idx]
    y_train, y_test = y[train_idx], y[test_idx]
    
    # –ù–∞–≤—á–∏—Ç–∏ –º–æ–¥–µ–ª—å
    model = LogisticRegression(max_iter=10000, random_state=42)
    model.fit(X_train, y_train)
    
    # –û—Ü—ñ–Ω–∏—Ç–∏
    score = model.score(X_test, y_test)
    scores.append(score)
    
    print(f"Fold {fold}: {score:.4f}")

print(f"\nMean: {np.mean(scores):.4f}")
print(f"Std: {np.std(scores):.4f}")
```

### –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ KFold

```python
kf = KFold(
    n_splits=5,        # –ö—ñ–ª—å–∫—ñ—Å—Ç—å folds (–∑–∞–∑–≤–∏—á–∞–π 5 –∞–±–æ 10)
    shuffle=True,      # –ü–µ—Ä–µ–º—ñ—à–∞—Ç–∏ –ø–µ—Ä–µ–¥ —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è–º
    random_state=42    # Seed –¥–ª—è –≤—ñ–¥—Ç–≤–æ—Ä—é–≤–∞–Ω–æ—Å—Ç—ñ
)

# shuffle=True ‚Äî –í–ê–ñ–õ–ò–í–û!
# –ë–µ–∑ shuffle, —è–∫—â–æ –¥–∞–Ω—ñ –≤—ñ–¥—Å–æ—Ä—Ç–æ–≤–∞–Ω—ñ ‚Üí –ø–æ–≥–∞–Ω—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
```

---

## Stratified K-Fold (–¥–ª—è –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó)

### –ù–∞–≤—ñ—â–æ?

**–ü—Ä–æ–±–ª–µ–º–∞:** –Ω–µ—Å–±–∞–ª–∞–Ω—Å–æ–≤–∞–Ω—ñ –∫–ª–∞—Å–∏ –º–æ–∂—É—Ç—å —Ä–æ–∑–ø–æ–¥—ñ–ª—è—Ç–∏—Å—è –Ω–µ—Ä—ñ–≤–Ω–æ–º—ñ—Ä–Ω–æ –ø–æ folds.

```python
# –î–∞–Ω—ñ: 90% –∫–ª–∞—Å 0, 10% –∫–ª–∞—Å 1
y = np.array([0]*900 + [1]*100)

# –ó–≤–∏—á–∞–π–Ω–∏–π KFold
kf = KFold(n_splits=5, shuffle=True, random_state=42)

for fold, (train_idx, test_idx) in enumerate(kf.split(X), 1):
    y_test = y[test_idx]
    class_dist = np.bincount(y_test) / len(y_test)
    print(f"Fold {fold}: {class_dist}")

# –ú–æ–∂–ª–∏–≤–∏–π –≤–∏–≤—ñ–¥:
# Fold 1: [0.85, 0.15]  ‚Üê 85% –∫–ª–∞—Å 0, 15% –∫–ª–∞—Å 1
# Fold 2: [0.93, 0.07]  ‚Üê 93% –∫–ª–∞—Å 0, 7% –∫–ª–∞—Å 1
# –ù–µ–∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ! ‚ùå
```

### ‚úÖ Stratified K-Fold

**–ó–±–µ—Ä—ñ–≥–∞—î –ø—Ä–æ–ø–æ—Ä—Ü—ñ—ó –∫–ª–∞—Å—ñ–≤ —É –∫–æ–∂–Ω–æ–º—É fold**.

```python
from sklearn.model_selection import StratifiedKFold

skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

for fold, (train_idx, test_idx) in enumerate(skf.split(X, y), 1):
    y_test = y[test_idx]
    class_dist = np.bincount(y_test) / len(y_test)
    print(f"Fold {fold}: {class_dist}")

# –í–∏–≤—ñ–¥:
# Fold 1: [0.90, 0.10]  ‚Üê –¢–æ—á–Ω–æ —è–∫ –≤ –æ—Ä–∏–≥—ñ–Ω–∞–ª—ñ!
# Fold 2: [0.90, 0.10]
# Fold 3: [0.90, 0.10]
# Fold 4: [0.90, 0.10]
# Fold 5: [0.90, 0.10]
# –ö–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ! ‚úì
```

### –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –∑ cross_val_score

```python
from sklearn.model_selection import cross_val_score

# –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î StratifiedKFold –¥–ª—è classification
scores = cross_val_score(
    model, X, y,
    cv=5,  # –î–ª—è classification ‚Üí StratifiedKFold –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ
    scoring='accuracy'
)

# –ê–±–æ —è–≤–Ω–æ:
from sklearn.model_selection import StratifiedKFold
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

scores = cross_val_score(
    model, X, y,
    cv=skf,  # –ü–µ—Ä–µ–¥–∞—Ç–∏ –æ–±'—î–∫—Ç —è–≤–Ω–æ
    scoring='accuracy'
)
```

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—è:** –¥–ª—è –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó **–∑–∞–≤–∂–¥–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π StratifiedKFold**!

---

## Leave-One-Out Cross-Validation (LOOCV)

### –©–æ —Ü–µ?

**K = n** (–¥–µ n ‚Äî –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∑—Ä–∞–∑–∫—ñ–≤). –ö–æ–∂–µ–Ω –∑—Ä–∞–∑–æ–∫ –ø–æ —á–µ—Ä–∑—ñ —î test set.

```
Dataset: 10 –∑—Ä–∞–∑–∫—ñ–≤

Fold 1:  [TEST][TRAIN][TRAIN][TRAIN][TRAIN][TRAIN][TRAIN][TRAIN][TRAIN][TRAIN]
Fold 2:  [TRAIN][TEST][TRAIN][TRAIN][TRAIN][TRAIN][TRAIN][TRAIN][TRAIN][TRAIN]
Fold 3:  [TRAIN][TRAIN][TEST][TRAIN][TRAIN][TRAIN][TRAIN][TRAIN][TRAIN][TRAIN]
...
Fold 10: [TRAIN][TRAIN][TRAIN][TRAIN][TRAIN][TRAIN][TRAIN][TRAIN][TRAIN][TEST]

10 —ñ—Ç–µ—Ä–∞—Ü—ñ–π –¥–ª—è 10 –∑—Ä–∞–∑–∫—ñ–≤
```

### –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è

```python
from sklearn.model_selection import LeaveOneOut, cross_val_score

loo = LeaveOneOut()

# –î–ª—è –º–∞–ª–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç—É
X_small = X[:100]  # –¢—ñ–ª—å–∫–∏ 100 –∑—Ä–∞–∑–∫—ñ–≤
y_small = y[:100]

scores = cross_val_score(
    model, X_small, y_small,
    cv=loo,  # LOOCV
    scoring='accuracy'
)

print(f"Number of folds: {len(scores)}")  # 100
print(f"Mean accuracy: {scores.mean():.4f}")
print(f"Std: {scores.std():.4f}")
```

### –ü–µ—Ä–µ–≤–∞–≥–∏ —Ç–∞ –Ω–µ–¥–æ–ª—ñ–∫–∏

| –ü–µ—Ä–µ–≤–∞–≥–∏ | –ù–µ–¥–æ–ª—ñ–∫–∏ |
|----------|----------|
| ‚úÖ –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –¥–∞–Ω–∏—Ö | ‚ùå **–î—É–∂–µ –ø–æ–≤—ñ–ª—å–Ω–æ** (n —ñ—Ç–µ—Ä–∞—Ü—ñ–π) |
| ‚úÖ –î–µ—Ç–µ—Ä–º—ñ–Ω–æ–≤–∞–Ω–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç | ‚ùå –í–∏—Å–æ–∫–∞ variance –æ—Ü—ñ–Ω–∫–∏ |
| ‚úÖ –ü—ñ–¥—Ö–æ–¥–∏—Ç—å –¥–ª—è –¥—É–∂–µ –º–∞–ª–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç—ñ–≤ | ‚ùå –ù–µ –ø—Ä–∞—Ü—é—î –¥–ª—è –≤–µ–ª–∏–∫–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç—ñ–≤ |

**–ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏:**
- –î—É–∂–µ –º–∞–ª–∏–π –¥–∞—Ç–∞—Å–µ—Ç (< 100 –∑—Ä–∞–∑–∫—ñ–≤)
- –ö–æ–ª–∏ computational cost –Ω–µ –≤–∞–∂–ª–∏–≤–∏–π

**–ö–æ–ª–∏ –ù–ï –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏:**
- –°–µ—Ä–µ–¥–Ω—ñ —Ç–∞ –≤–µ–ª–∏–∫—ñ –¥–∞—Ç–∞—Å–µ—Ç–∏ (> 100)
- –ö–æ–ª–∏ –ø–æ—Ç—Ä—ñ–±–Ω–∞ —à–≤–∏–¥–∫—ñ—Å—Ç—å

---

## Time Series Cross-Validation

### –ß–æ–º—É –Ω–µ K-Fold –¥–ª—è time series?

```python
# ‚ùå –ü–û–ì–ê–ù–û: –∑–≤–∏—á–∞–π–Ω–∏–π K-Fold –¥–ª—è time series
# –ü–æ—Ä—É—à—É—î —á–∞—Å–æ–≤—É –ø–æ—Å–ª—ñ–¥–æ–≤–Ω—ñ—Å—Ç—å!

Dates: [Jan, Feb, Mar, Apr, May, Jun, Jul, Aug, Sep, Oct]

K-Fold –º–æ–∂–µ –¥–∞—Ç–∏:
Train: [Feb, Apr, Jun, Jul, Sep, Oct]
Test:  [Jan, Mar, May, Aug]

–ú–æ–¥–µ–ª—å –Ω–∞–≤—á–∞—î—Ç—å—Å—è –Ω–∞ –º–∞–π–±—É—Ç–Ω—å–æ–º—É (Oct) 
—ñ —Ç–µ—Å—Ç—É—î—Ç—å—Å—è –Ω–∞ –º–∏–Ω—É–ª–æ–º—É (Jan)! ‚ùå
```

### ‚úÖ TimeSeriesSplit

**Expanding window:** train set —Ä–æ—Å—Ç–µ, test –∑–∞–≤–∂–¥–∏ –≤ –º–∞–π–±—É—Ç–Ω—å–æ–º—É.

```
Dataset: 10 –º—ñ—Å—è—Ü—ñ–≤

Fold 1: [TRAIN][TRAIN] [TEST]
Fold 2: [TRAIN][TRAIN][TRAIN] [TEST]
Fold 3: [TRAIN][TRAIN][TRAIN][TRAIN] [TEST]
Fold 4: [TRAIN][TRAIN][TRAIN][TRAIN][TRAIN] [TEST]
Fold 5: [TRAIN][TRAIN][TRAIN][TRAIN][TRAIN][TRAIN] [TEST]

Train set —Ä–æ—Å—Ç–µ
Test –∑–∞–≤–∂–¥–∏ –ü–Ü–°–õ–Ø train (–≤ –º–∞–π–±—É—Ç–Ω—å–æ–º—É)
```

### –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è

```python
from sklearn.model_selection import TimeSeriesSplit
import numpy as np

# Time series –¥–∞–Ω—ñ (–≤—ñ–¥—Å–æ—Ä—Ç–æ–≤–∞–Ω—ñ –∑–∞ —á–∞—Å–æ–º!)
n_samples = 100
X = np.random.randn(n_samples, 5)
y = np.random.randn(n_samples)

tscv = TimeSeriesSplit(n_splits=5)

for fold, (train_idx, test_idx) in enumerate(tscv.split(X), 1):
    print(f"\nFold {fold}:")
    print(f"  Train: indices {train_idx[0]} to {train_idx[-1]} "
          f"({len(train_idx)} samples)")
    print(f"  Test:  indices {test_idx[0]} to {test_idx[-1]} "
          f"({len(test_idx)} samples)")

# –í–∏–≤—ñ–¥:
# Fold 1:
#   Train: indices 0 to 49 (50 samples)
#   Test:  indices 50 to 59 (10 samples)
# Fold 2:
#   Train: indices 0 to 59 (60 samples)
#   Test:  indices 60 to 69 (10 samples)
# ...
```

### –ó cross_val_score

```python
from sklearn.linear_model import Ridge

model = Ridge()

scores = cross_val_score(
    model, X, y,
    cv=TimeSeriesSplit(n_splits=5),
    scoring='r2'
)

print(f"Time Series CV Scores: {scores}")
print(f"Mean R¬≤: {scores.mean():.4f}")
```

### –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ TimeSeriesSplit

```python
tscv = TimeSeriesSplit(
    n_splits=5,        # –ö—ñ–ª—å–∫—ñ—Å—Ç—å splits
    max_train_size=None,  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∏–π —Ä–æ–∑–º—ñ—Ä train (None = –Ω–µ–æ–±–º–µ–∂–µ–Ω–∏–π)
    test_size=None,    # –§—ñ–∫—Å–æ–≤–∞–Ω–∏–π —Ä–æ–∑–º—ñ—Ä test set
    gap=0              # Gap –º—ñ–∂ train —ñ test (–∫—ñ–ª—å–∫—ñ—Å—Ç—å –ø—Ä–æ–ø—É—â–µ–Ω–∏—Ö –∑—Ä–∞–∑–∫—ñ–≤)
)

# –ó gap (–∫–æ—Ä–∏—Å–Ω–æ –¥–ª—è forecasting)
tscv_gap = TimeSeriesSplit(n_splits=5, gap=5)
# Train: [0...49], Test: [55...64] (5 –∑—Ä–∞–∑–∫—ñ–≤ –ø—Ä–æ–ø—É—â–µ–Ω–æ)
```

---

## Hyperparameter Tuning –∑ CV

### GridSearchCV

**–ü–µ—Ä–µ–±—ñ—Ä –≤—Å—ñ—Ö –∫–æ–º–±—ñ–Ω–∞—Ü—ñ–π –≥—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤** –∑ –∫—Ä–æ—Å-–≤–∞–ª—ñ–¥–∞—Ü—ñ—î—é.

```python
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier

# –ú–æ–¥–µ–ª—å
rf = RandomForestClassifier(random_state=42)

# –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –¥–ª—è –ø–µ—Ä–µ–±–æ—Ä—É
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [5, 10, 20, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# GridSearchCV
grid_search = GridSearchCV(
    rf,                      # –ú–æ–¥–µ–ª—å
    param_grid,              # –ü–∞—Ä–∞–º–µ—Ç—Ä–∏
    cv=5,                    # 5-fold CV
    scoring='accuracy',      # –ú–µ—Ç—Ä–∏–∫–∞
    n_jobs=-1,               # –ü–∞—Ä–∞–ª–µ–ª—ñ–∑–∞—Ü—ñ—è
    verbose=2                # –í–∏–≤–æ–¥–∏—Ç–∏ –ø—Ä–æ–≥—Ä–µ—Å
)

# –ù–∞–≤—á–∞–Ω–Ω—è (–ø–µ—Ä–µ–±–∏—Ä–∞—î –≤—Å—ñ –∫–æ–º–±—ñ–Ω–∞—Ü—ñ—ó)
grid_search.fit(X, y)

# –†–µ–∑—É–ª—å—Ç–∞—Ç–∏
print(f"Best parameters: {grid_search.best_params_}")
print(f"Best CV score: {grid_search.best_score_:.4f}")

# –ù–∞–π–∫—Ä–∞—â–∞ –º–æ–¥–µ–ª—å
best_model = grid_search.best_estimator_

# –í—Å—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
results = pd.DataFrame(grid_search.cv_results_)
print(results[['params', 'mean_test_score', 'std_test_score']].head(10))
```

**–ö—ñ–ª—å–∫—ñ—Å—Ç—å –∫–æ–º–±—ñ–Ω–∞—Ü—ñ–π:**
```python
# 3 * 4 * 3 * 3 = 108 –∫–æ–º–±—ñ–Ω–∞—Ü—ñ–π
# –ó cv=5 ‚Üí 108 * 5 = 540 fits!
```

### RandomizedSearchCV

**–í–∏–ø–∞–¥–∫–æ–≤–∏–π –≤–∏–±—ñ—Ä –∫–æ–º–±—ñ–Ω–∞—Ü—ñ–π** ‚Äî —à–≤–∏–¥—à–µ –∑–∞ GridSearch.

```python
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint, uniform

# –†–æ–∑–ø–æ–¥—ñ–ª–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤
param_distributions = {
    'n_estimators': randint(50, 300),       # –¶—ñ–ª–µ —á–∏—Å–ª–æ –≤—ñ–¥ 50 –¥–æ 300
    'max_depth': [5, 10, 20, None],
    'min_samples_split': randint(2, 20),
    'min_samples_leaf': randint(1, 10),
    'max_features': uniform(0.1, 0.9)       # Float –≤—ñ–¥ 0.1 –¥–æ 1.0
}

random_search = RandomizedSearchCV(
    rf,
    param_distributions,
    n_iter=50,           # –ö—ñ–ª—å–∫—ñ—Å—Ç—å –≤–∏–ø–∞–¥–∫–æ–≤–∏—Ö –∫–æ–º–±—ñ–Ω–∞—Ü—ñ–π
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    random_state=42,
    verbose=2
)

random_search.fit(X, y)

print(f"Best parameters: {random_search.best_params_}")
print(f"Best CV score: {random_search.best_score_:.4f}")
```

**–ü–µ—Ä–µ–≤–∞–≥–∏ RandomizedSearchCV:**
- ‚úÖ –®–≤–∏–¥—à–µ (–º–µ–Ω—à–µ fits)
- ‚úÖ –ú–æ–∂–µ –∑–Ω–∞–π—Ç–∏ –∫—Ä–∞—â–µ —Ä—ñ—à–µ–Ω–Ω—è (–≤–∏–ø–∞–¥–∫–æ–≤–∏–π –ø–æ—à—É–∫ —ñ–Ω–æ–¥—ñ –∫—Ä–∞—â–∏–π)
- ‚úÖ –ü—ñ–¥—Ö–æ–¥–∏—Ç—å –¥–ª—è continuous –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤

### –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è Grid vs Random

| –ê—Å–ø–µ–∫—Ç | GridSearchCV | RandomizedSearchCV |
|--------|--------------|-------------------|
| **–®–≤–∏–¥–∫—ñ—Å—Ç—å** | –ü–æ–≤—ñ–ª—å–Ω–æ (–≤—Å—ñ –∫–æ–º–±—ñ–Ω–∞—Ü—ñ—ó) | –®–≤–∏–¥–∫–æ (n_iter –∫–æ–º–±—ñ–Ω–∞—Ü—ñ–π) |
| **–ü–æ–∫—Ä–∏—Ç—Ç—è** | –í–∏—á–µ—Ä–ø–Ω–∏–π –ø–æ—à—É–∫ | –í–∏–ø–∞–¥–∫–æ–≤–∏–π –ø–æ—à—É–∫ |
| **Continuous params** | –ü–æ—Ç—Ä—ñ–±–Ω–æ –¥–∏—Å–∫—Ä–µ—Ç–∏–∑—É–≤–∞—Ç–∏ | –ü—Ä–∞—Ü—é—î –∑ —Ä–æ–∑–ø–æ–¥—ñ–ª–∞–º–∏ |
| **–ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏** | –ú–∞–ª–∏–π param_grid | –í–µ–ª–∏–∫–∏–π param space |

---

## cross_validate (—Ä–æ–∑—à–∏—Ä–µ–Ω–∞ –≤–µ—Ä—Å—ñ—è)

**cross_val_score** –ø–æ–≤–µ—Ä—Ç–∞—î —Ç—ñ–ª—å–∫–∏ –æ–¥–Ω—É –º–µ—Ç—Ä–∏–∫—É. **cross_validate** ‚Äî –±–∞–≥–∞—Ç–æ –º–µ—Ç—Ä–∏–∫ —ñ –¥–æ–¥–∞—Ç–∫–æ–≤—É —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é.

```python
from sklearn.model_selection import cross_validate

# –ö—ñ–ª—å–∫–∞ –º–µ—Ç—Ä–∏–∫
scoring = {
    'accuracy': 'accuracy',
    'precision': 'precision',
    'recall': 'recall',
    'f1': 'f1',
    'roc_auc': 'roc_auc'
}

cv_results = cross_validate(
    model, X, y,
    cv=5,
    scoring=scoring,
    return_train_score=True,  # –ü–æ–≤–µ—Ä–Ω—É—Ç–∏ train scores
    return_estimator=True     # –ü–æ–≤–µ—Ä–Ω—É—Ç–∏ fitted –º–æ–¥–µ–ª—ñ
)

# –†–µ–∑—É–ª—å—Ç–∞—Ç–∏
print("Test scores:")
for metric in scoring.keys():
    scores = cv_results[f'test_{metric}']
    print(f"  {metric}: {scores.mean():.4f} (+/- {scores.std():.4f})")

print("\nTrain scores:")
for metric in scoring.keys():
    scores = cv_results[f'train_{metric}']
    print(f"  {metric}: {scores.mean():.4f} (+/- {scores.std():.4f})")

# Fitted –º–æ–¥–µ–ª—ñ
models = cv_results['estimator']
print(f"\nNumber of fitted models: {len(models)}")

# –ß–∞—Å –≤–∏–∫–æ–Ω–∞–Ω–Ω—è
print(f"Fit time: {cv_results['fit_time'].mean():.3f}s")
print(f"Score time: {cv_results['score_time'].mean():.3f}s")
```

---

## –í–∏–±—ñ—Ä –∫—ñ–ª—å–∫–æ—Å—Ç—ñ folds (K)

### –ó–∞–≥–∞–ª—å–Ω—ñ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó

| –†–æ–∑–º—ñ—Ä –¥–∞—Ç–∞—Å–µ—Ç—É | –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—è | –ü—Ä–∏—á–∏–Ω–∞ |
|-----------------|--------------|---------|
| **–ú–∞–ª–∏–π** (< 1000) | K=10 –∞–±–æ LOOCV | –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –¥–∞–Ω–∏—Ö |
| **–°–µ—Ä–µ–¥–Ω—ñ–π** (1k-10k) | K=5 –∞–±–æ K=10 | –ë–∞–ª–∞–Ω—Å –º—ñ–∂ bias —ñ variance |
| **–í–µ–ª–∏–∫–∏–π** (> 10k) | K=3 –∞–±–æ K=5 | –®–≤–∏–¥–∫—ñ—Å—Ç—å |

### K=5 vs K=10

```python
# K=5: —à–≤–∏–¥—à–µ, –±—ñ–ª—å—à–∞ variance –æ—Ü—ñ–Ω–∫–∏
scores_5 = cross_val_score(model, X, y, cv=5)

# K=10: –ø–æ–≤—ñ–ª—å–Ω—ñ—à–µ, –º–µ–Ω—à–∞ variance –æ—Ü—ñ–Ω–∫–∏
scores_10 = cross_val_score(model, X, y, cv=10)

print(f"K=5:  Mean={scores_5.mean():.4f}, Std={scores_5.std():.4f}")
print(f"K=10: Mean={scores_10.mean():.4f}, Std={scores_10.std():.4f}")

# –ó–∞–∑–≤–∏—á–∞–π K=10 –º–∞—î –º–µ–Ω—à—É std
```

### –ö–æ–º–ø—Ä–æ–º—ñ—Å

```
K –º–∞–ª–∏–π (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥, K=3):
‚úÖ –®–≤–∏–¥—à–µ
‚úÖ –ë—ñ–ª—å—à–∏–π train set –≤ –∫–æ–∂–Ω–æ–º—É fold
‚ùå –ë—ñ–ª—å—à–∞ variance –æ—Ü—ñ–Ω–∫–∏
‚ùå –ú–µ–Ω—à–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –¥–∞–Ω–∏—Ö –¥–ª—è —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è

K –≤–µ–ª–∏–∫–∏–π (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥, K=10):
‚úÖ –ú–µ–Ω—à–∞ variance –æ—Ü—ñ–Ω–∫–∏
‚úÖ –ö—Ä–∞—â–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –¥–∞–Ω–∏—Ö
‚ùå –ü–æ–≤—ñ–ª—å–Ω—ñ—à–µ
‚ùå –ú–µ–Ω—à–∏–π train set –≤ –∫–æ–∂–Ω–æ–º—É fold
```

**–°—Ç–∞–Ω–¥–∞—Ä—Ç:** **K=5 –∞–±–æ K=10**

---

## Nested Cross-Validation

### –ù–∞–≤—ñ—â–æ?

**–ü—Ä–æ–±–ª–µ–º–∞:** —è–∫—â–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ CV —ñ –¥–ª—è model selection, —ñ –¥–ª—è –æ—Ü—ñ–Ω–∫–∏ ‚Üí –∑–∞–≤–∏—â–µ–Ω–∞ –æ—Ü—ñ–Ω–∫–∞ (optimistic bias).

**–†—ñ—à–µ–Ω–Ω—è:** –¥–≤–∞ —Ä—ñ–≤–Ω—ñ CV:
- **Outer CV** ‚Äî –æ—Ü—ñ–Ω–∫–∞ —É–∑–∞–≥–∞–ª—å–Ω–µ–Ω–Ω—è
- **Inner CV** ‚Äî hyperparameter tuning

```
Outer Loop (5 folds):
  Fold 1: [TRAIN+VAL][TRAIN+VAL][TRAIN+VAL][TRAIN+VAL][TEST]
           ‚îî‚îÄ‚îÄ Inner CV (–¥–ª—è tuning –Ω–∞ TRAIN+VAL)
  Fold 2: [TRAIN+VAL][TRAIN+VAL][TRAIN+VAL][TEST][TRAIN+VAL]
           ‚îî‚îÄ‚îÄ Inner CV
  ...
```

### –†–µ–∞–ª—ñ–∑–∞—Ü—ñ—è

```python
from sklearn.model_selection import cross_val_score, GridSearchCV
from sklearn.ensemble import RandomForestClassifier

# –ú–æ–¥–µ–ª—å –∑ tuning
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [5, 10, None]
}

# GridSearchCV (Inner CV)
grid_search = GridSearchCV(
    RandomForestClassifier(random_state=42),
    param_grid,
    cv=3,  # Inner CV: 3 folds
    scoring='accuracy'
)

# Outer CV
outer_scores = cross_val_score(
    grid_search,  # GridSearchCV —è–∫ estimator
    X, y,
    cv=5,  # Outer CV: 5 folds
    scoring='accuracy'
)

print(f"Nested CV scores: {outer_scores}")
print(f"Mean: {outer_scores.mean():.4f}")
print(f"Std: {outer_scores.std():.4f}")

# –¶–µ –ß–ï–°–ù–ê –æ—Ü—ñ–Ω–∫–∞ —É–∑–∞–≥–∞–ª—å–Ω–µ–Ω–Ω—è!
```

### –°—Ö–µ–º–∞

```
Total: 5 (outer) √ó 3 (inner) √ó 9 (param combinations) = 135 fits

Outer Fold 1:
  Inner CV –Ω–∞ 80% –¥–∞–Ω–∏—Ö:
    - –ü–µ—Ä–µ–±—Ä–∞—Ç–∏ –≤—Å—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ (3√ó3=9)
    - –í–∏–±—Ä–∞—Ç–∏ –Ω–∞–π–∫—Ä–∞—â—ñ
  –û—Ü—ñ–Ω–∏—Ç–∏ –Ω–∞ 20% (test –¥–ª—è outer fold 1)

Outer Fold 2:
  Inner CV –Ω–∞ 80% –¥–∞–Ω–∏—Ö:
    - –ó–Ω–æ–≤—É –ø–µ—Ä–µ–±—Ä–∞—Ç–∏ (–º–æ–∂—É—Ç—å –±—É—Ç–∏ —ñ–Ω—à—ñ –Ω–∞–π–∫—Ä–∞—â—ñ!)
  –û—Ü—ñ–Ω–∏—Ç–∏ –Ω–∞ 20%

...
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç:** –Ω–∞–¥—ñ–π–Ω–∞ –æ—Ü—ñ–Ω–∫–∞ —Ç–æ–≥–æ, —è–∫ –º–æ–¥–µ–ª—å (–∑ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∏–º tuning) –ø—Ä–∞—Ü—é–≤–∞—Ç–∏–º–µ –Ω–∞ –Ω–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö.

---

## –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è CV

### Regression

```python
from sklearn.model_selection import cross_val_score

# R¬≤ (–∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º)
scores = cross_val_score(model, X, y, cv=5)

# MSE
scores_mse = cross_val_score(
    model, X, y, cv=5, 
    scoring='neg_mean_squared_error'  # –ù–µ–≥–∞—Ç–∏–≤–Ω–∏–π MSE
)
mse_scores = -scores_mse  # –ö–æ–Ω–≤–µ—Ä—Ç—É–≤–∞—Ç–∏ –Ω–∞–∑–∞–¥

# MAE
scores_mae = cross_val_score(
    model, X, y, cv=5,
    scoring='neg_mean_absolute_error'
)

# –ö—ñ–ª—å–∫–∞ –º–µ—Ç—Ä–∏–∫
from sklearn.model_selection import cross_validate

cv_results = cross_validate(
    model, X, y, cv=5,
    scoring={
        'r2': 'r2',
        'mse': 'neg_mean_squared_error',
        'mae': 'neg_mean_absolute_error'
    }
)
```

### Classification

```python
# Accuracy (–∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º –¥–ª—è classification)
scores = cross_val_score(model, X, y, cv=5)

# Precision
scores_prec = cross_val_score(model, X, y, cv=5, scoring='precision')

# Recall
scores_rec = cross_val_score(model, X, y, cv=5, scoring='recall')

# F1
scores_f1 = cross_val_score(model, X, y, cv=5, scoring='f1')

# ROC-AUC
scores_auc = cross_val_score(model, X, y, cv=5, scoring='roc_auc')

# –î–ª—è multiclass
scores_f1_weighted = cross_val_score(
    model, X, y, cv=5, 
    scoring='f1_weighted'  # Weighted F1
)
```

### –ü–æ–≤–Ω–∏–π —Å–ø–∏—Å–æ–∫

```python
from sklearn.metrics import get_scorer_names

# –í—Å—ñ –¥–æ—Å—Ç—É–ø–Ω—ñ scoring functions
all_scorers = get_scorer_names()
print(f"Available scorers: {len(all_scorers)}")
print(all_scorers[:20])  # –ü–µ—Ä—à—ñ 20
```

---

## –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è CV —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤

### Boxplot

```python
import matplotlib.pyplot as plt
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression

# –ö—ñ–ª—å–∫–∞ –º–æ–¥–µ–ª–µ–π
models = {
    'Logistic Regression': LogisticRegression(max_iter=10000),
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
    'Gradient Boosting': GradientBoostingClassifier(random_state=42)
}

# CV scores –¥–ª—è –∫–æ–∂–Ω–æ—ó –º–æ–¥–µ–ª—ñ
cv_results = {}

for name, model in models.items():
    scores = cross_val_score(model, X, y, cv=10, scoring='accuracy')
    cv_results[name] = scores
    print(f"{name}: {scores.mean():.4f} (+/- {scores.std():.4f})")

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
plt.figure(figsize=(10, 6))

plt.boxplot(cv_results.values(), labels=cv_results.keys())
plt.ylabel('Accuracy', fontsize=12)
plt.title('10-Fold Cross-Validation Results', fontsize=14, fontweight='bold')
plt.grid(True, alpha=0.3, axis='y')
plt.xticks(rotation=15, ha='right')

# –°–µ—Ä–µ–¥–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è
means = [scores.mean() for scores in cv_results.values()]
plt.plot(range(1, len(means)+1), means, 'r^', markersize=10, label='Mean')
plt.legend()

plt.tight_layout()
plt.show()
```

### Learning Curve –∑ CV

```python
from sklearn.model_selection import learning_curve

train_sizes, train_scores, val_scores = learning_curve(
    model, X, y,
    train_sizes=np.linspace(0.1, 1.0, 10),
    cv=5,
    scoring='accuracy',
    n_jobs=-1
)

# –£—Å–µ—Ä–µ–¥–Ω–µ–Ω–Ω—è –ø–æ folds
train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)
val_mean = np.mean(val_scores, axis=1)
val_std = np.std(val_scores, axis=1)

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
plt.figure(figsize=(10, 6))

plt.plot(train_sizes, train_mean, 'o-', linewidth=2, label='Train Score')
plt.fill_between(train_sizes, train_mean - train_std, 
                 train_mean + train_std, alpha=0.1)

plt.plot(train_sizes, val_mean, 's-', linewidth=2, label='CV Score')
plt.fill_between(train_sizes, val_mean - val_std,
                 val_mean + val_std, alpha=0.1)

plt.xlabel('Training Set Size', fontsize=12)
plt.ylabel('Accuracy', fontsize=12)
plt.title('Learning Curves with 5-Fold CV', fontsize=14, fontweight='bold')
plt.legend(fontsize=11)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

---

## –ü–æ—à–∏—Ä–µ–Ω—ñ –ø–æ–º–∏–ª–∫–∏ ‚ùå

### 1. Preprocessing –ø–µ—Ä–µ–¥ CV

```python
# ‚ùå –ü–û–ì–ê–ù–û: –≤–∏—Ç—ñ–∫ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)  # –í–∏–∫–æ—Ä–∏—Å—Ç–∞–≤ –í–ï–°–¨ –¥–∞—Ç–∞—Å–µ—Ç!

scores = cross_val_score(model, X_scaled, y, cv=5)
# CV folds –±–∞—á–∞—Ç—å —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –æ–¥–∏–Ω –≤—ñ–¥ –æ–¥–Ω–æ–≥–æ —á–µ—Ä–µ–∑ scaling!

# ‚úÖ –î–û–ë–†–ï: –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π Pipeline
from sklearn.pipeline import Pipeline

pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('model', LogisticRegression())
])

scores = cross_val_score(pipeline, X, y, cv=5)
# Scaling –≤–∏–∫–æ–Ω—É—î—Ç—å—Å—è –í–°–ï–†–ï–î–ò–ù–Ü –∫–æ–∂–Ω–æ–≥–æ fold!
```

### 2. –û—Ü—ñ–Ω–∫–∞ –Ω–∞ –≤—Å—å–æ–º—É –¥–∞—Ç–∞—Å–µ—Ç—ñ –ø—ñ—Å–ª—è CV

```python
# ‚ùå –ü–û–ì–ê–ù–û
scores = cross_val_score(model, X, y, cv=5)
print(f"CV Score: {scores.mean()}")

# –ü–æ—Ç—ñ–º –Ω–∞–≤—á–∏—Ç–∏ –Ω–∞ –í–°–¨–û–ú–£ –¥–∞—Ç–∞—Å–µ—Ç—ñ —ñ –æ—Ü—ñ–Ω–∏—Ç–∏
model.fit(X, y)
final_score = model.score(X, y)  # ‚ùå –ó–∞–≤–∏—â–µ–Ω–∞ –æ—Ü—ñ–Ω–∫–∞!

# ‚úÖ –î–û–ë–†–ï: –∑–±–µ—Ä–µ–≥—Ç–∏ –æ–∫—Ä–µ–º–∏–π test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# CV –Ω–∞ train
scores = cross_val_score(model, X_train, y_train, cv=5)
print(f"CV Score: {scores.mean()}")

# –§—ñ–Ω–∞–ª—å–Ω–∞ –º–æ–¥–µ–ª—å –Ω–∞ –í–°–¨–û–ú–£ train
model.fit(X_train, y_train)

# –û—Ü—ñ–Ω–∫–∞ –Ω–∞ test (–Ω–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞–≤—Å—è –≤ CV!)
final_score = model.score(X_test, y_test)
```

### 3. –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è CV –¥–ª—è model selection –ë–ï–ó nested CV

```python
# ‚ùå –ü–û–ì–ê–ù–û: optimistic bias
models = [LogisticRegression(), RandomForest(), GradientBoosting()]

best_score = 0
best_model = None

for model in models:
    score = cross_val_score(model, X, y, cv=5).mean()
    if score > best_score:
        best_score = score
        best_model = model

print(f"Best CV score: {best_score}")  # –ó–∞–≤–∏—â–µ–Ω–∞!

# ‚úÖ –î–û–ë–†–ï: nested CV –∞–±–æ –æ–∫—Ä–µ–º–∏–π test set
# –í–∞—Ä—ñ–∞–Ω—Ç 1: –æ–∫—Ä–µ–º–∏–π test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# CV —Ç—ñ–ª—å–∫–∏ –Ω–∞ train –¥–ª—è –≤–∏–±–æ—Ä—É
best_score = 0
for model in models:
    score = cross_val_score(model, X_train, y_train, cv=5).mean()
    if score > best_score:
        best_score = score
        best_model = model

# –§—ñ–Ω–∞–ª—å–Ω–∞ –æ—Ü—ñ–Ω–∫–∞ –Ω–∞ test
best_model.fit(X_train, y_train)
final_score = best_model.score(X_test, y_test)
```

### 4. Shuffle –¥–ª—è time series

```python
# ‚ùå –ü–û–ì–ê–ù–û
scores = cross_val_score(model, X_timeseries, y_timeseries, cv=5)
# KFold –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º shuffle=True ‚Üí –ø–æ—Ä—É—à—É—î —á–∞—Å–æ–≤–∏–π –ø–æ—Ä—è–¥–æ–∫!

# ‚úÖ –î–û–ë–†–ï
from sklearn.model_selection import TimeSeriesSplit
tscv = TimeSeriesSplit(n_splits=5)
scores = cross_val_score(model, X_timeseries, y_timeseries, cv=tscv)
```

### 5. –ó–∞–±—É–≤–∞—Ç–∏ –ø—Ä–æ stratification –¥–ª—è –Ω–µ—Å–±–∞–ª–∞–Ω—Å–æ–≤–∞–Ω–∏—Ö –∫–ª–∞—Å—ñ–≤

```python
# –î–∞–Ω—ñ: 95% –∫–ª–∞—Å 0, 5% –∫–ª–∞—Å 1

# ‚ùå –ü–û–ì–ê–ù–û: –∑–≤–∏—á–∞–π–Ω–∏–π KFold
scores = cross_val_score(model, X, y, cv=5)
# –î–µ—è–∫—ñ folds –º–æ–∂—É—Ç—å –Ω–µ –º—ñ—Å—Ç–∏—Ç–∏ –∫–ª–∞—Å 1!

# ‚úÖ –î–û–ë–†–ï: StratifiedKFold (–∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –¥–ª—è classification)
scores = cross_val_score(model, X, y, cv=5)
# –ê–±–æ —è–≤–Ω–æ:
from sklearn.model_selection import StratifiedKFold
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_val_score(model, X, y, cv=skf)
```

---

## –ü—Ä–∞–∫—Ç–∏—á–Ω—ñ –ø–æ—Ä–∞–¥–∏ üí°

### 1. –ó–∞–≤–∂–¥–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π Pipeline –¥–ª—è preprocessing

```python
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestClassifier

# Pipeline –≥–∞—Ä–∞–Ω—Ç—É—î, —â–æ preprocessing —Ä–æ–±–∏—Ç—å—Å—è –í–°–ï–†–ï–î–ò–ù–Ü CV
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('pca', PCA(n_components=10)),
    ('classifier', RandomForestClassifier())
])

scores = cross_val_score(pipeline, X, y, cv=5)
```

### 2. –§—ñ–∫—Å—É–π random_state –¥–ª—è –≤—ñ–¥—Ç–≤–æ—Ä—é–≤–∞–Ω–æ—Å—Ç—ñ

```python
from sklearn.model_selection import StratifiedKFold

# –§—ñ–∫—Å–æ–≤–∞–Ω–∏–π seed
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

scores = cross_val_score(model, X, y, cv=cv)
# –ó–∞–≤–∂–¥–∏ –æ–¥–Ω–∞–∫–æ–≤—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
```

### 3. –ó–±–µ—Ä—ñ–≥–∞–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ CV

```python
import pandas as pd
from sklearn.model_selection import cross_validate

cv_results = cross_validate(
    model, X, y,
    cv=5,
    scoring=['accuracy', 'precision', 'recall', 'f1'],
    return_train_score=True
)

# –ö–æ–Ω–≤–µ—Ä—Ç—É–≤–∞—Ç–∏ –≤ DataFrame
df_results = pd.DataFrame(cv_results)
df_results.to_csv('cv_results.csv', index=False)

print(df_results.describe())
```

### 4. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π cross_validate –∑–∞–º—ñ—Å—Ç—å cross_val_score

```python
# cross_validate –¥–∞—î –±—ñ–ª—å—à–µ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó
cv_results = cross_validate(
    model, X, y,
    cv=5,
    scoring='accuracy',
    return_train_score=True,
    return_estimator=True  # –ó–±–µ—Ä–µ–≥—Ç–∏ fitted –º–æ–¥–µ–ª—ñ
)

# –ú–æ–∂–Ω–∞ –∞–Ω–∞–ª—ñ–∑—É–≤–∞—Ç–∏ train vs test
train_scores = cv_results['train_score']
test_scores = cv_results['test_score']

print(f"Train: {train_scores.mean():.4f} (+/- {train_scores.std():.4f})")
print(f"Test: {test_scores.mean():.4f} (+/- {test_scores.std():.4f})")

# –î—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ overfitting
if train_scores.mean() - test_scores.mean() > 0.1:
    print("‚ö†Ô∏è Possible overfitting")
```

### 5. –ü–µ—Ä–µ–≤—ñ—Ä—è–π —á–∞—Å –≤–∏–∫–æ–Ω–∞–Ω–Ω—è

```python
import time

start = time.time()
scores = cross_val_score(model, X, y, cv=10)
elapsed = time.time() - start

print(f"CV Time: {elapsed:.2f}s")
print(f"Time per fold: {elapsed/10:.2f}s")

# –Ø–∫—â–æ –∑–∞–Ω–∞–¥—Ç–æ –ø–æ–≤—ñ–ª—å–Ω–æ ‚Üí –∑–º–µ–Ω—à K –∞–±–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π RandomizedSearchCV
```

---

## –†–µ–∞–ª—å–Ω–∏–π –ø—Ä–∏–∫–ª–∞–¥: Model Selection –∑ CV

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import cross_val_score, GridSearchCV, StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC

print("="*70)
print("MODEL SELECTION WITH CROSS-VALIDATION")
print("="*70)

# –î–∞–Ω—ñ
cancer = load_breast_cancer()
X = cancer.data
y = cancer.target

print(f"\nDataset: {X.shape[0]} samples, {X.shape[1]} features")
print(f"Classes: {np.unique(y)}")

# CV strategy
cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)

# –ú–æ–¥–µ–ª—ñ –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è
models = {
    'Logistic Regression': Pipeline([
        ('scaler', StandardScaler()),
        ('clf', LogisticRegression(max_iter=10000, random_state=42))
    ]),
    
    'Decision Tree': DecisionTreeClassifier(random_state=42),
    
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
    
    'Gradient Boosting': GradientBoostingClassifier(random_state=42),
    
    'SVM': Pipeline([
        ('scaler', StandardScaler()),
        ('clf', SVC(random_state=42, probability=True))
    ])
}

# –ú–µ—Ç—Ä–∏–∫–∏
scoring = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']

# –û—Ü—ñ–Ω–∫–∞ –∫–æ–∂–Ω–æ—ó –º–æ–¥–µ–ª—ñ
results = []

print("\n" + "="*70)
print("EVALUATING MODELS")
print("="*70)

for name, model in models.items():
    print(f"\n{name}...")
    
    # Cross-validation
    cv_results = cross_validate(
        model, X, y,
        cv=cv,
        scoring=scoring,
        return_train_score=True,
        n_jobs=-1
    )
    
    # –ó–±–µ—Ä–µ–≥—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
    result = {'Model': name}
    
    for metric in scoring:
        train_scores = cv_results[f'train_{metric}']
        test_scores = cv_results[f'test_{metric}']
        
        result[f'{metric}_mean'] = test_scores.mean()
        result[f'{metric}_std'] = test_scores.std()
        result[f'train_{metric}_mean'] = train_scores.mean()
        result[f'gap_{metric}'] = train_scores.mean() - test_scores.mean()
    
    result['fit_time'] = cv_results['fit_time'].mean()
    results.append(result)
    
    print(f"  Accuracy: {result['accuracy_mean']:.4f} (+/- {result['accuracy_std']:.4f})")
    print(f"  ROC-AUC:  {result['roc_auc_mean']:.4f} (+/- {result['roc_auc_std']:.4f})")

# DataFrame –∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
df_results = pd.DataFrame(results)

print("\n" + "="*70)
print("RESULTS SUMMARY")
print("="*70)

# –¢–æ–ø –º–æ–¥–µ–ª—ñ –∑–∞ accuracy
print("\nRanked by Accuracy:")
print(df_results[['Model', 'accuracy_mean', 'accuracy_std']]\
      .sort_values('accuracy_mean', ascending=False)\
      .to_string(index=False))

print("\nRanked by ROC-AUC:")
print(df_results[['Model', 'roc_auc_mean', 'roc_auc_std']]\
      .sort_values('roc_auc_mean', ascending=False)\
      .to_string(index=False))

# Overfitting analysis
print("\nOverfitting Analysis (Gap Train-Test):")
for _, row in df_results.iterrows():
    gap = row['gap_accuracy']
    status = "‚úÖ" if gap < 0.05 else "‚ö†Ô∏è" if gap < 0.1 else "üî¥"
    print(f"  {status} {row['Model']}: {gap:.4f}")

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
fig, axes = plt.subplots(2, 2, figsize=(16, 12))

# 1. Accuracy comparison
model_names = df_results['Model'].values
acc_means = df_results['accuracy_mean'].values
acc_stds = df_results['accuracy_std'].values

x = np.arange(len(model_names))
axes[0, 0].bar(x, acc_means, yerr=acc_stds, alpha=0.7, capsize=5)
axes[0, 0].set_xticks(x)
axes[0, 0].set_xticklabels(model_names, rotation=45, ha='right')
axes[0, 0].set_ylabel('Accuracy', fontsize=11)
axes[0, 0].set_title('10-Fold CV: Accuracy Comparison', fontsize=13, fontweight='bold')
axes[0, 0].grid(True, alpha=0.3, axis='y')
axes[0, 0].set_ylim([0.85, 1.0])

# 2. Multiple metrics heatmap
metrics_to_plot = ['accuracy_mean', 'precision_mean', 'recall_mean', 
                   'f1_mean', 'roc_auc_mean']
heatmap_data = df_results[['Model'] + metrics_to_plot].set_index('Model')

import seaborn as sns
sns.heatmap(heatmap_data, annot=True, fmt='.3f', cmap='YlGnBu', 
            ax=axes[0, 1], cbar_kws={'label': 'Score'})
axes[0, 1].set_title('All Metrics Heatmap', fontsize=13, fontweight='bold')
axes[0, 1].set_yticklabels(axes[0, 1].get_yticklabels(), rotation=0)

# 3. Train vs Test (overfitting check)
train_acc = df_results['train_accuracy_mean'].values
test_acc = df_results['accuracy_mean'].values

width = 0.35
axes[1, 0].bar(x - width/2, train_acc, width, label='Train', alpha=0.8)
axes[1, 0].bar(x + width/2, test_acc, width, label='Test', alpha=0.8)
axes[1, 0].set_xticks(x)
axes[1, 0].set_xticklabels(model_names, rotation=45, ha='right')
axes[1, 0].set_ylabel('Accuracy', fontsize=11)
axes[1, 0].set_title('Train vs Test Accuracy', fontsize=13, fontweight='bold')
axes[1, 0].legend()
axes[1, 0].grid(True, alpha=0.3, axis='y')

# 4. Fit time comparison
fit_times = df_results['fit_time'].values

axes[1, 1].barh(model_names, fit_times, alpha=0.7)
axes[1, 1].set_xlabel('Time (seconds)', fontsize=11)
axes[1, 1].set_title('Average Fit Time per Fold', fontsize=13, fontweight='bold')
axes[1, 1].grid(True, alpha=0.3, axis='x')

plt.tight_layout()
plt.show()

# –ù–∞–π–∫—Ä–∞—â–∞ –º–æ–¥–µ–ª—å
best_idx = df_results['accuracy_mean'].idxmax()
best_model_name = df_results.loc[best_idx, 'Model']
best_accuracy = df_results.loc[best_idx, 'accuracy_mean']

print("\n" + "="*70)
print("RECOMMENDATION")
print("="*70)
print(f"Best model: {best_model_name}")
print(f"Accuracy: {best_accuracy:.4f}")
print(f"ROC-AUC: {df_results.loc[best_idx, 'roc_auc_mean']:.4f}")

# Hyperparameter tuning –¥–ª—è –Ω–∞–π–∫—Ä–∞—â–æ—ó –º–æ–¥–µ–ª—ñ
if best_model_name == 'Random Forest':
    print("\nPerforming hyperparameter tuning for Random Forest...")
    
    param_grid = {
        'n_estimators': [50, 100, 200],
        'max_depth': [5, 10, 20, None],
        'min_samples_split': [2, 5, 10]
    }
    
    grid_search = GridSearchCV(
        RandomForestClassifier(random_state=42),
        param_grid,
        cv=5,
        scoring='accuracy',
        n_jobs=-1,
        verbose=1
    )
    
    grid_search.fit(X, y)
    
    print(f"\nBest parameters: {grid_search.best_params_}")
    print(f"Best CV score: {grid_search.best_score_:.4f}")

print("="*70)
```

---

## –ü–æ–≤'—è–∑–∞–Ω—ñ —Ç–µ–º–∏

- [[01_Bias_Variance_Tradeoff]] ‚Äî CV –¥–æ–ø–æ–º–∞–≥–∞—î –æ—Ü—ñ–Ω–∏—Ç–∏ variance
- [[02_Overfitting_Underfitting]] ‚Äî –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ —á–µ—Ä–µ–∑ CV
- [[03_Train_Test_Split]] ‚Äî –±–∞–∑–æ–≤–∏–π –º–µ—Ç–æ–¥ –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó
- [[Hyperparameter_Tuning]] ‚Äî GridSearchCV, RandomizedSearchCV
- [[Model_Selection]] ‚Äî –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –º–æ–¥–µ–ª–µ–π

## –†–µ—Å—É—Ä—Å–∏

- [Scikit-learn: Cross-Validation](https://scikit-learn.org/stable/modules/cross_validation.html)
- [StatQuest: Cross Validation](https://www.youtube.com/watch?v=fSytzGwwBVw)
- [Machine Learning Mastery: k-Fold Cross-Validation](https://machinelearningmastery.com/k-fold-cross-validation/)

---

## –ö–ª—é—á–æ–≤—ñ –≤–∏—Å–Ω–æ–≤–∫–∏

> Cross-Validation ‚Äî —Ü–µ —Ç–µ—Ö–Ω—ñ–∫–∞ —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –¥–∞—Ç–∞—Å–µ—Ç—É –Ω–∞ –∫—ñ–ª—å–∫–∞ —á–∞—Å—Ç–∏–Ω (folds), –Ω–∞–≤—á–∞–Ω–Ω—è —Ç–∞ —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ –Ω–∞ —Ä—ñ–∑–Ω–∏—Ö –∫–æ–º–±—ñ–Ω–∞—Ü—ñ—è—Ö —Ü–∏—Ö —á–∞—Å—Ç–∏–Ω, —ñ —É—Å–µ—Ä–µ–¥–Ω–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤. –¶–µ –¥–∞—î –±—ñ–ª—å—à –Ω–∞–¥—ñ–π–Ω—É –æ—Ü—ñ–Ω–∫—É, –Ω—ñ–∂ –æ–¥–∏–Ω train-test split.

**–¢–∏–ø–∏ CV:**

- **K-Fold** ‚Äî —Å—Ç–∞–Ω–¥–∞—Ä—Ç (K=5 –∞–±–æ K=10)
- **Stratified K-Fold** ‚Äî –¥–ª—è –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó (–∑–±–µ—Ä—ñ–≥–∞—î –ø—Ä–æ–ø–æ—Ä—Ü—ñ—ó –∫–ª–∞—Å—ñ–≤)
- **TimeSeriesSplit** ‚Äî –¥–ª—è —á–∞—Å–æ–≤–∏—Ö —Ä—è–¥—ñ–≤
- **LOOCV** ‚Äî –¥–ª—è –¥—É–∂–µ –º–∞–ª–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç—ñ–≤

**Best Practices:**

```python
# 1. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π Pipeline –¥–ª—è preprocessing
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('model', LogisticRegression())
])

# 2. Stratified –¥–ª—è –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# 3. Cross-validate –∑–∞–º—ñ—Å—Ç—å cross_val_score
cv_results = cross_validate(pipeline, X, y, cv=cv, 
                            return_train_score=True)

# 4. Nested CV –¥–ª—è model selection
```

**–ü–µ—Ä–µ–≤–∞–≥–∏:**

- ‚úÖ –ù–∞–¥—ñ–π–Ω—ñ—à–∞ –æ—Ü—ñ–Ω–∫–∞ (–º–µ–Ω—à–∞ variance)
- ‚úÖ –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –¥–∞–Ω–∏—Ö
- ‚úÖ –û—Ü—ñ–Ω–∫–∞ stability –º–æ–¥–µ–ª—ñ
- ‚úÖ –í–∏—è–≤–ª–µ–Ω–Ω—è overfitting

**–ù–µ–¥–æ–ª—ñ–∫–∏:**
- ‚ùå –ü–æ–≤—ñ–ª—å–Ω—ñ—à–µ (K √ó fits)
- ‚ùå –ù–µ –ø—ñ–¥—Ö–æ–¥–∏—Ç—å –¥–ª—è –≤–µ–ª–∏–∫–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç—ñ–≤
- ‚ùå –°–∫–ª–∞–¥–Ω—ñ—à–∞ —Ä–µ–∞–ª—ñ–∑–∞—Ü—ñ—è

**–ó–æ–ª–æ—Ç–µ –ø—Ä–∞–≤–∏–ª–æ:** –∑–∞–≤–∂–¥–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π CV –¥–ª—è model selection —ñ hyperparameter tuning, –∞–ª–µ –∑–±–µ—Ä—ñ–≥–∞–π –æ–∫—Ä–µ–º–∏–π test set –¥–ª—è —Ñ—ñ–Ω–∞–ª—å–Ω–æ—ó –æ—Ü—ñ–Ω–∫–∏!

---

#ml #core-concepts #cross-validation #model-evaluation #hyperparameter-tuning #k-fold #stratified
