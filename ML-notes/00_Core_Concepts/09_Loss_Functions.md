# Loss Functions (–§—É–Ω–∫—Ü—ñ—ó –≤—Ç—Ä–∞—Ç)

## –©–æ —Ü–µ?

**Loss Function** ‚Äî —Ü–µ —Ñ—É–Ω–∫—Ü—ñ—è, —è–∫–∞ **–≤–∏–º—ñ—Ä—é—î –ø–æ–º–∏–ª–∫—É** –º–æ–¥–µ–ª—ñ. –í–æ–Ω–∞ –ø–æ–∫–∞–∑—É—î, **–Ω–∞—Å–∫—ñ–ª—å–∫–∏ –ø–æ–≥–∞–Ω–æ** –º–æ–¥–µ–ª—å –ø–µ—Ä–µ–¥–±–∞—á–∞—î –Ω–∞ –¥–∞–Ω–∏—Ö. –ú–µ—Ç–∞ –Ω–∞–≤—á–∞–Ω–Ω—è ‚Äî **–º—ñ–Ω—ñ–º—ñ–∑—É–≤–∞—Ç–∏** loss function.

**–ì–æ–ª–æ–≤–Ω–∞ —ñ–¥–µ—è:** —Ä—ñ–∑–Ω—ñ –∑–∞–¥–∞—á—ñ –ø–æ—Ç—Ä–µ–±—É—é—Ç—å —Ä—ñ–∑–Ω–∏—Ö loss functions. –í–∏–±—ñ—Ä –ø—Ä–∞–≤–∏–ª—å–Ω–æ—ó loss function –∫—Ä–∏—Ç–∏—á–Ω–æ –≤–∞–∂–ª–∏–≤–∏–π –¥–ª—è —É—Å–ø—ñ—Ö—É –º–æ–¥–µ–ª—ñ.

## –ù–∞–≤—ñ—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ?

- üéØ **–û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è –º–æ–¥–µ–ª—ñ** ‚Äî —â–æ –º—ñ–Ω—ñ–º—ñ–∑—É—î–º–æ –ø—ñ–¥ —á–∞—Å –Ω–∞–≤—á–∞–Ω–Ω—è
- üìä **Gradient descent** ‚Äî –æ–±—á–∏—Å–ª–µ–Ω–Ω—è –≥—Ä–∞–¥—ñ—î–Ω—Ç—ñ–≤
- üîç **Model training** ‚Äî —è–∫ –º–æ–¥–µ–ª—å –≤—á–∏—Ç—å—Å—è
- ‚öôÔ∏è **Custom objectives** ‚Äî —Å–ø–µ—Ü–∏—Ñ—ñ—á–Ω—ñ –±—ñ–∑–Ω–µ—Å-—Ü—ñ–ª—ñ
- üí° **Regularization** ‚Äî –¥–æ–¥–∞–≤–∞–Ω–Ω—è penalties
- üéöÔ∏è **Fine-tuning** ‚Äî –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –Ω–∞–≤—á–∞–Ω–Ω—è

## –ö–æ–ª–∏ –≤–∞–∂–ª–∏–≤–æ?

**–ó–∞–≤–∂–¥–∏!** Loss function ‚Äî —Ü–µ —Å–µ—Ä—Ü–µ ML –∞–ª–≥–æ—Ä–∏—Ç–º—É.

**–ö–ª—é—á–æ–≤—ñ –ø–∏—Ç–∞–Ω–Ω—è:**
- Regression —á–∏ classification?
- Binary —á–∏ multiclass?
- –ß–∏ —î outliers?
- –ß–∏ –≤–∞–∂–ª–∏–≤—ñ –≤—Å—ñ –ø–æ–º–∏–ª–∫–∏ –æ–¥–Ω–∞–∫–æ–≤–æ?

---

## –ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è Loss Functions

```
Loss Functions
‚îÇ
‚îú‚îÄ‚îÄ Regression Losses
‚îÇ   ‚îú‚îÄ‚îÄ Mean Squared Error (MSE / L2 Loss)
‚îÇ   ‚îú‚îÄ‚îÄ Mean Absolute Error (MAE / L1 Loss)
‚îÇ   ‚îú‚îÄ‚îÄ Huber Loss
‚îÇ   ‚îú‚îÄ‚îÄ Log-Cosh Loss
‚îÇ   ‚îî‚îÄ‚îÄ Quantile Loss
‚îÇ
‚îú‚îÄ‚îÄ Classification Losses
‚îÇ   ‚îú‚îÄ‚îÄ Binary Cross-Entropy (Log Loss)
‚îÇ   ‚îú‚îÄ‚îÄ Categorical Cross-Entropy
‚îÇ   ‚îú‚îÄ‚îÄ Sparse Categorical Cross-Entropy
‚îÇ   ‚îú‚îÄ‚îÄ Hinge Loss (SVM)
‚îÇ   ‚îî‚îÄ‚îÄ Focal Loss
‚îÇ
‚îî‚îÄ‚îÄ Advanced Losses
    ‚îú‚îÄ‚îÄ Contrastive Loss
    ‚îú‚îÄ‚îÄ Triplet Loss
    ‚îî‚îÄ‚îÄ Custom Business Losses
```

---

# REGRESSION LOSSES

## 1. Mean Squared Error (MSE / L2 Loss)

### –§–æ—Ä–º—É–ª–∞

$$\text{MSE} = \frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2$$

### –Ü–Ω—Ç—É—ó—Ü—ñ—è

**MSE** ‚Äî **–∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–∞** –ø–æ–º–∏–ª–∫–∞. **–°–∏–ª—å–Ω–æ –∫–∞—Ä–∞—î –≤–µ–ª–∏–∫—ñ –ø–æ–º–∏–ª–∫–∏**.

```
–ü–æ–º–∏–ª–∫–∏:     [1, 1, 1, 1, 1]  vs  [0, 0, 0, 0, 5]
–ö–≤–∞–¥—Ä–∞—Ç–∏:    [1, 1, 1, 1, 1]  vs  [0, 0, 0, 0, 25]

MSE:         5/5 = 1.0        vs  25/5 = 5.0

–î—Ä—É–≥–∏–π –≤–∞—Ä—ñ–∞–Ω—Ç –º–∞—î 1 –≤–µ–ª–∏–∫—É –ø–æ–º–∏–ª–∫—É ‚Üí MSE —É 5 —Ä–∞–∑—ñ–≤ –±—ñ–ª—å—à–µ!
```

### –ö–æ–¥

```python
import numpy as np

def mse_loss(y_true, y_pred):
    """Mean Squared Error"""
    return np.mean((y_true - y_pred) ** 2)

# –ü—Ä–∏–∫–ª–∞–¥
y_true = np.array([3.0, -0.5, 2.0, 7.0])
y_pred = np.array([2.5, 0.0, 2.0, 8.0])

loss = mse_loss(y_true, y_pred)
print(f"MSE Loss: {loss:.4f}")

# –ì—Ä–∞–¥—ñ—î–Ω—Ç MSE
def mse_gradient(y_true, y_pred):
    """–ì—Ä–∞–¥—ñ—î–Ω—Ç MSE –ø–æ predictions"""
    return 2 * (y_pred - y_true) / len(y_true)

gradient = mse_gradient(y_true, y_pred)
print(f"Gradient: {gradient}")
```

### –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è

```python
import matplotlib.pyplot as plt
import numpy as np

# –ü–æ–º–∏–ª–∫–∏
errors = np.linspace(-5, 5, 100)
mse_values = errors ** 2
mae_values = np.abs(errors)

plt.figure(figsize=(10, 6))
plt.plot(errors, mse_values, linewidth=2, label='MSE (L2)', color='blue')
plt.plot(errors, mae_values, linewidth=2, label='MAE (L1)', color='red')

plt.xlabel('Error (y_true - y_pred)', fontsize=12)
plt.ylabel('Loss', fontsize=12)
plt.title('MSE vs MAE Loss Functions', fontsize=14, fontweight='bold')
plt.legend(fontsize=11)
plt.grid(True, alpha=0.3)
plt.axvline(x=0, color='k', linestyle='--', alpha=0.3)
plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)

# –ê–Ω–æ—Ç–∞—Ü—ñ—è
plt.annotate('MSE –∫–∞—Ä–∞—î –≤–µ–ª–∏–∫—ñ –ø–æ–º–∏–ª–∫–∏\n—Å–∏–ª—å–Ω—ñ—à–µ (–∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–æ)', 
            xy=(3, 9), xytext=(1, 15),
            fontsize=10,
            bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.7),
            arrowprops=dict(arrowstyle='->', color='blue', lw=2))

plt.tight_layout()
plt.show()
```

### –ü–µ—Ä–µ–≤–∞–≥–∏ —Ç–∞ –Ω–µ–¥–æ–ª—ñ–∫–∏

| –ü–µ—Ä–µ–≤–∞–≥–∏ | –ù–µ–¥–æ–ª—ñ–∫–∏ |
|----------|----------|
| ‚úÖ –î–∏—Ñ–µ—Ä–µ–Ω—Ü—ñ–π–æ–≤–Ω–∞ (smooth) | ‚ùå –î—É–∂–µ —á—É—Ç–ª–∏–≤–∞ –¥–æ outliers |
| ‚úÖ –î–æ–±—Ä–µ –ø—Ä–∞—Ü—é—î –∑ gradient descent | ‚ùå –ù–µ –≤ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏—Ö –æ–¥–∏–Ω–∏—Ü—è—Ö |
| ‚úÖ –ü–æ–ø—É–ª—è—Ä–Ω–∞ –≤ ML | ‚ùå –ú–æ–∂–µ –¥–∞–≤–∞—Ç–∏ –≤–µ–ª–∏–∫—ñ –≥—Ä–∞–¥—ñ—î–Ω—Ç–∏ |
| ‚úÖ Probabilistic interpretation (Gaussian) | |

### –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏?

- ‚úÖ **Default** –¥–ª—è regression
- ‚úÖ –ö–æ–ª–∏ –≤–µ–ª–∏–∫—ñ –ø–æ–º–∏–ª–∫–∏ **–∫—Ä–∏—Ç–∏—á–Ω—ñ—à—ñ** –∑–∞ –º–∞–ª—ñ
- ‚úÖ –ö–æ–ª–∏ –Ω–µ–º–∞—î –∑–Ω–∞—á–Ω–∏—Ö outliers
- ‚úÖ Linear regression, neural networks
- ‚ùå –ö–æ–ª–∏ —î outliers (–≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π MAE –∞–±–æ Huber)

---

## 2. Mean Absolute Error (MAE / L1 Loss)

### –§–æ—Ä–º—É–ª–∞

$$\text{MAE} = \frac{1}{n}\sum_{i=1}^{n}|y_i - \hat{y}_i|$$

### –Ü–Ω—Ç—É—ó—Ü—ñ—è

**MAE** ‚Äî **–ª—ñ–Ω—ñ–π–Ω–∞** –ø–æ–º–∏–ª–∫–∞. **–í—Å—ñ –ø–æ–º–∏–ª–∫–∏ —Ä—ñ–≤–Ω–æ—Ü—ñ–Ω–Ω—ñ**.

```
–ü–æ–º–∏–ª–∫–∏:     [1, 1, 1, 1, 1]  vs  [0, 0, 0, 0, 5]
–ê–±—Å–æ–ª—é—Ç–Ω—ñ:   [1, 1, 1, 1, 1]  vs  [0, 0, 0, 0, 5]

MAE:         5/5 = 1.0        vs  5/5 = 1.0

–û–±–∏–¥–≤–∞ –≤–∞—Ä—ñ–∞–Ω—Ç–∏ –º–∞—é—Ç—å –æ–¥–Ω–∞–∫–æ–≤—É MAE!
```

### –ö–æ–¥

```python
def mae_loss(y_true, y_pred):
    """Mean Absolute Error"""
    return np.mean(np.abs(y_true - y_pred))

# –ü—Ä–∏–∫–ª–∞–¥
y_true = np.array([3.0, -0.5, 2.0, 7.0])
y_pred = np.array([2.5, 0.0, 2.0, 8.0])

loss = mae_loss(y_true, y_pred)
print(f"MAE Loss: {loss:.4f}")

# –ì—Ä–∞–¥—ñ—î–Ω—Ç MAE (subgradient –≤ 0)
def mae_gradient(y_true, y_pred):
    """–ì—Ä–∞–¥—ñ—î–Ω—Ç MAE"""
    return np.sign(y_pred - y_true) / len(y_true)

gradient = mae_gradient(y_true, y_pred)
print(f"Gradient: {gradient}")
```

### –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è MSE vs MAE

```python
import numpy as np

# –î–∞–Ω—ñ –∑ outlier
y_true = np.array([1, 2, 3, 4, 100])  # 100 ‚Äî outlier!
y_pred = np.array([1.1, 2.1, 2.9, 4.2, 5])

mse = mse_loss(y_true, y_pred)
mae = mae_loss(y_true, y_pred)

print("–ó outlier:")
print(f"  MSE: {mse:.2f}")  # –î—É–∂–µ –≤–µ–ª–∏–∫–∞ —á–µ—Ä–µ–∑ outlier!
print(f"  MAE: {mae:.2f}")  # –ú–µ–Ω—à —á—É—Ç–ª–∏–≤–∞

# –ë–µ–∑ outlier
y_true_clean = np.array([1, 2, 3, 4, 5])
y_pred_clean = np.array([1.1, 2.1, 2.9, 4.2, 4.8])

mse_clean = mse_loss(y_true_clean, y_pred_clean)
mae_clean = mae_loss(y_true_clean, y_pred_clean)

print("\n–ë–µ–∑ outlier:")
print(f"  MSE: {mse_clean:.2f}")
print(f"  MAE: {mae_clean:.2f}")
```

### –ü–µ—Ä–µ–≤–∞–≥–∏ —Ç–∞ –Ω–µ–¥–æ–ª—ñ–∫–∏

| –ü–µ—Ä–µ–≤–∞–≥–∏ | –ù–µ–¥–æ–ª—ñ–∫–∏ |
|----------|----------|
| ‚úÖ –°—Ç—ñ–π–∫–∞ –¥–æ outliers | ‚ùå –ù–µ –¥–∏—Ñ–µ—Ä–µ–Ω—Ü—ñ–π–æ–≤–Ω–∞ –≤ 0 |
| ‚úÖ –í –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏—Ö –æ–¥–∏–Ω–∏—Ü—è—Ö | ‚ùå –ü–æ–≤—ñ–ª—å–Ω—ñ—à–∞ –∫–æ–Ω–≤–µ—Ä–≥–µ–Ω—Ü—ñ—è |
| ‚úÖ –ü—Ä–æ—Å—Ç–∞ —ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü—ñ—è | ‚ùå Gradient –ø–æ—Å—Ç—ñ–π–Ω–∏–π (–º–æ–∂–µ –±—É—Ç–∏ –ø—Ä–æ–±–ª–µ–º–∞) |

### –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏?

- ‚úÖ –ö–æ–ª–∏ —î **outliers**
- ‚úÖ –ö–æ–ª–∏ –≤—Å—ñ –ø–æ–º–∏–ª–∫–∏ **—Ä—ñ–≤–Ω–æ—Ü—ñ–Ω–Ω—ñ**
- ‚úÖ –ü–æ—Ç—Ä—ñ–±–Ω–∞ **—ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü—ñ—è** (—Ç—ñ –∂ –æ–¥–∏–Ω–∏—Ü—ñ)
- ‚ùå –ö–æ–ª–∏ –≤–µ–ª–∏–∫—ñ –ø–æ–º–∏–ª–∫–∏ –∫—Ä–∏—Ç–∏—á–Ω—ñ—à—ñ (–≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π MSE)

---

## 3. Huber Loss

### –§–æ—Ä–º—É–ª–∞

$$L_{\delta}(y, \hat{y}) = \begin{cases}
\frac{1}{2}(y - \hat{y})^2 & \text{for } |y - \hat{y}| \leq \delta \\
\delta(|y - \hat{y}| - \frac{1}{2}\delta) & \text{otherwise}
\end{cases}$$

### –Ü–Ω—Ç—É—ó—Ü—ñ—è

**Huber Loss** ‚Äî —Ü–µ **–∫–æ–º–±—ñ–Ω–∞—Ü—ñ—è MSE —ñ MAE**:
- –î–ª—è –º–∞–ª–∏—Ö –ø–æ–º–∏–ª–æ–∫ ‚Üí MSE (–∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–∞)
- –î–ª—è –≤–µ–ª–∏–∫–∏—Ö –ø–æ–º–∏–ª–æ–∫ ‚Üí MAE (–ª—ñ–Ω—ñ–π–Ω–∞)

**–ü–∞—Ä–∞–º–µ—Ç—Ä $\delta$** –∫–æ–Ω—Ç—Ä–æ–ª—é—î –ø–µ—Ä–µ—Ö—ñ–¥ –º—ñ–∂ MSE —ñ MAE.

```
|error| < Œ¥  ‚Üí MSE (smooth, —à–≤–∏–¥–∫–∞ –∫–æ–Ω–≤–µ—Ä–≥–µ–Ω—Ü—ñ—è)
|error| ‚â• Œ¥  ‚Üí MAE (robust –¥–æ outliers)

–ù–∞–π–∫—Ä–∞—â–µ –∑ –æ–±–æ—Ö —Å–≤—ñ—Ç—ñ–≤!
```

### –ö–æ–¥

```python
def huber_loss(y_true, y_pred, delta=1.0):
    """Huber Loss"""
    error = y_true - y_pred
    is_small_error = np.abs(error) <= delta
    
    squared_loss = 0.5 * error ** 2
    linear_loss = delta * (np.abs(error) - 0.5 * delta)
    
    return np.where(is_small_error, squared_loss, linear_loss).mean()

# –ü—Ä–∏–∫–ª–∞–¥
y_true = np.array([1, 2, 3, 4, 100])
y_pred = np.array([1.1, 2.1, 2.9, 4.2, 5])

mse = mse_loss(y_true, y_pred)
mae = mae_loss(y_true, y_pred)
huber = huber_loss(y_true, y_pred, delta=1.0)

print(f"MSE:   {mse:.2f}")
print(f"MAE:   {mae:.2f}")
print(f"Huber: {huber:.2f}")
```

### –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è

```python
import matplotlib.pyplot as plt
import numpy as np

# –ü–æ–º–∏–ª–∫–∏
errors = np.linspace(-5, 5, 1000)
delta = 1.5

# Loss values
mse_vals = errors ** 2
mae_vals = np.abs(errors)

huber_vals = np.where(
    np.abs(errors) <= delta,
    0.5 * errors ** 2,
    delta * (np.abs(errors) - 0.5 * delta)
)

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
plt.figure(figsize=(12, 6))

plt.plot(errors, mse_vals, linewidth=2, label='MSE', alpha=0.7)
plt.plot(errors, mae_vals, linewidth=2, label='MAE', alpha=0.7)
plt.plot(errors, huber_vals, linewidth=3, label=f'Huber (Œ¥={delta})', color='green')

# Vertical lines at ¬±Œ¥
plt.axvline(x=delta, color='red', linestyle='--', alpha=0.5, label=f'¬±Œ¥')
plt.axvline(x=-delta, color='red', linestyle='--', alpha=0.5)

plt.xlabel('Error', fontsize=12)
plt.ylabel('Loss', fontsize=12)
plt.title('Huber Loss: Best of Both Worlds', fontsize=14, fontweight='bold')
plt.legend(fontsize=11)
plt.grid(True, alpha=0.3)
plt.ylim(0, 10)
plt.tight_layout()
plt.show()
```

### –í–∏–±—ñ—Ä Œ¥

```python
# –†—ñ–∑–Ω—ñ Œ¥
deltas = [0.5, 1.0, 2.0, 5.0]

fig, axes = plt.subplots(2, 2, figsize=(14, 10))
axes = axes.ravel()

for idx, delta in enumerate(deltas):
    errors = np.linspace(-5, 5, 1000)
    
    huber_vals = np.where(
        np.abs(errors) <= delta,
        0.5 * errors ** 2,
        delta * (np.abs(errors) - 0.5 * delta)
    )
    
    axes[idx].plot(errors, huber_vals, linewidth=2, color='green')
    axes[idx].axvline(x=delta, color='red', linestyle='--', alpha=0.5)
    axes[idx].axvline(x=-delta, color='red', linestyle='--', alpha=0.5)
    axes[idx].set_title(f'Œ¥ = {delta}', fontsize=12, fontweight='bold')
    axes[idx].set_xlabel('Error')
    axes[idx].set_ylabel('Loss')
    axes[idx].grid(True, alpha=0.3)
    axes[idx].set_ylim(0, 10)

plt.suptitle('Huber Loss with Different Œ¥', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.show()
```

### –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏?

- ‚úÖ **Outliers** –ø—Ä–∏—Å—É—Ç–Ω—ñ, –∞–ª–µ –Ω–µ –¥—É–∂–µ –±–∞–≥–∞—Ç–æ
- ‚úÖ –ü–æ—Ç—Ä—ñ–±–µ–Ω –±–∞–ª–∞–Ω—Å –º—ñ–∂ MSE —ñ MAE
- ‚úÖ Regression –∑ —à—É–º–Ω–∏–º–∏ –¥–∞–Ω–∏–º–∏
- ‚úÖ Reinforcement learning

---

## 4. Log-Cosh Loss

### –§–æ—Ä–º—É–ª–∞

$$L(y, \hat{y}) = \sum_{i=1}^{n} \log(\cosh(\hat{y}_i - y_i))$$

–¥–µ $\cosh(x) = \frac{e^x + e^{-x}}{2}$

### –Ü–Ω—Ç—É—ó—Ü—ñ—è

**Log-Cosh** ‚Äî —Ü–µ **smoother –≤–µ—Ä—Å—ñ—è MAE**, —è–∫–∞ –ø—Ä–∞—Ü—é—î —è–∫ MSE –¥–ª—è –º–∞–ª–∏—Ö –ø–æ–º–∏–ª–æ–∫.

### –ö–æ–¥

```python
def log_cosh_loss(y_true, y_pred):
    """Log-Cosh Loss"""
    error = y_pred - y_true
    return np.mean(np.log(np.cosh(error)))

# –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è
y_true = np.array([1, 2, 3, 4, 100])
y_pred = np.array([1.1, 2.1, 2.9, 4.2, 5])

losses = {
    'MSE': mse_loss(y_true, y_pred),
    'MAE': mae_loss(y_true, y_pred),
    'Huber': huber_loss(y_true, y_pred, delta=1.0),
    'Log-Cosh': log_cosh_loss(y_true, y_pred)
}

for name, loss in losses.items():
    print(f"{name:10s}: {loss:.4f}")
```

---

# CLASSIFICATION LOSSES

## 5. Binary Cross-Entropy (Log Loss)

### –§–æ—Ä–º—É–ª–∞

$$\text{BCE} = -\frac{1}{n}\sum_{i=1}^{n}[y_i\log(\hat{y}_i) + (1-y_i)\log(1-\hat{y}_i)]$$

–¥–µ:
- $y_i \in \{0, 1\}$ ‚Äî —Å–ø—Ä–∞–≤–∂–Ω—ñ–π –∫–ª–∞—Å
- $\hat{y}_i \in [0, 1]$ ‚Äî –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–∞ –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å –∫–ª–∞—Å—É 1

### –Ü–Ω—Ç—É—ó—Ü—ñ—è

**Binary Cross-Entropy** ‚Äî —Ü–µ **–ª–æ–≥–∞—Ä–∏—Ñ–º—ñ—á–Ω–∞** loss –¥–ª—è –±—ñ–Ω–∞—Ä–Ω–æ—ó –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó. **–°–∏–ª—å–Ω–æ –∫–∞—Ä–∞—î –≤–ø–µ–≤–Ω–µ–Ω—ñ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ñ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è**.

```
–°–ø—Ä–∞–≤–∂–Ω—ñ–π –∫–ª–∞—Å: y = 1

–ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è:  ≈∑ = 0.9  ‚Üí Loss = -log(0.9) ‚âà 0.10   ‚úì –ú–∞–ª–∞
–ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è:  ≈∑ = 0.5  ‚Üí Loss = -log(0.5) ‚âà 0.69   ‚ö†Ô∏è –°–µ—Ä–µ–¥–Ω—è
–ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è:  ≈∑ = 0.1  ‚Üí Loss = -log(0.1) ‚âà 2.30   ‚ùå –í–µ–ª–∏–∫–∞
–ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è:  ≈∑ = 0.01 ‚Üí Loss = -log(0.01) ‚âà 4.61  ‚ùå‚ùå –î—É–∂–µ –≤–µ–ª–∏–∫–∞!
```

### –ö–æ–¥

```python
def binary_cross_entropy(y_true, y_pred):
    """Binary Cross-Entropy Loss"""
    # Clip –¥–ª—è —É–Ω–∏–∫–Ω–µ–Ω–Ω—è log(0)
    epsilon = 1e-15
    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
    
    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))

# –ü—Ä–∏–∫–ª–∞–¥
y_true = np.array([1, 0, 1, 1, 0])
y_pred = np.array([0.9, 0.1, 0.8, 0.6, 0.2])

loss = binary_cross_entropy(y_true, y_pred)
print(f"Binary Cross-Entropy: {loss:.4f}")

# Gradient
def bce_gradient(y_true, y_pred):
    """–ì—Ä–∞–¥—ñ—î–Ω—Ç BCE"""
    epsilon = 1e-15
    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
    return -(y_true / y_pred - (1 - y_true) / (1 - y_pred)) / len(y_true)

gradient = bce_gradient(y_true, y_pred)
print(f"Gradient: {gradient}")
```

### –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è

```python
import matplotlib.pyplot as plt
import numpy as np

# –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω—ñ –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ
y_pred = np.linspace(0.01, 0.99, 100)

# Loss –¥–ª—è y_true = 1
loss_y1 = -np.log(y_pred)

# Loss –¥–ª—è y_true = 0
loss_y0 = -np.log(1 - y_pred)

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
plt.figure(figsize=(12, 6))

plt.plot(y_pred, loss_y1, linewidth=3, label='y_true = 1', color='blue')
plt.plot(y_pred, loss_y0, linewidth=3, label='y_true = 0', color='red')

plt.xlabel('Predicted Probability ≈∑', fontsize=12)
plt.ylabel('Loss', fontsize=12)
plt.title('Binary Cross-Entropy Loss', fontsize=14, fontweight='bold')
plt.legend(fontsize=12)
plt.grid(True, alpha=0.3)
plt.ylim(0, 5)

# –ê–Ω–æ—Ç–∞—Ü—ñ—ó
plt.annotate('–í–ø–µ–≤–Ω–µ–Ω–µ –ø—Ä–∞–≤–∏–ª—å–Ω–µ\n–ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è\n(–Ω–∏–∑—å–∫–∞ loss)', 
            xy=(0.95, 0.05), xytext=(0.7, 1.5),
            fontsize=10,
            bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.7),
            arrowprops=dict(arrowstyle='->', color='green', lw=2))

plt.annotate('–í–ø–µ–≤–Ω–µ–Ω–µ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–µ\n–ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è\n(–≤–∏—Å–æ–∫–∞ loss)', 
            xy=(0.05, 3), xytext=(0.2, 4),
            fontsize=10,
            bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.7),
            arrowprops=dict(arrowstyle='->', color='red', lw=2))

plt.tight_layout()
plt.show()
```

### –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏?

- ‚úÖ **Binary classification** (–æ—Å–Ω–æ–≤–Ω–∞ loss)
- ‚úÖ Logistic regression
- ‚úÖ Binary output layer –≤ neural networks
- ‚úÖ –ö–æ–ª–∏ –ø–æ—Ç—Ä—ñ–±–Ω—ñ –∫–∞–ª—ñ–±—Ä–æ–≤–∞–Ω—ñ –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ

---

## 6. Categorical Cross-Entropy

### –§–æ—Ä–º—É–ª–∞

$$\text{CCE} = -\frac{1}{n}\sum_{i=1}^{n}\sum_{j=1}^{C}y_{ij}\log(\hat{y}_{ij})$$

–¥–µ:
- $C$ ‚Äî –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∫–ª–∞—Å—ñ–≤
- $y_{ij}$ ‚Äî one-hot encoded true label
- $\hat{y}_{ij}$ ‚Äî –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–∞ –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å –∫–ª–∞—Å—É $j$ –¥–ª—è –∑—Ä–∞–∑–∫–∞ $i$

### –Ü–Ω—Ç—É—ó—Ü—ñ—è

**Categorical Cross-Entropy** ‚Äî —Ü–µ —É–∑–∞–≥–∞–ª—å–Ω–µ–Ω–Ω—è BCE –¥–ª—è **multiclass classification**.

```
–ü—Ä–∏–∫–ª–∞–¥: 3 –∫–ª–∞—Å–∏

True label: [0, 1, 0]  (–∫–ª–∞—Å 1)
Predicted:  [0.1, 0.7, 0.2]

Loss = -(0*log(0.1) + 1*log(0.7) + 0*log(0.2))
     = -log(0.7)
     ‚âà 0.357

–¢—ñ–ª—å–∫–∏ –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å —Å–ø—Ä–∞–≤–∂–Ω—å–æ–≥–æ –∫–ª–∞—Å—É –≤–ø–ª–∏–≤–∞—î!
```

### –ö–æ–¥

```python
def categorical_cross_entropy(y_true, y_pred):
    """Categorical Cross-Entropy Loss"""
    epsilon = 1e-15
    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
    
    return -np.mean(np.sum(y_true * np.log(y_pred), axis=1))

# –ü—Ä–∏–∫–ª–∞–¥
y_true = np.array([
    [0, 1, 0],  # –ö–ª–∞—Å 1
    [1, 0, 0],  # –ö–ª–∞—Å 0
    [0, 0, 1]   # –ö–ª–∞—Å 2
])

y_pred = np.array([
    [0.1, 0.7, 0.2],
    [0.8, 0.1, 0.1],
    [0.2, 0.2, 0.6]
])

loss = categorical_cross_entropy(y_true, y_pred)
print(f"Categorical Cross-Entropy: {loss:.4f}")

# –ß–µ—Ä–µ–∑ Keras
import tensorflow as tf

cce = tf.keras.losses.CategoricalCrossentropy()
loss_keras = cce(y_true, y_pred).numpy()
print(f"Keras CCE: {loss_keras:.4f}")
```

### Sparse Categorical Cross-Entropy

```python
# –Ø–∫—â–æ labels –Ω–µ one-hot encoded
y_true_sparse = np.array([1, 0, 2])  # Class indices

y_pred = np.array([
    [0.1, 0.7, 0.2],
    [0.8, 0.1, 0.1],
    [0.2, 0.2, 0.6]
])

def sparse_categorical_cross_entropy(y_true, y_pred):
    """Sparse CCE (labels as integers)"""
    epsilon = 1e-15
    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
    
    # Extract predicted probability for true class
    n_samples = len(y_true)
    log_probs = -np.log(y_pred[range(n_samples), y_true])
    
    return np.mean(log_probs)

loss_sparse = sparse_categorical_cross_entropy(y_true_sparse, y_pred)
print(f"Sparse CCE: {loss_sparse:.4f}")
```

---

## 7. Hinge Loss (SVM)

### –§–æ—Ä–º—É–ª–∞

$$L(y, \hat{y}) = \max(0, 1 - y \cdot \hat{y})$$

–¥–µ:
- $y \in \{-1, +1\}$ ‚Äî —Å–ø—Ä–∞–≤–∂–Ω—ñ–π –∫–ª–∞—Å
- $\hat{y} \in \mathbb{R}$ ‚Äî raw prediction (–Ω–µ –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å!)

### –Ü–Ω—Ç—É—ó—Ü—ñ—è

**Hinge Loss** –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –≤ **SVM**. –í–∏–º–∞–≥–∞—î –Ω–µ –ø—Ä–æ—Å—Ç–æ –ø—Ä–∞–≤–∏–ª—å–Ω–æ—ó –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó, –∞ **margin** (–≤—ñ–¥—Å—Ç—É–ø).

```
y = +1 (positive class)

≈∑ > +1  ‚Üí Loss = 0              ‚úì –ü—Ä–∞–≤–∏–ª—å–Ω–æ –∑ margin
≈∑ = +1  ‚Üí Loss = 0              ‚úì –ù–∞ –º–µ–∂—ñ
≈∑ = 0   ‚Üí Loss = max(0, 1-0) = 1   ‚ùå –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ
≈∑ = -1  ‚Üí Loss = max(0, 1-(-1)) = 2 ‚ùå –î—É–∂–µ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ
```

### –ö–æ–¥

```python
def hinge_loss(y_true, y_pred):
    """
    Hinge Loss (for SVM)
    
    y_true in {-1, +1}
    y_pred in R (raw scores)
    """
    return np.mean(np.maximum(0, 1 - y_true * y_pred))

# –ü—Ä–∏–∫–ª–∞–¥
y_true = np.array([1, -1, 1, -1, 1])
y_pred = np.array([2.0, -1.5, 0.5, -0.3, -0.1])

loss = hinge_loss(y_true, y_pred)
print(f"Hinge Loss: {loss:.4f}")

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
import matplotlib.pyplot as plt

y_pred_range = np.linspace(-3, 3, 100)

# Loss –¥–ª—è y = +1
loss_pos = np.maximum(0, 1 - y_pred_range)

# Loss –¥–ª—è y = -1
loss_neg = np.maximum(0, 1 + y_pred_range)

plt.figure(figsize=(10, 6))
plt.plot(y_pred_range, loss_pos, linewidth=2, label='y = +1')
plt.plot(y_pred_range, loss_neg, linewidth=2, label='y = -1')
plt.xlabel('Predicted Score ≈∑', fontsize=12)
plt.ylabel('Loss', fontsize=12)
plt.title('Hinge Loss', fontsize=14, fontweight='bold')
plt.legend(fontsize=11)
plt.grid(True, alpha=0.3)
plt.axhline(y=0, color='k', linewidth=0.5)
plt.axvline(x=0, color='k', linewidth=0.5)
plt.tight_layout()
plt.show()
```

### Squared Hinge Loss

```python
def squared_hinge_loss(y_true, y_pred):
    """Squared Hinge Loss (smoother)"""
    return np.mean(np.maximum(0, 1 - y_true * y_pred) ** 2)
```

---

## 8. Focal Loss

### –§–æ—Ä–º—É–ª–∞

$$\text{FL}(p_t) = -\alpha_t(1-p_t)^\gamma\log(p_t)$$

–¥–µ:
- $p_t$ ‚Äî predicted probability –¥–ª—è true class
- $\gamma$ ‚Äî focusing parameter (–∑–∞–∑–≤–∏—á–∞–π 2)
- $\alpha_t$ ‚Äî balancing parameter

### –Ü–Ω—Ç—É—ó—Ü—ñ—è

**Focal Loss** ‚Äî —Ü–µ –º–æ–¥–∏—Ñ—ñ–∫–∞—Ü—ñ—è cross-entropy –¥–ª—è **class imbalance**. **–ó–º–µ–Ω—à—É—î –≤–∞–≥—É easy examples** (–¥–µ –º–æ–¥–µ–ª—å –≤–ø–µ–≤–Ω–µ–Ω–∞).

```
Cross-Entropy:  -log(p_t)
Focal Loss:     -(1-p_t)^Œ≥ * log(p_t)

–Ø–∫—â–æ p_t = 0.9 (–≤–ø–µ–≤–Ω–µ–Ω–µ –ø—Ä–∞–≤–∏–ª—å–Ω–µ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è):
  CE:  -log(0.9) ‚âà 0.10
  FL:  -(1-0.9)^2 * log(0.9) = -0.01 * 0.10 ‚âà 0.001  ‚Üê –£ 100 —Ä–∞–∑—ñ–≤ –º–µ–Ω—à–µ!

–§–æ–∫—É—Å –Ω–∞ –≤–∞–∂–∫–∏—Ö –ø—Ä–∏–∫–ª–∞–¥–∞—Ö!
```

### –ö–æ–¥

```python
def focal_loss(y_true, y_pred, gamma=2.0, alpha=0.25):
    """
    Focal Loss –¥–ª—è binary classification
    
    Parameters:
    -----------
    gamma : float
        Focusing parameter (–∑–∞–∑–≤–∏—á–∞–π 2)
    alpha : float
        Balancing parameter
    """
    epsilon = 1e-15
    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
    
    # Cross entropy
    ce = -(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))
    
    # Focal term
    p_t = np.where(y_true == 1, y_pred, 1 - y_pred)
    focal_term = (1 - p_t) ** gamma
    
    # Alpha balancing
    alpha_t = np.where(y_true == 1, alpha, 1 - alpha)
    
    return np.mean(alpha_t * focal_term * ce)

# –ü—Ä–∏–∫–ª–∞–¥: imbalanced data
y_true = np.array([1, 0, 0, 0, 0, 0, 0, 0, 0, 1])  # 80% class 0
y_pred = np.array([0.9, 0.1, 0.2, 0.1, 0.15, 0.05, 0.2, 0.1, 0.05, 0.85])

bce = binary_cross_entropy(y_true, y_pred)
fl = focal_loss(y_true, y_pred, gamma=2.0)

print(f"BCE:        {bce:.4f}")
print(f"Focal Loss: {fl:.4f}")
```

### –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è

```python
import matplotlib.pyplot as plt
import numpy as np

# Predicted probabilities
p_t = np.linspace(0.01, 0.99, 100)

# Cross-Entropy
ce = -np.log(p_t)

# Focal Loss –∑ —Ä—ñ–∑–Ω–∏–º–∏ gamma
gammas = [0, 0.5, 1, 2, 5]

plt.figure(figsize=(12, 6))

plt.plot(p_t, ce, linewidth=3, label='CE (Œ≥=0)', linestyle='--')

for gamma in gammas[1:]:
    fl = -(1 - p_t) ** gamma * np.log(p_t)
    plt.plot(p_t, fl, linewidth=2, label=f'Focal (Œ≥={gamma})')

plt.xlabel('Predicted Probability p_t (for true class)', fontsize=12)
plt.ylabel('Loss', fontsize=12)
plt.title('Focal Loss: Down-weighting Easy Examples', fontsize=14, fontweight='bold')
plt.legend(fontsize=11)
plt.grid(True, alpha=0.3)
plt.ylim(0, 5)

# –ê–Ω–æ—Ç–∞—Ü—ñ—è
plt.annotate('Easy examples\n(high p_t)\nget low weight', 
            xy=(0.9, 0.1), xytext=(0.7, 2),
            fontsize=10,
            bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.7),
            arrowprops=dict(arrowstyle='->', color='green', lw=2))

plt.tight_layout()
plt.show()
```

### –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏?

- ‚úÖ **Class imbalance** (rare objects detection)
- ‚úÖ **Object detection** (RetinaNet)
- ‚úÖ –ö–æ–ª–∏ –±–∞–≥–∞—Ç–æ easy examples
- ‚úÖ Segmentation –∑ imbalanced classes

---

# ADVANCED LOSSES

## 9. Contrastive Loss

### –§–æ—Ä–º—É–ª–∞

$$L = (1-y) \cdot \frac{1}{2}D^2 + y \cdot \frac{1}{2}\max(0, m - D)^2$$

–¥–µ:
- $D$ ‚Äî distance –º—ñ–∂ embeddings
- $y \in \{0, 1\}$ ‚Äî similar (0) —á–∏ dissimilar (1)
- $m$ ‚Äî margin

### –Ü–Ω—Ç—É—ó—Ü—ñ—è

**Contrastive Loss** –¥–ª—è **siamese networks**:
- Similar pairs ‚Üí –º—ñ–Ω—ñ–º—ñ–∑—É–≤–∞—Ç–∏ distance
- Dissimilar pairs ‚Üí –º–∞–∫—Å–∏–º—ñ–∑—É–≤–∞—Ç–∏ distance (–¥–æ margin)

### –ö–æ–¥

```python
def contrastive_loss(embeddings1, embeddings2, labels, margin=1.0):
    """
    Contrastive Loss
    
    Parameters:
    -----------
    embeddings1, embeddings2 : array-like
        Embeddings –ø–∞—Ä–∏
    labels : array-like
        0 = similar, 1 = dissimilar
    margin : float
        Minimum distance for dissimilar pairs
    """
    # Euclidean distance
    distances = np.linalg.norm(embeddings1 - embeddings2, axis=1)
    
    # Similar pairs: minimize distance
    similar_loss = (1 - labels) * distances ** 2
    
    # Dissimilar pairs: maximize distance up to margin
    dissimilar_loss = labels * np.maximum(0, margin - distances) ** 2
    
    return np.mean(0.5 * (similar_loss + dissimilar_loss))

# –ü—Ä–∏–∫–ª–∞–¥
embeddings1 = np.array([[1, 2], [3, 4], [5, 6]])
embeddings2 = np.array([[1.1, 2.1], [8, 9], [5.2, 6.1]])
labels = np.array([0, 1, 0])  # similar, dissimilar, similar

loss = contrastive_loss(embeddings1, embeddings2, labels, margin=2.0)
print(f"Contrastive Loss: {loss:.4f}")
```

---

## 10. Triplet Loss

### –§–æ—Ä–º—É–ª–∞

$$L = \max(0, D(a, p) - D(a, n) + \text{margin})$$

–¥–µ:
- $a$ ‚Äî anchor
- $p$ ‚Äî positive (similar to anchor)
- $n$ ‚Äî negative (dissimilar to anchor)
- $D$ ‚Äî distance function

### –Ü–Ω—Ç—É—ó—Ü—ñ—è

**Triplet Loss** –¥–ª—è **face recognition**, **metric learning**:
- Distance(anchor, positive) –º–∞—î –±—É—Ç–∏ **–º–µ–Ω—à–µ** –∑–∞
- Distance(anchor, negative) + margin

### –ö–æ–¥

```python
def triplet_loss(anchor, positive, negative, margin=0.5):
    """Triplet Loss"""
    # Distances
    pos_dist = np.linalg.norm(anchor - positive, axis=1)
    neg_dist = np.linalg.norm(anchor - negative, axis=1)
    
    # Loss
    loss = np.maximum(0, pos_dist - neg_dist + margin)
    
    return np.mean(loss)

# –ü—Ä–∏–∫–ª–∞–¥
anchor = np.array([[1, 2]])
positive = np.array([[1.1, 2.1]])  # –°—Ö–æ–∂–∏–π
negative = np.array([[5, 6]])      # –†—ñ–∑–Ω–∏–π

loss = triplet_loss(anchor, positive, negative, margin=0.5)
print(f"Triplet Loss: {loss:.4f}")
```

---

## –ü–æ—Ä—ñ–≤–Ω—è–ª—å–Ω–∞ —Ç–∞–±–ª–∏—Ü—è

| Loss Function | –ó–∞–¥–∞—á–∞ | Robust –¥–æ outliers? | Smooth? | –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ |
|---------------|--------|---------------------|---------|---------------------|
| **MSE** | Regression | ‚ùå | ‚úÖ | Default regression, no outliers |
| **MAE** | Regression | ‚úÖ | ‚ùå (–≤ 0) | Outliers present |
| **Huber** | Regression | ‚úÖ | ‚úÖ | Balance MSE/MAE |
| **BCE** | Binary Class | N/A | ‚úÖ | Binary classification |
| **CCE** | Multiclass | N/A | ‚úÖ | Multiclass classification |
| **Hinge** | Binary Class | ‚úÖ | ‚ùå | SVM, margin important |
| **Focal** | Class (imb) | N/A | ‚úÖ | Class imbalance |
| **Contrastive** | Metric Learning | N/A | ‚úÖ | Siamese networks |
| **Triplet** | Metric Learning | N/A | ‚úÖ | Face recognition |

---

## –ü—Ä–∞–∫—Ç–∏—á–Ω—ñ –ø–æ—Ä–∞–¥–∏ üí°

### 1. –í–∏–±—ñ—Ä loss function

```python
# Regression:
# - Default: MSE
# - Outliers: MAE –∞–±–æ Huber
# - Specific quantiles: Quantile Loss

# Binary Classification:
# - Default: Binary Cross-Entropy
# - Class imbalance: Focal Loss
# - Margin important: Hinge Loss

# Multiclass:
# - Default: Categorical Cross-Entropy
# - Imbalanced: Weighted CCE –∞–±–æ Focal
```

### 2. Custom Loss Functions

```python
import tensorflow as tf

# Custom loss —É Keras
def custom_mse_with_penalty(y_true, y_pred):
    """MSE –∑ –¥–æ–¥–∞—Ç–∫–æ–≤–∏–º penalty –¥–ª—è –≤–µ–ª–∏–∫–∏—Ö –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω—å"""
    mse = tf.reduce_mean(tf.square(y_true - y_pred))
    penalty = tf.reduce_mean(tf.square(y_pred))  # Penalize large predictions
    return mse + 0.1 * penalty

# –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è
model.compile(optimizer='adam', loss=custom_mse_with_penalty)
```

### 3. Loss Weighting –¥–ª—è Imbalanced Classes

```python
from sklearn.utils.class_weight import compute_class_weight

# –û–±—á–∏—Å–ª–∏—Ç–∏ –≤–∞–≥–∏
class_weights = compute_class_weight('balanced', 
                                     classes=np.unique(y_train),
                                     y=y_train)

# –£ Keras
model.fit(X_train, y_train, class_weight=dict(enumerate(class_weights)))
```

### 4. Combine Multiple Losses

```python
def combined_loss(y_true, y_pred, alpha=0.5):
    """–ö–æ–º–±—ñ–Ω–∞—Ü—ñ—è MSE —ñ MAE"""
    mse = tf.reduce_mean(tf.square(y_true - y_pred))
    mae = tf.reduce_mean(tf.abs(y_true - y_pred))
    return alpha * mse + (1 - alpha) * mae
```

---

## –†–µ–∞–ª—å–Ω–∏–π –ø—Ä–∏–∫–ª–∞–¥: –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è Loss Functions

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.metrics import mean_squared_error, mean_absolute_error

print("="*70)
print("COMPARING LOSS FUNCTIONS FOR REGRESSION")
print("="*70)

# –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –¥–∞–Ω–∏—Ö –∑ outliers
np.random.seed(42)
X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)

# –î–æ–¥–∞—Ç–∏ outliers
outlier_indices = [10, 25, 50, 75, 90]
y[outlier_indices] += np.random.choice([-100, 100], size=len(outlier_indices))

# Split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# Models (—Ä—ñ–∑–Ω—ñ loss functions)
models = {
    'Linear (MSE)': LinearRegression(),
    'Ridge (MSE + L2)': Ridge(alpha=1.0),
    'Lasso (MSE + L1)': Lasso(alpha=1.0)
}

# Train and evaluate
results = []

for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    
    mse = mean_squared_error(y_test, y_pred)
    mae = mean_absolute_error(y_test, y_pred)
    
    results.append({
        'Model': name,
        'MSE': mse,
        'MAE': mae
    })
    
    print(f"\n{name}:")
    print(f"  MSE: {mse:.2f}")
    print(f"  MAE: {mae:.2f}")

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

for idx, (name, model) in enumerate(models.items()):
    # Predictions
    X_plot = np.linspace(X.min(), X.max(), 300).reshape(-1, 1)
    y_plot = model.predict(X_plot)
    
    # Plot
    axes[idx].scatter(X_train, y_train, alpha=0.6, s=50, label='Train')
    axes[idx].scatter(X_test, y_test, alpha=0.6, s=50, label='Test', color='orange')
    
    # Outliers
    axes[idx].scatter(X[outlier_indices], y[outlier_indices], 
                     s=200, marker='x', color='red', linewidths=3, 
                     label='Outliers', zorder=5)
    
    axes[idx].plot(X_plot, y_plot, 'g-', linewidth=2, label='Model')
    
    axes[idx].set_title(f'{name}\nMSE: {results[idx]["MSE"]:.1f} | MAE: {results[idx]["MAE"]:.1f}',
                       fontsize=11, fontweight='bold')
    axes[idx].set_xlabel('X')
    axes[idx].set_ylabel('y')
    axes[idx].legend(fontsize=9)
    axes[idx].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("\n" + "="*70)
print("–í–∏—Å–Ω–æ–≤–æ–∫:")
print("MSE (Linear) —Å–∏–ª—å–Ω–æ —Å—Ç—Ä–∞–∂–¥–∞—î –≤—ñ–¥ outliers")
print("L1 regularization (Lasso) –±—ñ–ª—å—à robust")
print("="*70)
```

---

## –ö–ª—é—á–æ–≤—ñ –≤–∏—Å–Ω–æ–≤–∫–∏

> Loss Function ‚Äî —Ü–µ —Å–µ—Ä—Ü–µ ML –∞–ª–≥–æ—Ä–∏—Ç–º—É. –í–æ–Ω–∞ –≤–∏–∑–Ω–∞—á–∞—î, —â–æ –º–æ–¥–µ–ª—å –æ–ø—Ç–∏–º—ñ–∑—É—î –ø—ñ–¥ —á–∞—Å –Ω–∞–≤—á–∞–Ω–Ω—è.

**Regression:**
- **MSE** ‚Äî default, –∫–∞—Ä–∞—î –≤–µ–ª–∏–∫—ñ –ø–æ–º–∏–ª–∫–∏
- **MAE** ‚Äî robust –¥–æ outliers
- **Huber** ‚Äî –±–∞–ª–∞–Ω—Å MSE/MAE

**Classification:**
- **Binary CE** ‚Äî binary classification
- **Categorical CE** ‚Äî multiclass
- **Focal Loss** ‚Äî class imbalance

**–í–∏–±—ñ—Ä:**
```
Outliers?          ‚Üí MAE –∞–±–æ Huber
Class imbalance?   ‚Üí Focal Loss –∞–±–æ weighted CE
Margin important?  ‚Üí Hinge Loss
Metric learning?   ‚Üí Triplet/Contrastive
```

**–í–∞–∂–ª–∏–≤–æ:**
- –†—ñ–∑–Ω—ñ losses ‚Üí —Ä—ñ–∑–Ω–∞ –ø–æ–≤–µ–¥—ñ–Ω–∫–∞ –º–æ–¥–µ–ª—ñ
- MSE —á—É—Ç–ª–∏–≤–∞ –¥–æ outliers
- Cross-Entropy –¥–ª—è classification
- –ú–æ–∂–Ω–∞ –∫–æ–º–±—ñ–Ω—É–≤–∞—Ç–∏ losses

---

#ml #loss-functions #mse #mae #cross-entropy #huber-loss #focal-loss #optimization
