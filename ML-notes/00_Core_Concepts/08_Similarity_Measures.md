# Similarity Measures (–ú–µ—Ç—Ä–∏–∫–∏ —Å—Ö–æ–∂–æ—Å—Ç—ñ)

## –©–æ —Ü–µ?

**Similarity Measures** ‚Äî —Ü–µ —Ñ—É–Ω–∫—Ü—ñ—ó, —è–∫—ñ **–≤–∏–º—ñ—Ä—é—é—Ç—å —Å—Ö–æ–∂—ñ—Å—Ç—å** –º—ñ–∂ –¥–≤–æ–º–∞ –æ–±'—î–∫—Ç–∞–º–∏. –ß–∏–º –±—ñ–ª—å—à–µ –∑–Ω–∞—á–µ–Ω–Ω—è, —Ç–∏–º –±—ñ–ª—å—à —Å—Ö–æ–∂—ñ –æ–±'—î–∫—Ç–∏. –¶–µ **–¥–æ–ø–æ–≤–Ω–µ–Ω–Ω—è** –¥–æ distance metrics.

**–ì–æ–ª–æ–≤–Ω–∞ —ñ–¥–µ—è:** 
```
Distance ‚Üî Similarity

Distance –≤–µ–ª–∏–∫–∞ ‚Üí –æ–±'—î–∫—Ç–∏ —Ä—ñ–∑–Ω—ñ
Similarity –≤–µ–ª–∏–∫–∞ ‚Üí –æ–±'—î–∫—Ç–∏ —Å—Ö–æ–∂—ñ

–ß–∞—Å—Ç–æ: Similarity = 1 / (1 + Distance)
```

## –ù–∞–≤—ñ—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ?

- üîç **Recommender Systems** ‚Äî –∑–Ω–∞–π—Ç–∏ —Å—Ö–æ–∂–∏—Ö –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤/—Ç–æ–≤–∞—Ä–∏
- üìÑ **Document Similarity** ‚Äî –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è —Ç–µ–∫—Å—Ç—ñ–≤
- üñºÔ∏è **Image Retrieval** ‚Äî –ø–æ—à—É–∫ —Å—Ö–æ–∂–∏—Ö –∑–æ–±—Ä–∞–∂–µ–Ω—å
- üß¨ **Bioinformatics** ‚Äî –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –î–ù–ö/–±—ñ–ª–∫—ñ–≤
- üìä **Collaborative Filtering** ‚Äî —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó
- üéØ **Clustering** ‚Äî –≥—Ä—É–ø—É–≤–∞–Ω–Ω—è —Å—Ö–æ–∂–∏—Ö –æ–±'—î–∫—Ç—ñ–≤

## –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏?

**–ü–æ—Ç—Ä—ñ–±–Ω–æ:**
- Recommender systems
- –ü–æ—à—É–∫ —Å—Ö–æ–∂–∏—Ö –æ–±'—î–∫—Ç—ñ–≤
- Collaborative filtering
- Feature matching

**–í—ñ–¥–º—ñ–Ω–Ω—ñ—Å—Ç—å –≤—ñ–¥ Distance:**
- Distance ‚Üí "–Ω–∞—Å–∫—ñ–ª—å–∫–∏ —Ä—ñ–∑–Ω—ñ?"
- Similarity ‚Üí "–Ω–∞—Å–∫—ñ–ª—å–∫–∏ —Å—Ö–æ–∂—ñ?"

---

## –ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è –º–µ—Ç—Ä–∏–∫

```
Similarity Measures
‚îÇ
‚îú‚îÄ‚îÄ Vector-based
‚îÇ   ‚îú‚îÄ‚îÄ Cosine Similarity
‚îÇ   ‚îú‚îÄ‚îÄ Dot Product
‚îÇ   ‚îî‚îÄ‚îÄ Pearson Correlation
‚îÇ
‚îú‚îÄ‚îÄ Set-based
‚îÇ   ‚îú‚îÄ‚îÄ Jaccard Similarity
‚îÇ   ‚îú‚îÄ‚îÄ Dice Coefficient
‚îÇ   ‚îî‚îÄ‚îÄ Overlap Coefficient
‚îÇ
‚îú‚îÄ‚îÄ Probabilistic
‚îÇ   ‚îú‚îÄ‚îÄ Kullback-Leibler Divergence
‚îÇ   ‚îî‚îÄ‚îÄ Jensen-Shannon Divergence
‚îÇ
‚îî‚îÄ‚îÄ Other
    ‚îú‚îÄ‚îÄ Spearman Correlation
    ‚îú‚îÄ‚îÄ Kendall Tau
    ‚îî‚îÄ‚îÄ Tanimoto Coefficient
```

---

# 1. Cosine Similarity

## –§–æ—Ä–º—É–ª–∞

$$\text{cosine\_similarity}(\mathbf{x}, \mathbf{y}) = \frac{\mathbf{x} \cdot \mathbf{y}}{\|\mathbf{x}\| \|\mathbf{y}\|} = \frac{\sum_{i=1}^{n} x_i y_i}{\sqrt{\sum_{i=1}^{n} x_i^2} \sqrt{\sum_{i=1}^{n} y_i^2}}$$

Range: $[-1, 1]$
- 1 ‚Üí —ñ–¥–µ–Ω—Ç–∏—á–Ω–∏–π –Ω–∞–ø—Ä—è–º–æ–∫
- 0 ‚Üí –æ—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω—ñ (–Ω–µ–∑–∞–ª–µ–∂–Ω—ñ)
- -1 ‚Üí –ø—Ä–æ—Ç–∏–ª–µ–∂–Ω–∏–π –Ω–∞–ø—Ä—è–º–æ–∫

## –Ü–Ω—Ç—É—ó—Ü—ñ—è

**Cosine Similarity** –≤–∏–º—ñ—Ä—é—î **–∫—É—Ç –º—ñ–∂ –≤–µ–∫—Ç–æ—Ä–∞–º–∏**, —ñ–≥–Ω–æ—Ä—É—é—á–∏ —ó—Ö –¥–æ–≤–∂–∏–Ω—É.

```
–ü—Ä–∏–∫–ª–∞–¥: –£–ø–æ–¥–æ–±–∞–Ω–Ω—è –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤

User A: [5, 4, 0, 0, 3]  (—Ñ—ñ–ª—å–º–∏: Action, Comedy, Drama, Horror, Sci-Fi)
User B: [10, 8, 0, 0, 6]  (—É 2 —Ä–∞–∑–∏ –±—ñ–ª—å—à–µ –æ—Ü—ñ–Ω–æ–∫, –∞–ª–µ —Ç—ñ –∂ –∂–∞–Ω—Ä–∏!)

Euclidean distance: –≤–µ–ª–∏–∫–∞ (—Ä—ñ–∑–Ω—ñ magnitude)
Cosine similarity: 1.0 (—Ç–æ–π —Å–∞–º–∏–π –Ω–∞–ø—Ä—è–º–æ–∫ —É–ø–æ–¥–æ–±–∞–Ω—å!)

–í–∏—Å–Ω–æ–≤–æ–∫: –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ –¥—É–∂–µ —Å—Ö–æ–∂—ñ –∑–∞ —Å–º–∞–∫–∞–º–∏
```

## –ö–æ–¥

```python
import numpy as np

def cosine_similarity(x, y):
    """–û–±—á–∏—Å–ª–∏—Ç–∏ Cosine Similarity"""
    dot_product = np.dot(x, y)
    norm_x = np.linalg.norm(x)
    norm_y = np.linalg.norm(y)
    return dot_product / (norm_x * norm_y)

# –ü—Ä–∏–∫–ª–∞–¥: –†–µ–π—Ç–∏–Ω–≥–∏ —Ñ—ñ–ª—å–º—ñ–≤
user_A = np.array([5, 4, 0, 0, 3])
user_B = np.array([10, 8, 0, 0, 6])
user_C = np.array([0, 0, 5, 5, 0])

sim_AB = cosine_similarity(user_A, user_B)
sim_AC = cosine_similarity(user_A, user_C)

print(f"Similarity(A, B): {sim_AB:.4f}")  # ~1.0 (–¥—É–∂–µ —Å—Ö–æ–∂—ñ)
print(f"Similarity(A, C): {sim_AC:.4f}")  # 0.0 (—Ä—ñ–∑–Ω—ñ –∂–∞–Ω—Ä–∏)

# –ß–µ—Ä–µ–∑ sklearn
from sklearn.metrics.pairwise import cosine_similarity as sklearn_cosine

users = np.array([user_A, user_B, user_C])
similarity_matrix = sklearn_cosine(users)

print("\nSimilarity Matrix:")
print(similarity_matrix)
```

## –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è

```python
import matplotlib.pyplot as plt
import numpy as np

# –í–µ–∫—Ç–æ—Ä–∏ –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤ (TF-IDF –¥–ª—è —Å–ª—ñ–≤)
doc1 = np.array([2, 3, 0, 1])  # "machine learning AI python"
doc2 = np.array([4, 6, 0, 2])  # –¢–æ–π —Å–∞–º–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç, –±—ñ–ª—å—à–µ —Å–ª—ñ–≤
doc3 = np.array([0, 1, 5, 0])  # –Ü–Ω—à–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç "data statistics R"

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è (–≤–∏–∫–æ—Ä–∏—Å—Ç–∞—î–º–æ –ø–µ—Ä—à—ñ 2 –≤–∏–º—ñ—Ä–∏)
docs_2d = np.array([
    [doc1[0], doc1[1]],
    [doc2[0], doc2[1]],
    [doc3[0], doc3[1]]
])

# –û–±—á–∏—Å–ª–∏—Ç–∏ —Å—Ö–æ–∂–æ—Å—Ç—ñ
sim_12 = cosine_similarity(doc1, doc2)
sim_13 = cosine_similarity(doc1, doc3)

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
plt.figure(figsize=(10, 8))

origin = [0, 0]

# –í–µ–∫—Ç–æ—Ä–∏
plt.quiver(*origin, docs_2d[0, 0], docs_2d[0, 1], angles='xy', scale_units='xy', 
          scale=1, color='blue', width=0.01, label=f'Doc1')
plt.quiver(*origin, docs_2d[1, 0], docs_2d[1, 1], angles='xy', scale_units='xy', 
          scale=1, color='green', width=0.01, label=f'Doc2')
plt.quiver(*origin, docs_2d[2, 0], docs_2d[2, 1], angles='xy', scale_units='xy', 
          scale=1, color='red', width=0.01, label=f'Doc3')

# –ê–Ω–æ—Ç–∞—Ü—ñ—ó
plt.text(docs_2d[0, 0]/2, docs_2d[0, 1]/2 + 0.5, 'Doc1\n(ML, AI)', 
        fontsize=10, ha='center',
        bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.7))
plt.text(docs_2d[1, 0]/2, docs_2d[1, 1]/2 - 0.5, 'Doc2\n(Same topic)', 
        fontsize=10, ha='center',
        bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.7))
plt.text(docs_2d[2, 0]/2 + 0.5, docs_2d[2, 1]/2, 'Doc3\n(Stats)', 
        fontsize=10, ha='center',
        bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.7))

plt.xlim(-1, 7)
plt.ylim(-1, 8)
plt.xlabel('Feature 1 (e.g., "machine")', fontsize=12)
plt.ylabel('Feature 2 (e.g., "learning")', fontsize=12)
plt.title(f'Cosine Similarity\nDoc1-Doc2: {sim_12:.3f} | Doc1-Doc3: {sim_13:.3f}', 
         fontsize=14, fontweight='bold')
plt.legend(fontsize=11)
plt.grid(True, alpha=0.3)
plt.axhline(y=0, color='k', linewidth=0.5)
plt.axvline(x=0, color='k', linewidth=0.5)
plt.tight_layout()
plt.show()
```

## –ü—Ä–∏–∫–ª–∞–¥: Document Similarity

```python
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# –î–æ–∫—É–º–µ–Ω—Ç–∏
documents = [
    "Machine learning is a subset of artificial intelligence",
    "Deep learning is a type of machine learning",
    "Natural language processing uses machine learning",
    "Statistics is important for data science"
]

# TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü—ñ—è
vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(documents)

# Cosine similarity matrix
similarity_matrix = cosine_similarity(tfidf_matrix)

print("Document Similarity Matrix:")
print(similarity_matrix)

# –ó–Ω–∞–π—Ç–∏ –Ω–∞–π–±—ñ–ª—å—à —Å—Ö–æ–∂—ñ –¥–æ–∫—É–º–µ–Ω—Ç–∏
for i in range(len(documents)):
    for j in range(i+1, len(documents)):
        sim = similarity_matrix[i, j]
        print(f"\nDoc{i} vs Doc{j}: {sim:.4f}")
        print(f"  Doc{i}: {documents[i][:50]}...")
        print(f"  Doc{j}: {documents[j][:50]}...")

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(10, 8))
sns.heatmap(similarity_matrix, annot=True, fmt='.3f', cmap='YlOrRd',
           xticklabels=[f'Doc{i}' for i in range(len(documents))],
           yticklabels=[f'Doc{i}' for i in range(len(documents))],
           cbar_kws={'label': 'Cosine Similarity'})
plt.title('Document Similarity Matrix (Cosine)', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.show()
```

## –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏?

- ‚úÖ **Text mining** (TF-IDF vectors)
- ‚úÖ **Recommender systems** (user-item ratings)
- ‚úÖ **Image retrieval** (feature vectors)
- ‚úÖ –ö–æ–ª–∏ –≤–∞–∂–ª–∏–≤–∏–π **–Ω–∞–ø—Ä—è–º–æ–∫**, –Ω–µ magnitude
- ‚úÖ **Sparse high-dimensional** data
- ‚ùå –ö–æ–ª–∏ magnitude –≤–∞–∂–ª–∏–≤–∞

---

# 2. Pearson Correlation Coefficient

## –§–æ—Ä–º—É–ª–∞

$$r = \frac{\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum_{i=1}^{n}(x_i - \bar{x})^2}\sqrt{\sum_{i=1}^{n}(y_i - \bar{y})^2}}$$

Range: $[-1, 1]$
- 1 ‚Üí perfect positive correlation
- 0 ‚Üí no correlation
- -1 ‚Üí perfect negative correlation

## –Ü–Ω—Ç—É—ó—Ü—ñ—è

**Pearson Correlation** –≤–∏–º—ñ—Ä—é—î **–ª—ñ–Ω—ñ–π–Ω—É –∑–∞–ª–µ–∂–Ω—ñ—Å—Ç—å** –º—ñ–∂ –¥–≤–æ–º–∞ –∑–º—ñ–Ω–Ω–∏–º–∏. –ù–∞ –≤—ñ–¥–º—ñ–Ω—É –≤—ñ–¥ Cosine, **—Ü–µ–Ω—Ç—Ä—É—î** –¥–∞–Ω—ñ (–≤—ñ–¥–Ω—ñ–º–∞—î mean).

```
–†—ñ–∑–Ω–∏—Ü—è Cosine vs Pearson:

User ratings:
A: [5, 5, 5, 1, 1]
B: [4, 4, 4, 2, 2]

Cosine: –≤–∏—Å–æ–∫–∞ —Å—Ö–æ–∂—ñ—Å—Ç—å (—Å—Ö–æ–∂–∏–π –ø–∞—Ç—Ç–µ—Ä–Ω)
Pearson: —â–µ –≤–∏—â–∞ (–≤—Ä–∞—Ö–æ–≤—É—î, —â–æ B —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–Ω–æ –Ω–∏–∂—á–µ)

Pearson –∫—Ä–∞—â–µ –¥–ª—è recommender systems!
```

## –ö–æ–¥

```python
import numpy as np

def pearson_correlation(x, y):
    """–û–±—á–∏—Å–ª–∏—Ç–∏ Pearson Correlation"""
    x_centered = x - np.mean(x)
    y_centered = y - np.mean(y)
    
    numerator = np.sum(x_centered * y_centered)
    denominator = np.sqrt(np.sum(x_centered**2) * np.sum(y_centered**2))
    
    return numerator / denominator

# –ü—Ä–∏–∫–ª–∞–¥: –†–µ–π—Ç–∏–Ω–≥–∏ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤
user_A = np.array([5, 5, 5, 1, 1])
user_B = np.array([4, 4, 4, 2, 2])
user_C = np.array([1, 1, 1, 5, 5])

corr_AB = pearson_correlation(user_A, user_B)
corr_AC = pearson_correlation(user_A, user_C)

print(f"Pearson(A, B): {corr_AB:.4f}")  # –ü–æ–∑–∏—Ç–∏–≤–Ω–∞ –∫–æ—Ä–µ–ª—è—Ü—ñ—è
print(f"Pearson(A, C): {corr_AC:.4f}")  # –ù–µ–≥–∞—Ç–∏–≤–Ω–∞ –∫–æ—Ä–µ–ª—è—Ü—ñ—è

# –ß–µ—Ä–µ–∑ numpy
corr_AB_np = np.corrcoef(user_A, user_B)[0, 1]
print(f"NumPy Pearson(A, B): {corr_AB_np:.4f}")

# –ß–µ—Ä–µ–∑ scipy
from scipy.stats import pearsonr
corr, p_value = pearsonr(user_A, user_B)
print(f"Scipy Pearson(A, B): {corr:.4f} (p-value: {p_value:.4f})")
```

## –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è Cosine vs Pearson

```python
import matplotlib.pyplot as plt
import numpy as np

# –î–∞–Ω—ñ
user_A = np.array([5, 5, 5, 1, 1])
user_B = np.array([4, 4, 4, 2, 2])  # –°–∏—Å—Ç–µ–º–∞—Ç–∏—á–Ω–æ –Ω–∏–∂—á–µ
user_C = np.array([1, 1, 1, 5, 5])  # –ü—Ä–æ—Ç–∏–ª–µ–∂–Ω—ñ –≤–ø–æ–¥–æ–±–∞–Ω–Ω—è

items = ['Item1', 'Item2', 'Item3', 'Item4', 'Item5']

# –û–±—á–∏—Å–ª–∏—Ç–∏ –º–µ—Ç—Ä–∏–∫–∏
cos_AB = cosine_similarity(user_A, user_B)
cos_AC = cosine_similarity(user_A, user_C)
pearson_AB = pearson_correlation(user_A, user_B)
pearson_AC = pearson_correlation(user_A, user_C)

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Subplot 1: A vs B
axes[0].plot(items, user_A, 'o-', linewidth=2, markersize=10, label='User A')
axes[0].plot(items, user_B, 's-', linewidth=2, markersize=10, label='User B')
axes[0].set_ylabel('Rating', fontsize=12)
axes[0].set_title(f'User A vs B\nCosine: {cos_AB:.3f} | Pearson: {pearson_AB:.3f}', 
                 fontsize=12, fontweight='bold')
axes[0].legend(fontsize=11)
axes[0].grid(True, alpha=0.3)
axes[0].set_ylim(0, 6)

# Subplot 2: A vs C
axes[1].plot(items, user_A, 'o-', linewidth=2, markersize=10, label='User A')
axes[1].plot(items, user_C, '^-', linewidth=2, markersize=10, label='User C', color='red')
axes[1].set_ylabel('Rating', fontsize=12)
axes[1].set_title(f'User A vs C\nCosine: {cos_AC:.3f} | Pearson: {pearson_AC:.3f}', 
                 fontsize=12, fontweight='bold')
axes[1].legend(fontsize=11)
axes[1].grid(True, alpha=0.3)
axes[1].set_ylim(0, 6)

plt.tight_layout()
plt.show()

print("–í–∏—Å–Ω–æ–≤–∫–∏:")
print("A vs B: —Å—Ö–æ–∂—ñ –ø–∞—Ç—Ç–µ—Ä–Ω–∏ (B —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–Ω–æ –Ω–∏–∂—á–µ)")
print(f"  Cosine:  {cos_AB:.3f}")
print(f"  Pearson: {pearson_AB:.3f} (–º–∞–π–∂–µ perfect correlation)")
print("\nA vs C: –ø—Ä–æ—Ç–∏–ª–µ–∂–Ω—ñ –≤–ø–æ–¥–æ–±–∞–Ω–Ω—è")
print(f"  Cosine:  {cos_AC:.3f}")
print(f"  Pearson: {pearson_AC:.3f} (perfect negative correlation)")
```

## –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏?

- ‚úÖ **Recommender systems** (centered ratings)
- ‚úÖ –ö–æ–ª–∏ —î **systematic bias** (–æ–¥–∏–Ω user –∑–∞–≤–∂–¥–∏ –≤–∏—â–µ)
- ‚úÖ –í–∏—è–≤–ª–µ–Ω–Ω—è **linear relationships**
- ‚úÖ **Feature selection** (correlation with target)
- ‚ùå Non-linear relationships (–≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π Spearman)

---

# 3. Jaccard Similarity

## –§–æ—Ä–º—É–ª–∞

$$J(\mathbf{A}, \mathbf{B}) = \frac{|\mathbf{A} \cap \mathbf{B}|}{|\mathbf{A} \cup \mathbf{B}|}$$

Range: $[0, 1]$
- 0 ‚Üí –∑–æ–≤—Å—ñ–º —Ä—ñ–∑–Ω—ñ
- 1 ‚Üí —ñ–¥–µ–Ω—Ç–∏—á–Ω—ñ

## –Ü–Ω—Ç—É—ó—Ü—ñ—è

**Jaccard Similarity** –¥–ª—è **–º–Ω–æ–∂–∏–Ω** ‚Äî —Å–ø—ñ–≤–≤—ñ–¥–Ω–æ—à–µ–Ω–Ω—è **—Å–ø—ñ–ª—å–Ω–∏—Ö –µ–ª–µ–º–µ–Ω—Ç—ñ–≤** –¥–æ **–≤—Å—ñ—Ö —É–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö**.

```
–ü—Ä–∏–∫–ª–∞–¥: –ü–æ–∫—É–ø–∫–∏

Customer A –∫—É–ø–∏–≤: {milk, bread, eggs, butter}
Customer B –∫—É–ø–∏–≤: {milk, bread, cheese}

–°–ø—ñ–ª—å–Ω—ñ: {milk, bread} ‚Üí 2 –µ–ª–µ–º–µ–Ω—Ç–∏
–í—Å—ñ —É–Ω—ñ–∫–∞–ª—å–Ω—ñ: {milk, bread, eggs, butter, cheese} ‚Üí 5 –µ–ª–µ–º–µ–Ω—Ç—ñ–≤

Jaccard = 2/5 = 0.4
```

## –ö–æ–¥

```python
def jaccard_similarity(set_a, set_b):
    """–û–±—á–∏—Å–ª–∏—Ç–∏ Jaccard Similarity –¥–ª—è –º–Ω–æ–∂–∏–Ω"""
    intersection = len(set_a & set_b)
    union = len(set_a | set_b)
    return intersection / union if union > 0 else 0

# –ü—Ä–∏–∫–ª–∞–¥: –ü–æ–∫—É–ø–∫–∏
customer_A = {'milk', 'bread', 'eggs', 'butter'}
customer_B = {'milk', 'bread', 'cheese'}
customer_C = {'apple', 'banana', 'orange'}

sim_AB = jaccard_similarity(customer_A, customer_B)
sim_AC = jaccard_similarity(customer_A, customer_C)

print(f"Jaccard(A, B): {sim_AB:.4f}")  # 0.4 (—Å—Ö–æ–∂—ñ)
print(f"Jaccard(A, C): {sim_AC:.4f}")  # 0.0 (—Ä—ñ–∑–Ω—ñ)

# –î–ª—è binary vectors
from sklearn.metrics import jaccard_score

# Binary representation
items = ['milk', 'bread', 'eggs', 'butter', 'cheese', 'apple']
vector_A = [1, 1, 1, 1, 0, 0]  # A –∫—É–ø–∏–≤ milk, bread, eggs, butter
vector_B = [1, 1, 0, 0, 1, 0]  # B –∫—É–ø–∏–≤ milk, bread, cheese

jaccard_sklearn = jaccard_score(vector_A, vector_B, average='binary')
print(f"Jaccard (binary vectors): {jaccard_sklearn:.4f}")
```

## –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è

```python
import matplotlib.pyplot as plt
from matplotlib_venn import venn2

# –ú–Ω–æ–∂–∏–Ω–∏
set_A = {'milk', 'bread', 'eggs', 'butter'}
set_B = {'milk', 'bread', 'cheese'}

# Jaccard
jaccard = jaccard_similarity(set_A, set_B)

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è Venn diagram
plt.figure(figsize=(10, 6))

venn = venn2([set_A, set_B], set_labels=('Customer A', 'Customer B'))

# –ö–æ–ª—å–æ—Ä–∏
venn.get_patch_by_id('10').set_color('lightblue')
venn.get_patch_by_id('01').set_color('lightcoral')
venn.get_patch_by_id('11').set_color('lightgreen')

plt.title(f'Jaccard Similarity = {jaccard:.3f}\n'
         f'Intersection: {len(set_A & set_B)} | Union: {len(set_A | set_B)}',
         fontsize=14, fontweight='bold')
plt.tight_layout()
plt.show()

print(f"Set A: {set_A}")
print(f"Set B: {set_B}")
print(f"Intersection: {set_A & set_B}")
print(f"Union: {set_A | set_B}")
print(f"Jaccard: {jaccard:.4f}")
```

## –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏?

- ‚úÖ **Recommender systems** (item sets)
- ‚úÖ **Market basket analysis**
- ‚úÖ **Document similarity** (word sets)
- ‚úÖ **Genomics** (gene sets)
- ‚úÖ Binary/set data
- ‚ùå Continuous values (–≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π Cosine/Pearson)

---

# 4. Dice Coefficient (S√∏rensen-Dice)

## –§–æ—Ä–º—É–ª–∞

$$\text{Dice}(\mathbf{A}, \mathbf{B}) = \frac{2|\mathbf{A} \cap \mathbf{B}|}{|\mathbf{A}| + |\mathbf{B}|}$$

Range: $[0, 1]$

## –Ü–Ω—Ç—É—ó—Ü—ñ—è

**Dice Coefficient** —Å—Ö–æ–∂–∞ –Ω–∞ Jaccard, –∞–ª–µ **–¥–∞—î –±—ñ–ª—å—à—É –≤–∞–≥—É —Å–ø—ñ–ª—å–Ω–∏–º –µ–ª–µ–º–µ–Ω—Ç–∞–º** (–º–Ω–æ–∂–∏—Ç—å –Ω–∞ 2).

```
A = {1, 2, 3, 4}
B = {3, 4, 5, 6}

Intersection: {3, 4} ‚Üí 2
Union: {1, 2, 3, 4, 5, 6} ‚Üí 6

Jaccard = 2/6 ‚âà 0.33
Dice = 2*2/(4+4) = 4/8 = 0.5  ‚Üê –í–∏—â–µ!
```

## –ö–æ–¥

```python
def dice_coefficient(set_a, set_b):
    """–û–±—á–∏—Å–ª–∏—Ç–∏ Dice Coefficient"""
    intersection = len(set_a & set_b)
    return 2 * intersection / (len(set_a) + len(set_b)) if (len(set_a) + len(set_b)) > 0 else 0

# –ü—Ä–∏–∫–ª–∞–¥
set_A = {1, 2, 3, 4}
set_B = {3, 4, 5, 6}

jaccard = jaccard_similarity(set_A, set_B)
dice = dice_coefficient(set_A, set_B)

print(f"Jaccard: {jaccard:.4f}")
print(f"Dice:    {dice:.4f}")

# –ó–≤'—è–∑–æ–∫ –º—ñ–∂ Jaccard —ñ Dice
# Dice = 2*Jaccard / (1 + Jaccard)
dice_from_jaccard = 2 * jaccard / (1 + jaccard)
print(f"Dice (from Jaccard): {dice_from_jaccard:.4f}")
```

## –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è Jaccard vs Dice

```python
import matplotlib.pyplot as plt
import numpy as np

# –†—ñ–∑–Ω—ñ —Ä–æ–∑–º—ñ—Ä–∏ –ø–µ—Ä–µ—Ç–∏–Ω—É
sizes = np.linspace(0, 1, 100)  # –ß–∞—Å—Ç–∫–∞ –ø–µ—Ä–µ—Ç–∏–Ω—É

jaccard_values = []
dice_values = []

for intersection_ratio in sizes:
    # –ü—Ä–∏–ø—É—Å—Ç–∏–º–æ |A| = |B| = 1
    # Intersection = intersection_ratio
    # Union = 2 - intersection_ratio
    
    jaccard = intersection_ratio / (2 - intersection_ratio)
    dice = 2 * intersection_ratio / 2
    
    jaccard_values.append(jaccard)
    dice_values.append(dice)

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
plt.figure(figsize=(10, 6))
plt.plot(sizes, jaccard_values, linewidth=2, label='Jaccard')
plt.plot(sizes, dice_values, linewidth=2, label='Dice', linestyle='--')
plt.xlabel('Intersection Ratio', fontsize=12)
plt.ylabel('Similarity', fontsize=12)
plt.title('Jaccard vs Dice Coefficient', fontsize=14, fontweight='bold')
plt.legend(fontsize=11)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

print("–°–ø–æ—Å—Ç–µ—Ä–µ–∂–µ–Ω–Ω—è:")
print("Dice –∑–∞–≤–∂–¥–∏ >= Jaccard (–¥–∞—î –±—ñ–ª—å—à—É –≤–∞–≥—É –ø–µ—Ä–µ—Ç–∏–Ω—É)")
```

---

# 5. Spearman Rank Correlation

## –§–æ—Ä–º—É–ª–∞

$$\rho = 1 - \frac{6\sum d_i^2}{n(n^2-1)}$$

–¥–µ $d_i$ ‚Äî —Ä—ñ–∑–Ω–∏—Ü—è —Ä–∞–Ω–≥—ñ–≤.

Range: $[-1, 1]$

## –Ü–Ω—Ç—É—ó—Ü—ñ—è

**Spearman** ‚Äî —Ü–µ **Pearson –¥–ª—è —Ä–∞–Ω–≥—ñ–≤**. –í–∏–º—ñ—Ä—é—î **–º–æ–Ω–æ—Ç–æ–Ω–Ω—É** (–Ω–µ –æ–±–æ–≤'—è–∑–∫–æ–≤–æ –ª—ñ–Ω—ñ–π–Ω—É) –∑–∞–ª–µ–∂–Ω—ñ—Å—Ç—å.

```
–ü—Ä–∏–∫–ª–∞–¥: Salary vs Experience

Experience: [1, 2, 5, 10, 15] years
Salary:     [30, 35, 55, 85, 100] k$

–ù–µ–ª—ñ–Ω—ñ–π–Ω–∞ –∑–∞–ª–µ–∂–Ω—ñ—Å—Ç—å (exponential growth)
Pearson: –º–æ–∂–µ –±—É—Ç–∏ —Å–µ—Ä–µ–¥–Ω—è
Spearman: –≤–∏—Å–æ–∫–∞ (monotonic increase)
```

## –ö–æ–¥

```python
from scipy.stats import spearmanr, pearsonr
import numpy as np

# –ü—Ä–∏–∫–ª–∞–¥: –ù–µ–ª—ñ–Ω—ñ–π–Ω–∞ –∑–∞–ª–µ–∂–Ω—ñ—Å—Ç—å
x = np.array([1, 2, 5, 10, 15])
y = np.array([30, 35, 55, 85, 100])  # –ü—Ä–∏–±–ª–∏–∑–Ω–æ exponential

# Pearson (–ª—ñ–Ω—ñ–π–Ω–∞)
pearson_corr, _ = pearsonr(x, y)

# Spearman (–º–æ–Ω–æ—Ç–æ–Ω–Ω–∞)
spearman_corr, _ = spearmanr(x, y)

print(f"Pearson:  {pearson_corr:.4f}")
print(f"Spearman: {spearman_corr:.4f}")

# –î–ª—è —Å–∏–ª—å–Ω–æ –Ω–µ–ª—ñ–Ω—ñ–π–Ω–æ—ó
x_nonlinear = np.array([1, 2, 3, 4, 5])
y_nonlinear = np.array([1, 4, 9, 16, 25])  # –ö–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–∞

pearson_nl, _ = pearsonr(x_nonlinear, y_nonlinear)
spearman_nl, _ = spearmanr(x_nonlinear, y_nonlinear)

print(f"\n–ù–µ–ª—ñ–Ω—ñ–π–Ω–∞ –∑–∞–ª–µ–∂–Ω—ñ—Å—Ç—å (y=x¬≤):")
print(f"  Pearson:  {pearson_nl:.4f}")
print(f"  Spearman: {spearman_nl:.4f}")  # Perfect monotonic!
```

## –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è

```python
import matplotlib.pyplot as plt
import numpy as np

# –†—ñ–∑–Ω—ñ —Ç–∏–ø–∏ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç–µ–π
x = np.linspace(1, 10, 50)

fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# 1. –õ—ñ–Ω—ñ–π–Ω–∞
y_linear = 2*x + 1 + np.random.normal(0, 1, 50)
pearson_1, _ = pearsonr(x, y_linear)
spearman_1, _ = spearmanr(x, y_linear)

axes[0, 0].scatter(x, y_linear, alpha=0.6)
axes[0, 0].set_title(f'Linear\nPearson: {pearson_1:.3f} | Spearman: {spearman_1:.3f}',
                    fontsize=12, fontweight='bold')
axes[0, 0].grid(True, alpha=0.3)

# 2. –ö–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–∞
y_quadratic = x**2 + np.random.normal(0, 5, 50)
pearson_2, _ = pearsonr(x, y_quadratic)
spearman_2, _ = spearmanr(x, y_quadratic)

axes[0, 1].scatter(x, y_quadratic, alpha=0.6, color='orange')
axes[0, 1].set_title(f'Quadratic\nPearson: {pearson_2:.3f} | Spearman: {spearman_2:.3f}',
                    fontsize=12, fontweight='bold')
axes[0, 1].grid(True, alpha=0.3)

# 3. Exponential
y_exp = np.exp(x/5) + np.random.normal(0, 5, 50)
pearson_3, _ = pearsonr(x, y_exp)
spearman_3, _ = spearmanr(x, y_exp)

axes[1, 0].scatter(x, y_exp, alpha=0.6, color='green')
axes[1, 0].set_title(f'Exponential\nPearson: {pearson_3:.3f} | Spearman: {spearman_3:.3f}',
                    fontsize=12, fontweight='bold')
axes[1, 0].grid(True, alpha=0.3)

# 4. No correlation
y_random = np.random.normal(50, 10, 50)
pearson_4, _ = pearsonr(x, y_random)
spearman_4, _ = spearmanr(x, y_random)

axes[1, 1].scatter(x, y_random, alpha=0.6, color='red')
axes[1, 1].set_title(f'No Correlation\nPearson: {pearson_4:.3f} | Spearman: {spearman_4:.3f}',
                    fontsize=12, fontweight='bold')
axes[1, 1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

## –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏?

- ‚úÖ **Non-linear monotonic** relationships
- ‚úÖ **Ordinal data** (rankings)
- ‚úÖ –ö–æ–ª–∏ —î **outliers** (robust)
- ‚úÖ **Feature selection** (non-linear)
- ‚ùå –ö–æ–ª–∏ –ø–æ—Ç—Ä—ñ–±–Ω–∞ —Å–∞–º–µ –ª—ñ–Ω—ñ–π–Ω–∞ –∑–∞–ª–µ–∂–Ω—ñ—Å—Ç—å

---

# 6. Kullback-Leibler Divergence (KL Divergence)

## –§–æ—Ä–º—É–ª–∞

$$D_{KL}(P \| Q) = \sum_{i} P(i) \log \frac{P(i)}{Q(i)}$$

Range: $[0, \infty)$
- 0 ‚Üí —Ä–æ–∑–ø–æ–¥—ñ–ª–∏ —ñ–¥–µ–Ω—Ç–∏—á–Ω—ñ
- ‚àû ‚Üí –∑–æ–≤—Å—ñ–º —Ä—ñ–∑–Ω—ñ

**–í–∞–∂–ª–∏–≤–æ:** KL Divergence **–Ω–µ —Å–∏–º–µ—Ç—Ä–∏—á–Ω–∞**: $D_{KL}(P \| Q) \neq D_{KL}(Q \| P)$

## –Ü–Ω—Ç—É—ó—Ü—ñ—è

**KL Divergence** –≤–∏–º—ñ—Ä—é—î, **—Å–∫—ñ–ª—å–∫–∏ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó –≤—Ç—Ä–∞—á–∞—î—Ç—å—Å—è**, —è–∫—â–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ —Ä–æ–∑–ø–æ–¥—ñ–ª $Q$ –∑–∞–º—ñ—Å—Ç—å —Å–ø—Ä–∞–≤–∂–Ω—å–æ–≥–æ $P$.

```
–ü—Ä–∏–∫–ª–∞–¥: –ú–æ–¥–µ–ª—é–≤–∞–Ω–Ω—è –∫–∏–¥–∞–Ω–Ω—è –∫—É–±–∏–∫–∞

P (—Å–ø—Ä–∞–≤–∂–Ω—ñ–π):  [1/6, 1/6, 1/6, 1/6, 1/6, 1/6]
Q (–º–æ–¥–µ–ª—å):     [0.2, 0.2, 0.2, 0.2, 0.1, 0.1]

KL(P || Q) = —Å–∫—ñ–ª—å–∫–∏ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó –≤—Ç—Ä–∞—á–∞—î–º–æ
```

## –ö–æ–¥

```python
import numpy as np

def kl_divergence(p, q):
    """
    –û–±—á–∏—Å–ª–∏—Ç–∏ KL Divergence
    
    D_KL(P || Q) = Œ£ P(i) log(P(i)/Q(i))
    """
    # –£–Ω–∏–∫–∞—î–º–æ log(0)
    q = np.where(q == 0, 1e-10, q)
    p = np.where(p == 0, 1e-10, p)
    
    return np.sum(p * np.log(p / q))

# –ü—Ä–∏–∫–ª–∞–¥
p_true = np.array([0.2, 0.3, 0.5])  # –°–ø—Ä–∞–≤–∂–Ω—ñ–π —Ä–æ–∑–ø–æ–¥—ñ–ª
q_model = np.array([0.25, 0.25, 0.5])  # –ú–æ–¥–µ–ª—å

kl_pq = kl_divergence(p_true, q_model)
kl_qp = kl_divergence(q_model, p_true)

print(f"KL(P || Q): {kl_pq:.6f}")
print(f"KL(Q || P): {kl_qp:.6f}")
print(f"Asymmetric: {kl_pq != kl_qp}")

# –ß–µ—Ä–µ–∑ scipy
from scipy.stats import entropy

kl_scipy = entropy(p_true, q_model)
print(f"Scipy KL(P || Q): {kl_scipy:.6f}")
```

## –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è

```python
import matplotlib.pyplot as plt
import numpy as np

# –î–≤–∞ —Ä–æ–∑–ø–æ–¥—ñ–ª–∏
x = np.array([1, 2, 3, 4, 5])
p = np.array([0.1, 0.2, 0.4, 0.2, 0.1])  # –°–ø—Ä–∞–≤–∂–Ω—ñ–π
q = np.array([0.2, 0.2, 0.2, 0.2, 0.2])  # Uniform (–º–æ–¥–µ–ª—å)

# KL
kl = kl_divergence(p, q)

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
plt.figure(figsize=(10, 6))

width = 0.35
plt.bar(x - width/2, p, width, label='P (True)', alpha=0.7, color='blue')
plt.bar(x + width/2, q, width, label='Q (Model)', alpha=0.7, color='red')

plt.xlabel('Category', fontsize=12)
plt.ylabel('Probability', fontsize=12)
plt.title(f'KL Divergence D(P || Q) = {kl:.4f}', fontsize=14, fontweight='bold')
plt.legend(fontsize=11)
plt.grid(True, alpha=0.3, axis='y')
plt.tight_layout()
plt.show()
```

## Jensen-Shannon Divergence (Symmetric)

```python
def js_divergence(p, q):
    """
    Jensen-Shannon Divergence (symmetric version of KL)
    
    JS(P, Q) = 0.5 * KL(P || M) + 0.5 * KL(Q || M)
    –¥–µ M = (P + Q) / 2
    """
    m = (p + q) / 2
    return 0.5 * kl_divergence(p, m) + 0.5 * kl_divergence(q, m)

# –ü—Ä–∏–∫–ª–∞–¥
p = np.array([0.2, 0.3, 0.5])
q = np.array([0.25, 0.25, 0.5])

js = js_divergence(p, q)
js_reverse = js_divergence(q, p)

print(f"JS(P, Q): {js:.6f}")
print(f"JS(Q, P): {js_reverse:.6f}")
print(f"Symmetric: {np.isclose(js, js_reverse)}")

# –ß–µ—Ä–µ–∑ scipy
from scipy.spatial.distance import jensenshannon

js_scipy = jensenshannon(p, q) ** 2  # scipy –ø–æ–≤–µ—Ä—Ç–∞—î sqrt
print(f"Scipy JS: {js_scipy:.6f}")
```

## –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏?

- ‚úÖ **Comparing probability distributions**
- ‚úÖ **Information theory**
- ‚úÖ **Model evaluation** (generative models)
- ‚úÖ **NLP** (topic modeling)
- ‚ùå –ö–æ–ª–∏ –ø–æ—Ç—Ä—ñ–±–Ω–∞ —Å–∏–º–µ—Ç—Ä–∏—á–Ω–∞ –º–µ—Ç—Ä–∏–∫–∞ (–≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π JS)

---

# –ü–æ—Ä—ñ–≤–Ω—è–ª—å–Ω–∞ —Ç–∞–±–ª–∏—Ü—è

| –ú–µ—Ç—Ä–∏–∫–∞ | –¢–∏–ø –¥–∞–Ω–∏—Ö | Range | –°–∏–º–µ—Ç—Ä–∏—á–Ω–∞? | –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ |
|---------|-----------|-------|-------------|---------------------|
| **Cosine** | Vectors | [-1, 1] | ‚úÖ | Text, sparse, direction –≤–∞–∂–ª–∏–≤–∏–π |
| **Pearson** | Vectors | [-1, 1] | ‚úÖ | Linear correlation, recommenders |
| **Spearman** | Rankings | [-1, 1] | ‚úÖ | Monotonic, ordinal data |
| **Jaccard** | Sets | [0, 1] | ‚úÖ | Binary, sets, market basket |
| **Dice** | Sets | [0, 1] | ‚úÖ | Sets (–±—ñ–ª—å—à–∞ –≤–∞–≥–∞ intersection) |
| **KL Divergence** | Distributions | [0, ‚àû) | ‚ùå | Probability distributions |
| **JS Divergence** | Distributions | [0, 1] | ‚úÖ | Symmetric KL |

---

## –ü—Ä–∞–∫—Ç–∏—á–Ω—ñ –ø—Ä–∏–∫–ª–∞–¥–∏

### 1. Movie Recommender System

```python
import numpy as np
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity

# User-Item ratings matrix
ratings = pd.DataFrame({
    'Movie1': [5, 4, 0, 0, 3],
    'Movie2': [4, 5, 0, 0, 4],
    'Movie3': [0, 0, 5, 4, 0],
    'Movie4': [0, 0, 4, 5, 0],
    'Movie5': [3, 4, 0, 0, 5]
}, index=['User1', 'User2', 'User3', 'User4', 'User5'])

print("Ratings Matrix:")
print(ratings)

# Cosine similarity –º—ñ–∂ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞–º–∏
user_similarity = cosine_similarity(ratings)
user_similarity_df = pd.DataFrame(user_similarity, 
                                  index=ratings.index,
                                  columns=ratings.index)

print("\nUser Similarity (Cosine):")
print(user_similarity_df)

# –ó–Ω–∞–π—Ç–∏ –Ω–∞–π–±—ñ–ª—å—à —Å—Ö–æ–∂–∏—Ö –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤ –¥–ª—è User1
user1_similarities = user_similarity_df.loc['User1'].drop('User1').sort_values(ascending=False)
print(f"\nMost similar to User1:")
print(user1_similarities)

# –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó
def recommend_movies(user, ratings, user_similarity, n_recommendations=2):
    """–†–µ–∫–æ–º–µ–Ω–¥—É–≤–∞—Ç–∏ —Ñ—ñ–ª—å–º–∏ –Ω–∞ –æ—Å–Ω–æ–≤—ñ —Å—Ö–æ–∂–∏—Ö –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤"""
    # –ó–Ω–∞–π—Ç–∏ —Å—Ö–æ–∂–∏—Ö –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤
    similar_users = user_similarity_df.loc[user].drop(user).sort_values(ascending=False)
    
    # –§—ñ–ª—å–º–∏, —è–∫—ñ user —â–µ –Ω–µ –¥–∏–≤–∏–≤—Å—è
    user_movies = ratings.loc[user]
    unwatched = user_movies[user_movies == 0].index
    
    # –û—Ü—ñ–Ω–∫–∏ –≤—ñ–¥ —Å—Ö–æ–∂–∏—Ö –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤
    recommendations = {}
    for movie in unwatched:
        weighted_sum = 0
        similarity_sum = 0
        
        for similar_user in similar_users.index[:3]:  # Top 3
            if ratings.loc[similar_user, movie] > 0:
                weight = similar_users[similar_user]
                weighted_sum += weight * ratings.loc[similar_user, movie]
                similarity_sum += weight
        
        if similarity_sum > 0:
            recommendations[movie] = weighted_sum / similarity_sum
    
    # –¢–æ–ø —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó
    return sorted(recommendations.items(), key=lambda x: x[1], reverse=True)[:n_recommendations]

recommendations = recommend_movies('User1', ratings, user_similarity_df)
print(f"\nRecommendations for User1:")
for movie, score in recommendations:
    print(f"  {movie}: predicted rating = {score:.2f}")
```

### 2. Document Clustering

```python
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics.pairwise import cosine_similarity
import matplotlib.pyplot as plt
import seaborn as sns

# –î–æ–∫—É–º–µ–Ω—Ç–∏
documents = [
    "Machine learning is a subset of artificial intelligence",
    "Deep learning uses neural networks with multiple layers",
    "Natural language processing is a branch of AI",
    "Statistics is the foundation of data science",
    "Probability theory is used in statistical modeling",
    "Data visualization helps understand patterns in data"
]

# TF-IDF
vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(documents)

# Cosine similarity
similarity_matrix = cosine_similarity(tfidf_matrix)

# Clustering
clustering = AgglomerativeClustering(n_clusters=3, metric='cosine', linkage='average')
labels = clustering.fit_predict(tfidf_matrix.toarray())

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è similarity matrix
plt.figure(figsize=(10, 8))
sns.heatmap(similarity_matrix, annot=True, fmt='.2f', cmap='YlOrRd',
           xticklabels=[f'Doc{i+1}' for i in range(len(documents))],
           yticklabels=[f'Doc{i+1}' for i in range(len(documents))],
           cbar_kws={'label': 'Cosine Similarity'})
plt.title('Document Similarity Matrix', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.show()

# –ü–æ–∫–∞–∑–∞—Ç–∏ –∫–ª–∞—Å—Ç–µ—Ä–∏
print("\nDocument Clusters:")
for i, (doc, label) in enumerate(zip(documents, labels)):
    print(f"Cluster {label}: Doc{i+1} - {doc[:50]}...")
```

---

## –ü–æ—à–∏—Ä–µ–Ω—ñ –ø–æ–º–∏–ª–∫–∏ ‚ùå

### 1. –ü–ª—É—Ç–∞—Ç–∏ Distance —ñ Similarity

```python
# ‚ùå –ü–û–ì–ê–ù–û
from sklearn.metrics.pairwise import cosine_similarity
distance = 1 - cosine_similarity(x, y)  # –¶–µ Cosine DISTANCE, –Ω–µ similarity!

# ‚úÖ –î–û–ë–†–ï
similarity = cosine_similarity(x, y)  # Range [0, 1] –¥–ª—è normalized
```

### 2. –ù–µ –Ω–æ—Ä–º–∞–ª—ñ–∑—É–≤–∞—Ç–∏ –¥–ª—è Cosine

```python
# Cosine similarity –≤–∂–µ –Ω–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω–∞ (–¥—ñ–ª–∏—Ç—å –Ω–∞ –Ω–æ—Ä–º–∏)
# –ê–ª–µ —è–∫—â–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—à —è–∫ Distance –≤ KNN:

from sklearn.neighbors import KNeighborsClassifier

# ‚úÖ –î–û–ë–†–ï: cosine metric –≤–±—É–¥–æ–≤–∞–Ω–∞
knn = KNeighborsClassifier(metric='cosine')
```

### 3. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ Pearson –¥–ª—è sparse data

```python
# ‚ùå –ü–û–ì–ê–ù–û: Pearson –¥–ª—è sparse TF-IDF
# –ë–∞–≥–∞—Ç–æ –Ω—É–ª—ñ–≤ ‚Üí mean –∑–º—ñ—â–µ–Ω–∞

# ‚úÖ –î–û–ë–†–ï: Cosine –¥–ª—è sparse
```

---

## –ü—Ä–∞–∫—Ç–∏—á–Ω—ñ –ø–æ—Ä–∞–¥–∏ üí°

### 1. –í–∏–±—ñ—Ä –º–µ—Ç—Ä–∏–∫–∏ –∑–∞ —Ç–∏–ø–æ–º –¥–∞–Ω–∏—Ö

```python
# Dense continuous ‚Üí Pearson –∞–±–æ Cosine
# Sparse continuous ‚Üí Cosine
# Binary/Sets ‚Üí Jaccard –∞–±–æ Dice
# Rankings ‚Üí Spearman
# Distributions ‚Üí KL –∞–±–æ JS
```

### 2. Normalization –≤–∞–∂–ª–∏–≤–∞

```python
from sklearn.preprocessing import normalize

# –î–ª—è Cosine similarity
X_normalized = normalize(X, norm='l2')  # Unit vectors
similarity = X_normalized @ X_normalized.T  # Dot product = Cosine
```

### 3. –û–±–∏—Ä–∞–π symmetric –º–µ—Ç—Ä–∏–∫—É

```python
# KL divergence asymmetric ‚Üí –ø—Ä–æ–±–ª–µ–º–∏
# –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π JS divergence (symmetric) –∞–±–æ
# Average: (KL(P||Q) + KL(Q||P)) / 2
```

---

## –ö–ª—é—á–æ–≤—ñ –≤–∏—Å–Ω–æ–≤–∫–∏

> Similarity Measures –≤–∏–º—ñ—Ä—é—é—Ç—å —Å—Ö–æ–∂—ñ—Å—Ç—å –º—ñ–∂ –æ–±'—î–∫—Ç–∞–º–∏. –í–∏–±—ñ—Ä –º–µ—Ç—Ä–∏–∫–∏ –∑–∞–ª–µ–∂–∏—Ç—å –≤—ñ–¥ —Ç–∏–ø—É –¥–∞–Ω–∏—Ö —Ç–∞ –∑–∞–¥–∞—á—ñ.

**–û—Å–Ω–æ–≤–Ω—ñ –º–µ—Ç—Ä–∏–∫–∏:**
- **Cosine** ‚Äî –∫—É—Ç –º—ñ–∂ –≤–µ–∫—Ç–æ—Ä–∞–º–∏ (text, sparse data)
- **Pearson** ‚Äî –ª—ñ–Ω—ñ–π–Ω–∞ –∫–æ—Ä–µ–ª—è—Ü—ñ—è (recommenders, centered)
- **Spearman** ‚Äî –º–æ–Ω–æ—Ç–æ–Ω–Ω–∞ –∑–∞–ª–µ–∂–Ω—ñ—Å—Ç—å (rankings, non-linear)
- **Jaccard** ‚Äî –¥–ª—è –º–Ω–æ–∂–∏–Ω (sets, binary)
- **KL/JS** ‚Äî –¥–ª—è —Ä–æ–∑–ø–æ–¥—ñ–ª—ñ–≤ –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç–µ–π

**–í–∏–±—ñ—Ä:**
```
Text/Sparse       ‚Üí Cosine
Recommenders      ‚Üí Pearson (centered ratings)
Rankings          ‚Üí Spearman
Binary/Sets       ‚Üí Jaccard
Distributions     ‚Üí KL/JS
```

**Distance ‚Üî Similarity:**
```
Similarity = 1 / (1 + Distance)
–∞–±–æ
Distance = 1 - Similarity  (–¥–ª—è normalized)
```

---

#ml #similarity-measures #cosine #pearson #jaccard #spearman #kl-divergence #recommender-systems
