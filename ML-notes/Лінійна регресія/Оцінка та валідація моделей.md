
**Оцінка та валідація моделей** — це методи для перевірки якості моделі машинного навчання, щоб зрозуміти, наскільки добре вона узагальнює знання на нових даних.

**Головна ідея:** не просто навчити модель, а переконатися, що вона працює на даних, які **ніколи не бачила**.

## Навіщо це потрібно?

### Проблема

Модель може **ідеально** працювати на тренувальних даних, але **погано** на нових:

```
Train Accuracy: 99% ✓
Test Accuracy: 65% ✗

→ Модель ПЕРЕОБУЧИЛАСЬ (overfitting)!
```

### Мета

- Виявити **переобучення** (overfitting)
- Виявити **недонавчання** (underfitting)
- Вибрати найкращу модель
- Налаштувати гіперпараметри
- Оцінити реальну якість на production

## Розділення даних

### Базовий підхід: Train/Test Split

```
Всі дані (100%)
    ↓
├─ Train (70-80%) → Навчання моделі
└─ Test (20-30%)  → Фінальна оцінка
```

**Приклад:**

```python
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, 
    test_size=0.2,      # 20% на тест
    random_state=42,    # Відтворюваність
    stratify=y          # Зберегти пропорції класів
)
```

### Розширений підхід: Train/Validation/Test Split

```
Всі дані (100%)
    ↓
├─ Train (60-70%)      → Навчання моделі
├─ Validation (10-20%) → Підбір гіперпараметрів
└─ Test (20%)          → Фінальна оцінка (торкаємось 1 раз!)
```

**Чому 3 частини?**

|Набір|Призначення|Використання|
|---|---|---|
|**Train**|Навчання моделі|Багато разів|
|**Validation**|Вибір моделі/параметрів|Багато разів|
|**Test**|Остаточна оцінка|**ТІЛЬКИ 1 РАЗ!**|

**Приклад:**

```python
# Крок 1: Відділити test set (20%)
X_temp, X_test, y_temp, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Крок 2: Розділити залишок на train (75% від 80% = 60%) і validation (25% від 80% = 20%)
X_train, X_val, y_train, y_val = train_test_split(
    X_temp, y_temp, test_size=0.25, random_state=42
)

# Результат: Train=60%, Validation=20%, Test=20%
```

## Простий приклад: Передбачення цін будинків

### Дані

1000 будинків з ознаками (площа, кімнати, локація) та ціною.

### Розділення

- **Train:** 600 будинків
- **Validation:** 200 будинків
- **Test:** 200 будинків

### Процес

**Крок 1:** Навчити модель на Train

```python
model = LinearRegression()
model.fit(X_train, y_train)
```

**Крок 2:** Оцінити на Validation

```python
val_score = model.score(X_val, y_val)
# R² = 0.75
```

**Крок 3:** Підібрати гіперпараметри (якщо потрібно)

```python
# Спробувати polynomial degree=2
model = Pipeline([
    ('poly', PolynomialFeatures(degree=2)),
    ('linear', LinearRegression())
])
model.fit(X_train, y_train)
val_score = model.score(X_val, y_val)
# R² = 0.82 ← Краще!
```

**Крок 4:** ТІЛЬКИ в кінці — оцінити на Test

```python
test_score = model.score(X_test, y_test)
# R² = 0.80 ← Фінальна оцінка
```

### Інтерпретація

- **Train R² = 0.85**, **Val R² = 0.82**, **Test R² = 0.80**
    - ✅ Модель добре генералізує, невелика різниця
- **Train R² = 0.99**, **Val R² = 0.65**, **Test R² = 0.63**
    - ❌ Переобучення! Модель запам'ятала Train, але не навчилась

## Cross-Validation (Крос-валідація)

### Проблема Train/Val/Test

- Втрачаємо частину даних для навчання
- Результат залежить від того, як випадково розділили
- Для малих датасетів це критично

### Рішення: k-Fold Cross-Validation

**Ідея:** Розділити дані на k частин (folds), по черзі використовувати кожну як validation.

```
k=5 (5-Fold CV):

Fold 1: [Test][Train][Train][Train][Train]
Fold 2: [Train][Test][Train][Train][Train]
Fold 3: [Train][Train][Test][Train][Train]
Fold 4: [Train][Train][Train][Test][Train]
Fold 5: [Train][Train][Train][Train][Test]

Фінальна оцінка = середнє з 5 результатів
```

### Алгоритм

1. Розділити дані на k частин
2. Для кожної частини i:
    - Використати i-ту частину як validation
    - Інші k-1 частин як train
    - Навчити модель і оцінити
3. Усереднити k результатів

### Приклад коду

```python
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LogisticRegression

model = LogisticRegression()

# 5-Fold Cross-Validation
scores = cross_val_score(
    model, X, y,
    cv=5,              # Кількість folds
    scoring='accuracy' # Метрика
)

print(f"Scores: {scores}")
# [0.82, 0.85, 0.79, 0.83, 0.81]

print(f"Mean: {scores.mean():.3f}")
# Mean: 0.820

print(f"Std: {scores.std():.3f}")
# Std: 0.020
```

### Інтерпретація стандартного відхилення

|Std|Інтерпретація|
|---|---|
|< 0.02|Стабільна модель ✓|
|0.02-0.05|Прийнятна варіативність|
|> 0.05|Нестабільна модель, залежить від даних ✗|

## Варіації Cross-Validation

### 1. Stratified k-Fold

**Проблема:** У звичайному k-fold розподіл класів може бути нерівномірним.

**Рішення:** Зберігати пропорцію класів в кожному fold.

```python
from sklearn.model_selection import StratifiedKFold

skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

for train_idx, val_idx in skf.split(X, y):
    X_train, X_val = X[train_idx], X[val_idx]
    y_train, y_val = y[train_idx], y[val_idx]
    
    # Навчання і оцінка
```

**Приклад:**

Дані: 100 зразків, 70% клас 0, 30% клас 1

**Звичайний k-fold:** Може дати fold з 90% класу 0 **Stratified k-fold:** Кожен fold матиме ~70% класу 0, ~30% класу 1 ✓

### 2. Leave-One-Out Cross-Validation (LOOCV)

**Ідея:** k = n (кількість зразків)

- Кожен зразок по черзі є validation
- Train на всіх інших

**Переваги:**

- Використовує майже всі дані для навчання
- Детермінований результат (без randomness)

**Недоліки:**

- Дуже повільно (n ітерацій)
- Висока variance оцінки

**Коли використовувати:** Дуже малі датасети (<100 зразків)

```python
from sklearn.model_selection import LeaveOneOut

loo = LeaveOneOut()
scores = cross_val_score(model, X, y, cv=loo)
```

### 3. Time Series Split

**Проблема:** Для часових рядів не можна просто перемішувати дані!

**Рішення:** Валідація завжди на **майбутніх** даних.

```
Time Series Split:

Fold 1: [Train          ][Val]
Fold 2: [Train               ][Val]
Fold 3: [Train                    ][Val]
Fold 4: [Train                         ][Val]

→ Завжди навчаємось на минулому, тестуємо на майбутньому
```

```python
from sklearn.model_selection import TimeSeriesSplit

tscv = TimeSeriesSplit(n_splits=5)

for train_idx, val_idx in tscv.split(X):
    X_train, X_val = X[train_idx], X[val_idx]
    y_train, y_val = y[train_idx], y[val_idx]
```

### 4. Group k-Fold

**Проблема:** Дані згруповані (наприклад, кілька фото одного пацієнта).

**Рішення:** Вся група має бути або в train, або в validation.

```python
from sklearn.model_selection import GroupKFold

# groups: [1, 1, 1, 2, 2, 3, 3, 3]
gkf = GroupKFold(n_splits=3)

for train_idx, val_idx in gkf.split(X, y, groups=groups):
    # Група ніколи не розділяється між train і validation
```

## Складний приклад: Діагностика захворювання

### Дані

- 1000 пацієнтів
- 20 ознак (аналізи крові, вік, тиск тощо)
- 2 класи: здоровий (70%), хворий (30%)
- **Незбалансовані класи!**

### Завдання

Вибрати найкращу модель серед:

1. Logistic Regression
2. Random Forest
3. SVM

### Процес

**Крок 1:** Розділити на Train/Test

```python
X_train, X_test, y_train, y_test = train_test_split(
    X, y, 
    test_size=0.2,
    stratify=y,  # Важливо для незбалансованих класів!
    random_state=42
)
```

**Крок 2:** Stratified 5-Fold CV на Train

```python
from sklearn.model_selection import cross_validate

models = {
    'LogReg': LogisticRegression(),
    'RF': RandomForestClassifier(),
    'SVM': SVC(probability=True)
}

results = {}

for name, model in models.items():
    cv_results = cross_validate(
        model, X_train, y_train,
        cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),
        scoring=['accuracy', 'roc_auc', 'f1'],
        return_train_score=True
    )
    
    results[name] = {
        'train_acc': cv_results['train_accuracy'].mean(),
        'val_acc': cv_results['test_accuracy'].mean(),
        'val_auc': cv_results['test_roc_auc'].mean(),
        'val_f1': cv_results['test_f1'].mean(),
        'std_acc': cv_results['test_accuracy'].std()
    }
```

**Крок 3:** Порівняти результати

```
Model    | Train Acc | Val Acc | Val AUC | Val F1 | Std
---------|-----------|---------|---------|--------|-----
LogReg   | 0.82      | 0.80    | 0.85    | 0.75   | 0.02 ✓ Стабільно
RF       | 0.98      | 0.83    | 0.88    | 0.78   | 0.03 ← Переобучення
SVM      | 0.85      | 0.84    | 0.89    | 0.80   | 0.02 ← Найкраще! ✓
```

**Висновок:** SVM має найкращі метрики на validation.

**Крок 4:** Навчити SVM на всьому Train, оцінити на Test

```python
best_model = SVC(probability=True)
best_model.fit(X_train, y_train)

test_acc = best_model.score(X_test, y_test)
test_auc = roc_auc_score(y_test, best_model.predict_proba(X_test)[:, 1])

print(f"Test Accuracy: {test_acc:.3f}")  # 0.835
print(f"Test AUC: {test_auc:.3f}")       # 0.890
```

**Інтерпретація:**

- Val AUC = 0.89, Test AUC = 0.89 ✓
- Модель дійсно добре генералізує!

## Bias-Variance Trade-off

### Концепція

**Помилка моделі складається з трьох компонентів:**

$$\text{Error} = \text{Bias}^2 + \text{Variance} + \text{Irreducible Error}$$

### Визначення

|Компонент|Що це|Причина|
|---|---|---|
|**Bias (зміщення)**|Помилка через спрощення моделі|Модель занадто проста|
|**Variance (дисперсія)**|Чутливість до тренувальних даних|Модель занадто складна|
|**Irreducible Error**|Шум в даних|Не можна зменшити|

### Візуально

```
Висока Bias (Underfitting):
Train Error: Високий
Test Error:  Високий
→ Модель занадто проста

Висока Variance (Overfitting):
Train Error: Низький
Test Error:  Високий
→ Модель занадто складна

Оптимальний баланс:
Train Error: Помірний
Test Error:  Помірний (близько до Train)
→ Модель в самий раз ✓
```

### Приклад: Поліноміальна регресія

```python
import numpy as np
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

# Генерація даних
X = np.linspace(0, 10, 100).reshape(-1, 1)
y = 2 * X.ravel() + 5 + np.random.normal(0, 2, 100)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

degrees = [1, 2, 5, 10, 15]
results = []

for d in degrees:
    model = Pipeline([
        ('poly', PolynomialFeatures(degree=d)),
        ('linear', LinearRegression())
    ])
    
    model.fit(X_train, y_train)
    
    train_mse = mean_squared_error(y_train, model.predict(X_train))
    test_mse = mean_squared_error(y_test, model.predict(X_test))
    
    results.append({
        'degree': d,
        'train_mse': train_mse,
        'test_mse': test_mse
    })

# Результати:
Degree | Train MSE | Test MSE | Статус
-------|-----------|----------|--------
1      | 4.2       | 4.5      | High Bias (Underfitting)
2      | 4.0       | 4.1      | Оптимально ✓
5      | 3.5       | 5.0      | Початок overfitting
10     | 2.0       | 12.0     | High Variance (Overfitting)
15     | 0.5       | 50.0     | Extreme Overfitting
```

### Графік

```
Error
  ↑
  |         Test Error
  |           ╱
  |         ╱
  |       ╱  ← Оптимум
  |     ╱
  |   ╱_____ Train Error
  |  
  |________________→ Model Complexity
   Simple      Complex
```

## Learning Curves (Криві навчання)

### Що це

Графік помилки (Train і Validation) залежно від **розміру тренувального набору**.

### Приклад

```python
from sklearn.model_selection import learning_curve

train_sizes, train_scores, val_scores = learning_curve(
    model, X, y,
    train_sizes=np.linspace(0.1, 1.0, 10),
    cv=5,
    scoring='neg_mean_squared_error'
)

plt.plot(train_sizes, -train_scores.mean(axis=1), label='Train Error')
plt.plot(train_sizes, -val_scores.mean(axis=1), label='Validation Error')
plt.xlabel('Training Set Size')
plt.ylabel('Error')
plt.legend()
```

### Інтерпретація

**1. High Bias (Underfitting):**

```
Error
  ↑
  | Train ────────
  | Val   ────────
  |     (обидві високі, не зменшуються)
  |________________→ Training Size
  
→ Більше даних НЕ допоможе
→ Потрібна складніша модель
```

**2. High Variance (Overfitting):**

```
Error
  ↑
  | Val   ─────────
  |         ╲
  |          ╲
  | Train     ────
  |________________→ Training Size
  
→ Великий розрив між Train і Val
→ Більше даних ДОПОМОЖЕ
→ Або зменшити складність моделі
```

**3. Оптимально:**

```
Error
  ↑
  | Val   ─╲___
  | Train ─╱
  |     (сходяться до низького значення)
  |________________→ Training Size
  
→ Модель добре генералізує ✓
```

## Hyperparameter Tuning

### Grid Search

**Ідея:** Перебрати всі комбінації гіперпараметрів.

```python
from sklearn.model_selection import GridSearchCV

param_grid = {
    'C': [0.1, 1, 10, 100],
    'kernel': ['linear', 'rbf'],
    'gamma': [0.001, 0.01, 0.1]
}

grid_search = GridSearchCV(
    SVC(),
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1,      # Паралельно
    verbose=1
)

grid_search.fit(X_train, y_train)

print(f"Best params: {grid_search.best_params_}")
print(f"Best CV score: {grid_search.best_score_:.3f}")

# Використати найкращу модель
best_model = grid_search.best_estimator_
```

**Недолік:** Повільно при багатьох параметрах (4 × 2 × 3 = 24 комбінації × 5 folds = 120 навчань!)

### Random Search

**Ідея:** Випадково вибрати n комбінацій.

```python
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import uniform, randint

param_distributions = {
    'C': uniform(0.1, 100),
    'kernel': ['linear', 'rbf'],
    'gamma': uniform(0.001, 0.1)
}

random_search = RandomizedSearchCV(
    SVC(),
    param_distributions,
    n_iter=20,      # Тільки 20 комбінацій
    cv=5,
    scoring='accuracy',
    random_state=42
)

random_search.fit(X_train, y_train)
```

**Переваги:**

- Швидше (контролюємо n_iter)
- Часто знаходить так само добре як Grid Search

### Порівняння

|Метод|Переваги|Недоліки|
|---|---|---|
|**Grid Search**|Гарантовано перевіряє всі|Повільно|
|**Random Search**|Швидше, ефективніше для великих просторів|Може пропустити оптимум|

## Overfitting vs Underfitting

### Визначення

|Стан|Train Error|Test Error|Причина|Рішення|
|---|---|---|---|---|
|**Underfitting**|Високий|Високий|Модель занадто проста|Складніша модель, більше ознак|
|**Good Fit**|Низький|Низький (близько до Train)|Оптимальна складність|Нічого не змінювати ✓|
|**Overfitting**|Дуже низький|Високий|Модель занадто складна|Регуляризація, більше даних, менше ознак|

### Діагностика

```python
# Ознаки overfitting:
train_acc = 0.99  # Дуже високо
val_acc = 0.70    # Набагато нижче
gap = train_acc - val_acc  # > 0.1 → Overfitting!

# Ознаки underfitting:
train_acc = 0.65  # Низько
val_acc = 0.63    # Також низько
# Обидві помилки високі → Underfitting!
```

### Рішення для Overfitting

1. **Більше даних** — найефективніше, але не завжди можливо
2. **Регуляризація** (L1/L2) — штраф за складність
3. **Зменшити складність моделі:**
    - Decision Tree: зменшити max_depth
    - Neural Network: менше layers/neurons
    - Polynomial: менший degree
4. **Dropout** (для нейронних мереж)
5. **Early stopping** — зупинити навчання, коли val error зростає
6. **Feature selection** — видалити непотрібні ознаки
7. **Cross-validation** — краща оцінка якості

### Рішення для Underfitting

1. **Складніша модель:**
    - Linear → Polynomial
    - Logistic Regression → SVM/Random Forest
2. **Більше ознак** — feature engineering
3. **Менша регуляризація** (збільшити C)
4. **Більше часу навчання** (epochs для neural networks)

## Практичний workflow

### Покроковий процес

```
1. Розділити дані
   ├─ Train/Val/Test (60/20/20)
   └─ Stratify для незбалансованих класів

2. Exploratory Data Analysis (EDA)
   ├─ Перевірити розподіли
   ├─ Знайти outliers
   └─ Correlation matrix

3. Feature Engineering
   ├─ Обробити missing values
   ├─ Encoding категоріальних
   ├─ Scaling числових
   └─ Створити нові ознаки

4. Baseline модель
   ├─ Проста модель (LogReg, Decision Tree)
   └─ Оцінка на validation

5. Складніші моделі
   ├─ Random Forest, SVM, Gradient Boosting
   └─ 5-Fold CV для кожної

6. Hyperparameter Tuning
   ├─ Grid/Random Search
   └─ На найкращій моделі

7. Фінальна оцінка
   ├─ Навчити на Train+Val
   └─ Оцінити на Test (ОДИН РАЗ!)

8. Діагностика
   ├─ Learning curves
   ├─ Confusion matrix
   └─ Feature importance
```

## Приклад повного коду

```python
import numpy as np
import pandas as pd
from sklearn.model_selection import (
    train_test_split,
    cross_validate,
    GridSearchCV,
    StratifiedKFold
)
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import classification_report, roc_auc_score

# 1. Завантажити дані
from sklearn.datasets import make_classification
X, y = make_classification(
    n_samples=1000, n_features=20, n_informative=15,
    n_redundant=5, random_state=42
)

# 2. Розділити Train/Test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

# 3. Масштабування
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 4. Визначити моделі
models = {
    'LogReg': LogisticRegression(max_iter=1000),
    'RandomForest': RandomForestClassifier(random_state=42),
    'SVM': SVC(probability=True, random_state=42)
}

# 5. Cross-Validation для порівняння
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
results = {}

for name, model in models.items():
    cv_results = cross_validate(
        model, X_train_scaled, y_train,
        cv=cv,
        scoring=['accuracy', 'roc_auc'],
        return_train_score=True
    )
    
    results[name] = {
        'train_acc': cv_results['train_accuracy'].mean(),
        'val_acc': cv_results['test_accuracy'].mean(),
        'val_auc': cv_results['test_roc_auc'].mean(),
        'std_acc': cv_results['test_accuracy'].std()
    }
    
    print(f"{name}:")
    print(f"  Train Acc: {results[name]['train_acc']:.3f}")
    print(f"  Val Acc:   {results[name]['val_acc']:.3f} ± {results[name]['std_acc']:.3f}")
    print(f"  Val AUC:   {results[name]['val_auc']:.3f}")
    print()

# 6. Hyperparameter Tuning на найкращій моделі
# Припустимо, RandomForest найкраща
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [5, 10, 15, None],
    'min_samples_split': [2, 5, 10]
}

grid_search = GridSearchCV(
    RandomForestClassifier(random_state=42),
    param_grid,
    cv=5,
    scoring='roc_auc',
    n_jobs=-1,
    verbose=1
)

grid_search.fit(X_train_scaled, y_train)

print(f"Best params: {grid_search.best_params_}")
print(f"Best CV AUC: {grid_search.best_score_:.3f}")

# 7. Фінальна оцінка на Test
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test_scaled)
y_pred_proba = best_model.predict_proba(X_test_scaled)[:, 1]

print("\n=== Test Set Results ===")
print(classification_report(y_test, y_pred))
print(f"Test AUC: {roc_auc_score(y_test, y_pred_proba):.3f}")
```

## Поширені помилки

### ❌ Помилка 1: Використання Test для підбору параметрів

```python
# ПОГАНО!
for param in params:
    model.set_params(param)
    model.fit(X_train, y_train)
    score = model.score(X_test, y_test)  # ← Використали Test!
```

**Проблема:** Test "засвітився", оцінка буде оптимістичною.

**Правильно:** Використовувати Validation або CV.

### ❌ Помилка 2: Не стратифікувати при розділенні

```python
# ПОГАНО для незбалансованих класів
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# ДОБРЕ
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y
)
```

### ❌ Помилка 3: Data Leakage через масштабування

```python
# ПОГАНО! (fit на всіх даних)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
X_train, X_test = train_test_split(X_scaled, ...)

# ДОБРЕ (fit тільки на train)
X_train, X_test = train_test_split(X, ...)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)  # ← тільки transform!
```

### ❌ Помилка 4: Ігнорувати випадковість

```python
# ПОГАНО (результати не відтворюються)
train_test_split(X, y, test_size=0.2)

# ДОБРЕ
train_test_split(X, y, test_size=0.2, random_state=42)
```

### ❌ Помилка 5: Використовувати accuracy для незбалансованих класів

```python
# Якщо 95% клас 0, модель "завжди 0" → accuracy=95%!

# Використовуй:
- ROC-AUC
- F1-Score
- Precision/Recall
- Balanced Accuracy
```

## Практичні поради

1. **Завжди розділяй дані СПОЧАТКУ** — перед будь-якою обробкою
2. **Використовуй stratify** для незбалансованих класів
3. **Test торкаєшся ТІЛЬКИ 1 РАЗ** — в самому кінці
4. **CV для малих даних** — 5-10 fold зазвичай достатньо
5. **Перевіряй на overfitting** — різниця train/val > 10% = проблема
6. **Learning curves допомагають** — зрозуміти, потрібно більше даних чи складніша модель
7. **Random Search часто краще** — швидше і ефективніше за Grid Search
8. **Фіксуй random_state** — для відтворюваності
9. **Масштабуй правильно** — fit на train, transform на val/test
10. **Візуалізуй** — графіки кажуть більше за числа

## Ключові висновки

> Оцінка моделі — це не просто accuracy на test set, а систематичний процес перевірки якості генералізації.

**Основні принципи:**

- **Train** → навчання
- **Validation** → вибір моделі/параметрів
- **Test** → остаточна оцінка (1 раз!)
- **Cross-Validation** → надійна оцінка на малих даних
- **Bias-Variance Trade-off** → баланс між простотою та складністю

**Золоте правило:** $$\text{Test Error} \approx \text{Validation Error} \approx \text{Train Error} \implies \text{Good Model}$$

Якщо великий розрив — є проблема (overfitting або underfitting)!