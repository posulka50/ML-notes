## Що це

Градієнтний спуск — ітераційний метод **мінімізації функцій**, який знаходить оптимальні параметри моделі (β), коли **аналітичне рішення складне або недоступне**.  
Використовується у машинному навчанні для регресії, нейронних мереж тощо.

---

## Основна ідея

- Починаємо з випадкових параметрів β₀  
- Крок за кроком змінюємо β у напрямку **найшвидшого зменшення функції втрат**  

Формула оновлення:

$$
\beta := \beta - \alpha \nabla J(\beta)
$$

- $\alpha$ — швидкість навчання (learning rate)  
- $\nabla J(\beta)$ — градієнт функції втрат J(β)  

---

## Приклад для лінійної регресії

- Функція втрат: **MSE**

$$
J(\beta) = \frac{1}{n} \sum_{i=1}^{n} (y_i - X_i \beta)^2
$$

- Градієнт:

$$
\nabla J(\beta) = -\frac{2}{n} X^T (y - X \beta)
$$

- Оновлення коефіцієнтів:

$$
\beta := \beta - \alpha \left(-\frac{2}{n} X^T (y - X \beta)\right)
= \beta + \frac{2\alpha}{n} X^T (y - X \beta)
$$

---

## Вибір learning rate (α)

- **Занадто маленький α** → повільна конвергенція  
- **Занадто великий α** → градієнтний спуск «скаче» і не сходиться  
- Часто використовують **адаптивні алгоритми** (Adam, RMSProp)  

---

## Варіанти градієнтного спуску

| Метод | Особливості |
|-------|------------|
| Batch Gradient Descent | Використовує всі дані на кожному кроці; стабільно, але повільно |
| Stochastic GD | Один приклад на кроці; швидко, але шумно |
| Mini-batch GD | Компроміс: невеликі підмножини даних; часто найефективніше |

---

## Інтуїція

- Уяви функцію втрат як **гору чи долину**  
- Градієнт показує напрямок **найкрутішого підйому**  
- Ми йдемо в **протилежний бік**, щоб знайти мінімум

---

## Застосування

- Лінійна та поліноміальна регресія  
- Логістична регресія  
- Нейронні мережі  
- Багатовимірні моделі, коли аналітичне рішення неможливе  

---



