
**Поліноміальна регресія** — це розширення лінійної регресії, яке моделює **нелінійні залежності** між змінними за допомогою поліномів вищих степенів.

**Головна ідея:** замість прямої лінії використовуємо криву, щоб краще описати складні залежності в даних.

## Формула

Для одної змінної: $$y = \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3 + ... + \beta_d x^d + \varepsilon$$

де $d$ — степінь полінома.

**Важливо:** це все ще **лінійна модель** відносно параметрів $\beta$, тому можемо використовувати МНК!

## Приклади степенів

|Степінь|Назва|Формула|Графік|
|---|---|---|---|
|1|Лінійна|$y = \beta_0 + \beta_1 x$|Пряма лінія|
|2|Квадратична|$y = \beta_0 + \beta_1 x + \beta_2 x^2$|Парабола|
|3|Кубічна|$y = \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3$|S-подібна крива|
|d|Поліном степеня d|$y = \sum_{i=0}^{d} \beta_i x^i$|Складна крива|

## Простий приклад: траєкторія м'яча

### Дані

М'яч кинули вгору. Висота (м) через t секунд:

|Час (сек)|Висота (м)|
|---|---|
|0|0|
|1|15|
|2|20|
|3|15|
|4|0|

### Лінійна регресія (d=1)

$$y = 0 + 0x$$

**Проблема:** пряма не описує параболічну траєкторію!

### Квадратична регресія (d=2)

$$y = \beta_0 + \beta_1 t + \beta_2 t^2$$

Після МНК отримуємо: $$y = 0 + 10t - 2.5t^2$$

**Перевірка:**

- $t=0$: $y = 0$ ✓
- $t=1$: $y = 10 - 2.5 = 7.5$ (близько до 15)
- $t=2$: $y = 20 - 10 = 10$ (близько до 20)

**Висновок:** квадратична модель набагато краще описує фізичну реальність!

## Складний приклад: продажі vs температура

### Дані

Продажі морозива (тис. шт.) залежно від температури:

|Температура (°C)|Продажі|
|---|---|
|-10|5|
|0|10|
|10|30|
|20|65|
|30|85|
|40|70|

### Аналіз

**Спостереження:** продажі спочатку ростуть (тепліше → більше купують), але при дуже високій температурі падають (люди залишаються вдома).

### Моделі

**Лінійна (d=1):** $$y = 20 + 2t$$

- R² = 0.65
- **Проблема:** не бачить спаду при високих температурах

**Квадратична (d=2):** $$y = 10 + 3.5t - 0.05t^2$$

- R² = 0.92
- **Краще:** описує підйом і спад

**Кубічна (d=3):** $$y = 5 + 4t - 0.08t^2 + 0.001t^3$$

- R² = 0.96
- **Проблема:** можливе переобучення

### Передбачення

Для $t = 25°C$:

|Модель|Формула|Прогноз|
|---|---|---|
|Лінійна|$20 + 2(25)$|70|
|Квадратична|$10 + 3.5(25) - 0.05(625)$|76.25|
|Кубічна|складніше|78.5|

**Реальне значення:** ~77 → квадратична найточніша!

## Як вибрати степінь полінома?

### Методи

1. **Візуальний аналіз** — подивитися на графік даних
2. **Cross-validation** — перевірити на валідаційних даних
3. **Інформаційні критерії** — AIC, BIC (карають за складність)
4. **R² adjusted** — враховує кількість параметрів

### Правила

|Ситуація|Рекомендація|
|---|---|
|Дані лінійні|d = 1|
|Одна "горбина"|d = 2|
|Дві "горбини"|d = 3|
|Складна залежність|d = 4-5 (але обережно!)|

⚠️ **Правило:** d > 5 рідко має сенс — високий ризик переобучення!

## Переобучення (Overfitting)

### Проблема

Чим вищий степінь, тим точніше модель описує **тренувальні** дані, але гірше **генералізує** на нових.

### Приклад

```
d=1: Train R²=0.70, Test R²=0.68  ← Недонавчання
d=2: Train R²=0.85, Test R²=0.83  ← Оптимально ✓
d=5: Train R²=0.99, Test R²=0.65  ← Переобучення ✗
d=10: Train R²=1.00, Test R²=0.30 ← Сильне переобучення ✗✗
```

	
## Створення поліноміальних ознак

### Матричне представлення

Для $x$ і $d=3$ створюємо: $$X_{poly} = [1, x, x^2, x^3]$$

**Приклад:** Якщо $x = 2$: $$X_{poly} = [1, 2, 4, 8]$$

### Для багатьох змінних

Якщо є $x_1, x_2$ і $d=2$, створюємо: $$[1, x_1, x_2, x_1^2, x_1 x_2, x_2^2]$$

**Важливо:** кількість ознак зростає комбінаторно!

## Переваги та недоліки

### Переваги ✓

|Перевага|Пояснення|
|---|---|
|**Нелінійні залежності**|Моделює криві, параболи, складні форми|
|**Простота**|Все ще використовуємо МНК|
|**Інтерпретованість**|Можна бачити вплив степенів|
|**Гнучкість**|Легко змінювати степінь|

### Недоліки ✗

|Недолік|Пояснення|
|---|---|
|**Переобучення**|Високі степені підганяються під шум|
|**Екстраполяція**|Погано передбачає за межами даних|
|**Мультиколінеарність**|$x, x^2, x^3$ сильно корелюють|
|**Вибір степеня**|Немає автоматичного способу|
|**Інтерпретація**|Важко пояснити $\beta_5 x^5$|

## Регуляризація

Для запобігання переобучення комбінуємо з регуляризацією:

### Ridge Polynomial Regression

$$J(\beta) = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 + \lambda \sum_{j=1}^{d} \beta_j^2$$

**Ефект:** дозволяє використовувати вищі степені без сильного переобучення.

## Альтернативи поліноміальній регресії

|Метод|Коли краще|
|---|---|
|**Splines**|Потрібна локальна гнучкість|
|**GAM** (Generalized Additive Models)|Багато змінних, складні залежності|
|**Decision Trees**|Нелінійні взаємодії між змінними|
|**Neural Networks**|Дуже складні залежності|

## Коли використовувати

### Ідеально підходить ✓

- **Фізичні процеси** (траєкторії, температурні залежності)
- **Невеликі датасети** з явною нелінійністю
- **Потрібна інтерпретованість** (в межах d ≤ 3)
- **Швидкий baseline** для нелінійних задач

### Краще використати інше ✗

- Дані **лінійні** → Лінійна регресія
- **Багато змінних** і складні взаємодії → Tree-based моделі
- **Екстраполяція** важлива → Фізичні/статистичні моделі
- **Дуже нелінійні** дані → Neural Networks, GAM

## Практичні поради

1. **Почніть з d=2** — часто достатньо для більшості задач
2. **Візуалізуйте** — графік покаже, чи потрібен вищий степінь
3. **Використовуйте cross-validation** для вибору d
4. **Нормалізуйте дані** — особливо для високих степенів
5. **d ≤ 5** — вищі степені рідко мають практичний сенс
6. **Комбінуйте з регуляризацією** — Ridge допомагає при високих d
7. **Перевіряйте на test set** — Train R² може бути оманливим

## Ключові висновки

> Поліноміальна регресія розширює лінійну регресію для моделювання нелінійних залежностей через додавання степенів змінних.

**Основні принципи:**

- Створюємо нові ознаки: $x, x^2, x^3, ..., x^d$
- Застосовуємо звичайну лінійну регресію (МНК)
- Вищий степінь = більша гнучкість, але ризик переобучення
- Оптимальний d вибираємо через валідацію

**Головне правило:** $$\text{Простота} > \text{Складність}$$

Якщо d=2 дає хороші результати, не використовуй d=10!