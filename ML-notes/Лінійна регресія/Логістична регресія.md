
**Логістична регресія** — це алгоритм машинного навчання для **бінарної класифікації**, який передбачає ймовірність належності об'єкта до певного класу.

**Головна ідея:** перетворити лінійну комбінацію ознак у ймовірність за допомогою **сигмоїдної функції**.

## Основна формула

### Лінійна комбінація

$$z = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_p x_p = \beta^T X$$

### Сигмоїдна функція (логістична функція)

$$\sigma(z) = \frac{1}{1 + e^{-z}}$$

### Передбачення ймовірності

$$P(y=1|X) = \sigma(\beta^T X) = \frac{1}{1 + e^{-\beta^T X}}$$

## Чому саме сигмоїда?

### Властивості

- **Діапазон:** $[0, 1]$ — ідеально для ймовірностей!
- **Монотонна:** зростає від 0 до 1
- **S-подібна крива:** плавний перехід між класами
- **Інтерпретована:** логарифм шансів лінійний

### Графік сигмоїди

```
P(y=1)
  1 |           ────────
    |         ╱
0.5 |       ╱
    |     ╱
  0 | ───
    |________________
         z (лінійна комбінація)
```

**Інтуїція:**

- Коли $z \to +\infty$: $P(y=1) \to 1$ (впевнено клас 1)
- Коли $z \to -\infty$: $P(y=1) \to 0$ (впевнено клас 0)
- Коли $z = 0$: $P(y=1) = 0.5$ (невизначеність)

## Простий приклад: схвалення кредиту

### Дані

|ID|Дохід (тис. $)|Кредитний рейтинг|Схвалено|
|---|---|---|---|
|1|30|400|0 (Ні)|
|2|50|600|1 (Так)|
|3|40|550|0 (Ні)|
|4|70|750|1 (Так)|
|5|60|700|1 (Так)|

### Модель

Після навчання отримали коефіцієнти: $$z = -5 + 0.05 \times \text{Дохід} + 0.01 \times \text{Рейтинг}$$

### Передбачення для нового клієнта

Клієнт: Дохід = 55 тис. $, Рейтинг = 650

**Крок 1:** Обчислити лінійну комбінацію $$z = -5 + 0.05(55) + 0.01(650) = -5 + 2.75 + 6.5 = 4.25$$

**Крок 2:** Застосувати сигмоїду $$P(Схвалено=1) = \frac{1}{1 + e^{-4.25}} = \frac{1}{1 + 0.014} = 0.986$$

**Крок 3:** Прийняти рішення (threshold = 0.5) $$P = 0.986 > 0.5 \Rightarrow \text{Схвалено!} \checkmark$$

**Інтерпретація:** Ймовірність схвалення кредиту — 98.6%

## Складний приклад: діагностика діабету

### Дані

100 пацієнтів з ознаками:

|Ознака|Опис|Діапазон|
|---|---|---|
|Glucose|Рівень глюкози|70-200 mg/dL|
|BMI|Індекс маси тіла|18-45|
|Age|Вік|21-81|
|**Outcome**|Діабет (0/1)|0 або 1|

### Розподіл

- Здорові (y=0): 65 пацієнтів
- Хворі на діабет (y=1): 35 пацієнтів

### Навчання моделі

Припустимо, отримали: $$z = -8 + 0.05 \times \text{Glucose} + 0.1 \times \text{BMI} + 0.02 \times \text{Age}$$

### Приклад 1: Молода людина з нормальними показниками

**Дані:** Glucose=90, BMI=22, Age=25

$$z = -8 + 0.05(90) + 0.1(22) + 0.02(25)$$ $$z = -8 + 4.5 + 2.2 + 0.5 = -0.8$$

$$P(\text{Діабет}=1) = \frac{1}{1 + e^{0.8}} = \frac{1}{1 + 2.23} = 0.31$$

**Рішення:** $P = 0.31 < 0.5 \Rightarrow$ **Здоровий** ✓

**Інтерпретація:** Ризик діабету 31% — низький

### Приклад 2: Людина середнього віку з високими показниками

**Дані:** Glucose=180, BMI=35, Age=55

$$z = -8 + 0.05(180) + 0.1(35) + 0.02(55)$$ $$z = -8 + 9 + 3.5 + 1.1 = 5.6$$

$$P(\text{Діабет}=1) = \frac{1}{1 + e^{-5.6}} = \frac{1}{1 + 0.0037} = 0.996$$

**Рішення:** $P = 0.996 > 0.5 \Rightarrow$ **Діабет** ⚠️

**Інтерпретація:** Ризик діабету 99.6% — дуже високий!

### Приклад 3: Граничний випадок

**Дані:** Glucose=120, BMI=28, Age=45

$$z = -8 + 0.05(120) + 0.1(28) + 0.02(45)$$ $$z = -8 + 6 + 2.8 + 0.9 = 1.7$$

$$P(\text{Діабет}=1) = \frac{1}{1 + e^{-1.7}} = \frac{1}{1 + 0.183} = 0.845$$

**Рішення:** $P = 0.845 > 0.5 \Rightarrow$ **Діабет** (але з меншою впевненістю)

**Інтерпретація:** Ризик діабету 84.5% — високий, але не критичний

## Функція втрат: Log-Loss (Cross-Entropy)

### Для одного прикладу

$$L(y, \hat{p}) = -[y \log(\hat{p}) + (1-y) \log(1-\hat{p})]$$

де:

- $y$ — реальний клас (0 або 1)
- $\hat{p}$ — передбачена ймовірність $P(y=1)$

### Для всього датасету

$$J(\beta) = -\frac{1}{n} \sum_{i=1}^{n} [y_i \log(\hat{p}_i) + (1-y_i) \log(1-\hat{p}_i)]$$

### Інтуїція

**Якщо $y=1$ (реальний клас — позитивний):**

- $\hat{p} = 0.9 \Rightarrow L = -\log(0.9) = 0.105$ (мала помилка ✓)
- $\hat{p} = 0.1 \Rightarrow L = -\log(0.1) = 2.303$ (велика помилка ✗)

**Якщо $y=0$ (реальний клас — негативний):**

- $\hat{p} = 0.1 \Rightarrow L = -\log(0.9) = 0.105$ (мала помилка ✓)
- $\hat{p} = 0.9 \Rightarrow L = -\log(0.1) = 2.303$ (велика помилка ✗)

> Log-loss **суворо карає впевнені, але неправильні передбачення!**

## Оптимізація: Gradient Descent

### Градієнт функції втрат

$$\frac{\partial J}{\partial \beta_j} = \frac{1}{n} \sum_{i=1}^{n} (\hat{p}_i - y_i) x_{ij}$$

### Оновлення коефіцієнтів

$$\beta_j := \beta_j - \alpha \frac{\partial J}{\partial \beta_j}$$

де $\alpha$ — learning rate.

**Відмінність від лінійної регресії:**

- Лінійна: $\frac{\partial J}{\partial \beta_j} = \frac{2}{n} \sum (\hat{y}_i - y_i) x_{ij}$
- Логістична: $\frac{\partial J}{\partial \beta_j} = \frac{1}{n} \sum (\hat{p}_i - y_i) x_{ij}$

**Схоже, але** $\hat{y}_i$ vs $\hat{p}_i$ мають різний зміст!

## Інтерпретація коефіцієнтів: Odds Ratio

### Шанси (Odds)

$$\text{Odds} = \frac{P(y=1)}{P(y=0)} = \frac{P(y=1)}{1 - P(y=1)}$$

### Логарифм шансів (Log-Odds / Logit)

$$\log(\text{Odds}) = z = \beta^T X$$

### Odds Ratio

Коли $x_j$ збільшується на 1 одиницю: $$\text{Odds Ratio} = e^{\beta_j}$$

### Приклад

У моделі діагностики діабету: $\beta_{\text{Glucose}} = 0.05$

$$\text{OR} = e^{0.05} = 1.051$$

**Інтерпретація:** При збільшенні глюкози на 1 mg/dL, шанси мати діабет зростають у **1.051 рази** (або на 5.1%).

### Таблиця інтерпретації

|$\beta_j$|$e^{\beta_j}$|Інтерпретація|
|---|---|---|
|0.69|2.0|Шанси подвоюються|
|1.10|3.0|Шанси потроюються|
|-0.69|0.5|Шанси зменшуються вдвічі|
|0|1.0|Ознака не впливає|

## Decision Threshold (Поріг рішення)

### Стандартний поріг: 0.5

$$\hat{y} = \begin{cases} 1 & \text{якщо } P(y=1) \geq 0.5 \ 0 & \text{якщо } P(y=1) < 0.5 \end{cases}$$

### Налаштування порогу

**Приклад: діагностика раку**

|Поріг|Що відбувається|Коли використовувати|
|---|---|---|
|0.3|Більше позитивних передбачень|Критично не пропустити хворого (high recall)|
|0.5|Збалансовано|Стандартний випадок|
|0.7|Менше позитивних передбачень|Важливо не помилитися з позитивом (high precision)|

### Trade-off

```
Поріг 0.3:
- Recall (чутливість): 95% ✓ — майже всіх хворих знайшли
- Precision (точність): 60% ✗ — багато false positives

Поріг 0.7:
- Recall: 70% ✗ — пропустили деяких хворих
- Precision: 90% ✓ — майже всі позитиви правильні
```

## Метрики оцінки

### Confusion Matrix (Матриця плутанини)

```
                Передбачено
                 0       1
Реально  0    TN=50   FP=10   ← Specificity
         1    FN=5    TP=35   ← Recall/Sensitivity
              ↑       ↑
              NPV   Precision
```

### Основні метрики

|Метрика|Формула|Значення|
|---|---|---|
|**Accuracy**|$\frac{TP + TN}{TP + TN + FP + FN}$|Загальна точність|
|**Precision**|$\frac{TP}{TP + FP}$|Скільки з позитивних правильні|
|**Recall**|$\frac{TP}{TP + FN}$|Скільки реальних позитивних знайшли|
|**F1-Score**|$2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}$|Гармонійне середнє|
|**Specificity**|$\frac{TN}{TN + FP}$|Скільки негативних правильно визначили|

### Приклад розрахунку

Для матриці вище:

- **Accuracy** = $\frac{50+35}{100} = 0.85$ (85%)
- **Precision** = $\frac{35}{35+10} = 0.78$ (78%)
- **Recall** = $\frac{35}{35+5} = 0.88$ (88%)
- **F1-Score** = $2 \times \frac{0.78 \times 0.88}{0.78 + 0.88} = 0.82$ (82%)

### ROC-AUC (Receiver Operating Characteristic)

**ROC крива** — графік залежності True Positive Rate (Recall) від False Positive Rate:

```
TPR (Recall)
  1 |    ╱────
    |   ╱
    |  ╱
0.5 | ╱
    |╱
  0 |________
    0   0.5  1  FPR
    
Діагональ = випадкове гадання (AUC=0.5)
Ідеальна модель = AUC=1.0
Добра модель = AUC>0.8
```

**AUC (Area Under Curve):**

- AUC = 1.0 → Ідеальна модель
- AUC = 0.9-1.0 → Відмінна модель
- AUC = 0.8-0.9 → Добра модель
- AUC = 0.7-0.8 → Прийнятна модель
- AUC = 0.5 → Як підкидання монети

## Багатокласова класифікація

### One-vs-Rest (OvR)

Для K класів будуємо K бінарних класифікаторів:

- Класифікатор 1: "Клас 1" vs "Всі інші"
- Класифікатор 2: "Клас 2" vs "Всі інші"
- ...
- Класифікатор K: "Клас K" vs "Всі інші"

**Передбачення:** обираємо клас з найвищою ймовірністю.

### Softmax Regression (Multinomial Logistic Regression)

Узагальнення для K класів: $$P(y=k|X) = \frac{e^{z_k}}{\sum_{j=1}^{K} e^{z_j}}$$

де $z_k = \beta_k^T X$ для кожного класу.

## Переваги та недоліки

### Переваги ✓

|Перевага|Пояснення|
|---|---|
|**Інтерпретованість**|Легко зрозуміти вплив ознак (odds ratio)|
|**Ймовірності**|Видає калібровані ймовірності, а не просто клас|
|**Швидкість**|Швидке навчання та передбачення|
|**Регуляризація**|Легко додати L1/L2 (Lasso/Ridge)|
|**Працює з малими даними**|Не потребує великих датасетів|
|**Baseline**|Чудовий початковий алгоритм|

### Недоліки ✗

|Недолік|Пояснення|
|---|---|
|**Лінійна межа**|Передбачає лінійну розділимість класів|
|**Нелінійні залежності**|Погано працює зі складними взаємодіями|
|**Чутливість до outliers**|Екстремальні значення впливають на коефіцієнти|
|**Мультиколінеарність**|Проблеми при сильній кореляції ознак|
|**Незбалансовані класи**|Потребує спеціальної обробки|

## Регуляризація

### L1 (Lasso)

$$J(\beta) = -\frac{1}{n} \sum \text{log-loss} + \lambda \sum_{j=1}^{p} |\beta_j|$$

- Зануляє непотрібні коефіцієнти
- Робить відбір ознак

### L2 (Ridge)

$$J(\beta) = -\frac{1}{n} \sum \text{log-loss} + \lambda \sum_{j=1}^{p} \beta_j^2$$

- Зменшує величину коефіцієнтів
- Допомагає при мультиколінеарності

### Elastic Net

$$J(\beta) = -\frac{1}{n} \sum \text{log-loss} + \lambda [\alpha \sum |\beta_j| + (1-\alpha) \sum \beta_j^2]$$

## Приклад коду (Python)

```python
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    classification_report, 
    confusion_matrix,
    roc_auc_score,
    roc_curve
)
import matplotlib.pyplot as plt

# Генерація даних
from sklearn.datasets import make_classification
X, y = make_classification(
    n_samples=1000,
    n_features=10,
    n_informative=5,
    n_redundant=2,
    random_state=42
)

# Розділення даних
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Нормалізація (ВАЖЛИВО!)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Модель
model = LogisticRegression(
    penalty='l2',           # Ridge регуляризація
    C=1.0,                  # Inverse of λ (менше C = більша регуляризація)
    solver='lbfgs',         # Алгоритм оптимізації
    max_iter=1000,
    random_state=42
)

# Навчання
model.fit(X_train_scaled, y_train)

# Передбачення
y_pred = model.predict(X_test_scaled)
y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]

# Оцінка
print("=== Базові метрики ===")
print(f"Accuracy: {model.score(X_test_scaled, y_test):.4f}")
print(f"\nROC-AUC: {roc_auc_score(y_test, y_pred_proba):.4f}")

print("\n=== Classification Report ===")
print(classification_report(y_test, y_pred))

print("\n=== Confusion Matrix ===")
cm = confusion_matrix(y_test, y_pred)
print(cm)

# Коефіцієнти
print("\n=== Коефіцієнти ===")
print(f"Intercept: {model.intercept_[0]:.4f}")
for i, coef in enumerate(model.coef_[0]):
    print(f"Feature {i}: {coef:.4f} (Odds Ratio: {np.exp(coef):.4f})")

# ROC крива
fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)
plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, linewidth=2, label=f'ROC (AUC={roc_auc_score(y_test, y_pred_proba):.3f})')
plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate (Recall)')
plt.title('ROC Curve')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
```

## Коли використовувати

### Ідеально підходить ✓

- **Бінарна класифікація** з лінійно розділимими класами
- **Потрібні ймовірності** (не просто клас, а P(y=1))
- **Інтерпретованість важлива** (медицина, фінанси)
- **Швидкий baseline** перед складнішими моделями
- **Малі/середні датасети** (до 100K прикладів)

### Краще використати інше ✗

- Класи **нелінійно розділимі** → SVM з kernel, Random Forest
- **Складні взаємодії** між ознаками → Tree-based models
- **Дуже великі дані** → SGD Classifier, Neural Networks
- **Зображення, текст, аудіо** → Neural Networks

## Практичні поради

1. **Завжди нормалізуйте дані** — StandardScaler або MinMaxScaler
2. **Починайте з простої моделі** — без регуляризації, threshold=0.5
3. **Перевіряйте на незбалансованість** — використовуйте `class_weight='balanced'`
4. **Підбирайте поріг** — 0.5 не завжди оптимальний
5. **Використовуйте ROC-AUC** — краща метрика, ніж accuracy
6. **Комбінуйте з feature engineering** — interaction terms, polynomial features
7. **Регуляризація завжди** — L2 за замовчуванням, L1 для відбору ознак

## Ключові висновки

> Логістична регресія перетворює лінійну комбінацію ознак у ймовірність за допомогою сигмоїдної функції.

**Основні принципи:**

- Використовує сигмоїду: $\sigma(z) = \frac{1}{1 + e^{-z}}$
- Оптимізує log-loss функцію
- Дає калібровані ймовірності P(y=1|X)
- Коефіцієнти інтерпретуються через odds ratio
- Threshold можна налаштовувати під задачу

**Формула передбачення:** $$P(y=1|X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x_1 + ... + \beta_p x_p)}}$$