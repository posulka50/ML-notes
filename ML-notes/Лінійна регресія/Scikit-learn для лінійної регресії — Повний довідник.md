
–ü–æ–≤–Ω–∏–π –Ω–∞–±—ñ—Ä –∫–ª–∞—Å—ñ–≤, –º–µ—Ç–æ–¥—ñ–≤ —Ç–∞ —Ñ—É–Ω–∫—Ü—ñ–π scikit-learn –¥–ª—è —Ä–æ–±–æ—Ç–∏ –∑ –ª—ñ–Ω—ñ–π–Ω–æ—é —Ä–µ–≥—Ä–µ—Å—ñ—î—é —Ç–∞ —Å—É–º—ñ–∂–Ω–∏–º–∏ —Ç–µ–º–∞–º–∏.

---

## üì¶ –Ü–º–ø–æ—Ä—Ç–∏

```python
# –û—Å–Ω–æ–≤–Ω—ñ –º–æ–¥–µ–ª—ñ
from sklearn.linear_model import (
    LinearRegression,      # –ó–≤–∏—á–∞–π–Ω–∞ –ª—ñ–Ω—ñ–π–Ω–∞ —Ä–µ–≥—Ä–µ—Å—ñ—è
    Ridge,                 # Ridge —Ä–µ–≥—Ä–µ—Å—ñ—è (L2)
    Lasso,                 # Lasso —Ä–µ–≥—Ä–µ—Å—ñ—è (L1)
    ElasticNet,            # Elastic Net (L1 + L2)
    LogisticRegression,    # –õ–æ–≥—ñ—Å—Ç–∏—á–Ω–∞ —Ä–µ–≥—Ä–µ—Å—ñ—è
    RidgeCV,               # Ridge –∑ –≤–±—É–¥–æ–≤–∞–Ω–æ—é cross-validation
    LassoCV,               # Lasso –∑ –≤–±—É–¥–æ–≤–∞–Ω–æ—é CV
    ElasticNetCV,          # ElasticNet –∑ CV
    SGDRegressor,          # –°—Ç–æ—Ö–∞—Å—Ç–∏—á–Ω–∏–π –≥—Ä–∞–¥—ñ—î–Ω—Ç–Ω–∏–π —Å–ø—É—Å–∫
    SGDClassifier          # SGD –¥–ª—è –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó
)

# –ü–æ–ª—ñ–Ω–æ–º—ñ–∞–ª—å–Ω—ñ –æ–∑–Ω–∞–∫–∏
from sklearn.preprocessing import (
    PolynomialFeatures,    # –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –ø–æ–ª—ñ–Ω–æ–º—ñ–∞–ª—å–Ω–∏—Ö –æ–∑–Ω–∞–∫
    StandardScaler,        # –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è (mean=0, std=1)
    MinMaxScaler,          # –ú–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è [0, 1]
    RobustScaler,          # –†–æ–±–∞—Å—Ç–Ω–µ –º–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è (—Å—Ç—ñ–π–∫–µ –¥–æ –≤–∏–∫–∏–¥—ñ–≤)
    LabelEncoder,          # –ö–æ–¥—É–≤–∞–Ω–Ω—è –º—ñ—Ç–æ–∫
    OneHotEncoder          # One-hot –∫–æ–¥—É–≤–∞–Ω–Ω—è
)

# –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö
from sklearn.model_selection import (
    train_test_split,      # –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –Ω–∞ train/test
    cross_val_score,       # Cross-validation –∑ –æ—Ü—ñ–Ω–∫–æ—é
    cross_validate,        # CV –∑ –¥–µ—Ç–∞–ª—å–Ω–∏–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
    KFold,                 # K-Fold CV
    StratifiedKFold,       # Stratified K-Fold
    TimeSeriesSplit,       # Time series split
    LeaveOneOut,           # Leave-One-Out CV
    GroupKFold,            # Group K-Fold
    GridSearchCV,          # Grid search –¥–ª—è –≥—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤
    RandomizedSearchCV,    # Random search
    learning_curve,        # –ö—Ä–∏–≤—ñ –Ω–∞–≤—á–∞–Ω–Ω—è
    validation_curve       # –ö—Ä–∏–≤—ñ –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó
)

# –ú–µ—Ç—Ä–∏–∫–∏ —Ä–µ–≥—Ä–µ—Å—ñ—ó
from sklearn.metrics import (
    mean_squared_error,           # MSE
    mean_absolute_error,          # MAE
    r2_score,                     # R¬≤
    mean_absolute_percentage_error, # MAPE
    explained_variance_score      # Explained variance
)

# –ú–µ—Ç—Ä–∏–∫–∏ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó
from sklearn.metrics import (
    accuracy_score,        # Accuracy
    precision_score,       # Precision
    recall_score,          # Recall
    f1_score,              # F1-score
    roc_auc_score,         # ROC-AUC
    roc_curve,             # ROC –∫—Ä–∏–≤–∞
    confusion_matrix,      # Confusion matrix
    classification_report  # –ü–æ–≤–Ω–∏–π –∑–≤—ñ—Ç
)

# Pipelines
from sklearn.pipeline import Pipeline, make_pipeline

# Utilities
from sklearn.datasets import (
    make_regression,       # –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è —Ä–µ–≥—Ä–µ—Å—ñ–π–Ω–∏—Ö –¥–∞–Ω–∏—Ö
    make_classification,   # –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ–π–Ω–∏—Ö –¥–∞–Ω–∏—Ö
    load_diabetes,         # –î–∞—Ç–∞—Å–µ—Ç –¥—ñ–∞–±–µ—Ç—É
    load_boston            # –î–∞—Ç–∞—Å–µ—Ç —Ü—ñ–Ω –±—É–¥–∏–Ω–∫—ñ–≤ (deprecated)
)

# –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª–µ–π
import pickle
import joblib
```

---

## 1Ô∏è‚É£ LinearRegression ‚Äî –ó–≤–∏—á–∞–π–Ω–∞ –ª—ñ–Ω—ñ–π–Ω–∞ —Ä–µ–≥—Ä–µ—Å—ñ—è

### –û—Å–Ω–æ–≤–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏

```python
LinearRegression(
    fit_intercept=True,    # –ß–∏ –æ–±—á–∏—Å–ª—é–≤–∞—Ç–∏ Œ≤‚ÇÄ (intercept)
    normalize=False,       # Deprecated (–≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π StandardScaler)
    copy_X=True,           # –ö–æ–ø—ñ—é–≤–∞—Ç–∏ X (—â–æ–± –Ω–µ –∑–º—ñ–Ω—é–≤–∞—Ç–∏ –æ—Ä–∏–≥—ñ–Ω–∞–ª)
    n_jobs=None,           # –ö—ñ–ª—å–∫—ñ—Å—Ç—å —è–¥–µ—Ä (-1 = –≤—Å—ñ)
    positive=False         # –ü—Ä–∏–º—É—Å–∏—Ç–∏ –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç–∏ –±—É—Ç–∏ –ø–æ–∑–∏—Ç–∏–≤–Ω–∏–º–∏
)
```

### –ú–µ—Ç–æ–¥–∏

```python
model = LinearRegression()

# –ù–∞–≤—á–∞–Ω–Ω—è
model.fit(X_train, y_train)

# –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
y_pred = model.predict(X_test)

# –û—Ü—ñ–Ω–∫–∞ (R¬≤)
score = model.score(X_test, y_test)

# –î–æ—Å—Ç—É–ø –¥–æ –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç—ñ–≤
model.coef_          # Œ≤‚ÇÅ, Œ≤‚ÇÇ, ..., Œ≤‚Çö
model.intercept_     # Œ≤‚ÇÄ

# –ö—ñ–ª—å–∫—ñ—Å—Ç—å –æ–∑–Ω–∞–∫
model.n_features_in_
model.feature_names_in_  # –Ø–∫—â–æ X –±—É–≤ DataFrame
```

### –ü—Ä–∏–∫–ª–∞–¥

```python
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

# –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –¥–∞–Ω–∏—Ö
X = np.random.randn(100, 5)
y = 3*X[:, 0] + 2*X[:, 1] - X[:, 2] + np.random.randn(100)*0.5

# –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# –ú–æ–¥–µ–ª—å
model = LinearRegression()
model.fit(X_train, y_train)

# –†–µ–∑—É–ª—å—Ç–∞—Ç–∏
print(f"Coefficients: {model.coef_}")
print(f"Intercept: {model.intercept_}")
print(f"R¬≤ train: {model.score(X_train, y_train):.3f}")
print(f"R¬≤ test: {model.score(X_test, y_test):.3f}")

# –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
y_pred = model.predict(X_test)
print(f"MSE: {mean_squared_error(y_test, y_pred):.3f}")
print(f"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.3f}")
```

---

## 2Ô∏è‚É£ Ridge ‚Äî L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—è

### –û—Å–Ω–æ–≤–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏

```python
Ridge(
    alpha=1.0,             # Œª - —Å–∏–ª–∞ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—ó (–±—ñ–ª—å—à–µ = —Å–∏–ª—å–Ω—ñ—à–∞)
    fit_intercept=True,
    copy_X=True,
    max_iter=None,         # –ú–∞–∫—Å–∏–º—É–º —ñ—Ç–µ—Ä–∞—Ü—ñ–π (–¥–ª—è solver='sag')
    tol=1e-4,              # –¢–æ–ª–µ—Ä–∞–Ω—Ç–Ω—ñ—Å—Ç—å –¥–ª—è –∑—É–ø–∏–Ω–∫–∏
    solver='auto',         # 'auto', 'svd', 'cholesky', 'lsqr', 'sag', 'saga'
    positive=False,
    random_state=None
)
```

### –ú–µ—Ç–æ–¥–∏ (–∞–Ω–∞–ª–æ–≥—ñ—á–Ω—ñ LinearRegression)

```python
model = Ridge(alpha=1.0)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
score = model.score(X_test, y_test)
```

### –ü—Ä–∏–∫–ª–∞–¥ –∑ –ø—ñ–¥–±–æ—Ä–æ–º alpha

```python
from sklearn.linear_model import Ridge
from sklearn.model_selection import GridSearchCV

# –ü—ñ–¥–±—ñ—Ä alpha
param_grid = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100]}

ridge = Ridge()
grid_search = GridSearchCV(
    ridge, param_grid, cv=5, scoring='r2'
)
grid_search.fit(X_train, y_train)

print(f"Best alpha: {grid_search.best_params_['alpha']}")
print(f"Best R¬≤: {grid_search.best_score_:.3f}")

# –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –Ω–∞–π–∫—Ä–∞—â–æ—ó –º–æ–¥–µ–ª—ñ
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)
```

### RidgeCV ‚Äî –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∏–π –ø—ñ–¥–±—ñ—Ä alpha

```python
from sklearn.linear_model import RidgeCV

# –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∏–π –≤–∏–±—ñ—Ä alpha
alphas = [0.001, 0.01, 0.1, 1, 10, 100]
model = RidgeCV(alphas=alphas, cv=5)
model.fit(X_train, y_train)

print(f"Best alpha: {model.alpha_}")
print(f"R¬≤ test: {model.score(X_test, y_test):.3f}")
```

---

## 3Ô∏è‚É£ Lasso ‚Äî L1 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—è

### –û—Å–Ω–æ–≤–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏

```python
Lasso(
    alpha=1.0,             # Œª - —Å–∏–ª–∞ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—ó
    fit_intercept=True,
    max_iter=1000,         # –ú–∞–∫—Å–∏–º—É–º —ñ—Ç–µ—Ä–∞—Ü—ñ–π
    tol=1e-4,
    positive=False,
    selection='cyclic',    # 'cyclic' –∞–±–æ 'random'
    random_state=None
)
```

### –û—Å–æ–±–ª–∏–≤—ñ—Å—Ç—å: –≤—ñ–¥–±—ñ—Ä –æ–∑–Ω–∞–∫

```python
from sklearn.linear_model import Lasso

model = Lasso(alpha=0.1)
model.fit(X_train, y_train)

# –Ø–∫—ñ –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç–∏ –∑–∞–Ω—É–ª—è—Ç—å—Å—è?
print("Coefficients:", model.coef_)
print("Non-zero features:", np.sum(model.coef_ != 0))

# –í—ñ–¥–±—ñ—Ä –Ω–∞–π–≤–∞–∂–ª–∏–≤—ñ—à–∏—Ö –æ–∑–Ω–∞–∫
important_features = np.where(model.coef_ != 0)[0]
print(f"Important features: {important_features}")
```

### LassoCV

```python
from sklearn.linear_model import LassoCV

model = LassoCV(
    alphas=None,           # –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –≥–µ–Ω–µ—Ä—É—î alphas
    cv=5,
    max_iter=10000,
    random_state=42
)
model.fit(X_train, y_train)

print(f"Best alpha: {model.alpha_}")
print(f"Non-zero coefs: {np.sum(model.coef_ != 0)}")
```

---

## 4Ô∏è‚É£ ElasticNet ‚Äî L1 + L2

### –û—Å–Ω–æ–≤–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏

```python
ElasticNet(
    alpha=1.0,             # –ó–∞–≥–∞–ª—å–Ω–∞ —Å–∏–ª–∞ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—ó
    l1_ratio=0.5,          # –ë–∞–ª–∞–Ω—Å L1/L2: 0=Ridge, 1=Lasso
    fit_intercept=True,
    max_iter=1000,
    tol=1e-4,
    positive=False,
    selection='cyclic',
    random_state=None
)
```

### –ü—Ä–∏–∫–ª–∞–¥

```python
from sklearn.linear_model import ElasticNet

model = ElasticNet(
    alpha=0.1,      # –°–∏–ª–∞ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—ó
    l1_ratio=0.5    # 50% L1 + 50% L2
)
model.fit(X_train, y_train)

print(f"R¬≤ test: {model.score(X_test, y_test):.3f}")
print(f"Non-zero coefs: {np.sum(model.coef_ != 0)}")
```

### ElasticNetCV

```python
from sklearn.linear_model import ElasticNetCV

model = ElasticNetCV(
    l1_ratio=[0.1, 0.5, 0.7, 0.9, 0.95, 0.99, 1],
    alphas=[0.001, 0.01, 0.1, 1],
    cv=5,
    random_state=42
)
model.fit(X_train, y_train)

print(f"Best alpha: {model.alpha_}")
print(f"Best l1_ratio: {model.l1_ratio_}")
```

---

## 5Ô∏è‚É£ LogisticRegression ‚Äî –õ–æ–≥—ñ—Å—Ç–∏—á–Ω–∞ —Ä–µ–≥—Ä–µ—Å—ñ—è

### –û—Å–Ω–æ–≤–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏

```python
LogisticRegression(
    penalty='l2',          # 'l1', 'l2', 'elasticnet', None
    C=1.0,                 # Inverse of Œª (–º–µ–Ω—à–µ = —Å–∏–ª—å–Ω—ñ—à–∞ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—è)
    solver='lbfgs',        # 'lbfgs', 'liblinear', 'newton-cg', 'sag', 'saga'
    max_iter=100,
    multi_class='auto',    # 'ovr', 'multinomial'
    class_weight=None,     # 'balanced' –¥–ª—è –Ω–µ–∑–±–∞–ª–∞–Ω—Å–æ–≤–∞–Ω–∏—Ö –∫–ª–∞—Å—ñ–≤
    random_state=None,
    l1_ratio=None          # –î–ª—è penalty='elasticnet'
)
```

### –ú–µ—Ç–æ–¥–∏

```python
model = LogisticRegression()
model.fit(X_train, y_train)

# –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –∫–ª–∞—Å—ñ–≤
y_pred = model.predict(X_test)

# –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç–µ–π
y_proba = model.predict_proba(X_test)
# [[P(class=0), P(class=1)], ...]

# Log-odds
y_decision = model.decision_function(X_test)

# Accuracy
score = model.score(X_test, y_test)

# –ö–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç–∏
model.coef_        # Œ≤‚ÇÅ, Œ≤‚ÇÇ, ..., Œ≤‚Çö
model.intercept_   # Œ≤‚ÇÄ
model.classes_     # –£–Ω—ñ–∫–∞–ª—å–Ω—ñ –∫–ª–∞—Å–∏
```

### –ü—Ä–∏–∫–ª–∞–¥ –∑ –º–µ—Ç—Ä–∏–∫–∞–º–∏

```python
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    classification_report, 
    confusion_matrix,
    roc_auc_score,
    roc_curve
)
import matplotlib.pyplot as plt

# –ú–æ–¥–µ–ª—å
model = LogisticRegression(
    penalty='l2',
    C=1.0,
    solver='lbfgs',
    max_iter=1000,
    random_state=42
)
model.fit(X_train, y_train)

# –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
y_pred = model.predict(X_test)
y_proba = model.predict_proba(X_test)[:, 1]

# –ú–µ—Ç—Ä–∏–∫–∏
print("=== Classification Report ===")
print(classification_report(y_test, y_pred))

print("\n=== Confusion Matrix ===")
print(confusion_matrix(y_test, y_pred))

print(f"\nROC-AUC: {roc_auc_score(y_test, y_proba):.3f}")

# ROC Curve
fpr, tpr, thresholds = roc_curve(y_test, y_proba)
plt.plot(fpr, tpr, label=f'ROC (AUC={roc_auc_score(y_test, y_proba):.3f})')
plt.plot([0, 1], [0, 1], 'k--', label='Random')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend()
plt.show()
```

---

## 6Ô∏è‚É£ SGDRegressor / SGDClassifier ‚Äî –°—Ç–æ—Ö–∞—Å—Ç–∏—á–Ω–∏–π –≥—Ä–∞–¥—ñ—î–Ω—Ç–Ω–∏–π —Å–ø—É—Å–∫

### SGDRegressor

```python
from sklearn.linear_model import SGDRegressor

SGDRegressor(
    loss='squared_error',  # 'squared_error', 'huber', 'epsilon_insensitive'
    penalty='l2',          # 'l1', 'l2', 'elasticnet'
    alpha=0.0001,          # –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—è
    l1_ratio=0.15,         # –î–ª—è elasticnet
    max_iter=1000,
    tol=1e-3,
    learning_rate='invscaling',  # 'constant', 'optimal', 'invscaling', 'adaptive'
    eta0=0.01,             # –ü–æ—á–∞—Ç–∫–æ–≤–∏–π learning rate
    random_state=None
)
```

### –ü—Ä–∏–∫–ª–∞–¥

```python
from sklearn.linear_model import SGDRegressor

model = SGDRegressor(
    loss='squared_error',
    penalty='l2',
    alpha=0.0001,
    max_iter=1000,
    random_state=42
)
model.fit(X_train, y_train)

print(f"R¬≤ test: {model.score(X_test, y_test):.3f}")
```

### SGDClassifier

```python
from sklearn.linear_model import SGDClassifier

model = SGDClassifier(
    loss='log_loss',       # 'hinge' (SVM), 'log_loss' (LogReg)
    penalty='l2',
    alpha=0.0001,
    max_iter=1000,
    class_weight='balanced',
    random_state=42
)
model.fit(X_train, y_train)
```

---

## 7Ô∏è‚É£ PolynomialFeatures ‚Äî –ü–æ–ª—ñ–Ω–æ–º—ñ–∞–ª—å–Ω–∞ —Ä–µ–≥—Ä–µ—Å—ñ—è

### –ü–∞—Ä–∞–º–µ—Ç—Ä–∏

```python
PolynomialFeatures(
    degree=2,              # –°—Ç–µ–ø—ñ–Ω—å –ø–æ–ª—ñ–Ω–æ–º–∞
    interaction_only=False, # –¢—ñ–ª—å–∫–∏ –≤–∑–∞—î–º–æ–¥—ñ—ó (x1*x2), –±–µ–∑ x1¬≤, x2¬≤
    include_bias=True      # –î–æ–¥–∞–≤–∞—Ç–∏ –∫–æ–ª–æ–Ω–∫—É –∑ 1 (–¥–ª—è intercept)
)
```

### –ü—Ä–∏–∫–ª–∞–¥

```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import Pipeline

# –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –ø–æ–ª—ñ–Ω–æ–º—ñ–∞–ª—å–Ω–∏—Ö –æ–∑–Ω–∞–∫
poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X)

print(f"Original features: {X.shape[1]}")
print(f"Polynomial features: {X_poly.shape[1]}")
print(f"Feature names: {poly.get_feature_names_out()}")

# –ê–±–æ –∑ Pipeline
model = Pipeline([
    ('poly', PolynomialFeatures(degree=2)),
    ('linear', LinearRegression())
])
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
```

### –ü—Ä–∏–∫–ª–∞–¥ –∑ —Ä—ñ–∑–Ω–∏–º–∏ —Å—Ç–µ–ø–µ–Ω—è–º–∏

```python
from sklearn.metrics import mean_squared_error
import numpy as np

degrees = [1, 2, 3, 4, 5]
results = []

for d in degrees:
    model = Pipeline([
        ('poly', PolynomialFeatures(degree=d)),
        ('linear', LinearRegression())
    ])
    
    model.fit(X_train, y_train)
    
    train_mse = mean_squared_error(y_train, model.predict(X_train))
    test_mse = mean_squared_error(y_test, model.predict(X_test))
    
    results.append({
        'degree': d,
        'train_mse': train_mse,
        'test_mse': test_mse
    })
    
    print(f"Degree {d}: Train MSE={train_mse:.3f}, Test MSE={test_mse:.3f}")
```

---

## 8Ô∏è‚É£ Preprocessing ‚Äî –ú–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è —Ç–∞ –∫–æ–¥—É–≤–∞–Ω–Ω—è

### StandardScaler ‚Äî –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è

```python
from sklearn.preprocessing import StandardScaler

# –ü–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—è: (x - mean) / std
scaler = StandardScaler()

# Fit + transform –Ω–∞ train
X_train_scaled = scaler.fit_transform(X_train)

# –¢—ñ–ª—å–∫–∏ transform –Ω–∞ test (–≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ train!)
X_test_scaled = scaler.transform(X_test)

# –î–æ—Å—Ç—É–ø –¥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤
print(f"Mean: {scaler.mean_}")
print(f"Std: {scaler.scale_}")
```

### MinMaxScaler ‚Äî –º–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è [0, 1]

```python
from sklearn.preprocessing import MinMaxScaler

# –ü–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—è: (x - min) / (max - min)
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X_train)
```

### RobustScaler ‚Äî —Å—Ç—ñ–π–∫–µ –¥–æ –≤–∏–∫–∏–¥—ñ–≤

```python
from sklearn.preprocessing import RobustScaler

# –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î –º–µ–¥—ñ–∞–Ω—É —Ç–∞ IQR –∑–∞–º—ñ—Å—Ç—å mean/std
scaler = RobustScaler()
X_scaled = scaler.fit_transform(X_train)
```

### OneHotEncoder ‚Äî –∫–æ–¥—É–≤–∞–Ω–Ω—è –∫–∞—Ç–µ–≥–æ—Ä—ñ–∞–ª—å–Ω–∏—Ö

```python
from sklearn.preprocessing import OneHotEncoder

encoder = OneHotEncoder(sparse_output=False, drop='first')
X_encoded = encoder.fit_transform(X_categorical)

# –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞: pandas get_dummies
import pandas as pd
X_encoded = pd.get_dummies(df, columns=['category'], drop_first=True)
```

---

## 9Ô∏è‚É£ Model Selection ‚Äî –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è —Ç–∞ –≤–∞–ª—ñ–¥–∞—Ü—ñ—è

### train_test_split

```python
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,         # 20% –Ω–∞ —Ç–µ—Å—Ç
    random_state=42,       # –í—ñ–¥—Ç–≤–æ—Ä—é–≤–∞–Ω—ñ—Å—Ç—å
    shuffle=True,          # –ü–µ—Ä–µ–º—ñ—à–∞—Ç–∏
    stratify=y             # –ó–±–µ—Ä–µ–≥—Ç–∏ –ø—Ä–æ–ø–æ—Ä—Ü—ñ—ó –∫–ª–∞—Å—ñ–≤ (–¥–ª—è –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó)
)
```

### cross_val_score ‚Äî –ø—Ä–æ—Å—Ç–∞ CV

```python
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import Ridge

model = Ridge(alpha=1.0)

# 5-fold cross-validation
scores = cross_val_score(
    model, X, y,
    cv=5,                  # –ö—ñ–ª—å–∫—ñ—Å—Ç—å folds
    scoring='r2'           # –ú–µ—Ç—Ä–∏–∫–∞: 'r2', 'neg_mean_squared_error', etc.
)

print(f"CV scores: {scores}")
print(f"Mean: {scores.mean():.3f} ¬± {scores.std():.3f}")
```

### cross_validate ‚Äî –¥–µ—Ç–∞–ª—å–Ω–∞ CV

```python
from sklearn.model_selection import cross_validate

results = cross_validate(
    model, X, y,
    cv=5,
    scoring=['r2', 'neg_mean_squared_error'],
    return_train_score=True
)

print(f"Train R¬≤: {results['train_r2'].mean():.3f}")
print(f"Val R¬≤: {results['test_r2'].mean():.3f}")
print(f"Val MSE: {-results['test_neg_mean_squared_error'].mean():.3f}")
```

### KFold —Ç–∞ StratifiedKFold

```python
from sklearn.model_selection import KFold, StratifiedKFold

# KFold –¥–ª—è —Ä–µ–≥—Ä–µ—Å—ñ—ó
kf = KFold(n_splits=5, shuffle=True, random_state=42)

for train_idx, val_idx in kf.split(X):
    X_train_fold, X_val_fold = X[train_idx], X[val_idx]
    y_train_fold, y_val_fold = y[train_idx], y[val_idx]
    
    # –ù–∞–≤—á–∞–Ω–Ω—è —Ç–∞ –æ—Ü—ñ–Ω–∫–∞
    model.fit(X_train_fold, y_train_fold)
    score = model.score(X_val_fold, y_val_fold)
    print(f"Fold score: {score:.3f}")

# StratifiedKFold –¥–ª—è –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_val_score(model, X, y, cv=skf)
```

### TimeSeriesSplit

```python
from sklearn.model_selection import TimeSeriesSplit

tscv = TimeSeriesSplit(n_splits=5)

for train_idx, val_idx in tscv.split(X):
    X_train, X_val = X[train_idx], X[val_idx]
    y_train, y_val = y[train_idx], y[val_idx]
    
    model.fit(X_train, y_train)
    score = model.score(X_val, y_val)
```

---

## üîü GridSearchCV —Ç–∞ RandomizedSearchCV

### GridSearchCV ‚Äî –ø–µ—Ä–µ–±—ñ—Ä —É—Å—ñ—Ö –∫–æ–º–±—ñ–Ω–∞—Ü—ñ–π

```python
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import Ridge

param_grid = {
    'alpha': [0.001, 0.01, 0.1, 1, 10, 100],
    'solver': ['auto', 'svd', 'lsqr']
}

grid_search = GridSearchCV(
    Ridge(),
    param_grid,
    cv=5,
    scoring='r2',
    n_jobs=-1,             # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –≤—Å—ñ —è–¥—Ä–∞
    verbose=1,
    return_train_score=True
)

grid_search.fit(X_train, y_train)

# –†–µ–∑—É–ª—å—Ç–∞—Ç–∏
print(f"Best params: {grid_search.best_params_}")
print(f"Best CV score: {grid_search.best_score_:.3f}")

# –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –Ω–∞–π–∫—Ä–∞—â–æ—ó –º–æ–¥–µ–ª—ñ
best_model = grid_search.best_estimator_
test_score = best_model.score(X_test, y_test)
print(f"Test score: {test_score:.3f}")

# –í—Å—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
import pandas as pd
results_df = pd.DataFrame(grid_search.cv_results_)
print(results_df[['params', 'mean_test_score', 'std_test_score']])
```

### RandomizedSearchCV ‚Äî –≤–∏–ø–∞–¥–∫–æ–≤–∏–π –ø–æ—à—É–∫

```python
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import uniform, randint

param_distributions = {
    'alpha': uniform(0.001, 100),     # –ù–µ–ø–µ—Ä–µ—Ä–≤–Ω–∏–π —Ä–æ–∑–ø–æ–¥—ñ–ª
    'solver': ['auto', 'svd', 'lsqr']
}

random_search = RandomizedSearchCV(
    Ridge(),
    param_distributions,
    n_iter=20,             # –ö—ñ–ª—å–∫—ñ—Å—Ç—å –∫–æ–º–±—ñ–Ω–∞—Ü—ñ–π –¥–ª—è –ø—Ä–æ–±—É–≤–∞–Ω–Ω—è
    cv=5,
    scoring='r2',
    random_state=42,
    n_jobs=-1
)

random_search.fit(X_train, y_train)
print(f"Best params: {random_search.best_params_}")
```

### Pipeline –∑ GridSearch

```python
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Ridge

pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('ridge', Ridge())
])

param_grid = {
    'ridge__alpha': [0.1, 1, 10],     # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π –Ω–∞–∑–≤—É –∫—Ä–æ–∫—É + '__'
    'ridge__solver': ['auto', 'svd']
}

grid_search = GridSearchCV(pipeline, param_grid, cv=5)
grid_search.fit(X_train, y_train)
```

---

## 1Ô∏è‚É£1Ô∏è‚É£ Metrics ‚Äî –ú–µ—Ç—Ä–∏–∫–∏ –æ—Ü—ñ–Ω–∫–∏

### –ú–µ—Ç—Ä–∏–∫–∏ —Ä–µ–≥—Ä–µ—Å—ñ—ó

```python
from sklearn.metrics import (
    mean_squared_error,
    mean_absolute_error,
    r2_score,
    mean_absolute_percentage_error,
    explained_variance_score
)

y_pred = model.predict(X_test)

# MSE
mse = mean_squared_error(y_test, y_pred)
print(f"MSE: {mse:.3f}")

# RMSE
rmse = np.sqrt(mse)
print(f"RMSE: {rmse:.3f}")

# MAE
mae = mean_absolute_error(y_test, y_pred)
print(f"MAE: {mae:.3f}")

# R¬≤
r2 = r2_score(y_test, y_pred)
print(f"R¬≤: {r2:.3f}")

# MAPE
mape = mean_absolute_percentage_error(y_test, y_pred)
print(f"MAPE: {mape:.3f}")

# Explained Variance
ev = explained_variance_score(y_test, y_pred)
print(f"Explained Variance: {ev:.3f}")
```

### –ú–µ—Ç—Ä–∏–∫–∏ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó

```python
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    roc_auc_score,
    confusion_matrix,
    classification_report
)

y_pred = model.predict(X_test)
y_proba = model.predict_proba(X_test)[:, 1]  # –ô–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å –∫–ª–∞—Å—É 1

# Accuracy
acc = accuracy_score(y_test, y_pred)
print(f"Accuracy: {acc:.3f}")

# Precision, Recall, F1
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
print(f"Precision: {precision:.3f}")
print(f"Recall: {recall:.3f}")
print(f"F1: {f1:.3f}")

# ROC-AUC
auc = roc_auc_score(y_test, y_proba)
print(f"ROC-AUC: {auc:.3f}")

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(cm)

# Classification Report
print("\nClassification Report:")
print(classification_report(y_test, y_pred))
```

---

## 1Ô∏è‚É£2Ô∏è‚É£ Learning Curves —Ç–∞ Validation Curves

### learning_curve ‚Äî –∫—Ä–∏–≤—ñ –Ω–∞–≤—á–∞–Ω–Ω—è

```python
from sklearn.model_selection import learning_curve
import matplotlib.pyplot as plt

train_sizes, train_scores, val_scores = learning_curve(
    model, X, y,
    train_sizes=np.linspace(0.1, 1.0, 10),
    cv=5,
    scoring='r2',
    n_jobs=-1
)

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
train_mean = train_scores.mean(axis=1)
train_std = train_scores.std(axis=1)
val_mean = val_scores.mean(axis=1)
val_std = val_scores.std(axis=1)

plt.figure(figsize=(10, 6))
plt.plot(train_sizes, train_mean, label='Train Score')
plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.2)
plt.plot(train_sizes, val_mean, label='Validation Score')
plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.2)
plt.xlabel('Training Set Size')
plt.ylabel('Score')
plt.title('Learning Curves')
plt.legend()
plt.grid()
plt.show()
```

### validation_curve ‚Äî –∫—Ä–∏–≤—ñ –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó

```python
from sklearn.model_selection import validation_curve

param_range = [0.001, 0.01, 0.1, 1, 10, 100]
train_scores, val_scores = validation_curve(
    Ridge(), X, y,
    param_name='alpha',
    param_range=param_range,
    cv=5,
    scoring='r2'
)

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
plt.figure(figsize=(10, 6))
plt.plot(param_range, train_scores.mean(axis=1), label='Train Score')
plt.plot(param_range, val_scores.mean(axis=1), label='Validation Score')
plt.xscale('log')
plt.xlabel('Alpha (regularization)')
plt.ylabel('R¬≤')
plt.title('Validation Curve for Ridge')
plt.legend()
plt.grid()
plt.show()
```

---

## 1Ô∏è‚É£3Ô∏è‚É£ Pipeline ‚Äî –ö–æ–Ω–≤–µ—î—Ä–∏ –æ–±—Ä–æ–±–∫–∏

### –ü—Ä–æ—Å—Ç–∏–π Pipeline

```python
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Ridge

pipeline = Pipeline([
    ('scaler', StandardScaler()),      # –ö—Ä–æ–∫ 1: –º–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è
    ('ridge', Ridge(alpha=1.0))        # –ö—Ä–æ–∫ 2: –º–æ–¥–µ–ª—å
])

# –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è —è–∫ –∑–≤–∏—á–∞–π–Ω–∞ –º–æ–¥–µ–ª—å
pipeline.fit(X_train, y_train)
y_pred = pipeline.predict(X_test)
score = pipeline.score(X_test, y_test)

# –î–æ—Å—Ç—É–ø –¥–æ –æ–∫—Ä–µ–º–∏—Ö –∫—Ä–æ–∫—ñ–≤
scaler = pipeline.named_steps['scaler']
model = pipeline.named_steps['ridge']
print(f"Ridge coefficients: {model.coef_}")
```

### make_pipeline ‚Äî —Å–∫–æ—Ä–æ—á–µ–Ω–∏–π —Å–∏–Ω—Ç–∞–∫—Å–∏—Å

```python
from sklearn.pipeline import make_pipeline

pipeline = make_pipeline(
    StandardScaler(),
    PolynomialFeatures(degree=2),
    Ridge(alpha=1.0)
)

pipeline.fit(X_train, y_train)
```

### Pipeline –∑ –ø–æ–ª—ñ–Ω–æ–º—ñ–∞–ª—å–Ω–∏–º–∏ –æ–∑–Ω–∞–∫–∞–º–∏

```python
from sklearn.preprocessing import PolynomialFeatures

pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('poly', PolynomialFeatures(degree=2)),
    ('ridge', Ridge(alpha=1.0))
])

pipeline.fit(X_train, y_train)
y_pred = pipeline.predict(X_test)
```

---

## 1Ô∏è‚É£4Ô∏è‚É£ –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ç–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª–µ–π

### pickle

```python
import pickle

# –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è
with open('model.pkl', 'wb') as f:
    pickle.dump(model, f)

# –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è
with open('model.pkl', 'rb') as f:
    loaded_model = pickle.load(f)

y_pred = loaded_model.predict(X_test)
```

### joblib ‚Äî –∫—Ä–∞—â–µ –¥–ª—è sklearn

```python
import joblib

# –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è
joblib.dump(model, 'model.joblib')

# –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è
loaded_model = joblib.load('model.joblib')

y_pred = loaded_model.predict(X_test)
```

### –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è Pipeline

```python
# Pipeline —Ç–∞–∫–æ–∂ –∑–±–µ—Ä—ñ–≥–∞—î—Ç—å—Å—è —Ü—ñ–ª–∫–æ–º
joblib.dump(pipeline, 'pipeline.joblib')
loaded_pipeline = joblib.load('pipeline.joblib')
```

---

## 1Ô∏è‚É£5Ô∏è‚É£ –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è —Å–∏–Ω—Ç–µ—Ç–∏—á–Ω–∏—Ö –¥–∞–Ω–∏—Ö

### make_regression

```python
from sklearn.datasets import make_regression

X, y = make_regression(
    n_samples=1000,        # –ö—ñ–ª—å–∫—ñ—Å—Ç—å –∑—Ä–∞–∑–∫—ñ–≤
    n_features=10,         # –ö—ñ–ª—å–∫—ñ—Å—Ç—å –æ–∑–Ω–∞–∫
    n_informative=5,       # –Ü–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ñ –æ–∑–Ω–∞–∫–∏
    n_targets=1,           # –û–¥–Ω–∞ —Ü—ñ–ª—å–æ–≤–∞ –∑–º—ñ–Ω–Ω–∞
    noise=10.0,            # –®—É–º
    random_state=42
)

print(f"X shape: {X.shape}")
print(f"y shape: {y.shape}")
```

### make_classification

```python
from sklearn.datasets import make_classification

X, y = make_classification(
    n_samples=1000,
    n_features=20,
    n_informative=15,
    n_redundant=5,
    n_classes=2,
    weights=[0.7, 0.3],    # –ù–µ–∑–±–∞–ª–∞–Ω—Å–æ–≤–∞–Ω—ñ –∫–ª–∞—Å–∏
    random_state=42
)
```

### –†–µ–∞–ª—å–Ω—ñ –¥–∞—Ç–∞—Å–µ—Ç–∏

```python
from sklearn.datasets import load_diabetes, load_iris

# –†–µ–≥—Ä–µ—Å—ñ—è: –¥—ñ–∞–±–µ—Ç
diabetes = load_diabetes()
X, y = diabetes.data, diabetes.target
feature_names = diabetes.feature_names

# –ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è: —ñ—Ä–∏—Å
iris = load_iris()
X, y = iris.data, iris.target
class_names = iris.target_names
```

---

## 1Ô∏è‚É£6Ô∏è‚É£ –ü–æ–≤–Ω–∏–π –ø—Ä–∏–∫–ª–∞–¥: End-to-End Pipeline

```python
import numpy as np
import pandas as pd
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Ridge
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_squared_error, r2_score
import joblib

# 1. –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –¥–∞–Ω–∏—Ö
X, y = make_regression(
    n_samples=1000, 
    n_features=20, 
    n_informative=10,
    noise=10, 
    random_state=42
)

# 2. –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# 3. Pipeline
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('ridge', Ridge())
])

# 4. Grid Search
param_grid = {
    'ridge__alpha': [0.001, 0.01, 0.1, 1, 10, 100]
}

grid_search = GridSearchCV(
    pipeline, 
    param_grid, 
    cv=5, 
    scoring='r2',
    n_jobs=-1,
    verbose=1
)

# 5. –ù–∞–≤—á–∞–Ω–Ω—è
grid_search.fit(X_train, y_train)

# 6. –†–µ–∑—É–ª—å—Ç–∞—Ç–∏
print(f"Best alpha: {grid_search.best_params_['ridge__alpha']}")
print(f"Best CV R¬≤: {grid_search.best_score_:.3f}")

# 7. –û—Ü—ñ–Ω–∫–∞ –Ω–∞ test
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)

print(f"\n=== Test Set Results ===")
print(f"R¬≤: {r2_score(y_test, y_pred):.3f}")
print(f"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.3f}")

# 8. –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è
joblib.dump(best_model, 'best_ridge_model.joblib')
print("\nModel saved to 'best_ridge_model.joblib'")
```

---

## 1Ô∏è‚É£7Ô∏è‚É£ –ö–æ—Ä–∏—Å–Ω—ñ –ø–æ—Ä–∞–¥–∏

### 1. –ó–∞–≤–∂–¥–∏ –º–∞—Å—à—Ç–∞–±—É–π –¥–∞–Ω—ñ –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–æ–≤–∞–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π

```python
# –ü–û–ì–ê–ù–û (–±–µ–∑ –º–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è)
model = Ridge(alpha=1.0)
model.fit(X_train, y_train)

# –î–û–ë–†–ï (–∑ –º–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è–º)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
model.fit(X_train_scaled, y_train)
```

### 2. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π Pipeline –¥–ª—è —É–Ω–∏–∫–Ω–µ–Ω–Ω—è data leakage

```python
# –ü–û–ì–ê–ù–û (–º–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è –¥–æ —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è)
X_scaled = scaler.fit_transform(X)
X_train, X_test = train_test_split(X_scaled, ...)

# –î–û–ë–†–ï (Pipeline –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –ø—Ä–∞–≤–∏–ª—å–Ω–æ)
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('model', Ridge())
])
```

### 3. –§—ñ–∫—Å—É–π random_state –¥–ª—è –≤—ñ–¥—Ç–≤–æ—Ä—é–≤–∞–Ω–æ—Å—Ç—ñ

```python
# –í—Å—é–¥–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π –æ–¥–Ω–∞–∫–æ–≤–∏–π random_state
train_test_split(X, y, random_state=42)
cross_val_score(model, X, y, cv=KFold(random_state=42))
model = Ridge(random_state=42)
```

### 4. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π n_jobs=-1 –¥–ª—è –ø—Ä–∏—Å–∫–æ—Ä–µ–Ω–Ω—è

```python
# –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î –≤—Å—ñ –¥–æ—Å—Ç—É–ø–Ω—ñ —è–¥—Ä–∞
cross_val_score(model, X, y, cv=5, n_jobs=-1)
GridSearchCV(model, param_grid, n_jobs=-1)
```

### 5. –ü–µ—Ä–µ–≤—ñ—Ä—è–π –Ω–∞ overfitting

```python
train_score = model.score(X_train, y_train)
test_score = model.score(X_test, y_test)

if train_score - test_score > 0.1:
    print("‚ö†Ô∏è Overfitting detected!")
```

---

## 1Ô∏è‚É£8Ô∏è‚É£ –ß–µ–∫-–ª–∏—Å—Ç –¥–ª—è —Ä–æ–±–æ—Ç–∏ –∑ —Ä–µ–≥—Ä–µ—Å—ñ—î—é

```python
# ‚úÖ 1. –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –¥–∞–Ω—ñ
X, y = load_data()

# ‚úÖ 2. –†–æ–∑–¥—ñ–ª–∏—Ç–∏ –Ω–∞ train/test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# ‚úÖ 3. –°—Ç–≤–æ—Ä–∏—Ç–∏ Pipeline –∑ –º–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è–º
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('model', Ridge())
])

# ‚úÖ 4. Grid Search –¥–ª—è –≥—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤
param_grid = {'model__alpha': [0.1, 1, 10]}
grid_search = GridSearchCV(pipeline, param_grid, cv=5)
grid_search.fit(X_train, y_train)

# ‚úÖ 5. –û—Ü—ñ–Ω–∏—Ç–∏ –Ω–∞ test
best_model = grid_search.best_estimator_
test_score = best_model.score(X_test, y_test)

# ‚úÖ 6. –ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ –Ω–∞ overfitting
train_score = best_model.score(X_train, y_train)
print(f"Train R¬≤: {train_score:.3f}, Test R¬≤: {test_score:.3f}")

# ‚úÖ 7. –ó–±–µ—Ä–µ–≥—Ç–∏ –º–æ–¥–µ–ª—å
joblib.dump(best_model, 'model.joblib')
```

---

## –†–µ–∑—é–º–µ –æ—Å–Ω–æ–≤–Ω–∏—Ö –∫–ª–∞—Å—ñ–≤

|–ö–ª–∞—Å|–ü—Ä–∏–∑–Ω–∞—á–µ–Ω–Ω—è|–ö–ª—é—á–æ–≤—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏|
|---|---|---|
|`LinearRegression`|–ó–≤–∏—á–∞–π–Ω–∞ –ª—ñ–Ω—ñ–π–Ω–∞ —Ä–µ–≥—Ä–µ—Å—ñ—è|`fit_intercept`|
|`Ridge`|L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—è|`alpha`|
|`Lasso`|L1 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—è (–≤—ñ–¥–±—ñ—Ä –æ–∑–Ω–∞–∫)|`alpha`|
|`ElasticNet`|L1 + L2|`alpha`, `l1_ratio`|
|`LogisticRegression`|–ë—ñ–Ω–∞—Ä–Ω–∞ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è|`C`, `penalty`|
|`SGDRegressor`|SGD –¥–ª—è —Ä–µ–≥—Ä–µ—Å—ñ—ó|`loss`, `penalty`, `alpha`|
|`PolynomialFeatures`|–ü–æ–ª—ñ–Ω–æ–º—ñ–∞–ª—å–Ω—ñ –æ–∑–Ω–∞–∫–∏|`degree`|
|`StandardScaler`|–ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è|-|
|`Pipeline`|–ö–æ–Ω–≤–µ—î—Ä –æ–±—Ä–æ–±–∫–∏|-|
|`GridSearchCV`|–ü—ñ–¥–±—ñ—Ä –≥—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤|`param_grid`, `cv`|
|`cross_val_score`|Cross-validation|`cv`, `scoring`|

---

## –ö–æ—Ä–∏—Å–Ω—ñ –ø–æ—Å–∏–ª–∞–Ω–Ω—è

- [Scikit-learn Documentation](https://scikit-learn.org/stable/documentation.html)
- [Linear Models User Guide](https://scikit-learn.org/stable/modules/linear_model.html)
- [Model Selection Guide](https://scikit-learn.org/stable/model_selection.html)
- [Metrics Guide](https://scikit-learn.org/stable/modules/model_evaluation.html)
