
Регуляризація — метод для **контролю складності моделі**, який додає штраф за великі значення коефіцієнтів.  
Мета: **зменшити переобучення** та зробити модель більш стабільною.

## Основні види

### 1️⃣ Ridge (L2)

- Додає штраф на квадрат величини коефіцієнтів.
- Функція втрат:

$$
J(\beta) = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 + \lambda \sum_{j=1}^{p} \beta_j^2
$$

- $\lambda$ — гіперпараметр регуляризації.
- Ефект: зменшує величину коефіцієнтів, особливо при мультиколінеарності.

---

### 2️⃣ Lasso (L1)

- Додає штраф на **модуль коефіцієнтів**.
- Функція втрат:

$$
J(\beta) = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 + \lambda \sum_{j=1}^{p} |\beta_j|
$$

- Може **зануляти непотрібні коефіцієнти** → робить відбір ознак.
- Використовується, коли важливо отримати **просту модель**.

---

### 3️⃣ Elastic Net

- Комбінація L1 та L2.
- Функція втрат:

$$
J(\beta) = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 + \lambda \left( \alpha \sum_{j=1}^{p} |\beta_j| + (1-\alpha) \sum_{j=1}^{p} \beta_j^2 \right)
$$

- $\alpha \in [0,1]$ — баланс між L1 і L2.
- Застосовується, коли Lasso занадто агресивно зануляє коефіцієнти, а Ridge — занадто м’яко.

---

## Інтуїція

- Модель “не любить” великі коефіцієнти → **менше переобучення**.
- L2 = «тягнемо β до нуля плавно».  
- L1 = «зануляємо деякі β повністю».

---

## Вибір λ

- Малий λ → майже як звичайна лінійна регресія (МНК).  
- Великий λ → сильна регуляризація, коефіцієнти малі або нуль.  
- Підбирають через **cross-validation** або **GridSearch**.

---

## Зв’язок з ММП

- Регуляризація = априорне припущення про β:
  - Ridge → β ~ Normal(0, σ²)  
  - Lasso → β ~ Laplace(0, b)  

> Тобто регуляризація — це ММП з апріорним розподілом.

---

## Застосування

- Великі набори ознак
- Мультиколінеарність
- Просторові/тимчасові дані
- Машинне навчання (Ridge, Lasso, ElasticNet)

## Ключові висновки

- Регуляризація **зменшує переобучення**.  
- L1 = відбір ознак, L2 = стабільність коефіцієнтів.  
- Elastic Net = компроміс між L1 та L2.
- λ підбирається на основі **якості моделі на валідації**.
