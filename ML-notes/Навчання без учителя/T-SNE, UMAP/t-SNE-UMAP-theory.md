# t-SNE та UMAP — Візуалізація високовимірних даних

**t-SNE (t-Distributed Stochastic Neighbor Embedding)** та **UMAP (Uniform Manifold Approximation and Projection)** — сучасні методи зменшення розмірності для **візуалізації** високовимірних даних.

**Головна ідея:** Перетворити високовимірні дані (100D, 1000D) в 2D/3D так, щоб **зберегти структуру** — схожі точки залишаються близько, різні — далеко.

---

## Що це

t-SNE та UMAP — це алгоритми, які:

- **Візуалізують високовимірні дані** в 2D/3D ✓
- **Зберігають локальну структуру** (сусідів) ✓
- **Знаходять нелінійні патерни** (на відміну від PCA) ✓
- **Розділяють кластери** краще за PCA ✓
- **НЕ для ML** (тільки для візуалізації!) ⚠️

---

## Навіщо

### Проблема PCA

```
PCA — лінійна проекція:
  ✗ Не бачить нелінійні структури
  ✗ Погано розділяє кластери що перекриваються
  ✗ Зберігає глобальну, а не локальну структуру

t-SNE/UMAP:
  ✓ Нелінійні
  ✓ Зберігають сусідство точок
  ✓ Краще розділяють кластери
  ✓ Чудова візуалізація
```

### Візуальне порівняння

```
Високовимірні дані (100D):
[невидимі для людини]

PCA → 2D:
  ●●●●●●
 ●●●●●●●●    ← кластери перекриваються
  ●●●●●●

t-SNE → 2D:
 ●●●           ●●●
●●●●●         ●●●●●   ← кластери чітко розділені
 ●●●           ●●●

UMAP → 2D:
 ●●●           ●●●
●●●●●         ●●●●●   ← як t-SNE, але швидше
 ●●●           ●●●
```

### Приклади задач

- Візуалізація результатів кластеризації
- Exploratory Data Analysis (EDA)
- Перевірка якості ембедингів (word2vec, BERT)
- Біоінформатика (single-cell RNA-seq)
- Візуалізація нейронних мереж (layers, activations)
- Аналіз зображень (MNIST, ImageNet)

---

## t-SNE — детально

### Основна ідея

**Мета:** Побудувати 2D карту, де **відстані між точками** відповідають **схожості** в оригінальному просторі.

**Ключові принципи:**

1. В **високовимірному** просторі рахуємо **ймовірності** того, що точки є сусідами
2. В **низьковимірному** (2D) також рахуємо ймовірності
3. Підбираємо 2D координати так, щоб **ймовірності співпадали**

---

### Інтуїція (простими словами)

Уяви, що ти маєш 100D дані про міста (економіка, клімат, культура, ...).

**Крок 1:** В 100D просторі рахуємо "схожість" між містами
```
Київ ↔ Львів: дуже схожі (близькі сусіди)
Київ ↔ Токіо: не схожі (далекі)
```

**Крок 2:** Випадково кидаємо міста на 2D карту
```
Спочатку — хаос:
  Токіо   Київ
     Львів
```

**Крок 3:** Поступово "тягнемо" точки так, щоб:
- Схожі міста (Київ-Львів) → близько на карті
- Різні міста (Київ-Токіо) → далеко на карті

**Крок 4:** Після кількох тисяч ітерацій:
```
Кінцева карта:
  Київ Львів  (близько)
  
  
      Токіо   (далеко)
```

---

### Математика (без страшних формул)

**Крок 1: Ймовірності в високовимірному просторі**

Для кожної пари точок $(x_i, x_j)$ рахуємо:

$$p_{j|i} = \frac{\exp(-||x_i - x_j||^2 / 2\sigma_i^2)}{\sum_{k \neq i} \exp(-||x_i - x_k||^2 / 2\sigma_i^2)}$$

**Інтуїція:**
- Якщо $x_i$ і $x_j$ близько → $p_{j|i}$ велике (висока ймовірність сусідства)
- Якщо далеко → $p_{j|i}$ мале

**Симетризація:**
$$p_{ij} = \frac{p_{j|i} + p_{i|j}}{2n}$$

---

**Крок 2: Ймовірності в низьковимірному просторі (2D)**

Для 2D точок $(y_i, y_j)$ використовуємо **t-розподіл** (Student's t):

$$q_{ij} = \frac{(1 + ||y_i - y_j||^2)^{-1}}{\sum_{k \neq l}(1 + ||y_k - y_l||^2)^{-1}}$$

**Чому t-розподіл?**
- Має "важкі хвости" (heavy tails)
- Дозволяє розташувати несхожі точки далі в 2D
- Вирішує "crowding problem" PCA

---

**Крок 3: Мінімізація розбіжності (KL divergence)**

Підбираємо $y_i$ (2D координати) так, щоб мінімізувати:

$$KL(P||Q) = \sum_{i \neq j} p_{ij} \log \frac{p_{ij}}{q_{ij}}$$

**Інтуїція:**
- Якщо $p_{ij}$ велике (схожі в 100D), а $q_{ij}$ мале (далеко в 2D) → **великий штраф**
- Алгоритм "тягне" їх ближче в 2D

**Оптимізація:** Gradient descent (кілька тисяч ітерацій)

---

### Параметри t-SNE

**1. perplexity** (найважливіший!)

```
Інтуїція: "Скільки сусідів врахувати?"

perplexity = 5:
  Кожна точка дивиться на ~5 найближчих сусідів
  ✓ Для малих датасетів (< 1000 точок)
  
perplexity = 30 (за замовчуванням):
  ~30 сусідів
  ✓ Універсальний вибір
  
perplexity = 50-100:
  Багато сусідів
  ✓ Для великих датасетів (> 10,000)
```

**Візуальний ефект:**

```
perplexity = 5:
●●  ●●  ●●    ← багато малих кластерів

perplexity = 30:
 ●●●   ●●●    ← середні кластери
 ●●●   ●●●

perplexity = 100:
 ●●●●●●●●●    ← великі, об'єднані кластери
```

**Правило:** `perplexity` має бути між 5 і 50, зазвичай 30.

---

**2. learning_rate**

```
Швидкість градієнтного спуску

learning_rate = 10-100: повільно, але стабільно
learning_rate = 200 (default): швидко
learning_rate = 1000: дуже швидко, може не зійтись
```

---

**3. n_iter**

```
Кількість ітерацій оптимізації

n_iter = 250 (мінімум): швидко, але низька якість
n_iter = 1000 (default): нормально
n_iter = 5000: висока якість, повільно
```

---

**4. early_exaggeration**

```
На перших ітераціях "роздуває" кластери

Допомагає уникнути локальних мінімумів
```

---

### Переваги t-SNE

✅ **Чудова візуалізація** — найкраща для складних даних  
✅ **Нелінійна** — знаходить складні структури  
✅ **Добре розділяє кластери**  
✅ **Зберігає локальну структуру** (сусідів)  

---

### Недоліки t-SNE

❌ **Дуже повільний** — O(n²) складність  
❌ **Стохастичний** — різні результати при різних запусках  
❌ **НЕ зберігає глобальну структуру** — відстані між кластерами без значення  
❌ **НЕ для ML** — не можна передбачити для нових точок  
❌ **Чутливий до параметрів** — особливо perplexity  
❌ **Crowding problem** — в 2D не вистачає місця для всіх точок  

---

### Важливі обмеження t-SNE

**1. Відстані між кластерами безглузді**

```
t-SNE результат:

Кластер A     Кластер B
  ●●●            ●●●
   ●              ●
                        Кластер C
                          ●●●
                           ●

НЕПРАВИЛЬНА інтерпретація:
"A і B ближчі, ніж C" ✗

ПРАВИЛЬНА:
"A, B, C — окремі групи" ✓
"Відстань між ними не має значення"
```

**2. Розмір кластера безглуздий**

```
Кластер 1 (великий)     Кластер 2 (малий)
  ●●●●●                    ●●
  ●●●●●                    ●●
  ●●●●●

НЕ означає, що Кластер 1 важливіший або більший!
Розмір залежить від perplexity і випадковості.
```

**3. Не можна порівнювати різні t-SNE результати**

```
Запуск 1:          Запуск 2:
  ●●●   ●●●          ●●●
   ●     ●          ●●●●●
                     ●●●

Обидва правильні!
Але орієнтація/відстані різні
```

---

## UMAP — детально

### Основна ідея

**UMAP = швидша, краща альтернатива t-SNE**

**Ключові відмінності від t-SNE:**
- Базується на теорії **многовидів** (manifold theory)
- **Швидший** (10-100x)
- **Краще зберігає глобальну структуру**
- **Можна використати для нових точок** (transform)
- **Стабільніший** до параметрів

---

### Інтуїція (простими словами)

Уяви, що високовимірні дані лежать на якійсь **кривій поверхні** (manifold) в 100D просторі.

**UMAP:**
1. Будує **граф** сусідства в високовимірному просторі
2. "Розгортає" цей граф в 2D
3. Зберігає топологічну структуру (хто з ким сусідить)

**Аналогія:**

```
Маємо м'яч (3D manifold):
    ●●●
   ●●●●●
    ●●●

UMAP "розгортає" його в 2D:
  ●●●●●●●
  ●●●●●●●
  ●●●●●●●

Сусіди залишаються сусідами!
```

---

### Математика (спрощено)

**Крок 1: Побудувати граф в високовимірному просторі**

Для кожної точки $x_i$:
- Знайти $k$ найближчих сусідів
- З'єднати ребрами з вагами (схожість)

**Крок 2: Оптимізувати граф в низьковимірному просторі**

Мінімізувати функцію (подібна до t-SNE, але інша):

$$\sum_{ij} w_{ij} \log \frac{w_{ij}}{q_{ij}} + (1-w_{ij})\log \frac{1-w_{ij}}{1-q_{ij}}$$

де:
- $w_{ij}$ — вага ребра в високовимірному графі
- $q_{ij}$ — вага в низьковимірному графі

**Інтуїція:**
- Якщо $w_{ij}$ велике (сусіди) → зробити $q_{ij}$ великим (близько в 2D)
- Якщо $w_{ij}$ мале (не сусіди) → зробити $q_{ij}$ малим (далеко в 2D)

---

### Параметри UMAP

**1. n_neighbors** (аналог perplexity в t-SNE)

```
Інтуїція: "Розмір околиці для аналізу локальної структури"

n_neighbors = 5-10:
  Фокус на дуже локальній структурі
  ●●  ●●  ●●    ← багато малих кластерів
  
n_neighbors = 15 (default):
  Баланс локальної та глобальної структури
  ✓ Рекомендовано для початку
  
n_neighbors = 50-100:
  Більше фокусу на глобальній структурі
   ●●●●●●●●●   ← великі кластери
```

**Правило:** 
- Менше даних → менший n_neighbors (5-10)
- Більше даних → більший n_neighbors (30-50)

---

**2. min_dist**

```
Інтуїція: "Наскільки щільно пакувати точки в 2D?"

min_dist = 0.0:
  Максимально щільно
  ●●●●●●●    ← точки майже зливаються
  
min_dist = 0.1 (default):
  Помірна щільність
  ● ● ● ● ●  ✓
  
min_dist = 0.5-1.0:
  Розріджено
  ●   ●   ●  ← великі проміжки
```

**Використання:**
- `min_dist = 0.0` → для чіткої візуалізації кластерів
- `min_dist = 0.1-0.3` → універсально
- `min_dist = 0.5+` → для збереження глобальної структури

---

**3. metric**

```
Метрика відстані в високовимірному просторі

metric = 'euclidean' (default)  ✓ для більшості
metric = 'cosine'               ✓ для text embeddings
metric = 'manhattan'
metric = 'correlation'
```

---

**4. n_components**

```
Розмірність виходу

n_components = 2 (default)  ✓ візуалізація
n_components = 3            ✓ 3D візуалізація
n_components = 10-50        ✓ для ML (preprocessing)
```

---

### Переваги UMAP

✅ **Швидкий** — 10-100x швидше за t-SNE  
✅ **Краще зберігає глобальну структуру**  
✅ **Можна використати для нових точок** (transform)  
✅ **Стабільніший** до параметрів  
✅ **Працює з великими даними** (мільйони точок)  
✅ **Можна використати для ML** (як preprocessing)  

---

### Недоліки UMAP

❌ **Складніша математика** (важче зрозуміти)  
❌ **Все ще стохастичний** (різні запуски → різні результати)  
❌ **Потребує налаштування** (n_neighbors, min_dist)  
❌ **Нова бібліотека** (менше ресурсів/документації ніж t-SNE)  

---

## Порівняння t-SNE vs UMAP vs PCA

### Швидкість

```
PCA:
  ✓✓✓ Дуже швидко (секунди)
  O(min(n×d², d³))

UMAP:
  ✓✓ Швидко (хвилини)
  O(n^1.14)

t-SNE:
  ✗ Повільно (години для великих даних)
  O(n²) або O(n log n) з Barnes-Hut
```

**Benchmark (10,000 точок, 100 ознак):**

```
PCA:     2 секунди
UMAP:    30 секунд
t-SNE:   5 хвилин (Barnes-Hut)
```

---

### Якість візуалізації

```
Складні нелінійні структури:

PCA:     ✗✗ Погано
UMAP:    ✓✓✓ Відмінно
t-SNE:   ✓✓✓ Відмінно

Розділення кластерів:

PCA:     ✗ Перекриття
UMAP:    ✓✓ Добре
t-SNE:   ✓✓✓ Найкраще
```

---

### Глобальна vs локальна структура

```
Локальна структура (сусідство):
  PCA:   ✗
  UMAP:  ✓✓✓
  t-SNE: ✓✓✓

Глобальна структура (відстані між кластерами):
  PCA:   ✓✓✓
  UMAP:  ✓✓
  t-SNE: ✗
```

---

### Стабільність

```
Детермінованість:
  PCA:   ✓✓✓ Завжди однаковий результат
  UMAP:  ✗ Різні запуски → різні результати (але схожі)
  t-SNE: ✗ Дуже різні результати

Чутливість до параметрів:
  PCA:   ✓✓✓ Немає критичних параметрів
  UMAP:  ✓✓ Стабільний (n_neighbors не дуже критичний)
  t-SNE: ✗ Дуже чутливий до perplexity
```

---

### Використання для ML

```
Preprocessing для ML:
  PCA:   ✓✓✓ Так, дуже часто
  UMAP:  ✓ Можна, але рідше
  t-SNE: ✗ НІ! Тільки візуалізація

Передбачення для нових точок:
  PCA:   ✓✓✓ transform()
  UMAP:  ✓ transform() (з обмеженнями)
  t-SNE: ✗ Немає методу
```

---

### Порівняльна таблиця

| Аспект | PCA | t-SNE | UMAP |
|--------|-----|-------|------|
| **Швидкість** | ✓✓✓ Дуже швидко | ✗ Повільно | ✓✓ Швидко |
| **Візуалізація кластерів** | ✗ Погано | ✓✓✓ Відмінно | ✓✓✓ Відмінно |
| **Локальна структура** | ✗ Ні | ✓✓✓ Так | ✓✓✓ Так |
| **Глобальна структура** | ✓✓✓ Так | ✗ Ні | ✓✓ Частково |
| **Для ML** | ✓✓✓ Так | ✗ Ні | ✓ Можна |
| **Великі дані (>100k)** | ✓✓✓ Так | ✗ Ні | ✓✓ Так |
| **Детермінованість** | ✓✓✓ Так | ✗ Ні | ✗ Ні |
| **Нові точки** | ✓✓✓ transform() | ✗ Ні | ✓ transform() |
| **Складність розуміння** | ✓✓✓ Проста | ✓✓ Середня | ✓ Складна |

---

## Коли що використовувати

### PCA використовуй для:

✅ Швидкий EDA (exploratory data analysis)  
✅ Preprocessing для ML  
✅ Розуміння глобальної структури даних  
✅ Визначення важливих компонент  
✅ Коли потрібна швидкість (великі дані)  

**Приклад:** Швидко зрозуміти, чи є в даних якась структура

---

### t-SNE використовуй для:

✅ **Найкраща візуалізація** складних даних  
✅ Exploratory Data Analysis  
✅ Перевірка якості кластеризації  
✅ Візуалізація ембедингів (word2vec, BERT)  
✅ Коли важлива краса картинки (презентації, статті)  

**Приклад:** Показати бізнесу, що ML модель знайшла цікаві групи клієнтів

---

### UMAP використовуй для:

✅ Візуалізація великих даних (>10,000 точок)  
✅ Коли важлива і локальна, і глобальна структура  
✅ Preprocessing для ML (замість PCA)  
✅ Коли потрібна швидкість (швидше за t-SNE)  
✅ Інтерактивна візуалізація (можна додавати нові точки)  

**Приклад:** Візуалізація мільйона клітин (single-cell biology)

---

## Практичний workflow

### Крок 1: Почни з PCA

```python
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Швидко подивитись на дані
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, alpha=0.5)
plt.title(f'PCA (variance: {pca.explained_variance_ratio_.sum():.1%})')
plt.show()
```

**Якщо PCA показує добре розділення → зупинись, достатньо!**

---

### Крок 2: Якщо PCA погано → спробуй UMAP

```python
import umap

# UMAP для кращої візуалізації
reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, random_state=42)
X_umap = reducer.fit_transform(X_scaled)

plt.scatter(X_umap[:, 0], X_umap[:, 1], c=labels, alpha=0.5)
plt.title('UMAP')
plt.show()
```

**Якщо UMAP добре → використовуй його!**

---

### Крок 3: Якщо хочеш найкращу візуалізацію → t-SNE

```python
from sklearn.manifold import TSNE

# t-SNE для ідеальної візуалізації (повільно!)
tsne = TSNE(n_components=2, perplexity=30, random_state=42)
X_tsne = tsne.fit_transform(X_scaled)

plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=labels, alpha=0.5)
plt.title('t-SNE')
plt.show()
```

---

### Крок 4: Порівняй всі три

```python
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# PCA
axes[0].scatter(X_pca[:, 0], X_pca[:, 1], c=labels, alpha=0.5)
axes[0].set_title('PCA (швидко, глобально)')

# UMAP
axes[1].scatter(X_umap[:, 0], X_umap[:, 1], c=labels, alpha=0.5)
axes[1].set_title('UMAP (швидко, локально+глобально)')

# t-SNE
axes[2].scatter(X_tsne[:, 0], X_tsne[:, 1], c=labels, alpha=0.5)
axes[2].set_title('t-SNE (повільно, найкраща візуалізація)')

plt.tight_layout()
plt.show()
```

---

## Поширені помилки

### ❌ Помилка 1: Використання t-SNE/UMAP для ML

```python
# ПОГАНО
X_tsne = tsne.fit_transform(X_train)
model.fit(X_tsne, y_train)  # ✗

# ДОБРЕ
X_pca = pca.fit_transform(X_train)
model.fit(X_pca, y_train)  # ✓
```

**Чому:** t-SNE/UMAP оптимізовані для візуалізації, не для ML!

---

### ❌ Помилка 2: Інтерпретація відстаней між кластерами в t-SNE

```python
# t-SNE результат
Кластер A     Кластер B
  ●●●            ●●●

НЕПРАВИЛЬНО: "A і B схожі, бо близько" ✗
ПРАВИЛЬНО: "A і B — окремі групи" ✓
```

---

### ❌ Помилка 3: Не масштабувати дані

```python
# ПОГАНО
tsne = TSNE()
X_tsne = tsne.fit_transform(X)  # ✗

# ДОБРЕ
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
tsne = TSNE()
X_tsne = tsne.fit_transform(X_scaled)  # ✓
```

---

### ❌ Помилка 4: Один запуск t-SNE/UMAP

```python
# ПОГАНО
tsne = TSNE(random_state=42)
X_tsne = tsne.fit_transform(X_scaled)
# Показуємо результат ✗

# ДОБРЕ
# Запустити кілька разів з різними random_state
for seed in [42, 123, 456]:
    tsne = TSNE(random_state=seed)
    X_tsne = tsne.fit_transform(X_scaled)
    # Порівняти результати

# Якщо схожі → OK
# Якщо дуже різні → погана якість даних або параметрів
```

---

### ❌ Помилка 5: Неправильний perplexity/n_neighbors

```python
# ПОГАНО (perplexity > n_samples)
tsne = TSNE(perplexity=100)
X_tsne = tsne.fit_transform(X[:50])  # тільки 50 точок ✗

# ДОБРЕ
n_samples = len(X)
perplexity = min(30, n_samples // 3)  # евристика
tsne = TSNE(perplexity=perplexity)
```

---

## Поради та best practices

### 1. Завжди масштабуй дані

```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Потім застосуй t-SNE/UMAP на X_scaled
```

---

### 2. Експериментуй з параметрами

```python
# t-SNE
for perplexity in [5, 10, 30, 50]:
    tsne = TSNE(perplexity=perplexity, random_state=42)
    X_tsne = tsne.fit_transform(X_scaled)
    # Візуалізуй

# UMAP
for n_neighbors in [5, 15, 30, 50]:
    reducer = umap.UMAP(n_neighbors=n_neighbors, random_state=42)
    X_umap = reducer.fit_transform(X_scaled)
    # Візуалізуй
```

---

### 3. Використовуй PCA перед t-SNE/UMAP для великих даних

```python
# Якщо дуже багато ознак (>100)
pca = PCA(n_components=50)  # спочатку зменшити
X_pca = pca.fit_transform(X_scaled)

# Потім t-SNE/UMAP
tsne = TSNE()
X_tsne = tsne.fit_transform(X_pca)

# Швидше і краще!
```

---

### 4. Фіксуй random_state для відтворюваності

```python
tsne = TSNE(random_state=42)
reducer = umap.UMAP(random_state=42)
```

---

### 5. Для дуже великих даних: sampling

```python
# Якщо >100,000 точок
if len(X) > 100000:
    # Візуалізувати вибірку
    idx = np.random.choice(len(X), size=10000, replace=False)
    X_sample = X_scaled[idx]
    
    tsne = TSNE()
    X_tsne = tsne.fit_transform(X_sample)
```

---

## Спеціальні можливості UMAP

### 1. Supervised UMAP (з мітками)

```python
# Можна використати мітки для кращої візуалізації
reducer = umap.UMAP(n_neighbors=15, random_state=42)

# З мітками (semi-supervised)
X_umap = reducer.fit_transform(X_scaled, y=labels)

# Кластери будуть краще розділені!
```

---

### 2. Inverse transform (відновлення)

```python
# UMAP може "відновити" оригінальні дані (приблизно)
reducer = umap.UMAP(n_components=2)

X_umap = reducer.fit_transform(X_scaled)
X_restored = reducer.inverse_transform(X_umap)

# Перевірка якості
from sklearn.metrics import mean_squared_error
mse = mean_squared_error(X_scaled, X_restored)
print(f"Reconstruction MSE: {mse:.4f}")
```

---

### 3. Metric learning

```python
# Різні метрики для різних типів даних
reducer = umap.UMAP(metric='cosine')  # для text embeddings
reducer = umap.UMAP(metric='manhattan')  # для categorical
reducer = umap.UMAP(metric='euclidean')  # default
```

---

## Ключові висновки

> t-SNE та UMAP — потужні інструменти для візуалізації високовимірних даних, які зберігають локальну структуру та чудово розділяють кластери.

**t-SNE:**
- ✅ Найкраща візуалізація
- ✅ Зберігає локальну структуру
- ❌ Повільний
- ❌ Не зберігає глобальну структуру
- ❌ НЕ для ML

**UMAP:**
- ✅ Швидкий (10-100x швидше t-SNE)
- ✅ Краще зберігає глобальну структуру
- ✅ Можна для ML (але PCA краще)
- ✅ Можна для нових точок (transform)
- ❌ Складніша математика

**PCA:**
- ✅ Найшвидший
- ✅ Детермінований
- ✅ Ідеально для ML
- ❌ Тільки лінійний
- ❌ Погана візуалізація складних даних

**Золоте правило:**
1. **PCA** — для ML та швидкого EDA
2. **UMAP** — для візуалізації великих даних
3. **t-SNE** — для найкрасивішої візуалізації (презентації, статті)

**Порада:** Почни з PCA, якщо не задовольняє → UMAP → t-SNE

---

## Наступні кроки

1. **Практика sklearn** → наступний документ
2. **t-SNE gallery** → подивись приклади візуалізацій
3. **UMAP documentation** → поглиблене вивчення
4. **Plotly** — інтерактивна візуалізація t-SNE/UMAP
