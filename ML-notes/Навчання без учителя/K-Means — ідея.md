# K-Means — ідея

**Задача:** розділити дані на K кластерів (груп) так, щоб точки всередині кластера були схожі, а між кластерами — відрізнялися.

---

## Основна ідея

Алгоритм **K-Means** шукає K центроїдів (центрів кластерів) і призначає кожну точку до найближчого центроїда.

### Приклад

Маємо точки клієнтів: вік, дохід.  
Хочемо розділити на 3 групи: молоді з низьким доходом, середній вік з середнім доходом, літні з високим доходом.

---

## Алгоритм

### Словами (як це працює)

Уяви, що маєш купу точок на площині і хочеш розділити їх на 3 групи. Як це зробити?

**Крок 1: Вибираємо початкові центри груп**

- Випадково кидаємо 3 "прапорці" на карту — це будуть початкові центри кластерів
- Наприклад, якщо маємо 100 точок і K=3, то випадково вибираємо 3 точки як центри

**Крок 2: Призначаємо кожну точку до найближчого центру**

- Для кожної точки дивимося: до якого прапорця вона найближча?
- Якщо точка ближче до 1-го прапорця, ніж до 2-го і 3-го — вона йде в 1-й кластер
- Робимо це для ВСІХ точок
- Тепер кожна точка "знає", до якої групи вона належить

**Крок 3: Переміщуємо центри в середину своїх груп**

- Для кожного кластера беремо всі його точки і знаходимо їх середнє положення
- Переставляємо прапорець в це середнє положення
- Це як знайти "центр ваги" групи точок

**Крок 4: Повторюємо кроки 2-3**

- Знову дивимося, чи не стали деякі точки ближче до інших прапорців після їх переміщення
- Якщо так — перепризначаємо їх
- Знову рахуємо нові центри
- Повторюємо, поки прапорці перестануть рухатися (збіжність)

**Результат:** Точки розділені на K груп, де кожна група зібрана навколо свого центру.

---

### Математично (формули)

**Вхід:**

- Дані: $X = {x_1, x_2, ..., x_n}$, де $x_i \in \mathbb{R}^d$
- Кількість кластерів: $K$

**Крок 1:** Ініціалізація

- Випадково вибираємо K точок як початкові центроїди: $\mu_1, \mu_2, ..., \mu_K$

**Крок 2:** Призначення (Assignment)

- Для кожної точки $x_i$ знаходимо найближчий центроїд: $$c_i = \arg\min_{k} |x_i - \mu_k|^2$$

де $c_i$ — номер кластера для точки $x_i$

**Пояснення формули:** шукаємо таке $k$ (номер кластера), при якому відстань $|x_i - \mu_k|^2$ мінімальна

**Крок 3:** Оновлення (Update)

- Перераховуємо центроїди як середнє точок у кластері: $$\mu_k = \frac{1}{|C_k|} \sum_{x_i \in C_k} x_i$$

де $C_k$ — множина точок у кластері $k$

**Пояснення формули:** беремо всі точки кластера $k$, складаємо їх іділимо на кількість точок — це і є середнє (центр)

**Крок 4:** Повторення

- Повторюємо кроки 2-3 до збіжності (центроїди перестають рухатися)

---

## Функція втрат (Inertia)

### Що це означає?

**Інертність (Inertia)** — це міра "компактності" кластерів. Вона показує, наскільки близько точки знаходяться до своїх центрів.

**Простими словами:**

- Чим ближче точки до свого центра → тим менше Inertia → краще кластеризація
- Чим далі точки від центра → тим більше Inertia → гірше кластеризація

**Аналогія:** Уяви, що центр кластера — це школа, а точки — це учні. Inertia — це сумарна відстань, яку всі учні мають пройти до школи. Добра кластеризація = школа розташована так, що всі учні живуть близько.

---

### Математично

K-Means мінімізує **within-cluster sum of squares (WCSS)**:

$$J = \sum_{k=1}^{K} \sum_{x_i \in C_k} |x_i - \mu_k|^2$$

**Розшифровка:**

- $|x_i - \mu_k|^2$ — квадрат відстані від точки $x_i$ до центра її кластера $\mu_k$
- $\sum_{x_i \in C_k}$ — сумуємо по всіх точках кластера $k$
- $\sum_{k=1}^{K}$ — сумуємо по всіх кластерах

**Ціль:** мінімізувати сумарну відстань від точок до їх центроїдів.

**Приклад обчислення:**

```
Кластер 1: {x₁=[1,2], x₂=[2,1]}, центр μ₁=[1.5, 1.5]
  d²(x₁, μ₁) = (1-1.5)² + (2-1.5)² = 0.25 + 0.25 = 0.5
  d²(x₂, μ₁) = (2-1.5)² + (1-1.5)² = 0.25 + 0.25 = 0.5
  Сума для кластера 1 = 0.5 + 0.5 = 1.0

Кластер 2: {x₃=[8,8], x₄=[9,10]}, центр μ₂=[8.5, 9]
  d²(x₃, μ₂) = (8-8.5)² + (8-9)² = 0.25 + 1 = 1.25
  d²(x₄, μ₂) = (9-8.5)² + (10-9)² = 0.25 + 1 = 1.25
  Сума для кластера 2 = 1.25 + 1.25 = 2.5

Загальна Inertia: J = 1.0 + 2.5 = 3.5
```

---

## Приклад (вручну)

**Дані:**

```
x₁ = [1, 2]   (точка з координатами x=1, y=2)
x₂ = [2, 1]   (x=2, y=1)
x₃ = [8, 8]   (x=8, y=8)
x₄ = [9, 10]  (x=9, y=10)
```

**K = 2** (хочемо 2 кластери)

### Словами що відбувається

Уяви, що ці 4 точки розкидані на площині:

- Точки x₁ і x₂ знаходяться ліворуч внизу (близько координат 1-2)
- Точки x₃ і x₄ знаходяться праворуч вгорі (близько координат 8-10)

Очевидно, що вони розпадаються на 2 групи: {x₁, x₂} і {x₃, x₄}

Подивимося, як K-Means це знаходить автоматично:

---

### Ітерація 1

**Ініціалізація:**

```
μ₁ = [1, 2]  (випадково вибрали x₁ як перший центр)
μ₂ = [8, 8]  (випадково вибрали x₃ як другий центр)
```

**Призначення (Assignment):**

Тепер для кожної точки рахуємо відстань до обох центрів і вибираємо ближчий:

```
Точка x₁ = [1, 2]:
  відстань до μ₁ = √((1-1)² + (2-2)²) = 0      → ближче до μ₁
  відстань до μ₂ = √((1-8)² + (2-8)²) = √85    → далі
  ⟹ x₁ йде в кластер 1

Точка x₂ = [2, 1]:
  відстань до μ₁ = √((2-1)² + (1-2)²) = √2     → ближче до μ₁
  відстань до μ₂ = √((2-8)² + (1-8)²) = √85    → далі
  ⟹ x₂ йде в кластер 1

Точка x₃ = [8, 8]:
  відстань до μ₁ = √((8-1)² + (8-2)²) = √85    → далі
  відстань до μ₂ = √((8-8)² + (8-8)²) = 0      → ближче до μ₂
  ⟹ x₃ йде в кластер 2

Точка x₄ = [9, 10]:
  відстань до μ₁ = √((9-1)² + (10-2)²) = √128  → далі
  відстань до μ₂ = √((9-8)² + (10-8)²) = √5    → ближче до μ₂
  ⟹ x₄ йде в кластер 2
```

**Результат призначення:**

```
Кластер 1: {x₁, x₂} — лівий нижній
Кластер 2: {x₃, x₄} — правий верхній
```

**Оновлення (Update):**

Тепер рахуємо нові центри як середнє точок у кожному кластері:

```
Новий μ₁ = (x₁ + x₂) / 2 
        = ([1,2] + [2,1]) / 2 
        = [3,3] / 2 
        = [1.5, 1.5]

Новий μ₂ = (x₃ + x₄) / 2 
        = ([8,8] + [9,10]) / 2 
        = [17,18] / 2 
        = [8.5, 9]
```

Центри злегка зрушилися до "справжнього" центру своїх груп!

---

### Ітерація 2

**Призначення (перевіряємо, чи змінилися кластери):**

Знову рахуємо відстані з новими центрами:

```
Точка x₁ = [1, 2]:
  відстань до μ₁=[1.5, 1.5] = √((1-1.5)² + (2-1.5)²) = √0.5  → ближче
  відстань до μ₂=[8.5, 9]   = √((1-8.5)² + (2-9)²) = √105    → далі
  ⟹ залишається в кластері 1 ✓

Точка x₂ = [2, 1]:
  відстань до μ₁ = √((2-1.5)² + (1-1.5)²) = √0.5              → ближче
  відстань до μ₂ = √((2-8.5)² + (1-9)²) = √106                → далі
  ⟹ залишається в кластері 1 ✓

Точка x₃ = [8, 8]:
  відстань до μ₁ = √((8-1.5)² + (8-1.5)²) = √84.5             → далі
  відстань до μ₂ = √((8-8.5)² + (8-9)²) = √1.25               → ближче
  ⟹ залишається в кластері 2 ✓

Точка x₄ = [9, 10]:
  відстань до μ₁ = √((9-1.5)² + (10-1.5)²) = √128.5           → далі
  відстань до μ₂ = √((9-8.5)² + (10-9)²) = √1.25              → ближче
  ⟹ залишається в кластері 2 ✓
```

**Висновок:** Жодна точка не змінила кластер → **алгоритм зійшовся!**

Кінцевий результат:

- Кластер 1: {x₁, x₂} з центром μ₁ = [1.5, 1.5]
- Кластер 2: {x₃, x₄} з центром μ₂ = [8.5, 9]

---

### Коротка версія (для швидкого розуміння)

**Ітерація 1**

**Ініціалізація:**

```
μ₁ = [1, 2]  (випадково вибрали x₁)
μ₂ = [8, 8]  (випадково вибрали x₃)
```

**Призначення:**

```
d(x₁, μ₁) = 0      → c₁ = 1
d(x₂, μ₁) = √2     → c₂ = 1
d(x₃, μ₂) = 0      → c₃ = 2
d(x₄, μ₂) = √5     → c₄ = 2
```

Кластери: `C₁ = {x₁, x₂}`, `C₂ = {x₃, x₄}`

**Оновлення:**

```
μ₁ = (x₁ + x₂)/2 = [1.5, 1.5]
μ₂ = (x₃ + x₄)/2 = [8.5, 9]
```

### Ітерація 2

**Призначення:** (перевіряємо, чи змінилися кластери)

```
d(x₁, μ₁) = √0.5   → c₁ = 1
d(x₂, μ₁) = √0.5   → c₂ = 1
d(x₃, μ₂) = √1.25  → c₃ = 2
d(x₄, μ₂) = √1.25  → c₄ = 2
```

Кластери не змінилися → **збіжність!**

---

## Вибір K (Elbow Method)

**Проблема:** як вибрати правильну кількість кластерів?

**Метод ліктя (Elbow Method):**

1. Запускаємо K-Means для K = 1, 2, 3, ..., 10
2. Обчислюємо WCSS (Inertia) для кожного K
3. Будуємо графік WCSS(K)
4. Шукаємо "лікоть" — точку, де зменшення WCSS сповільнюється

```
WCSS
 │
 │ ●
 │   ●
 │     ●
 │       ●──●──●──●
 │                    ← "лікоть" при K=3
 └─────────────────── K
    1  2  3  4  5  6
```

---

## Переваги та недоліки

### ✅ Переваги

- Простий та швидкий
- Добре масштабується (O(n·K·d·t), де t — ітерації)
- Працює на великих даних

### ❌ Недоліки

- Потрібно задавати K заздалегідь
- Чутливий до ініціалізації (K-Means++)
- Припускає сферичні кластери
- Чутливий до викидів
- Не працює з кластерами складної форми

---

## Коли використовувати

**Добре працює:**

- Кластери сферичної форми
- Приблизно однаковий розмір кластерів
- Відомо приблизну кількість груп

**Погано працює:**

- Кластери складної форми (еліпси, "банани")
- Різна щільність кластерів
- Багато викидів

**Альтернативи:**

- DBSCAN — для кластерів довільної форми
- Hierarchical — коли K невідоме
- GMM — для еліптичних кластерів

---

## Зв'язок з іншими методами

**K-Means** — це окремий випадок:

- **Expectation-Maximization (EM)** для Gaussian Mixture Models
- **Lloyd's algorithm** для векторного квантування
- **Hard assignment** замість soft (GMM)

---

## Ключові терміни

- **Центроїд (Centroid)** — центр кластера, μₖ
- **Inertia (WCSS)** — сума квадратів відстаней до центроїдів
- **Збіжність (Convergence)** — коли центроїди перестають рухатися
- **Elbow Method** — метод вибору K
- **Silhouette Score** — метрика якості кластеризації

---

## Псевдокод

```python
def kmeans(X, K, max_iter=100):
    # 1. Ініціалізація
    centroids = random_sample(X, K)
    
    for iteration in range(max_iter):
        # 2. Призначення
        clusters = assign_to_nearest(X, centroids)
        
        # 3. Оновлення
        new_centroids = compute_centroids(X, clusters)
        
        # 4. Перевірка збіжності
        if centroids == new_centroids:
            break
        
        centroids = new_centroids
    
    return clusters, centroids
```

---

## Математичні деталі

### Відстань

За замовчуванням використовується **евклідова відстань**: $$d(x, \mu) = \sqrt{\sum_{j=1}^{d} (x_j - \mu_j)^2}$$

Для оптимізації мінімізуємо квадрат відстані (без √): $$d^2(x, \mu) = \sum_{j=1}^{d} (x_j - \mu_j)^2$$

### Оновлення центроїдів

Оптимальний центроїд — це **середнє арифметичне** точок кластера: $$\mu_k^* = \arg\min_{\mu} \sum_{x_i \in C_k} |x_i - \mu|^2$$

Розв'язок (похідна = 0): $$\mu_k^* = \frac{1}{|C_k|} \sum_{x_i \in C_k} x_i$$

---

## Складність

- **Час:** $O(n \cdot K \cdot d \cdot t)$
    
    - $n$ — кількість точок
    - $K$ — кількість кластерів
    - $d$ — кількість ознак (розмірність)
    - $t$ — кількість ітерацій (зазвичай < 100)
- **Пам'ять:** $O(n \cdot d + K \cdot d)$
    

---

## Наступні кроки

1. **[[K-Means++ — ініціалізація]] — краща стартова точка
2. **Mini-Batch K-Means** — швидша версія для великих даних
3. **Silhouette Score** — оцінка якості кластеризації
4. **DBSCAN** — кластеризація без фіксованого K
5. **Gaussian Mixture Models** — м'яка (soft) версія K-Means