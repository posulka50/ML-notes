	
–ü–æ–≤–Ω–∏–π –ø—Ä–∞–∫—Ç–∏—á–Ω–∏–π –≥–∞–π–¥ –ø–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—é DBSCAN –≤ scikit-learn –∑ –ø—Ä–∏–∫–ª–∞–¥–∞–º–∏ –∫–æ–¥—É.

---

## üì¶ –û—Å–Ω–æ–≤–Ω—ñ —ñ–º–ø–æ—Ä—Ç–∏

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# DBSCAN
from sklearn.cluster import DBSCAN, OPTICS

# Metrics
from sklearn.metrics import (
    silhouette_score,
    davies_bouldin_score,
    calinski_harabasz_score
)

# Preprocessing
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# Neighbors (–¥–ª—è –≤–∏–±–æ—Ä—É eps)
from sklearn.neighbors import NearestNeighbors

# Data
from sklearn.datasets import make_moons, make_circles, make_blobs
```

---

## 1Ô∏è‚É£ DBSCAN ‚Äî –æ—Å–Ω–æ–≤–Ω–∏–π –∫–ª–∞—Å

### –ü–∞—Ä–∞–º–µ—Ç—Ä–∏

```python
DBSCAN(
    eps=0.5,                   # –†–∞–¥—ñ—É—Å –æ–∫–æ–ª–∏—Ü—ñ (epsilon)
    min_samples=5,             # –ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ç–æ—á–æ–∫ –¥–ª—è core point
    metric='euclidean',        # –ú–µ—Ç—Ä–∏–∫–∞ –≤—ñ–¥—Å—Ç–∞–Ω—ñ: 'euclidean', 'manhattan', 'cosine', etc.
    metric_params=None,        # –î–æ–¥–∞—Ç–∫–æ–≤—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –¥–ª—è –º–µ—Ç—Ä–∏–∫–∏
    algorithm='auto',          # 'auto', 'ball_tree', 'kd_tree', 'brute'
    leaf_size=30,              # –†–æ–∑–º—ñ—Ä –ª–∏—Å—Ç–∞ –¥–ª—è ball_tree/kd_tree
    p=None,                    # –ü–∞—Ä–∞–º–µ—Ç—Ä –¥–ª—è Minkowski metric (p=2 ‚Üí euclidean)
    n_jobs=None                # –ö—ñ–ª—å–∫—ñ—Å—Ç—å —è–¥–µ—Ä (-1 = –≤—Å—ñ)
)
```

### –ê—Ç—Ä–∏–±—É—Ç–∏ –ø—ñ—Å–ª—è fit

```python
dbscan = DBSCAN(eps=0.5, min_samples=5)
dbscan.fit(X)

# –î–æ—Å—Ç—É–ø–Ω—ñ –∞—Ç—Ä–∏–±—É—Ç–∏:
dbscan.labels_              # –ú—ñ—Ç–∫–∏ –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤ (n_samples,) [-1 = noise]
dbscan.core_sample_indices_ # –Ü–Ω–¥–µ–∫—Å–∏ core points
dbscan.components_          # –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç–∏ core points (n_core_samples, n_features)
```

### –ú–µ—Ç–æ–¥–∏

```python
# –ù–∞–≤—á–∞–Ω–Ω—è
dbscan.fit(X)

# –ù–∞–≤—á–∞–Ω–Ω—è + –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
labels = dbscan.fit_predict(X)

# –í–ê–ñ–õ–ò–í–û: DBSCAN –Ω–µ –º–∞—î –º–µ—Ç–æ–¥—É predict()!
# –ù–µ –º–æ–∂–Ω–∞ –ø–µ—Ä–µ–¥–±–∞—á–∏—Ç–∏ –¥–ª—è –Ω–æ–≤–∏—Ö —Ç–æ—á–æ–∫
```

---

## 2Ô∏è‚É£ –ë–∞–∑–æ–≤–∏–π –ø—Ä–∏–∫–ª–∞–¥

```python
import numpy as np
from sklearn.cluster import DBSCAN
from sklearn.datasets import make_moons
import matplotlib.pyplot as plt

# 1. –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –¥–∞–Ω–∏—Ö (–¥–≤–∞ "–ø—ñ–≤–º—ñ—Å—è—Ü—ñ")
X, _ = make_moons(n_samples=300, noise=0.05, random_state=42)

# 2. DBSCAN
dbscan = DBSCAN(eps=0.3, min_samples=5)
labels = dbscan.fit_predict(X)

# 3. –†–µ–∑—É–ª—å—Ç–∞—Ç–∏
n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
n_noise = list(labels).count(-1)

print(f"–ö–ª–∞—Å—Ç–µ—Ä—ñ–≤ –∑–Ω–∞–π–¥–µ–Ω–æ: {n_clusters}")
print(f"Noise —Ç–æ—á–æ–∫: {n_noise}")
print(f"Core points: {len(dbscan.core_sample_indices_)}")

# 4. –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
plt.figure(figsize=(10, 6))

# –ö–ª–∞—Å—Ç–µ—Ä–∏
unique_labels = set(labels)
colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))

for k, col in zip(unique_labels, colors):
    if k == -1:
        # Noise ‚Äî —á–æ—Ä–Ω—ñ —Ç–æ—á–∫–∏
        col = 'black'
        marker = 'x'
        label = 'Noise'
    else:
        marker = 'o'
        label = f'Cluster {k}'
    
    class_member_mask = (labels == k)
    xy = X[class_member_mask]
    plt.scatter(xy[:, 0], xy[:, 1], c=[col], marker=marker, 
                s=50, alpha=0.6, label=label)

# Core points (–±—ñ–ª—å—à—ñ —Ç–æ—á–∫–∏)
core_samples_mask = np.zeros_like(labels, dtype=bool)
core_samples_mask[dbscan.core_sample_indices_] = True

plt.scatter(X[core_samples_mask, 0], X[core_samples_mask, 1],
            s=100, facecolors='none', edgecolors='red',
            linewidths=2, label='Core points')

plt.title(f'DBSCAN: {n_clusters} clusters, {n_noise} noise points')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
```

---

## 3Ô∏è‚É£ –í–∏–±—ñ—Ä –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ eps —ñ min_samples

### –ú–µ—Ç–æ–¥ 1: k-distance –≥—Ä–∞—Ñ—ñ–∫ (–Ω–∞–π–∫—Ä–∞—â–∏–π!)

```python
from sklearn.neighbors import NearestNeighbors
import numpy as np
import matplotlib.pyplot as plt

def plot_k_distance(X, k=5):
    """
    –ë—É–¥—É—î k-distance –≥—Ä–∞—Ñ—ñ–∫ –¥–ª—è –≤–∏–±–æ—Ä—É eps
    
    k ‚Äî –∑–∞–∑–≤–∏—á–∞–π –¥–æ—Ä—ñ–≤–Ω—é—î min_samples
    """
    # –ó–Ω–∞–π—Ç–∏ k –Ω–∞–π–±–ª–∏–∂—á–∏—Ö —Å—É—Å—ñ–¥—ñ–≤
    neighbors = NearestNeighbors(n_neighbors=k)
    neighbors.fit(X)
    
    distances, indices = neighbors.kneighbors(X)
    
    # –í—ñ–¥—Å—Ç–∞–Ω—å –¥–æ k-–≥–æ —Å—É—Å—ñ–¥–∞ (—ñ–Ω–¥–µ–∫—Å k-1, –±–æ –≤–∫–ª—é—á–∞—î —Å–∞–º—É —Ç–æ—á–∫—É)
    k_distances = distances[:, k-1]
    
    # –í—ñ–¥—Å–æ—Ä—Ç—É–≤–∞—Ç–∏ –∑–∞ —Å–ø–∞–¥–∞–Ω–Ω—è–º
    k_distances = np.sort(k_distances)[::-1]
    
    # –ì—Ä–∞—Ñ—ñ–∫
    plt.figure(figsize=(10, 6))
    plt.plot(k_distances)
    plt.ylabel(f'{k}-distance', fontsize=12)
    plt.xlabel('Data points sorted by distance', fontsize=12)
    plt.title(f'k-distance Graph (k={k})', fontsize=14)
    plt.grid(True, alpha=0.3)
    
    # –î–æ–¥–∞—Ç–∏ –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω—É –ª—ñ–Ω—ñ—é –¥–ª—è –≤—ñ–∑—É–∞–ª—å–Ω–æ–≥–æ –ø–æ—à—É–∫—É "–∫–æ–ª—ñ–Ω–∞"
    plt.axhline(y=np.median(k_distances), color='r', linestyle='--', 
                alpha=0.5, label=f'Median: {np.median(k_distances):.3f}')
    plt.legend()
    plt.show()
    
    # –í–∏–≤–µ–¥–µ–Ω–Ω—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ–π
    print(f"–†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–∏–π eps (–≤—ñ–∑—É–∞–ª—å–Ω–æ –≤–∏–∑–Ω–∞—á–∏—Ç–∏ '–∫–æ–ª—ñ–Ω–æ' –Ω–∞ –≥—Ä–∞—Ñ—ñ–∫—É)")
    print(f"–û—Ä—ñ—î–Ω—Ç–æ–≤–Ω–æ:")
    print(f"  - 25% percentile: {np.percentile(k_distances, 25):.3f}")
    print(f"  - 50% percentile: {np.percentile(k_distances, 50):.3f}")
    print(f"  - 75% percentile: {np.percentile(k_distances, 75):.3f}")

# –ü—Ä–∏–∫–ª–∞–¥
from sklearn.datasets import make_moons
X, _ = make_moons(n_samples=300, noise=0.05, random_state=42)

plot_k_distance(X, k=5)
```

**–Ø–∫ —á–∏—Ç–∞—Ç–∏ –≥—Ä–∞—Ñ—ñ–∫:**

```
k-distance
  ‚Üë
  ‚îÇ‚óè
  ‚îÇ ‚óè
  ‚îÇ  ‚óè
  ‚îÇ   ‚óè‚óè
  ‚îÇ     ‚óè‚óè
  ‚îÇ       ‚óè‚óè‚óè  ‚Üê "–∫–æ–ª—ñ–Ω–æ" (—Ä—ñ–∑–∫–µ —Å–ø–æ–≤—ñ–ª—å–Ω–µ–Ω–Ω—è)
  ‚îÇ          ‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  ‚îÇ
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí Points

eps ‚âà –∑–Ω–∞—á–µ–Ω–Ω—è y –≤ —Ç–æ—á—Ü—ñ –∫–æ–ª—ñ–Ω–∞
```

---

### –ú–µ—Ç–æ–¥ 2: –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∏–π –ø–æ—à—É–∫ –∫–æ–ª—ñ–Ω–∞

```python
from kneed import KneeLocator  # pip install kneed

def find_optimal_eps(X, k=5):
    """–ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –∑–Ω–∞—Ö–æ–¥–∏—Ç—å eps —á–µ—Ä–µ–∑ –ø–æ—à—É–∫ –∫–æ–ª—ñ–Ω–∞"""
    neighbors = NearestNeighbors(n_neighbors=k)
    neighbors.fit(X)
    distances, _ = neighbors.kneighbors(X)
    
    k_distances = np.sort(distances[:, k-1])
    
    # –ü–æ—à—É–∫ –∫–æ–ª—ñ–Ω–∞
    kneedle = KneeLocator(
        range(len(k_distances)), 
        k_distances,
        curve='convex',
        direction='increasing'
    )
    
    optimal_eps = k_distances[kneedle.knee] if kneedle.knee else None
    
    return optimal_eps

# –ü—Ä–∏–∫–ª–∞–¥
optimal_eps = find_optimal_eps(X, k=5)
print(f"–û–ø—Ç–∏–º–∞–ª—å–Ω–∏–π eps: {optimal_eps:.3f}")
```

---

### –ú–µ—Ç–æ–¥ 3: Grid Search –∑ –º–µ—Ç—Ä–∏–∫–∞–º–∏

```python
from sklearn.metrics import silhouette_score

def grid_search_dbscan(X, eps_range, min_samples_range):
    """
    Grid search –¥–ª—è DBSCAN –∑ –æ—Ü—ñ–Ω–∫–æ—é Silhouette Score
    """
    results = []
    
    for eps in eps_range:
        for min_samples in min_samples_range:
            dbscan = DBSCAN(eps=eps, min_samples=min_samples)
            labels = dbscan.fit_predict(X)
            
            # –ü—Ä–æ–ø—É—Å—Ç–∏—Ç–∏, —è–∫—â–æ –≤—Å—ñ —Ç–æ—á–∫–∏ noise –∞–±–æ –æ–¥–∏–Ω –∫–ª–∞—Å—Ç–µ—Ä
            n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
            n_noise = list(labels).count(-1)
            
            if n_clusters < 2 or n_clusters == len(X):
                continue
            
            # Silhouette (—Ç—ñ–ª—å–∫–∏ –¥–ª—è –Ω–µ-noise —Ç–æ—á–æ–∫)
            mask = labels != -1
            if sum(mask) > 1:
                try:
                    score = silhouette_score(X[mask], labels[mask])
                except:
                    score = -1
            else:
                score = -1
            
            results.append({
                'eps': eps,
                'min_samples': min_samples,
                'n_clusters': n_clusters,
                'n_noise': n_noise,
                'noise_pct': n_noise / len(X) * 100,
                'silhouette': score
            })
    
    df = pd.DataFrame(results)
    df = df.sort_values('silhouette', ascending=False)
    
    return df

# –ü—Ä–∏–∫–ª–∞–¥
eps_range = np.arange(0.1, 1.0, 0.05)
min_samples_range = range(3, 15)

results_df = grid_search_dbscan(X, eps_range, min_samples_range)

print("Top 5 –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ–π:")
print(results_df.head(10))

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
best_params = results_df.iloc[0]
print(f"\n–ù–∞–π–∫—Ä–∞—â–∞ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è:")
print(f"  eps: {best_params['eps']:.3f}")
print(f"  min_samples: {int(best_params['min_samples'])}")
print(f"  Silhouette: {best_params['silhouette']:.3f}")
print(f"  –ö–ª–∞—Å—Ç–µ—Ä—ñ–≤: {int(best_params['n_clusters'])}")
print(f"  Noise: {best_params['noise_pct']:.1f}%")
```

---

### –ú–µ—Ç–æ–¥ 4: Heatmap –¥–ª—è Grid Search

```python
def plot_dbscan_heatmap(X, eps_range, min_samples_range, metric='silhouette'):
    """
    –ë—É–¥—É—î heatmap –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö –∫–æ–º–±—ñ–Ω–∞—Ü—ñ–π –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤
    """
    results = np.zeros((len(min_samples_range), len(eps_range)))
    
    for i, min_samples in enumerate(min_samples_range):
        for j, eps in enumerate(eps_range):
            dbscan = DBSCAN(eps=eps, min_samples=min_samples)
            labels = dbscan.fit_predict(X)
            
            n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
            mask = labels != -1
            
            if n_clusters < 2 or sum(mask) < 2:
                results[i, j] = -1
            else:
                try:
                    if metric == 'silhouette':
                        results[i, j] = silhouette_score(X[mask], labels[mask])
                    elif metric == 'n_clusters':
                        results[i, j] = n_clusters
                    elif metric == 'noise_pct':
                        results[i, j] = list(labels).count(-1) / len(X) * 100
                except:
                    results[i, j] = -1
    
    # Heatmap
    plt.figure(figsize=(12, 8))
    sns.heatmap(
        results,
        xticklabels=[f'{e:.2f}' for e in eps_range],
        yticklabels=min_samples_range,
        cmap='viridis',
        annot=False,
        fmt='.2f'
    )
    plt.xlabel('eps', fontsize=12)
    plt.ylabel('min_samples', fontsize=12)
    plt.title(f'DBSCAN Grid Search - {metric}', fontsize=14)
    plt.tight_layout()
    plt.show()

# –ü—Ä–∏–∫–ª–∞–¥
eps_range = np.arange(0.1, 1.0, 0.05)
min_samples_range = range(3, 15)

plot_dbscan_heatmap(X, eps_range, min_samples_range, metric='silhouette')
plot_dbscan_heatmap(X, eps_range, min_samples_range, metric='n_clusters')
```

---

## 4Ô∏è‚É£ –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –∑ K-Means

```python
from sklearn.cluster import KMeans
from sklearn.datasets import make_moons
import matplotlib.pyplot as plt

# –î–∞–Ω—ñ –∑ –Ω–µ–ª—ñ–Ω—ñ–π–Ω–æ—é —Å—Ç—Ä—É–∫—Ç—É—Ä–æ—é
X, _ = make_moons(n_samples=300, noise=0.05, random_state=42)

# K-Means
kmeans = KMeans(n_clusters=2, random_state=42)
labels_kmeans = kmeans.fit_predict(X)

# DBSCAN
dbscan = DBSCAN(eps=0.3, min_samples=5)
labels_dbscan = dbscan.fit_predict(X)

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
fig, axes = plt.subplots(1, 2, figsize=(15, 6))

# K-Means
axes[0].scatter(X[:, 0], X[:, 1], c=labels_kmeans, cmap='viridis', s=50)
axes[0].scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],
                c='red', marker='X', s=200, edgecolors='black', linewidths=2)
axes[0].set_title('K-Means (K=2)\n‚úó –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ —Ä–æ–∑–¥—ñ–ª—è—î –ø—ñ–≤–º—ñ—Å—è—Ü—ñ', fontsize=12)
axes[0].grid(True, alpha=0.3)

# DBSCAN
for k in set(labels_dbscan):
    if k == -1:
        col = 'black'
        marker = 'x'
    else:
        col = plt.cm.viridis(k / max(labels_dbscan))
        marker = 'o'
    
    mask = labels_dbscan == k
    axes[1].scatter(X[mask, 0], X[mask, 1], c=[col], marker=marker, s=50)

axes[1].set_title(f'DBSCAN (eps=0.3, min_samples=5)\n‚úì –ü—Ä–∞–≤–∏–ª—å–Ω–æ –∑–Ω–∞—Ö–æ–¥–∏—Ç—å –ø—ñ–≤–º—ñ—Å—è—Ü—ñ', fontsize=12)
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

---

## 5Ô∏è‚É£ –ü—Ä–∞–∫—Ç–∏—á–Ω—ñ –ø—Ä–∏–∫–ª–∞–¥–∏

### –ü—Ä–∏–∫–ª–∞–¥ 1: –î–≤–∞ –∫—ñ–ª—å—Ü—è

```python
from sklearn.datasets import make_circles

# –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –¥–∞–Ω–∏—Ö
X, _ = make_circles(n_samples=500, factor=0.5, noise=0.05, random_state=42)

# –ú–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# DBSCAN
dbscan = DBSCAN(eps=0.3, min_samples=10)
labels = dbscan.fit_predict(X_scaled)

# –†–µ–∑—É–ª—å—Ç–∞—Ç–∏
n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
n_noise = list(labels).count(-1)

print(f"–ö–ª–∞—Å—Ç–µ—Ä—ñ–≤: {n_clusters}")
print(f"Noise: {n_noise} ({n_noise/len(X)*100:.1f}%)")

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
plt.figure(figsize=(10, 6))

for k in set(labels):
    if k == -1:
        plt.scatter(X[labels == k, 0], X[labels == k, 1], 
                   c='black', marker='x', s=50, label='Noise')
    else:
        plt.scatter(X[labels == k, 0], X[labels == k, 1], 
                   s=50, label=f'Cluster {k}')

plt.title('DBSCAN –Ω–∞ –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∏—á–Ω–∏—Ö –∫–æ–ª–∞—Ö')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
```

---

### –ü—Ä–∏–∫–ª–∞–¥ 2: –í–∏—è–≤–ª–µ–Ω–Ω—è –∞–Ω–æ–º–∞–ª—ñ–π —É —Ç—Ä–∞–Ω–∑–∞–∫—Ü—ñ—è—Ö

```python
import numpy as np
import pandas as pd

# –°–∏–º—É–ª—è—Ü—ñ—è –¥–∞–Ω–∏—Ö —Ç—Ä–∞–Ω–∑–∞–∫—Ü—ñ–π
np.random.seed(42)

# –ù–æ—Ä–º–∞–ª—å–Ω—ñ —Ç—Ä–∞–Ω–∑–∞–∫—Ü—ñ—ó (2 –≥—Ä—É–ø–∏: –¥—Ä—ñ–±–Ω—ñ —Ç–∞ —Å–µ—Ä–µ–¥–Ω—ñ –ø–æ–∫—É–ø–∫–∏)
normal_small = np.random.normal(loc=50, scale=10, size=(400, 2))
normal_medium = np.random.normal(loc=200, scale=30, size=(300, 2))

# –ê–Ω–æ–º–∞–ª—ñ—ó (–¥—É–∂–µ –≤–µ–ª–∏–∫—ñ –∞–±–æ –Ω–µ–∑–≤–∏—á–Ω—ñ —Ç—Ä–∞–Ω–∑–∞–∫—Ü—ñ—ó)
anomalies = np.random.uniform(low=500, high=1000, size=(20, 2))

# –û–±'—î–¥–Ω–∞–Ω–Ω—è
X = np.vstack([normal_small, normal_medium, anomalies])

df = pd.DataFrame(X, columns=['Amount', 'Frequency'])
df['Type'] = ['Normal']*700 + ['Anomaly']*20  # –°–ø—Ä–∞–≤–∂–Ω—ñ –º—ñ—Ç–∫–∏ (–¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏)

# –ú–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# DBSCAN –¥–ª—è –≤–∏—è–≤–ª–µ–Ω–Ω—è –∞–Ω–æ–º–∞–ª—ñ–π
dbscan = DBSCAN(eps=0.5, min_samples=10)
labels = dbscan.fit_predict(X_scaled)

df['Cluster'] = labels
df['Predicted'] = ['Anomaly' if l == -1 else 'Normal' for l in labels]

# –†–µ–∑—É–ª—å—Ç–∞—Ç–∏
print("=== Confusion Matrix ===")
print(pd.crosstab(df['Type'], df['Predicted']))

print(f"\n–ê–Ω–æ–º–∞–ª—ñ–π –≤–∏—è–≤–ª–µ–Ω–æ: {list(labels).count(-1)}")
print(f"–°–ø—Ä–∞–≤–∂–Ω—ñ—Ö –∞–Ω–æ–º–∞–ª—ñ–π: {sum(df['Type'] == 'Anomaly')}")

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
plt.figure(figsize=(12, 5))

# –°–ø—Ä–∞–≤–∂–Ω—ñ –º—ñ—Ç–∫–∏
plt.subplot(1, 2, 1)
scatter = plt.scatter(df['Amount'], df['Frequency'], 
                     c=(df['Type'] == 'Anomaly'), cmap='coolwarm', s=50)
plt.xlabel('Amount')
plt.ylabel('Frequency')
plt.title('–°–ø—Ä–∞–≤–∂–Ω—ñ –º—ñ—Ç–∫–∏')
plt.colorbar(scatter, label='Anomaly')

# DBSCAN —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
plt.subplot(1, 2, 2)
for cluster in set(labels):
    if cluster == -1:
        plt.scatter(df[df['Cluster'] == cluster]['Amount'],
                   df[df['Cluster'] == cluster]['Frequency'],
                   c='red', marker='x', s=100, label='Anomaly (DBSCAN)')
    else:
        plt.scatter(df[df['Cluster'] == cluster]['Amount'],
                   df[df['Cluster'] == cluster]['Frequency'],
                   s=50, label=f'Cluster {cluster}')

plt.xlabel('Amount')
plt.ylabel('Frequency')
plt.title('DBSCAN —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏')
plt.legend()

plt.tight_layout()
plt.show()
```

---

### –ü—Ä–∏–∫–ª–∞–¥ 3: –°–µ–≥–º–µ–Ω—Ç–∞—Ü—ñ—è –∫–ª—ñ—î–Ω—Ç—ñ–≤ –∑–∞ –≥–µ–æ–ª–æ–∫–∞—Ü—ñ—î—é

```python
import numpy as np

# –°–∏–º—É–ª—è—Ü—ñ—è GPS –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç –∫–ª—ñ—î–Ω—Ç—ñ–≤ —É –º—ñ—Å—Ç—ñ
np.random.seed(42)

# 3 —Ä–∞–π–æ–Ω–∏ –º—ñ—Å—Ç–∞
downtown = np.random.normal(loc=[40.7128, -74.0060], scale=0.01, size=(100, 2))
suburb_north = np.random.normal(loc=[40.7589, -73.9851], scale=0.015, size=(80, 2))
suburb_south = np.random.normal(loc=[40.6782, -73.9442], scale=0.012, size=(70, 2))

# –û–¥–∏–Ω–æ—á–Ω—ñ –∫–ª—ñ—î–Ω—Ç–∏ (–≤–∏–∫–∏–¥–∏)
outliers = np.random.uniform(low=[40.65, -74.05], high=[40.80, -73.90], size=(10, 2))

X = np.vstack([downtown, suburb_north, suburb_south, outliers])

# DBSCAN (eps –≤ –≥—Ä–∞–¥—É—Å–∞—Ö, ~0.01¬∞ ‚âà 1.1 –∫–º)
dbscan = DBSCAN(eps=0.02, min_samples=10)
labels = dbscan.fit_predict(X)

# –†–µ–∑—É–ª—å—Ç–∞—Ç–∏
n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
n_noise = list(labels).count(-1)

print(f"–†–∞–π–æ–Ω—ñ–≤ –∑–Ω–∞–π–¥–µ–Ω–æ: {n_clusters}")
print(f"–û–¥–∏–Ω–æ—á–Ω–∏—Ö –∫–ª—ñ—î–Ω—Ç—ñ–≤: {n_noise}")

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
plt.figure(figsize=(10, 8))

for k in set(labels):
    if k == -1:
        plt.scatter(X[labels == k, 1], X[labels == k, 0],
                   c='black', marker='x', s=100, label='Outliers', zorder=5)
    else:
        plt.scatter(X[labels == k, 1], X[labels == k, 0],
                   s=50, alpha=0.6, label=f'–†–∞–π–æ–Ω {k+1}')

plt.xlabel('Longitude')
plt.ylabel('Latitude')
plt.title(f'–°–µ–≥–º–µ–Ω—Ç–∞—Ü—ñ—è –∫–ª—ñ—î–Ω—Ç—ñ–≤ –∑–∞ –≥–µ–æ–ª–æ–∫–∞—Ü—ñ—î—é\n{n_clusters} —Ä–∞–π–æ–Ω—ñ–≤, {n_noise} outliers')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
```

---

## 6Ô∏è‚É£ –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –¥–ª—è –≤–∏—Å–æ–∫–æ–≤–∏–º—ñ—Ä–Ω–∏—Ö –¥–∞–Ω–∏—Ö

### PCA –ø—Ä–æ–µ–∫—Ü—ñ—è

```python
from sklearn.decomposition import PCA
from sklearn.datasets import load_iris

# –í–∏—Å–æ–∫–æ–≤–∏–º—ñ—Ä–Ω—ñ –¥–∞–Ω—ñ (4D)
iris = load_iris()
X = iris.data

# –ú–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# DBSCAN
dbscan = DBSCAN(eps=0.5, min_samples=5)
labels = dbscan.fit_predict(X_scaled)

# PCA –¥–ª—è –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
plt.figure(figsize=(12, 5))

# PCA –ø—Ä–æ–µ–∫—Ü—ñ—è
plt.subplot(1, 2, 1)
for k in set(labels):
    if k == -1:
        plt.scatter(X_pca[labels == k, 0], X_pca[labels == k, 1],
                   c='black', marker='x', s=100, label='Noise')
    else:
        plt.scatter(X_pca[labels == k, 0], X_pca[labels == k, 1],
                   s=50, alpha=0.6, label=f'Cluster {k}')

plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')
plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')
plt.title('DBSCAN (PCA –ø—Ä–æ–µ–∫—Ü—ñ—è)')
plt.legend()
plt.grid(True, alpha=0.3)

# –°–ø—Ä–∞–≤–∂–Ω—ñ –º—ñ—Ç–∫–∏ (–¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è)
plt.subplot(1, 2, 2)
scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=iris.target, cmap='viridis', s=50)
plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')
plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')
plt.title('–°–ø—Ä–∞–≤–∂–Ω—ñ –º—ñ—Ç–∫–∏')
plt.colorbar(scatter, label='Species')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

---

## 7Ô∏è‚É£ OPTICS ‚Äî –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ DBSCAN

```python
from sklearn.cluster import OPTICS

# OPTICS –Ω–µ –ø–æ—Ç—Ä–µ–±—É—î eps!
optics = OPTICS(
    min_samples=5,
    xi=0.05,          # –ö—Ä—É—Ç–∏–∑–Ω–∞ –¥–ª—è –≤–∏—è–≤–ª–µ–Ω–Ω—è –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤ (0-1)
    min_cluster_size=10
)

labels = optics.fit_predict(X)

# Reachability plot
plt.figure(figsize=(12, 5))

# –ö–ª–∞—Å—Ç–µ—Ä–∏
plt.subplot(1, 2, 1)
for k in set(labels):
    if k == -1:
        plt.scatter(X[labels == k, 0], X[labels == k, 1],
                   c='black', marker='x', s=50, label='Noise')
    else:
        plt.scatter(X[labels == k, 0], X[labels == k, 1],
                   s=50, label=f'Cluster {k}')
plt.title('OPTICS Clustering')
plt.legend()
plt.grid(True, alpha=0.3)

# Reachability plot
plt.subplot(1, 2, 2)
space = np.arange(len(X))
reachability = optics.reachability_[optics.ordering_]
plt.plot(space, reachability, 'k-', alpha=0.5)
plt.fill_between(space, 0, reachability, alpha=0.3)
plt.ylabel('Reachability Distance')
plt.xlabel('Sample Index (ordered)')
plt.title('Reachability Plot')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

---

## 8Ô∏è‚É£ –û—Ü—ñ–Ω–∫–∞ —è–∫–æ—Å—Ç—ñ

```python
from sklearn.metrics import silhouette_score, davies_bouldin_score

def evaluate_dbscan(X, labels):
    """
    –û—Ü—ñ–Ω–∏—Ç–∏ —è–∫—ñ—Å—Ç—å DBSCAN –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—ó
    """
    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
    n_noise = list(labels).count(-1)
    n_samples = len(X)
    
    print("=== DBSCAN Results ===")
    print(f"–ö–ª–∞—Å—Ç–µ—Ä—ñ–≤: {n_clusters}")
    print(f"Noise —Ç–æ—á–æ–∫: {n_noise} ({n_noise/n_samples*100:.1f}%)")
    print(f"Core points: {sum(labels != -1)}")
    
    # –ú–µ—Ç—Ä–∏–∫–∏ (–±–µ–∑ noise)
    mask = labels != -1
    
    if sum(mask) > 1 and len(set(labels[mask])) > 1:
        sil_score = silhouette_score(X[mask], labels[mask])
        db_score = davies_bouldin_score(X[mask], labels[mask])
        
        print(f"\n=== Metrics (excluding noise) ===")
        print(f"Silhouette Score: {sil_score:.3f}")
        print(f"Davies-Bouldin Index: {db_score:.3f}")
        
        # –Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü—ñ—è
        print(f"\n–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü—ñ—è:")
        if sil_score > 0.7:
            print("  ‚úì –í—ñ–¥–º—ñ–Ω–Ω–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—è")
        elif sil_score > 0.5:
            print("  ‚úì –î–æ–±—Ä–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—è")
        elif sil_score > 0.25:
            print("  ‚ö† –°–ª–∞–±–∫–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—è")
        else:
            print("  ‚úó –ü–æ–≥–∞–Ω–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—è")
    else:
        print("\n‚ö† –ù–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤ –¥–ª—è –º–µ—Ç—Ä–∏–∫")
    
    # –†–æ–∑–ø–æ–¥—ñ–ª –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞—Ö
    print(f"\n=== Cluster Sizes ===")
    for k in sorted(set(labels)):
        if k == -1:
            continue
        count = list(labels).count(k)
        print(f"Cluster {k}: {count} points ({count/n_samples*100:.1f}%)")

# –ü—Ä–∏–∫–ª–∞–¥
evaluate_dbscan(X_scaled, labels)
```

---

## 9Ô∏è‚É£ –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ç–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è

```python
import joblib

# –ù–∞–≤—á–∞–Ω–Ω—è
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

dbscan = DBSCAN(eps=0.5, min_samples=5)
labels = dbscan.fit_predict(X_scaled)

# –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è
joblib.dump(scaler, 'scaler.pkl')
joblib.dump(dbscan, 'dbscan_model.pkl')

# –í–ê–ñ–õ–ò–í–û: –ó–±–µ—Ä–µ–≥—Ç–∏ —Ç–∞–∫–æ–∂ core_sample_indices_ –¥–ª—è –º–æ–∂–ª–∏–≤–æ—Å—Ç—ñ —ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü—ñ—ó
model_data = {
    'dbscan': dbscan,
    'scaler': scaler,
    'labels_train': labels,
    'X_train_scaled': X_scaled
}
joblib.dump(model_data, 'dbscan_full.pkl')

# –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è
loaded_data = joblib.load('dbscan_full.pkl')
loaded_dbscan = loaded_data['dbscan']
loaded_scaler = loaded_data['scaler']

print(f"Core points: {len(loaded_dbscan.core_sample_indices_)}")
```

**–í–ê–ñ–õ–ò–í–û:** DBSCAN –Ω–µ –º–∞—î –º–µ—Ç–æ–¥—É `predict()` –¥–ª—è –Ω–æ–≤–∏—Ö —Ç–æ—á–æ–∫!

### –Ø–∫ –ø–µ—Ä–µ–¥–±–∞—á–∏—Ç–∏ –¥–ª—è –Ω–æ–≤–∏—Ö —Ç–æ—á–æ–∫?

```python
def predict_dbscan(new_X, dbscan, X_train, scaler, eps):
    """
    –°–ø–æ—Å—ñ–± –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –¥–ª—è –Ω–æ–≤–∏—Ö —Ç–æ—á–æ–∫ —á–µ—Ä–µ–∑ –Ω–∞–π–±–ª–∏–∂—á–æ–≥–æ —Å—É—Å—ñ–¥–∞
    
    –û–ë–ú–ï–ñ–ï–ù–ù–Ø: –¶–µ –Ω–∞–±–ª–∏–∂–µ–Ω–Ω—è, –Ω–µ –æ—Ñ—ñ—Ü—ñ–π–Ω–∏–π –º–µ—Ç–æ–¥
    """
    from sklearn.neighbors import NearestNeighbors
    
    # –ú–∞—Å—à—Ç–∞–±—É–≤–∞—Ç–∏ –Ω–æ–≤—ñ –¥–∞–Ω—ñ
    new_X_scaled = scaler.transform(new_X)
    
    # –ó–Ω–∞–π—Ç–∏ –Ω–∞–π–±–ª–∏–∂—á–æ–≥–æ —Å—É—Å—ñ–¥–∞ –∑ —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö
    nn = NearestNeighbors(n_neighbors=1)
    nn.fit(X_train)
    
    distances, indices = nn.kneighbors(new_X_scaled)
    
    # –Ø–∫—â–æ –≤—ñ–¥—Å—Ç–∞–Ω—å < eps ‚Üí –ø—Ä–∏—Å–≤–æ—ó—Ç–∏ –º—ñ—Ç–∫—É –Ω–∞–π–±–ª–∏–∂—á–æ–≥–æ
    # –Ü–Ω–∞–∫—à–µ ‚Üí noise (-1)
    predictions = []
    for dist, idx in zip(distances.flatten(), indices.flatten()):
        if dist <= eps:
            predictions.append(dbscan.labels_[idx])
        else:
            predictions.append(-1)  # Noise
    
    return np.array(predictions)

# –ü—Ä–∏–∫–ª–∞–¥
new_data = np.array([[1.5, 2.5], [5.0, 5.0]])
new_labels = predict_dbscan(
    new_data, 
    loaded_dbscan, 
    loaded_data['X_train_scaled'],
    loaded_scaler,
    eps=0.5
)

print(f"–ù–æ–≤—ñ —Ç–æ—á–∫–∏: {new_labels}")
```

---

## üîü –ü–æ—Ä–∞–¥–∏ —Ç–∞ best practices

### 1. –ó–∞–≤–∂–¥–∏ –º–∞—Å—à—Ç–∞–±—É–π –¥–∞–Ω—ñ

```python
# –ü–û–ì–ê–ù–û
dbscan = DBSCAN(eps=0.5)
dbscan.fit(X)

# –î–û–ë–†–ï
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
dbscan = DBSCAN(eps=0.5)
dbscan.fit(X_scaled)
```

---

### 2. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π k-distance –≥—Ä–∞—Ñ—ñ–∫

```python
# –ó–∞–≤–∂–¥–∏ –±—É–¥—É–π k-distance –ø–µ—Ä–µ–¥ DBSCAN
plot_k_distance(X_scaled, k=5)
# ‚Üí –í–∏–∑–Ω–∞—á eps –≤—ñ–∑—É–∞–ª—å–Ω–æ
```

---

### 3. MinPts –µ–≤—Ä–∏—Å—Ç–∏–∫–∞

```python
# –ü—Ä–∞–≤–∏–ª–æ: MinPts = 2 √ó dimensionality
n_features = X.shape[1]
min_samples = 2 * n_features

# –ê–ª–µ –Ω–µ –º–µ–Ω—à–µ 4
min_samples = max(min_samples, 4)
```

---

### 4. –ü–µ—Ä–µ–≤—ñ—Ä—è–π % noise

```python
n_noise = list(labels).count(-1)
noise_pct = n_noise / len(X) * 100

if noise_pct > 20:
    print("‚ö†Ô∏è –ó–∞–Ω–∞–¥—Ç–æ –±–∞–≥–∞—Ç–æ noise! –°–ø—Ä–æ–±—É–π:")
    print("  - –ó–±—ñ–ª—å—à–∏—Ç–∏ eps")
    print("  - –ó–º–µ–Ω—à–∏—Ç–∏ min_samples")
```

---

### 5. –í—ñ–∑—É–∞–ª—ñ–∑—É–π core/border/noise

```python
# Core points
core_mask = np.zeros_like(labels, dtype=bool)
core_mask[dbscan.core_sample_indices_] = True

# Border points (–Ω–µ core, –∞–ª–µ –Ω–µ noise)
border_mask = (labels != -1) & (~core_mask)

# Noise
noise_mask = (labels == -1)

plt.scatter(X[core_mask, 0], X[core_mask, 1], c='blue', label='Core', s=50)
plt.scatter(X[border_mask, 0], X[border_mask, 1], c='cyan', label='Border', s=50)
plt.scatter(X[noise_mask, 0], X[noise_mask, 1], c='red', marker='x', label='Noise', s=100)
plt.legend()
```

---

### 6. –î–ª—è –≤–µ–ª–∏–∫–∏—Ö –¥–∞–Ω–∏—Ö ‚Äî –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó

```python
# –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π ball_tree –∞–±–æ kd_tree –¥–ª—è –ø—Ä–∏—Å–∫–æ—Ä–µ–Ω–Ω—è
dbscan = DBSCAN(
    eps=0.5,
    min_samples=5,
    algorithm='ball_tree',  # –∞–±–æ 'kd_tree'
    leaf_size=30,
    n_jobs=-1  # –ü–∞—Ä–∞–ª–µ–ª—ñ–∑–∞—Ü—ñ—è
)
```

---

## –ß–µ–∫-–ª–∏—Å—Ç –¥–ª—è DBSCAN

```python
# ‚úÖ 1. –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –¥–∞–Ω—ñ
X = load_data()

# ‚úÖ 2. EDA
print(X.shape)
print(pd.DataFrame(X).describe())

# ‚úÖ 3. –ú–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è (–û–ë–û–í'–Ø–ó–ö–û–í–û!)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# ‚úÖ 4. –í–∏–±—ñ—Ä eps —á–µ—Ä–µ–∑ k-distance –≥—Ä–∞—Ñ—ñ–∫
plot_k_distance(X_scaled, k=5)
# –í—ñ–∑—É–∞–ª—å–Ω–æ –≤–∏–∑–Ω–∞—á–∏—Ç–∏ eps

# ‚úÖ 5. –í–∏–±—ñ—Ä min_samples
min_samples = 2 * X.shape[1]  # –∞–±–æ 4-5 –¥–ª—è 2D

# ‚úÖ 6. –ù–∞–≤—á–∞–Ω–Ω—è DBSCAN
dbscan = DBSCAN(eps=chosen_eps, min_samples=min_samples)
labels = dbscan.fit_predict(X_scaled)

# ‚úÖ 7. –û—Ü—ñ–Ω–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
evaluate_dbscan(X_scaled, labels)

# ‚úÖ 8. –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
visualize_clusters(X, labels)

# ‚úÖ 9. –Ø–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ ‚Äî Grid Search
results = grid_search_dbscan(X_scaled, eps_range, min_samples_range)

# ‚úÖ 10. –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è
joblib.dump({'dbscan': dbscan, 'scaler': scaler}, 'model.pkl')
```

---

## –ü–æ—Ä—ñ–≤–Ω—è–ª—å–Ω–∞ —Ç–∞–±–ª–∏—Ü—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤

|–°–∏—Ç—É–∞—Ü—ñ—è|eps|min_samples|–û—á—ñ–∫—É–≤–∞–Ω–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç|
|---|---|---|---|
|–©—ñ–ª—å–Ω—ñ –∫–ª–∞—Å—Ç–µ—Ä–∏|–ú–∞–ª–∏–π (0.1-0.3)|–°–µ—Ä–µ–¥–Ω—ñ–π (5-10)|–ë–∞–≥–∞—Ç–æ –º–∞–ª–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤|
|–†–æ–∑—Ä—ñ–¥–∂–µ–Ω—ñ –∫–ª–∞—Å—Ç–µ—Ä–∏|–í–µ–ª–∏–∫–∏–π (0.5-1.0)|–ú–∞–ª–∏–π (3-5)|–ú–∞–ª–æ –≤–µ–ª–∏–∫–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤|
|–ë–∞–≥–∞—Ç–æ —à—É–º—É|–°–µ—Ä–µ–¥–Ω—ñ–π|–í–µ–ª–∏–∫–∏–π (10-20)|–ú–µ–Ω—à–µ noise|
|–í–∏—Å–æ–∫—ñ —Ä–æ–∑–º—ñ—Ä–Ω–æ—Å—Ç—ñ|–í–µ–ª–∏–∫–∏–π|–í–µ–ª–∏–∫–∏–π|–°–∫–ª–∞–¥–Ω–æ ‚òπ|
|–ì–µ–æ–≥—Ä–∞—Ñ—ñ—á–Ω—ñ –¥–∞–Ω—ñ|~0.01-0.05¬∞|10-20|–†–∞–π–æ–Ω–∏ –º—ñ—Å—Ç–∞|

---

## –ö–æ—Ä–∏—Å–Ω—ñ –ø–æ—Å–∏–ª–∞–Ω–Ω—è

- [sklearn DBSCAN docs](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html)
- [sklearn OPTICS docs](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.OPTICS.html)
- [sklearn Clustering Guide](https://scikit-learn.org/stable/modules/clustering.html#dbscan)

---

**–°—Ç–≤–æ—Ä–µ–Ω–æ –¥–ª—è –ø—Ä–∞–∫—Ç–∏—á–Ω–æ–≥–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è DBSCAN —É –ø—Ä–æ—î–∫—Ç–∞—Ö** üöÄ