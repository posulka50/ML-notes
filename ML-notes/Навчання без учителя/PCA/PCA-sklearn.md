# PCA ‚Äî sklearn –ø—Ä–∞–∫—Ç–∏–∫–∞

–ü–æ–≤–Ω–∏–π –ø—Ä–∞–∫—Ç–∏—á–Ω–∏–π –≥–∞–π–¥ –ø–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—é PCA –≤ scikit-learn –∑ –ø—Ä–∏–∫–ª–∞–¥–∞–º–∏ –∫–æ–¥—É.

---

## üì¶ –û—Å–Ω–æ–≤–Ω—ñ —ñ–º–ø–æ—Ä—Ç–∏

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# PCA
from sklearn.decomposition import PCA, IncrementalPCA, KernelPCA

# Preprocessing
from sklearn.preprocessing import StandardScaler

# Metrics
from sklearn.metrics import (
    mean_squared_error,
    r2_score
)

# ML models (–¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è)
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score

# Data
from sklearn.datasets import load_iris, load_digits, make_classification
```

---

## 1Ô∏è‚É£ PCA ‚Äî –æ—Å–Ω–æ–≤–Ω–∏–π –∫–ª–∞—Å

### –ü–∞—Ä–∞–º–µ—Ç—Ä–∏

```python
PCA(
    n_components=None,         # –ö—ñ–ª—å–∫—ñ—Å—Ç—å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç –∞–±–æ variance to keep
    copy=True,                 # –ö–æ–ø—ñ—é–≤–∞—Ç–∏ –¥–∞–Ω—ñ
    whiten=False,              # –ù–æ—Ä–º–∞–ª—ñ–∑—É–≤–∞—Ç–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∏ –¥–æ –æ–¥. variance
    svd_solver='auto',         # 'auto', 'full', 'arpack', 'randomized'
    tol=0.0,                   # –¢–æ–ª–µ—Ä–∞–Ω—Ç–Ω—ñ—Å—Ç—å –¥–ª—è 'arpack'
    iterated_power='auto',     # –ö—ñ–ª—å–∫—ñ—Å—Ç—å —ñ—Ç–µ—Ä–∞—Ü—ñ–π –¥–ª—è 'randomized'
    n_oversamples=10,          # –î–ª—è 'randomized' solver
    power_iteration_normalizer='auto',  # –î–ª—è 'randomized'
    random_state=None          # Seed –¥–ª—è –≤—ñ–¥—Ç–≤–æ—Ä—é–≤–∞–Ω–æ—Å—Ç—ñ
)
```

**n_components –≤–∞—Ä—ñ–∞–Ω—Ç–∏:**

```python
# 1. Integer ‚Äî —Ç–æ—á–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç
pca = PCA(n_components=2)  # —Ç—ñ–ª—å–∫–∏ 2 –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∏

# 2. Float (0.0 - 1.0) ‚Äî variance to keep
pca = PCA(n_components=0.95)  # –∑–±–µ—Ä–µ–≥—Ç–∏ 95% variance

# 3. None ‚Äî –≤—Å—ñ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∏
pca = PCA()  # max(n_samples, n_features) –∫–æ–º–ø–æ–Ω–µ–Ω—Ç

# 4. String 'mle' ‚Äî –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∏–π –≤–∏–±—ñ—Ä —á–µ—Ä–µ–∑ MLE
pca = PCA(n_components='mle')  # –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–æ
```

---

### –ê—Ç—Ä–∏–±—É—Ç–∏ –ø—ñ—Å–ª—è fit

```python
pca = PCA(n_components=2)
pca.fit(X)

# –î–æ—Å—Ç—É–ø–Ω—ñ –∞—Ç—Ä–∏–±—É—Ç–∏:
pca.components_              # –ì–æ–ª–æ–≤–Ω—ñ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∏ (n_components, n_features)
pca.explained_variance_      # Variance –∫–æ–∂–Ω–æ—ó –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∏ (n_components,)
pca.explained_variance_ratio_  # % variance –∫–æ–∂–Ω–æ—ó –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∏ (n_components,)
pca.singular_values_         # –°–∏–Ω–≥—É–ª—è—Ä–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è
pca.mean_                    # –°–µ—Ä–µ–¥–Ω—î –∑–Ω–∞—á–µ–Ω–Ω—è –¥–ª—è —Ü–µ–Ω—Ç—Ä—É–≤–∞–Ω–Ω—è
pca.n_components_            # –ö—ñ–ª—å–∫—ñ—Å—Ç—å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç (–ø—ñ—Å–ª—è fit)
pca.n_features_              # –ö—ñ–ª—å–∫—ñ—Å—Ç—å –æ–∑–Ω–∞–∫ –≤ X
pca.n_features_in_           # –ö—ñ–ª—å–∫—ñ—Å—Ç—å –≤—Ö—ñ–¥–Ω–∏—Ö –æ–∑–Ω–∞–∫
pca.n_samples_               # –ö—ñ–ª—å–∫—ñ—Å—Ç—å –∑—Ä–∞–∑–∫—ñ–≤
pca.noise_variance_          # –û—Ü—ñ–Ω–∫–∞ variance —à—É–º—É
```

---

### –ú–µ—Ç–æ–¥–∏

```python
# –ù–∞–≤—á–∞–Ω–Ω—è
pca.fit(X)

# –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ—è (–∑–º–µ–Ω—à–µ–Ω–Ω—è —Ä–æ–∑–º—ñ—Ä–Ω–æ—Å—Ç—ñ)
X_pca = pca.transform(X)

# –ù–∞–≤—á–∞–Ω–Ω—è + —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ—è
X_pca = pca.fit_transform(X)

# –ó–≤–æ—Ä–æ—Ç–Ω–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ—è (–≤—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è)
X_restored = pca.inverse_transform(X_pca)

# Score (—Å–µ—Ä–µ–¥–Ω—è log-likelihood)
score = pca.score(X)

# Score –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –∑—Ä–∞–∑–∫–∞
scores = pca.score_samples(X)
```

---

## 2Ô∏è‚É£ –ë–∞–∑–æ–≤–∏–π –ø—Ä–∏–∫–ª–∞–¥

```python
import numpy as np
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import load_iris
import matplotlib.pyplot as plt

# 1. –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –¥–∞–Ω—ñ (4D)
iris = load_iris()
X = iris.data
y = iris.target

print(f"–û—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∞ —Ä–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å: {X.shape}")  # (150, 4)

# 2. –ú–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è (–û–ë–û–í'–Ø–ó–ö–û–í–û!)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 3. PCA
pca = PCA(n_components=2)  # 4D ‚Üí 2D
X_pca = pca.fit_transform(X_scaled)

print(f"–ù–æ–≤–∞ —Ä–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å: {X_pca.shape}")  # (150, 2)

# 4. Explained variance
print(f"\nExplained variance ratio:")
print(f"PC1: {pca.explained_variance_ratio_[0]:.2%}")
print(f"PC2: {pca.explained_variance_ratio_[1]:.2%}")
print(f"Total: {pca.explained_variance_ratio_.sum():.2%}")

# 5. –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
plt.figure(figsize=(10, 6))
scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', s=50, alpha=0.6)
plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')
plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')
plt.title('PCA of Iris Dataset')
plt.colorbar(scatter, label='Species')
plt.grid(True, alpha=0.3)
plt.show()
```

---

## 3Ô∏è‚É£ –í–∏–±—ñ—Ä –∫—ñ–ª—å–∫–æ—Å—Ç—ñ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç

### –ú–µ—Ç–æ–¥ 1: Explained Variance Ratio

```python
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# –ú–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# PCA –∑ —É—Å—ñ–º–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏
pca = PCA()
pca.fit(X_scaled)

# Explained variance
explained_var = pca.explained_variance_ratio_
cumsum_var = np.cumsum(explained_var)

print("Explained Variance –ø–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º:")
for i, (var, cum) in enumerate(zip(explained_var, cumsum_var)):
    print(f"PC{i+1}: {var:.3f} (cumulative: {cum:.3f})")

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
fig, axes = plt.subplots(1, 2, figsize=(15, 5))

# Individual variance
axes[0].bar(range(1, len(explained_var)+1), explained_var, alpha=0.7)
axes[0].set_xlabel('Principal Component')
axes[0].set_ylabel('Explained Variance Ratio')
axes[0].set_title('Explained Variance per Component')
axes[0].grid(True, alpha=0.3)

# Cumulative variance
axes[1].plot(range(1, len(cumsum_var)+1), cumsum_var, 'o-', linewidth=2)
axes[1].axhline(y=0.95, color='r', linestyle='--', label='95% threshold')
axes[1].axhline(y=0.90, color='orange', linestyle='--', label='90% threshold')
axes[1].set_xlabel('Number of Components')
axes[1].set_ylabel('Cumulative Explained Variance')
axes[1].set_title('Cumulative Explained Variance')
axes[1].legend()
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# –í–∏–±—ñ—Ä –∫—ñ–ª—å–∫–æ—Å—Ç—ñ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç –¥–ª—è 95% variance
n_components_95 = np.argmax(cumsum_var >= 0.95) + 1
print(f"\n–î–ª—è 95% variance –ø–æ—Ç—Ä—ñ–±–Ω–æ {n_components_95} –∫–æ–º–ø–æ–Ω–µ–Ω—Ç")
```

---

### –ú–µ—Ç–æ–¥ 2: Elbow Method (Scree Plot)

```python
# Scree plot
plt.figure(figsize=(10, 6))
plt.plot(range(1, len(explained_var)+1), explained_var, 'o-', linewidth=2, markersize=8)
plt.xlabel('Component Number')
plt.ylabel('Explained Variance Ratio')
plt.title('Scree Plot')
plt.grid(True, alpha=0.3)
plt.show()

# –®—É–∫–∞–π "–ª—ñ–∫–æ—Ç—å" ‚Äî —Ç–æ—á–∫—É –¥–µ –≥—Ä–∞—Ñ—ñ–∫ —Ä—ñ–∑–∫–æ –ø–∞–¥–∞—î
```

---

### –ú–µ—Ç–æ–¥ 3: –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∏–π –≤–∏–±—ñ—Ä (0.95 variance)

```python
# –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –≤–∏–±—Ä–∞—Ç–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∏ –¥–ª—è 95% variance
pca = PCA(n_components=0.95)
X_pca = pca.fit_transform(X_scaled)

print(f"–ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –≤–∏–±—Ä–∞–Ω–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç: {pca.n_components_}")
print(f"Explained variance: {pca.explained_variance_ratio_.sum():.2%}")
```

---

## 4Ô∏è‚É£ –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç

### Loadings (–≤–∫–ª–∞–¥ –æ–∑–Ω–∞–∫ —É –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∏)

```python
from sklearn.datasets import load_iris
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# –î–∞–Ω—ñ
iris = load_iris()
X = iris.data
feature_names = iris.feature_names

# –ú–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è + PCA
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

pca = PCA(n_components=2)
pca.fit(X_scaled)

# Loadings (–∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∏)
loadings = pca.components_.T * np.sqrt(pca.explained_variance_)

# DataFrame –¥–ª—è –∑—Ä—É—á–Ω–æ—Å—Ç—ñ
loadings_df = pd.DataFrame(
    loadings,
    columns=['PC1', 'PC2'],
    index=feature_names
)

print("Loadings (–≤–∫–ª–∞–¥ –æ–∑–Ω–∞–∫ —É –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∏):")
print(loadings_df)

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
fig, axes = plt.subplots(1, 2, figsize=(15, 5))

# Heatmap
sns.heatmap(loadings_df.T, annot=True, fmt='.2f', cmap='RdBu_r', 
            center=0, ax=axes[0], cbar_kws={'label': 'Loading'})
axes[0].set_title('Feature Loadings on Principal Components')

# Biplot
axes[1].scatter(loadings[:, 0], loadings[:, 1], s=100, alpha=0.7)
for i, txt in enumerate(feature_names):
    axes[1].annotate(txt, (loadings[i, 0], loadings[i, 1]), 
                    fontsize=10, ha='center')
axes[1].axhline(0, color='gray', linestyle='--', alpha=0.5)
axes[1].axvline(0, color='gray', linestyle='--', alpha=0.5)
axes[1].set_xlabel('PC1')
axes[1].set_ylabel('PC2')
axes[1].set_title('Feature Loadings Biplot')
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

---

### Biplot (–¥–∞–Ω—ñ + loadings —Ä–∞–∑–æ–º)

```python
def biplot(X_pca, loadings, labels=None, feature_names=None):
    """
    Biplot: –ø–æ–∫–∞–∑—É—î —ñ –¥–∞–Ω—ñ —ñ loadings –Ω–∞ –æ–¥–Ω–æ–º—É –≥—Ä–∞—Ñ—ñ–∫—É
    """
    plt.figure(figsize=(12, 8))
    
    # –¢–æ—á–∫–∏ –¥–∞–Ω–∏—Ö
    if labels is not None:
        scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, 
                            cmap='viridis', s=50, alpha=0.6)
        plt.colorbar(scatter, label='Class')
    else:
        plt.scatter(X_pca[:, 0], X_pca[:, 1], s=50, alpha=0.6)
    
    # –°—Ç—Ä—ñ–ª–∫–∏ –¥–ª—è –æ–∑–Ω–∞–∫
    if feature_names is not None:
        for i, feature in enumerate(feature_names):
            plt.arrow(0, 0, loadings[i, 0]*5, loadings[i, 1]*5,
                     color='red', alpha=0.5, head_width=0.1)
            plt.text(loadings[i, 0]*5.5, loadings[i, 1]*5.5, feature,
                    color='red', fontsize=10, ha='center')
    
    plt.xlabel('PC1')
    plt.ylabel('PC2')
    plt.title('PCA Biplot')
    plt.grid(True, alpha=0.3)
    plt.axhline(0, color='gray', linestyle='--', alpha=0.5)
    plt.axvline(0, color='gray', linestyle='--', alpha=0.5)
    plt.show()

# –ü—Ä–∏–∫–ª–∞–¥
biplot(X_pca, loadings, labels=y, feature_names=feature_names)
```

---

## 5Ô∏è‚É£ –ü—Ä–∞–∫—Ç–∏—á–Ω—ñ –ø—Ä–∏–∫–ª–∞–¥–∏

### –ü—Ä–∏–∫–ª–∞–¥ 1: –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è MNIST —Ü–∏—Ñ—Ä

```python
from sklearn.datasets import load_digits
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

# 1. –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ MNIST (64D: 8x8 –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è)
digits = load_digits()
X = digits.data  # (1797, 64)
y = digits.target

print(f"–û—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∞ —Ä–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å: {X.shape}")

# 2. –ú–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 3. PCA: 64D ‚Üí 2D
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

print(f"–ù–æ–≤–∞ —Ä–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å: {X_pca.shape}")
print(f"Explained variance: {pca.explained_variance_ratio_.sum():.2%}")

# 4. –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
plt.figure(figsize=(12, 8))
scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='tab10', s=30, alpha=0.6)
plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')
plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')
plt.title('PCA of MNIST Digits (64D ‚Üí 2D)')
plt.colorbar(scatter, label='Digit', ticks=range(10))
plt.grid(True, alpha=0.3)
plt.show()

# 5. –ü–æ–∫–∞–∑–∞—Ç–∏ –¥–µ–∫—ñ–ª—å–∫–∞ —Ü–∏—Ñ—Ä
fig, axes = plt.subplots(2, 5, figsize=(12, 5))
for i, ax in enumerate(axes.flat):
    ax.imshow(digits.images[i], cmap='gray')
    ax.set_title(f'Label: {y[i]}\nPC1: {X_pca[i,0]:.1f}')
    ax.axis('off')
plt.tight_layout()
plt.show()
```

---

### –ü—Ä–∏–∫–ª–∞–¥ 2: –†–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—è –∑–æ–±—Ä–∞–∂–µ–Ω—å (—à—É–º–æ–æ—á–∏—â–µ–Ω–Ω—è)

```python
from sklearn.datasets import load_digits
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# 1. –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –¥–∞–Ω—ñ
digits = load_digits()
X = digits.data / 16.0  # –Ω–æ—Ä–º–∞–ª—ñ–∑—É–≤–∞—Ç–∏ [0, 1]

# 2. –î–æ–¥–∞—Ç–∏ —à—É–º
np.random.seed(42)
noise = np.random.normal(0, 0.5, X.shape)
X_noisy = X + noise
X_noisy = np.clip(X_noisy, 0, 1)  # –æ–±–º–µ–∂–∏—Ç–∏ [0, 1]

# 3. PCA –∑ –º–∞–ª–æ—é –∫—ñ–ª—å–∫—ñ—Å—Ç—é –∫–æ–º–ø–æ–Ω–µ–Ω—Ç (—à—É–º–æ–æ—á–∏—â–µ–Ω–Ω—è)
n_components = 20  # –∑–∞–º—ñ—Å—Ç—å 64
pca = PCA(n_components=n_components)
X_compressed = pca.fit_transform(X_noisy)

# 4. –†–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—è (inverse transform)
X_restored = pca.inverse_transform(X_compressed)

# 5. –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
fig, axes = plt.subplots(3, 10, figsize=(15, 5))

for i in range(10):
    # –û—Ä–∏–≥—ñ–Ω–∞–ª
    axes[0, i].imshow(X[i].reshape(8, 8), cmap='gray')
    axes[0, i].axis('off')
    if i == 0:
        axes[0, i].set_ylabel('Original', fontsize=12)
    
    # –ó–∞—à—É–º–ª–µ–Ω–∏–π
    axes[1, i].imshow(X_noisy[i].reshape(8, 8), cmap='gray')
    axes[1, i].axis('off')
    if i == 0:
        axes[1, i].set_ylabel('Noisy', fontsize=12)
    
    # –í—ñ–¥–Ω–æ–≤–ª–µ–Ω–∏–π
    axes[2, i].imshow(X_restored[i].reshape(8, 8), cmap='gray')
    axes[2, i].axis('off')
    if i == 0:
        axes[2, i].set_ylabel(f'Restored\n({n_components} PCs)', fontsize=12)

plt.suptitle('PCA Denoising', fontsize=14)
plt.tight_layout()
plt.show()

# MSE
from sklearn.metrics import mean_squared_error
mse_noisy = mean_squared_error(X, X_noisy)
mse_restored = mean_squared_error(X, X_restored)

print(f"MSE (–æ—Ä–∏–≥—ñ–Ω–∞–ª vs –∑–∞—à—É–º–ª–µ–Ω–∏–π): {mse_noisy:.4f}")
print(f"MSE (–æ—Ä–∏–≥—ñ–Ω–∞–ª vs –≤—ñ–¥–Ω–æ–≤–ª–µ–Ω–∏–π): {mse_restored:.4f}")
print(f"–ü–æ–∫—Ä–∞—â–µ–Ω–Ω—è: {(1 - mse_restored/mse_noisy)*100:.1f}%")
```

---

### –ü—Ä–∏–∫–ª–∞–¥ 3: –ü—Ä–∏—Å–∫–æ—Ä–µ–Ω–Ω—è ML –∞–ª–≥–æ—Ä–∏—Ç–º—ñ–≤

```python
from sklearn.datasets import make_classification
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import time

# 1. –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è —Å–∫–ª–∞–¥–Ω–∏—Ö –¥–∞–Ω–∏—Ö (100 –æ–∑–Ω–∞–∫)
X, y = make_classification(
    n_samples=1000,
    n_features=100,
    n_informative=20,
    n_redundant=60,
    n_repeated=20,
    random_state=42
)

print(f"–î–∞–Ω—ñ: {X.shape}")

# 2. –ú–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 3. –ú–æ–¥–µ–ª—å –±–µ–∑ PCA
clf = RandomForestClassifier(n_estimators=100, random_state=42)

start = time.time()
score_before = cross_val_score(clf, X_scaled, y, cv=5).mean()
time_before = time.time() - start

print(f"\n=== –ë–µ–∑ PCA ===")
print(f"–¢–æ—á–Ω—ñ—Å—Ç—å: {score_before:.3f}")
print(f"–ß–∞—Å: {time_before:.2f} —Å–µ–∫")

# 4. –ó PCA (95% variance)
pca = PCA(n_components=0.95)
X_pca = pca.fit_transform(X_scaled)

print(f"\nPCA: {X.shape[1]} ‚Üí {pca.n_components_} –∫–æ–º–ø–æ–Ω–µ–Ω—Ç")
print(f"Explained variance: {pca.explained_variance_ratio_.sum():.2%}")

start = time.time()
score_after = cross_val_score(clf, X_pca, y, cv=5).mean()
time_after = time.time() - start

print(f"\n=== –ó PCA ===")
print(f"–¢–æ—á–Ω—ñ—Å—Ç—å: {score_after:.3f}")
print(f"–ß–∞—Å: {time_after:.2f} —Å–µ–∫")

# 5. –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è
print(f"\n=== –†–µ–∑—É–ª—å—Ç–∞—Ç ===")
print(f"–ó–º—ñ–Ω–∞ —Ç–æ—á–Ω–æ—Å—Ç—ñ: {(score_after - score_before):.3f}")
print(f"–ü—Ä–∏—Å–∫–æ—Ä–µ–Ω–Ω—è: {time_before/time_after:.1f}x")
print(f"–ó–º–µ–Ω—à–µ–Ω–Ω—è —Ä–æ–∑–º—ñ—Ä–Ω–æ—Å—Ç—ñ: {(1 - pca.n_components_/X.shape[1])*100:.0f}%")
```

---

### –ü—Ä–∏–∫–ª–∞–¥ 4: Eigenfaces (—Ä–æ–∑–ø—ñ–∑–Ω–∞–≤–∞–Ω–Ω—è –æ–±–ª–∏—á)

```python
from sklearn.datasets import fetch_lfw_people
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# 1. –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –¥–∞–Ω—ñ –æ–±–ª–∏—á
lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)
X = lfw_people.data
y = lfw_people.target
target_names = lfw_people.target_names

n_samples, h, w = lfw_people.images.shape
n_features = X.shape[1]

print(f"–ó–æ–±—Ä–∞–∂–µ–Ω—å: {n_samples}")
print(f"–†–æ–∑–º—ñ—Ä –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è: {h}x{w}")
print(f"–û–∑–Ω–∞–∫: {n_features}")

# 2. PCA –¥–ª—è eigenfaces
n_components = 150
pca = PCA(n_components=n_components, whiten=True, random_state=42)
X_pca = pca.fit_transform(X)

print(f"\nPCA: {n_features} ‚Üí {n_components}")
print(f"Explained variance: {pca.explained_variance_ratio_.sum():.2%}")

# 3. –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è eigenfaces (–≥–æ–ª–æ–≤–Ω–∏—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç)
eigenfaces = pca.components_.reshape((n_components, h, w))

fig, axes = plt.subplots(3, 5, figsize=(12, 8))
for i, ax in enumerate(axes.flat):
    ax.imshow(eigenfaces[i], cmap='gray')
    ax.set_title(f'Eigenface {i+1}\n{pca.explained_variance_ratio_[i]:.1%}')
    ax.axis('off')

plt.suptitle('Top 15 Eigenfaces', fontsize=14)
plt.tight_layout()
plt.show()

# 4. –†–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—è –æ–±–ª–∏—á—á—è
def reconstruct_face(idx, n_components_range):
    """–ü–æ–∫–∞–∑–∞—Ç–∏ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—é –∑ —Ä—ñ–∑–Ω–æ—é –∫—ñ–ª—å–∫—ñ—Å—Ç—é –∫–æ–º–ø–æ–Ω–µ–Ω—Ç"""
    original = lfw_people.images[idx]
    
    fig, axes = plt.subplots(1, len(n_components_range)+1, figsize=(15, 3))
    
    # –û—Ä–∏–≥—ñ–Ω–∞–ª
    axes[0].imshow(original, cmap='gray')
    axes[0].set_title('Original')
    axes[0].axis('off')
    
    # –†–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—ó
    for i, n_comp in enumerate(n_components_range):
        pca_temp = PCA(n_components=n_comp)
        X_temp = pca_temp.fit_transform(X)
        X_reconstructed = pca_temp.inverse_transform(X_temp[idx:idx+1])
        
        axes[i+1].imshow(X_reconstructed.reshape(h, w), cmap='gray')
        axes[i+1].set_title(f'{n_comp} PCs\n({pca_temp.explained_variance_ratio_.sum():.0%})')
        axes[i+1].axis('off')
    
    plt.tight_layout()
    plt.show()

# –ü—Ä–∏–∫–ª–∞–¥
reconstruct_face(0, [10, 50, 100, 150])
```

---

## 6Ô∏è‚É£ –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –∑/–±–µ–∑ PCA

```python
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import classification_report
import time

# 1. –î–∞–Ω—ñ
digits = load_digits()
X = digits.data
y = digits.target

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# 2. –ú–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# === –ë–ï–ó PCA ===
print("=== –ë–ï–ó PCA ===")
print(f"–†–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å: {X_train_scaled.shape}")

clf = LogisticRegression(max_iter=1000, random_state=42)

start = time.time()
clf.fit(X_train_scaled, y_train)
time_train_before = time.time() - start

start = time.time()
y_pred_before = clf.predict(X_test_scaled)
time_pred_before = time.time() - start

from sklearn.metrics import accuracy_score
acc_before = accuracy_score(y_test, y_pred_before)

print(f"–¢–æ—á–Ω—ñ—Å—Ç—å: {acc_before:.3f}")
print(f"–ß–∞—Å –Ω–∞–≤—á–∞–Ω–Ω—è: {time_train_before:.3f} —Å–µ–∫")
print(f"–ß–∞—Å –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è: {time_pred_before:.4f} —Å–µ–∫")

# === –ó PCA ===
print("\n=== –ó PCA (95% variance) ===")

pca = PCA(n_components=0.95, random_state=42)
X_train_pca = pca.fit_transform(X_train_scaled)
X_test_pca = pca.transform(X_test_scaled)

print(f"–†–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å: {X_train_pca.shape}")
print(f"Explained variance: {pca.explained_variance_ratio_.sum():.2%}")

clf_pca = LogisticRegression(max_iter=1000, random_state=42)

start = time.time()
clf_pca.fit(X_train_pca, y_train)
time_train_after = time.time() - start

start = time.time()
y_pred_after = clf_pca.predict(X_test_pca)
time_pred_after = time.time() - start

acc_after = accuracy_score(y_test, y_pred_after)

print(f"–¢–æ—á–Ω—ñ—Å—Ç—å: {acc_after:.3f}")
print(f"–ß–∞—Å –Ω–∞–≤—á–∞–Ω–Ω—è: {time_train_after:.3f} —Å–µ–∫")
print(f"–ß–∞—Å –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è: {time_pred_after:.4f} —Å–µ–∫")

# –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è
print("\n=== –ü–û–†–Ü–í–ù–Ø–ù–ù–Ø ===")
print(f"–ó–º—ñ–Ω–∞ —Ç–æ—á–Ω–æ—Å—Ç—ñ: {acc_after - acc_before:+.3f}")
print(f"–ü—Ä–∏—Å–∫–æ—Ä–µ–Ω–Ω—è –Ω–∞–≤—á–∞–Ω–Ω—è: {time_train_before/time_train_after:.1f}x")
print(f"–ü—Ä–∏—Å–∫–æ—Ä–µ–Ω–Ω—è –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è: {time_pred_before/time_pred_after:.1f}x")
print(f"–ó–º–µ–Ω—à–µ–Ω–Ω—è —Ä–æ–∑–º—ñ—Ä–Ω–æ—Å—Ç—ñ: {(1 - pca.n_components_/X.shape[1])*100:.0f}%")
```

---

## 7Ô∏è‚É£ –Ü–Ω—à—ñ –≤–∞—Ä—ñ–∞—Ü—ñ—ó PCA

### IncrementalPCA (–¥–ª—è –≤–µ–ª–∏–∫–∏—Ö –¥–∞–Ω–∏—Ö)

```python
from sklearn.decomposition import IncrementalPCA
import numpy as np

# –î–∞–Ω—ñ, —â–æ –Ω–µ –≤–ª–∞–∑—è—Ç—å —É –ø–∞–º'—è—Ç—å
n_samples = 10000
n_features = 1000

# IncrementalPCA –æ–±—Ä–æ–±–ª—è—î –±–∞—Ç—á–∞–º–∏
ipca = IncrementalPCA(n_components=50, batch_size=200)

# –Ü–º—ñ—Ç–∞—Ü—ñ—è –±–∞—Ç—á—ñ–≤
for i in range(0, n_samples, 200):
    X_batch = np.random.randn(200, n_features)  # –≥–µ–Ω–µ—Ä—É—î–º–æ –±–∞—Ç—á
    ipca.partial_fit(X_batch)

# –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ—è
X_new = np.random.randn(100, n_features)
X_transformed = ipca.transform(X_new)

print(f"Explained variance: {ipca.explained_variance_ratio_.sum():.2%}")
```

---

### KernelPCA (–Ω–µ–ª—ñ–Ω—ñ–π–Ω–∏–π)

```python
from sklearn.decomposition import KernelPCA
from sklearn.datasets import make_moons

# –ù–µ–ª—ñ–Ω—ñ–π–Ω—ñ –¥–∞–Ω—ñ
X, y = make_moons(n_samples=300, noise=0.05, random_state=42)

# –ó–≤–∏—á–∞–π–Ω–∏–π PCA (–ª—ñ–Ω—ñ–π–Ω–∏–π)
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

# Kernel PCA (–Ω–µ–ª—ñ–Ω—ñ–π–Ω–∏–π)
kpca = KernelPCA(n_components=2, kernel='rbf', gamma=10)
X_kpca = kpca.fit_transform(X)

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# –û—Ä–∏–≥—ñ–Ω–∞–ª
axes[0].scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', s=50)
axes[0].set_title('Original Data')
axes[0].grid(True, alpha=0.3)

# PCA
axes[1].scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', s=50)
axes[1].set_title('Linear PCA')
axes[1].grid(True, alpha=0.3)

# Kernel PCA
axes[2].scatter(X_kpca[:, 0], X_kpca[:, 1], c=y, cmap='viridis', s=50)
axes[2].set_title('Kernel PCA (RBF)')
axes[2].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

---

## 8Ô∏è‚É£ PCA –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—ó

```python
from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score

# 1. –î–∞–Ω—ñ (–≤–∏—Å–æ–∫–æ–≤–∏–º—ñ—Ä–Ω—ñ)
X, y_true = make_blobs(n_samples=300, n_features=50, centers=3, random_state=42)

# 2. –ú–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# === –ë–ï–ó PCA ===
kmeans_before = KMeans(n_clusters=3, random_state=42)
labels_before = kmeans_before.fit_predict(X_scaled)
sil_before = silhouette_score(X_scaled, labels_before)

print("=== –ë–ï–ó PCA ===")
print(f"–†–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å: {X_scaled.shape}")
print(f"Silhouette: {sil_before:.3f}")

# === –ó PCA ===
pca = PCA(n_components=0.95)
X_pca = pca.fit_transform(X_scaled)

kmeans_after = KMeans(n_clusters=3, random_state=42)
labels_after = kmeans_after.fit_predict(X_pca)
sil_after = silhouette_score(X_pca, labels_after)

print(f"\n=== –ó PCA ===")
print(f"–†–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å: {X_pca.shape}")
print(f"–ö–æ–º–ø–æ–Ω–µ–Ω—Ç: {pca.n_components_}")
print(f"Explained variance: {pca.explained_variance_ratio_.sum():.2%}")
print(f"Silhouette: {sil_after:.3f}")

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è (–ø—Ä–æ–µ–∫—Ü—ñ—è –Ω–∞ 2D)
pca_viz = PCA(n_components=2)
X_viz = pca_viz.fit_transform(X_scaled)

fig, axes = plt.subplots(1, 2, figsize=(15, 6))

axes[0].scatter(X_viz[:, 0], X_viz[:, 1], c=labels_before, cmap='viridis', s=50)
axes[0].set_title(f'K-Means –±–µ–∑ PCA\nSilhouette: {sil_before:.3f}')

axes[1].scatter(X_viz[:, 0], X_viz[:, 1], c=labels_after, cmap='viridis', s=50)
axes[1].set_title(f'K-Means –∑ PCA\nSilhouette: {sil_after:.3f}')

plt.tight_layout()
plt.show()
```

---

## 9Ô∏è‚É£ –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ç–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è

```python
import joblib
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# 1. –ù–∞–≤—á–∞–Ω–Ω—è
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

pca = PCA(n_components=0.95)
X_pca = pca.fit_transform(X_scaled)

# 2. –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è
model_data = {
    'pca': pca,
    'scaler': scaler,
    'n_components': pca.n_components_,
    'explained_variance': pca.explained_variance_ratio_.sum()
}

joblib.dump(model_data, 'pca_model.pkl')

# 3. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è
loaded_data = joblib.load('pca_model.pkl')
loaded_pca = loaded_data['pca']
loaded_scaler = loaded_data['scaler']

print(f"–ö–æ–º–ø–æ–Ω–µ–Ω—Ç: {loaded_data['n_components']}")
print(f"Explained variance: {loaded_data['explained_variance']:.2%}")

# 4. –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –Ω–∞ –Ω–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö
X_new = np.random.randn(10, X.shape[1])
X_new_scaled = loaded_scaler.transform(X_new)
X_new_pca = loaded_pca.transform(X_new_scaled)

print(f"\n–ù–æ–≤—ñ –¥–∞–Ω—ñ: {X_new.shape} ‚Üí {X_new_pca.shape}")
```

---

## üîü –ü–æ—Ä–∞–¥–∏ —Ç–∞ best practices

### 1. –ó–∞–≤–∂–¥–∏ –º–∞—Å—à—Ç–∞–±—É–π –¥–∞–Ω—ñ –ø–µ—Ä–µ–¥ PCA

```python
# –ü–û–ì–ê–ù–û (—Ä—ñ–∑–Ω–∏–π –º–∞—Å—à—Ç–∞–± –æ–∑–Ω–∞–∫)
pca = PCA(n_components=2)
pca.fit(X)

# –î–û–ë–†–ï
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
pca = PCA(n_components=2)
pca.fit(X_scaled)
```

---

### 2. –ü–µ—Ä–µ–≤—ñ—Ä—è–π explained variance

```python
# –°–∫—ñ–ª—å–∫–∏ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó –∑–±–µ—Ä—ñ–≥–∞—î–º–æ?
pca = PCA()
pca.fit(X_scaled)

cumsum = np.cumsum(pca.explained_variance_ratio_)
print(f"95% variance at component: {np.argmax(cumsum >= 0.95) + 1}")
```

---

### 3. PCA –¥–ª—è –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó vs –¥–ª—è ML

```python
# –î–ª—è –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó: 2-3 –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∏ (–∑–∞–≤–∂–¥–∏)
pca_viz = PCA(n_components=2)
X_viz = pca_viz.fit_transform(X_scaled)
plt.scatter(X_viz[:, 0], X_viz[:, 1])

# –î–ª—è ML: –∑–±–µ—Ä–µ–≥—Ç–∏ 95% variance
pca_ml = PCA(n_components=0.95)
X_ml = pca_ml.fit_transform(X_scaled)
```

---

### 4. –ù–µ –∑–∞–≤–∂–¥–∏ –ø–æ–∫—Ä–∞—â—É—î ML

```python
# –ó–ê–í–ñ–î–ò –ø–æ—Ä—ñ–≤–Ω—é–π –∑ baseline (–±–µ–∑ PCA)

# Baseline
score_before = cross_val_score(model, X_scaled, y, cv=5).mean()

# –ó PCA
X_pca = pca.fit_transform(X_scaled)
score_after = cross_val_score(model, X_pca, y, cv=5).mean()

if score_after < score_before:
    print("‚ö†Ô∏è PCA –ø–æ–≥—ñ—Ä—à–∏–ª–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç!")
```

---

### 5. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π whiten –¥–ª—è –¥–µ—è–∫–∏—Ö –∞–ª–≥–æ—Ä–∏—Ç–º—ñ–≤

```python
# whiten=True –Ω–æ—Ä–º–∞–ª—ñ–∑—É—î –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∏ –¥–æ –æ–¥–∏–Ω–∏—á–Ω–æ—ó variance
# –ö–æ—Ä–∏—Å–Ω–æ –¥–ª—è neural networks, SVM

pca = PCA(n_components=50, whiten=True)
X_pca = pca.fit_transform(X_scaled)
```

---

### 6. IncrementalPCA –¥–ª—è –≤–µ–ª–∏–∫–∏—Ö –¥–∞–Ω–∏—Ö

```python
# –Ø–∫—â–æ –¥–∞–Ω—ñ –Ω–µ –≤–ª–∞–∑—è—Ç—å —É –ø–∞–º'—è—Ç—å
if X.shape[0] > 10000:
    from sklearn.decomposition import IncrementalPCA
    ipca = IncrementalPCA(n_components=50, batch_size=1000)
    
    # –û–±—Ä–æ–±–ª—è—î–º–æ –±–∞—Ç—á–∞–º–∏
    for i in range(0, len(X), 1000):
        batch = X[i:i+1000]
        ipca.partial_fit(batch)
```

---

## –ß–µ–∫-–ª–∏—Å—Ç –¥–ª—è PCA

```python
# ‚úÖ 1. –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –¥–∞–Ω—ñ
X = load_data()

# ‚úÖ 2. EDA
print(X.shape)
print(pd.DataFrame(X).describe())

# ‚úÖ 3. –ú–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è (–û–ë–û–í'–Ø–ó–ö–û–í–û!)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# ‚úÖ 4. –í–∏–∑–Ω–∞—á–∏—Ç–∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω—É –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç
pca_temp = PCA()
pca_temp.fit(X_scaled)

cumsum = np.cumsum(pca_temp.explained_variance_ratio_)
n_components_95 = np.argmax(cumsum >= 0.95) + 1
print(f"–î–ª—è 95%: {n_components_95} –∫–æ–º–ø–æ–Ω–µ–Ω—Ç")

# ‚úÖ 5. –ù–∞–≤—á–∞–Ω–Ω—è PCA
pca = PCA(n_components=0.95)  # –∞–±–æ n_components_95
X_pca = pca.fit_transform(X_scaled)

# ‚úÖ 6. –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞
print(f"–†–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å: {X.shape} ‚Üí {X_pca.shape}")
print(f"Explained variance: {pca.explained_variance_ratio_.sum():.2%}")

# ‚úÖ 7. –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è (—è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ)
if pca.n_components_ >= 2:
    plt.scatter(X_pca[:, 0], X_pca[:, 1])
    plt.show()

# ‚úÖ 8. –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è ML –∑/–±–µ–∑ PCA
score_before = evaluate_model(X_scaled, y)
score_after = evaluate_model(X_pca, y)
print(f"–ë–µ–∑ PCA: {score_before:.3f}")
print(f"–ó PCA: {score_after:.3f}")

# ‚úÖ 9. –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è
joblib.dump({'pca': pca, 'scaler': scaler}, 'pca_model.pkl')
```

---

## –ö–æ—Ä–∏—Å–Ω—ñ –ø–æ—Å–∏–ª–∞–Ω–Ω—è

- [sklearn PCA docs](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)
- [sklearn IncrementalPCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.IncrementalPCA.html)
- [sklearn KernelPCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html)
- [Decomposition Guide](https://scikit-learn.org/stable/modules/decomposition.html)

---

**–°—Ç–≤–æ—Ä–µ–Ω–æ –¥–ª—è –ø—Ä–∞–∫—Ç–∏—á–Ω–æ–≥–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è PCA –≤ –ø—Ä–æ—î–∫—Ç–∞—Ö** üöÄ
