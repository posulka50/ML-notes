# Gaussian Mixture Models ‚Äî sklearn –ø—Ä–∞–∫—Ç–∏–∫–∞

–ü–æ–≤–Ω–∏–π –ø—Ä–∞–∫—Ç–∏—á–Ω–∏–π –≥–∞–π–¥ –ø–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—é GMM –≤ scikit-learn –∑ –ø—Ä–∏–∫–ª–∞–¥–∞–º–∏ –∫–æ–¥—É.

---

## üì¶ –û—Å–Ω–æ–≤–Ω—ñ —ñ–º–ø–æ—Ä—Ç–∏

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.patches import Ellipse

# GMM
from sklearn.mixture import GaussianMixture, BayesianGaussianMixture

# Metrics
from sklearn.metrics import (
    silhouette_score,
    davies_bouldin_score,
    calinski_harabasz_score,
    adjusted_rand_score
)

# Preprocessing
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# Data
from sklearn.datasets import make_blobs, load_iris, make_moons
```

---

## 1Ô∏è‚É£ GaussianMixture ‚Äî –æ—Å–Ω–æ–≤–Ω–∏–π –∫–ª–∞—Å

### –ü–∞—Ä–∞–º–µ—Ç—Ä–∏

```python
GaussianMixture(
    n_components=1,            # –ö—ñ–ª—å–∫—ñ—Å—Ç—å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç (–∫–ª–∞—Å—Ç–µ—Ä—ñ–≤)
    covariance_type='full',    # 'full', 'tied', 'diag', 'spherical'
    tol=1e-3,                  # –¢–æ–ª–µ—Ä–∞–Ω—Ç–Ω—ñ—Å—Ç—å –¥–ª—è –∑–±—ñ–∂–Ω–æ—Å—Ç—ñ EM
    reg_covar=1e-6,            # Regularization –¥–ª—è —Å—Ç–∞–±—ñ–ª—å–Ω–æ—Å—Ç—ñ
    max_iter=100,              # –ú–∞–∫—Å–∏–º—É–º —ñ—Ç–µ—Ä–∞—Ü—ñ–π EM
    n_init=1,                  # –ö—ñ–ª—å–∫—ñ—Å—Ç—å —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ–π
    init_params='kmeans',      # 'kmeans', 'k-means++', 'random', 'random_from_data'
    weights_init=None,         # –ü–æ—á–∞—Ç–∫–æ–≤—ñ –≤–∞–≥–∏ (œÄ‚Çñ)
    means_init=None,           # –ü–æ—á–∞—Ç–∫–æ–≤—ñ —Ü–µ–Ω—Ç—Ä–∏ (Œº‚Çñ)
    precisions_init=None,      # –ü–æ—á–∞—Ç–∫–æ–≤—ñ —Ç–æ—á–Ω–æ—Å—Ç—ñ (Œ£‚Çñ‚Åª¬π)
    random_state=None,         # Seed –¥–ª—è –≤—ñ–¥—Ç–≤–æ—Ä—é–≤–∞–Ω–æ—Å—Ç—ñ
    warm_start=False,          # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏
    verbose=0,                 # –í–∏–≤–æ–¥–∏—Ç–∏ –ø—Ä–æ–≥—Ä–µ—Å
    verbose_interval=10        # –ß–∞—Å—Ç–æ—Ç–∞ –≤–∏–≤–æ–¥—É
)
```

---

### –ê—Ç—Ä–∏–±—É—Ç–∏ –ø—ñ—Å–ª—è fit

```python
gmm = GaussianMixture(n_components=3)
gmm.fit(X)

# –î–æ—Å—Ç—É–ø–Ω—ñ –∞—Ç—Ä–∏–±—É—Ç–∏:
gmm.weights_              # –í–∞–≥–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç œÄ‚Çñ (n_components,)
gmm.means_                # –¶–µ–Ω—Ç—Ä–∏ Œº‚Çñ (n_components, n_features)
gmm.covariances_          # –ö–æ–≤–∞—Ä—ñ–∞—Ü—ñ–π–Ω—ñ –º–∞—Ç—Ä–∏—Ü—ñ Œ£‚Çñ (–∑–∞–ª–µ–∂–∏—Ç—å –≤—ñ–¥ —Ç–∏–ø—É)
gmm.precisions_           # –ó–≤–æ—Ä–æ—Ç–Ω—ñ –∫–æ–≤–∞—Ä—ñ–∞—Ü—ñ—ó Œ£‚Çñ‚Åª¬π
gmm.precisions_cholesky_  # Cholesky decomposition –¥–ª—è —à–≤–∏–¥–∫–æ—Å—Ç—ñ
gmm.converged_            # –ß–∏ –∑—ñ–π—à–æ–≤—Å—è EM –∞–ª–≥–æ—Ä–∏—Ç–º
gmm.n_iter_               # –ö—ñ–ª—å–∫—ñ—Å—Ç—å —ñ—Ç–µ—Ä–∞—Ü—ñ–π –¥–æ –∑–±—ñ–∂–Ω–æ—Å—Ç—ñ
gmm.lower_bound_          # Log-likelihood –Ω–∞ –æ—Å—Ç–∞–Ω–Ω—ñ–π —ñ—Ç–µ—Ä–∞—Ü—ñ—ó
```

---

### –ú–µ—Ç–æ–¥–∏

```python
# –ù–∞–≤—á–∞–Ω–Ω—è
gmm.fit(X)

# –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –º—ñ—Ç–æ–∫ (hard clustering)
labels = gmm.predict(X)

# –ù–∞–≤—á–∞–Ω–Ω—è + –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
labels = gmm.fit_predict(X)

# –ô–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ –Ω–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ (soft clustering)
probs = gmm.predict_proba(X)  # shape: (n_samples, n_components)

# Log-probability –¥–ª—è –∫–æ–∂–Ω–æ—ó —Ç–æ—á–∫–∏
log_probs = gmm.score_samples(X)  # shape: (n_samples,)

# –°–µ—Ä–µ–¥–Ω—è log-likelihood (–¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –º–æ–¥–µ–ª–µ–π)
avg_log_likelihood = gmm.score(X)

# BIC
bic = gmm.bic(X)

# AIC
aic = gmm.aic(X)

# –°–µ–º–ø–ª—é–≤–∞–Ω–Ω—è –Ω–æ–≤–∏—Ö —Ç–æ—á–æ–∫
new_samples, new_labels = gmm.sample(n_samples=100)
```

---

## 2Ô∏è‚É£ –ë–∞–∑–æ–≤–∏–π –ø—Ä–∏–∫–ª–∞–¥

```python
import numpy as np
from sklearn.mixture import GaussianMixture
from sklearn.datasets import make_blobs
import matplotlib.pyplot as plt

# 1. –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –¥–∞–Ω–∏—Ö
X, y_true = make_blobs(
    n_samples=300, 
    centers=3, 
    cluster_std=[1.0, 1.5, 0.5],  # —Ä—ñ–∑–Ω–∞ –¥–∏—Å–ø–µ—Ä—Å—ñ—è
    random_state=42
)

# 2. GMM
gmm = GaussianMixture(n_components=3, covariance_type='full', random_state=42)
gmm.fit(X)

# 3. –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
labels = gmm.predict(X)
probs = gmm.predict_proba(X)

# 4. –†–µ–∑—É–ª—å—Ç–∞—Ç–∏
print(f"–ó—ñ–π—à–æ–≤—Å—è: {gmm.converged_}")
print(f"–Ü—Ç–µ—Ä–∞—Ü—ñ–π: {gmm.n_iter_}")
print(f"Log-likelihood: {gmm.lower_bound_:.2f}")
print(f"BIC: {gmm.bic(X):.2f}")
print(f"AIC: {gmm.aic(X):.2f}")

print(f"\n–í–∞–≥–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç: {gmm.weights_}")
print(f"\n–¶–µ–Ω—Ç—Ä–∏:\n{gmm.means_}")

# 5. –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
plt.figure(figsize=(12, 5))

# Hard clustering
plt.subplot(1, 2, 1)
plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=50, alpha=0.6)
plt.scatter(gmm.means_[:, 0], gmm.means_[:, 1], 
            c='red', marker='X', s=200, edgecolors='black', linewidths=2,
            label='Centers')
plt.title('GMM - Hard Clustering')
plt.legend()
plt.grid(True, alpha=0.3)

# Soft clustering (–ø–æ–∫–∞–∑–∞—Ç–∏ –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ –¥–ª—è –ø–µ—Ä—à–æ–≥–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞)
plt.subplot(1, 2, 2)
plt.scatter(X[:, 0], X[:, 1], c=probs[:, 0], cmap='viridis', s=50, alpha=0.6)
plt.colorbar(label='P(component 0)')
plt.title('GMM - Soft Clustering (Component 0 probability)')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

---

## 3Ô∏è‚É£ –í–∏–±—ñ—Ä –∫—ñ–ª—å–∫–æ—Å—Ç—ñ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç (K)

### –ú–µ—Ç–æ–¥ 1: BIC (—Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–æ)

```python
from sklearn.mixture import GaussianMixture
import matplotlib.pyplot as plt

# –¢–µ—Å—Ç—É—î–º–æ —Ä—ñ–∑–Ω—ñ K
K_range = range(1, 11)
bic_scores = []
aic_scores = []

for k in K_range:
    gmm = GaussianMixture(n_components=k, covariance_type='full', random_state=42)
    gmm.fit(X)
    bic_scores.append(gmm.bic(X))
    aic_scores.append(gmm.aic(X))

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
plt.figure(figsize=(12, 5))

# BIC
plt.subplot(1, 2, 1)
plt.plot(K_range, bic_scores, 'o-', linewidth=2, markersize=8)
plt.xlabel('Number of components (K)')
plt.ylabel('BIC')
plt.title('BIC Score (lower is better)')
plt.grid(True, alpha=0.3)

# –û–ø—Ç–∏–º–∞–ª—å–Ω–∏–π K
optimal_k_bic = K_range[np.argmin(bic_scores)]
plt.axvline(x=optimal_k_bic, color='r', linestyle='--', 
            label=f'Optimal K={optimal_k_bic}')
plt.legend()

# AIC
plt.subplot(1, 2, 2)
plt.plot(K_range, aic_scores, 'o-', linewidth=2, markersize=8, color='orange')
plt.xlabel('Number of components (K)')
plt.ylabel('AIC')
plt.title('AIC Score (lower is better)')
plt.grid(True, alpha=0.3)

optimal_k_aic = K_range[np.argmin(aic_scores)]
plt.axvline(x=optimal_k_aic, color='r', linestyle='--',
            label=f'Optimal K={optimal_k_aic}')
plt.legend()

plt.tight_layout()
plt.show()

print(f"Optimal K (BIC): {optimal_k_bic}")
print(f"Optimal K (AIC): {optimal_k_aic}")
```

---

### –ú–µ—Ç–æ–¥ 2: –ö–æ–º–±—ñ–Ω–æ–≤–∞–Ω–∏–π (BIC + Silhouette)

```python
from sklearn.metrics import silhouette_score

K_range = range(2, 11)
results = []

for k in K_range:
    gmm = GaussianMixture(n_components=k, covariance_type='full', random_state=42)
    labels = gmm.fit_predict(X)
    
    results.append({
        'K': k,
        'BIC': gmm.bic(X),
        'AIC': gmm.aic(X),
        'Silhouette': silhouette_score(X, labels),
        'Log-likelihood': gmm.score(X)
    })

df_results = pd.DataFrame(results)
print(df_results)

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –≤—Å—ñ—Ö –º–µ—Ç—Ä–∏–∫
fig, axes = plt.subplots(2, 2, figsize=(15, 10))

# BIC
axes[0, 0].plot(df_results['K'], df_results['BIC'], 'o-')
axes[0, 0].set_title('BIC (min)')
axes[0, 0].set_xlabel('K')
axes[0, 0].grid(True, alpha=0.3)

# AIC
axes[0, 1].plot(df_results['K'], df_results['AIC'], 'o-', color='orange')
axes[0, 1].set_title('AIC (min)')
axes[0, 1].set_xlabel('K')
axes[0, 1].grid(True, alpha=0.3)

# Silhouette
axes[1, 0].plot(df_results['K'], df_results['Silhouette'], 'o-', color='green')
axes[1, 0].set_title('Silhouette Score (max)')
axes[1, 0].set_xlabel('K')
axes[1, 0].grid(True, alpha=0.3)

# Log-likelihood
axes[1, 1].plot(df_results['K'], df_results['Log-likelihood'], 'o-', color='purple')
axes[1, 1].set_title('Log-likelihood (max)')
axes[1, 1].set_xlabel('K')
axes[1, 1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

---

## 4Ô∏è‚É£ –¢–∏–ø–∏ –∫–æ–≤–∞—Ä—ñ–∞—Ü—ñ–π–Ω–∏—Ö –º–∞—Ç—Ä–∏—Ü—å

### –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –≤—Å—ñ—Ö —Ç–∏–ø—ñ–≤

```python
from sklearn.datasets import make_blobs

# –î–∞–Ω—ñ –∑ —Ä—ñ–∑–Ω–∏–º–∏ —Ñ–æ—Ä–º–∞–º–∏ –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤
X, _ = make_blobs(n_samples=300, centers=3, cluster_std=1.0, random_state=42)

# –î–æ–¥–∞–º–æ –∫–æ—Ä–µ–ª—è—Ü—ñ—é (—Ä–æ–∑—Ç—è–≥–Ω–µ–º–æ –¥–∞–Ω—ñ)
transformation = np.array([[2, 1], [0, 1]])
X_transformed = X @ transformation.T

covariance_types = ['spherical', 'diag', 'tied', 'full']

fig, axes = plt.subplots(2, 2, figsize=(15, 12))
axes = axes.ravel()

for idx, cov_type in enumerate(covariance_types):
    gmm = GaussianMixture(n_components=3, covariance_type=cov_type, random_state=42)
    labels = gmm.fit_predict(X_transformed)
    
    # –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
    axes[idx].scatter(X_transformed[:, 0], X_transformed[:, 1], 
                     c=labels, cmap='viridis', s=50, alpha=0.6)
    axes[idx].scatter(gmm.means_[:, 0], gmm.means_[:, 1],
                     c='red', marker='X', s=200, edgecolors='black', linewidths=2)
    
    # BIC
    bic = gmm.bic(X_transformed)
    axes[idx].set_title(f'{cov_type.capitalize()} (BIC={bic:.1f})')
    axes[idx].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

---

### –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –µ–ª—ñ–ø—Å—ñ–≤ (—Ñ–æ—Ä–º–∞ –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤)

```python
from matplotlib.patches import Ellipse
import matplotlib.pyplot as plt

def plot_gmm_ellipses(gmm, X, ax=None):
    """–í—ñ–∑—É–∞–ª—ñ–∑—É—î GMM –∫–ª–∞—Å—Ç–µ—Ä–∏ –∑ –µ–ª—ñ–ø—Å–∞–º–∏ –∫–æ–≤–∞—Ä—ñ–∞—Ü—ñ—ó"""
    if ax is None:
        ax = plt.gca()
    
    labels = gmm.predict(X)
    ax.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis', alpha=0.6)
    
    # –ï–ª—ñ–ø—Å–∏ –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞
    for i in range(gmm.n_components):
        if gmm.covariance_type == 'full':
            covariance = gmm.covariances_[i][:2, :2]
        elif gmm.covariance_type == 'tied':
            covariance = gmm.covariances_[:2, :2]
        elif gmm.covariance_type == 'diag':
            covariance = np.diag(gmm.covariances_[i][:2])
        elif gmm.covariance_type == 'spherical':
            covariance = np.eye(2) * gmm.covariances_[i]
        
        # –í–ª–∞—Å–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è —Ç–∞ –≤–µ–∫—Ç–æ—Ä–∏
        v, w = np.linalg.eigh(covariance)
        v = 2.0 * np.sqrt(2.0) * np.sqrt(v)  # 2 std dev
        u = w[0] / np.linalg.norm(w[0])
        
        # –ö—É—Ç –ø–æ–≤–æ—Ä–æ—Ç—É
        angle = np.arctan(u[1] / u[0])
        angle = 180.0 * angle / np.pi
        
        # –ï–ª—ñ–ø—Å
        ell = Ellipse(gmm.means_[i, :2], v[0], v[1], angle=180.0 + angle,
                     edgecolor='black', facecolor='none', linewidth=2)
        ax.add_patch(ell)
        
        ax.scatter(gmm.means_[i, 0], gmm.means_[i, 1], 
                  c='red', marker='X', s=200, edgecolors='black', linewidths=2)
    
    ax.set_xlabel('Feature 1')
    ax.set_ylabel('Feature 2')
    ax.grid(True, alpha=0.3)

# –ü—Ä–∏–∫–ª–∞–¥
gmm = GaussianMixture(n_components=3, covariance_type='full', random_state=42)
gmm.fit(X_transformed)

plt.figure(figsize=(10, 8))
plot_gmm_ellipses(gmm, X_transformed)
plt.title('GMM with Covariance Ellipses')
plt.show()
```

---

## 5Ô∏è‚É£ Soft vs Hard Clustering

```python
from sklearn.mixture import GaussianMixture

# GMM
gmm = GaussianMixture(n_components=3, random_state=42)
gmm.fit(X)

# Hard clustering (—è–∫ K-Means)
hard_labels = gmm.predict(X)

# Soft clustering (–π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ)
soft_probs = gmm.predict_proba(X)

# –ü—Ä–∏–∫–ª–∞–¥ –¥–ª—è –ø–µ—Ä—à–æ—ó —Ç–æ—á–∫–∏
print(f"–¢–æ—á–∫–∞ x‚ÇÅ = {X[0]}")
print(f"Hard label: {hard_labels[0]}")
print(f"Soft probabilities: {soft_probs[0]}")
print(f"  - Component 0: {soft_probs[0, 0]:.2%}")
print(f"  - Component 1: {soft_probs[0, 1]:.2%}")
print(f"  - Component 2: {soft_probs[0, 2]:.2%}")

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# Hard clustering
axes[0].scatter(X[:, 0], X[:, 1], c=hard_labels, cmap='viridis', s=50)
axes[0].set_title('Hard Clustering (predict)')

# Soft clustering - Component 0
axes[1].scatter(X[:, 0], X[:, 1], c=soft_probs[:, 0], 
               cmap='viridis', s=50, vmin=0, vmax=1)
axes[1].set_title('Soft Clustering - P(Component 0)')
plt.colorbar(axes[1].collections[0], ax=axes[1], label='Probability')

# Soft clustering - uncertainty (entropy)
from scipy.stats import entropy
uncertainties = entropy(soft_probs.T)
axes[2].scatter(X[:, 0], X[:, 1], c=uncertainties, cmap='Reds', s=50)
axes[2].set_title('Uncertainty (higher = more uncertain)')
plt.colorbar(axes[2].collections[0], ax=axes[2], label='Entropy')

plt.tight_layout()
plt.show()
```

---

## 6Ô∏è‚É£ –ü—Ä–∞–∫—Ç–∏—á–Ω—ñ –ø—Ä–∏–∫–ª–∞–¥–∏

### –ü—Ä–∏–∫–ª–∞–¥ 1: –°–µ–≥–º–µ–Ω—Ç–∞—Ü—ñ—è –∫–ª—ñ—î–Ω—Ç—ñ–≤ –∑ –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—è–º–∏

```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.mixture import GaussianMixture

# 1. –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –¥–∞–Ω–∏—Ö –∫–ª—ñ—î–Ω—Ç—ñ–≤
np.random.seed(42)
n_customers = 300

data = {
    'Age': np.random.randint(18, 70, n_customers),
    'Income': np.random.randint(20000, 150000, n_customers),
    'SpendingScore': np.random.randint(1, 100, n_customers),
}

df = pd.DataFrame(data)
print(df.head())

# 2. Preprocessing
scaler = StandardScaler()
X_scaled = scaler.fit_transform(df)

# 3. –í–∏–±—ñ—Ä K —á–µ—Ä–µ–∑ BIC
bic_scores = []
K_range = range(1, 10)

for k in K_range:
    gmm = GaussianMixture(n_components=k, random_state=42)
    gmm.fit(X_scaled)
    bic_scores.append(gmm.bic(X_scaled))

optimal_k = K_range[np.argmin(bic_scores)]
print(f"\nOptimal K (BIC): {optimal_k}")

# 4. –§—ñ–Ω–∞–ª—å–Ω–∞ –º–æ–¥–µ–ª—å
gmm = GaussianMixture(n_components=optimal_k, covariance_type='full', random_state=42)
df['Cluster'] = gmm.fit_predict(X_scaled)

# 5. –î–æ–¥–∞—Ç–∏ –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ
probs = gmm.predict_proba(X_scaled)
for i in range(optimal_k):
    df[f'Prob_Cluster_{i}'] = probs[:, i]

# 6. –ê–Ω–∞–ª—ñ–∑ –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤
print("\n=== Cluster Analysis ===")
cluster_summary = df.groupby('Cluster').agg({
    'Age': ['mean', 'std'],
    'Income': ['mean', 'std'],
    'SpendingScore': ['mean', 'std']
}).round(2)
print(cluster_summary)

print("\nCluster sizes:")
print(df['Cluster'].value_counts().sort_index())

# 7. –ó–Ω–∞–π—Ç–∏ "–Ω–µ–≤–ø–µ–≤–Ω–µ–Ω—ñ" —Ç–æ—á–∫–∏ (high entropy)
from scipy.stats import entropy
df['Uncertainty'] = entropy(probs.T)

print("\n=== Top 5 most uncertain customers ===")
uncertain_customers = df.nlargest(5, 'Uncertainty')[['Age', 'Income', 'SpendingScore', 'Cluster', 'Uncertainty']]
print(uncertain_customers)

# 8. –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
fig, axes = plt.subplots(1, 2, figsize=(15, 6))

# Income vs Spending Score
axes[0].scatter(df['Income'], df['SpendingScore'], 
                c=df['Cluster'], cmap='viridis', s=50, alpha=0.6)
axes[0].set_xlabel('Income')
axes[0].set_ylabel('Spending Score')
axes[0].set_title('Customer Segmentation')
axes[0].grid(True, alpha=0.3)

# Uncertainty
axes[1].scatter(df['Income'], df['SpendingScore'],
                c=df['Uncertainty'], cmap='Reds', s=50, alpha=0.6)
axes[1].set_xlabel('Income')
axes[1].set_ylabel('Spending Score')
axes[1].set_title('Uncertainty (red = uncertain)')
plt.colorbar(axes[1].collections[0], ax=axes[1], label='Entropy')
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

---

### –ü—Ä–∏–∫–ª–∞–¥ 2: –í–∏—è–≤–ª–µ–Ω–Ω—è –∞–Ω–æ–º–∞–ª—ñ–π —á–µ—Ä–µ–∑ GMM

```python
from sklearn.mixture import GaussianMixture
import numpy as np

# 1. –î–∞–Ω—ñ: –Ω–æ—Ä–º–∞–ª—å–Ω—ñ —Ç–æ—á–∫–∏ + –∞–Ω–æ–º–∞–ª—ñ—ó
np.random.seed(42)

# –ù–æ—Ä–º–∞–ª—å–Ω—ñ —Ç–æ—á–∫–∏ (3 –∫–ª–∞—Å—Ç–µ—Ä–∏)
X_normal_1 = np.random.randn(100, 2) * 0.5 + [0, 0]
X_normal_2 = np.random.randn(100, 2) * 0.5 + [3, 3]
X_normal_3 = np.random.randn(80, 2) * 0.5 + [0, 3]

# –ê–Ω–æ–º–∞–ª—ñ—ó
X_anomalies = np.random.uniform(low=-2, high=5, size=(20, 2))

X = np.vstack([X_normal_1, X_normal_2, X_normal_3, X_anomalies])
y_true = np.array([0]*100 + [1]*100 + [2]*80 + [3]*20)  # 3 = anomaly

# 2. GMM
gmm = GaussianMixture(n_components=3, covariance_type='full', random_state=42)
gmm.fit(X)

# 3. –û–±—á–∏—Å–ª–∏—Ç–∏ log-probability –¥–ª—è –∫–æ–∂–Ω–æ—ó —Ç–æ—á–∫–∏
log_probs = gmm.score_samples(X)

# 4. –í–∏—è–≤–ª–µ–Ω–Ω—è –∞–Ω–æ–º–∞–ª—ñ–π (–Ω–∏–∑—å–∫–∞ –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å)
threshold = np.percentile(log_probs, 5)  # –Ω–∏–∂–Ω—ñ 5%
anomalies_pred = log_probs < threshold

print(f"Threshold: {threshold:.2f}")
print(f"–í–∏—è–≤–ª–µ–Ω–æ –∞–Ω–æ–º–∞–ª—ñ–π: {anomalies_pred.sum()}")
print(f"–°–ø—Ä–∞–≤–∂–Ω—ñ—Ö –∞–Ω–æ–º–∞–ª—ñ–π: {(y_true == 3).sum()}")

# 5. –ú–µ—Ç—Ä–∏–∫–∏
from sklearn.metrics import classification_report

y_pred = (log_probs < threshold).astype(int)
y_true_binary = (y_true == 3).astype(int)

print("\n=== Classification Report ===")
print(classification_report(y_true_binary, y_pred, 
                           target_names=['Normal', 'Anomaly']))

# 6. –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
plt.figure(figsize=(15, 5))

# –°–ø—Ä–∞–≤–∂–Ω—ñ –º—ñ—Ç–∫–∏
plt.subplot(1, 3, 1)
plt.scatter(X[y_true != 3, 0], X[y_true != 3, 1], c='blue', s=50, alpha=0.6, label='Normal')
plt.scatter(X[y_true == 3, 0], X[y_true == 3, 1], c='red', marker='x', s=100, label='True Anomalies')
plt.title('True Labels')
plt.legend()
plt.grid(True, alpha=0.3)

# GMM –∫–ª–∞—Å—Ç–µ—Ä–∏
plt.subplot(1, 3, 2)
labels = gmm.predict(X)
plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=50, alpha=0.6)
plt.title('GMM Clusters')
plt.grid(True, alpha=0.3)

# –í–∏—è–≤–ª–µ–Ω—ñ –∞–Ω–æ–º–∞–ª—ñ—ó
plt.subplot(1, 3, 3)
plt.scatter(X[~anomalies_pred, 0], X[~anomalies_pred, 1], 
           c='blue', s=50, alpha=0.6, label='Normal')
plt.scatter(X[anomalies_pred, 0], X[anomalies_pred, 1],
           c='red', marker='x', s=100, label='Detected Anomalies')
plt.title('GMM Anomaly Detection')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# 7. Log-probability —Ä–æ–∑–ø–æ–¥—ñ–ª
plt.figure(figsize=(10, 5))
plt.hist(log_probs[y_true != 3], bins=50, alpha=0.7, label='Normal', density=True)
plt.hist(log_probs[y_true == 3], bins=20, alpha=0.7, label='Anomalies', density=True)
plt.axvline(x=threshold, color='red', linestyle='--', label=f'Threshold={threshold:.2f}')
plt.xlabel('Log-probability')
plt.ylabel('Density')
plt.title('Log-probability Distribution')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
```

---

### –ü—Ä–∏–∫–ª–∞–¥ 3: –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –Ω–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö

```python
from sklearn.mixture import GaussianMixture
import matplotlib.pyplot as plt

# 1. –ù–∞–≤—á–∏—Ç–∏ GMM –Ω–∞ —Ä–µ–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö
gmm = GaussianMixture(n_components=3, covariance_type='full', random_state=42)
gmm.fit(X)

# 2. –ó–≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ –Ω–æ–≤—ñ —Ç–æ—á–∫–∏
n_new_samples = 300
X_new, labels_new = gmm.sample(n_samples=n_new_samples)

# 3. –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
fig, axes = plt.subplots(1, 2, figsize=(15, 6))

# –û—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ
axes[0].scatter(X[:, 0], X[:, 1], c='blue', s=50, alpha=0.6)
axes[0].scatter(gmm.means_[:, 0], gmm.means_[:, 1],
               c='red', marker='X', s=200, edgecolors='black', linewidths=2)
axes[0].set_title('Original Data')
axes[0].grid(True, alpha=0.3)

# –ó–≥–µ–Ω–æ—Ä–æ–≤–∞–Ω—ñ –¥–∞–Ω—ñ
axes[1].scatter(X_new[:, 0], X_new[:, 1], c=labels_new, cmap='viridis', s=50, alpha=0.6)
axes[1].scatter(gmm.means_[:, 0], gmm.means_[:, 1],
               c='red', marker='X', s=200, edgecolors='black', linewidths=2)
axes[1].set_title('Generated Data (GMM samples)')
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# 4. –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è —Ä–æ–∑–ø–æ–¥—ñ–ª—ñ–≤
print("=== Component distribution ===")
print(f"Original: {np.bincount(gmm.predict(X)) / len(X)}")
print(f"Generated: {np.bincount(labels_new) / len(labels_new)}")
```

---

## 7Ô∏è‚É£ GMM vs K-Means

```python
from sklearn.cluster import KMeans
from sklearn.mixture import GaussianMixture
from sklearn.datasets import make_blobs

# –î–∞–Ω—ñ –∑ –µ–ª—ñ–ø—Ç–∏—á–Ω–∏–º–∏ –∫–ª–∞—Å—Ç–µ—Ä–∞–º–∏
X, _ = make_blobs(n_samples=300, centers=3, random_state=42)
transformation = np.array([[2, 0.5], [0.5, 1]])
X_elliptical = X @ transformation.T

# K-Means
kmeans = KMeans(n_clusters=3, random_state=42)
labels_kmeans = kmeans.fit_predict(X_elliptical)

# GMM
gmm = GaussianMixture(n_components=3, covariance_type='full', random_state=42)
labels_gmm = gmm.fit_predict(X_elliptical)

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
fig, axes = plt.subplots(1, 2, figsize=(15, 6))

# K-Means
axes[0].scatter(X_elliptical[:, 0], X_elliptical[:, 1], 
               c=labels_kmeans, cmap='viridis', s=50, alpha=0.6)
axes[0].scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],
               c='red', marker='X', s=200, edgecolors='black', linewidths=2)
axes[0].set_title(f'K-Means\nSilhouette: {silhouette_score(X_elliptical, labels_kmeans):.3f}')
axes[0].grid(True, alpha=0.3)

# GMM –∑ –µ–ª—ñ–ø—Å–∞–º–∏
axes[1].scatter(X_elliptical[:, 0], X_elliptical[:, 1],
               c=labels_gmm, cmap='viridis', s=50, alpha=0.6)
plot_gmm_ellipses(gmm, X_elliptical, ax=axes[1])
axes[1].set_title(f'GMM (full covariance)\nSilhouette: {silhouette_score(X_elliptical, labels_gmm):.3f}')

plt.tight_layout()
plt.show()

print(f"\nK-Means Silhouette: {silhouette_score(X_elliptical, labels_kmeans):.3f}")
print(f"GMM Silhouette: {silhouette_score(X_elliptical, labels_gmm):.3f}")
```

---

## 8Ô∏è‚É£ –û—Ü—ñ–Ω–∫–∞ —è–∫–æ—Å—Ç—ñ

```python
from sklearn.metrics import (
    silhouette_score,
    davies_bouldin_score,
    calinski_harabasz_score,
    adjusted_rand_score
)

def evaluate_gmm(X, gmm, y_true=None):
    """
    –û—Ü—ñ–Ω–∏—Ç–∏ —è–∫—ñ—Å—Ç—å GMM –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—ó
    """
    labels = gmm.predict(X)
    
    print("=== GMM Results ===")
    print(f"–ö–æ–º–ø–æ–Ω–µ–Ω—Ç: {gmm.n_components}")
    print(f"–ó—ñ–π—à–æ–≤—Å—è: {gmm.converged_}")
    print(f"–Ü—Ç–µ—Ä–∞—Ü—ñ–π: {gmm.n_iter_}")
    
    print(f"\n=== Model Selection ===")
    print(f"BIC: {gmm.bic(X):.2f}")
    print(f"AIC: {gmm.aic(X):.2f}")
    print(f"Log-likelihood: {gmm.score(X):.2f}")
    
    print(f"\n=== Clustering Metrics ===")
    sil_score = silhouette_score(X, labels)
    db_score = davies_bouldin_score(X, labels)
    ch_score = calinski_harabasz_score(X, labels)
    
    print(f"Silhouette Score: {sil_score:.3f}")
    print(f"Davies-Bouldin Index: {db_score:.3f}")
    print(f"Calinski-Harabasz Score: {ch_score:.1f}")
    
    if y_true is not None:
        ari = adjusted_rand_score(y_true, labels)
        print(f"\nAdjusted Rand Index: {ari:.3f}")
    
    # –Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü—ñ—è
    print(f"\n–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü—ñ—è:")
    if sil_score > 0.7:
        print("  ‚úì –í—ñ–¥–º—ñ–Ω–Ω–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—è")
    elif sil_score > 0.5:
        print("  ‚úì –î–æ–±—Ä–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—è")
    elif sil_score > 0.25:
        print("  ‚ö† –°–ª–∞–±–∫–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—è")
    else:
        print("  ‚úó –ü–æ–≥–∞–Ω–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—è")
    
    # –†–æ–∑–ø–æ–¥—ñ–ª –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞—Ö
    print(f"\n=== Cluster Sizes ===")
    unique, counts = np.unique(labels, return_counts=True)
    for cluster_id, count in zip(unique, counts):
        print(f"Cluster {cluster_id}: {count} points ({count/len(X)*100:.1f}%)")
    
    # –í–∞–≥–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç
    print(f"\n=== Component Weights ===")
    for i, weight in enumerate(gmm.weights_):
        print(f"Component {i}: {weight:.3f}")

# –ü—Ä–∏–∫–ª–∞–¥
evaluate_gmm(X_scaled, gmm, y_true=y_true)
```

---

## 9Ô∏è‚É£ –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –¥–ª—è –≤–∏—Å–æ–∫–æ–≤–∏–º—ñ—Ä–Ω–∏—Ö –¥–∞–Ω–∏—Ö (PCA)

```python
from sklearn.decomposition import PCA
from sklearn.datasets import load_iris

# 1. –î–∞–Ω—ñ (4D)
iris = load_iris()
X_iris = iris.data
y_iris = iris.target

# 2. –ú–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_iris)

# 3. GMM
gmm = GaussianMixture(n_components=3, covariance_type='full', random_state=42)
labels_gmm = gmm.fit_predict(X_scaled)
probs_gmm = gmm.predict_proba(X_scaled)

# 4. PCA –¥–ª—è –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

# 5. –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# GMM hard clustering
axes[0].scatter(X_pca[:, 0], X_pca[:, 1], c=labels_gmm, cmap='viridis', s=50, alpha=0.6)
axes[0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')
axes[0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')
axes[0].set_title('GMM - Hard Clustering')
axes[0].grid(True, alpha=0.3)

# GMM soft clustering (uncertainty)
from scipy.stats import entropy
uncertainty = entropy(probs_gmm.T)
axes[1].scatter(X_pca[:, 0], X_pca[:, 1], c=uncertainty, cmap='Reds', s=50, alpha=0.6)
axes[1].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')
axes[1].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')
axes[1].set_title('GMM - Uncertainty')
plt.colorbar(axes[1].collections[0], ax=axes[1], label='Entropy')
axes[1].grid(True, alpha=0.3)

# –°–ø—Ä–∞–≤–∂–Ω—ñ –º—ñ—Ç–∫–∏
axes[2].scatter(X_pca[:, 0], X_pca[:, 1], c=y_iris, cmap='viridis', s=50, alpha=0.6)
axes[2].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')
axes[2].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')
axes[2].set_title('True Labels')
axes[2].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# –ú–µ—Ç—Ä–∏–∫–∏
from sklearn.metrics import adjusted_rand_score
print(f"Adjusted Rand Index: {adjusted_rand_score(y_iris, labels_gmm):.3f}")
print(f"Silhouette Score: {silhouette_score(X_scaled, labels_gmm):.3f}")
```

---

## üîü –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ç–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è

```python
import joblib

# 1. –ù–∞–≤—á–∞–Ω–Ω—è
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

gmm = GaussianMixture(n_components=3, covariance_type='full', random_state=42)
gmm.fit(X_scaled)

# 2. –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è
model_data = {
    'gmm': gmm,
    'scaler': scaler,
    'optimal_k': 3,
    'bic': gmm.bic(X_scaled),
    'aic': gmm.aic(X_scaled)
}

joblib.dump(model_data, 'gmm_model.pkl')

# 3. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è
loaded_data = joblib.load('gmm_model.pkl')
loaded_gmm = loaded_data['gmm']
loaded_scaler = loaded_data['scaler']

print(f"–û–ø—Ç–∏–º–∞–ª—å–Ω–∏–π K: {loaded_data['optimal_k']}")
print(f"BIC: {loaded_data['bic']:.2f}")

# 4. –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –Ω–∞ –Ω–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö
new_data = np.array([[25, 50000, 75]])
new_data_scaled = loaded_scaler.transform(new_data)

# Hard clustering
new_label = loaded_gmm.predict(new_data_scaled)
print(f"\n–ù–æ–≤–∞ —Ç–æ—á–∫–∞ –Ω–∞–ª–µ–∂–∏—Ç—å –∫–ª–∞—Å—Ç–µ—Ä—É: {new_label[0]}")

# Soft clustering
new_probs = loaded_gmm.predict_proba(new_data_scaled)
print(f"–ô–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ: {new_probs[0]}")
for i, prob in enumerate(new_probs[0]):
    print(f"  Component {i}: {prob:.2%}")
```

---

## 1Ô∏è‚É£1Ô∏è‚É£ Bayesian GMM (–∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∏–π –≤–∏–±—ñ—Ä K)

```python
from sklearn.mixture import BayesianGaussianMixture

# BayesianGMM –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ "–≤–∏–º–∏–∫–∞—î" –∑–∞–π–≤—ñ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∏
# —á–µ—Ä–µ–∑ Dirichlet Process Prior

# –ó–∞–¥–∞—î–º–æ –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–£ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç
bgmm = BayesianGaussianMixture(
    n_components=10,           # –º–∞–∫—Å–∏–º—É–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç
    covariance_type='full',
    weight_concentration_prior=0.01,  # –º–µ–Ω—à–µ = –±—ñ–ª—å—à–µ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—ó
    random_state=42
)

bgmm.fit(X_scaled)

# –ï—Ñ–µ–∫—Ç–∏–≤–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç
effective_components = (bgmm.weights_ > 0.01).sum()
print(f"–ï—Ñ–µ–∫—Ç–∏–≤–Ω–∏—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç: {effective_components}")

# –í–∞–≥–∏
print(f"\n–í–∞–≥–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç:")
for i, weight in enumerate(bgmm.weights_):
    if weight > 0.01:
        print(f"  Component {i}: {weight:.3f}")

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
labels_bgmm = bgmm.predict(X_scaled)
plt.figure(figsize=(10, 6))
plt.scatter(X[:, 0], X[:, 1], c=labels_bgmm, cmap='viridis', s=50, alpha=0.6)
plt.title(f'Bayesian GMM (effective components: {effective_components})')
plt.colorbar(label='Cluster')
plt.grid(True, alpha=0.3)
plt.show()
```

---

## 1Ô∏è‚É£2Ô∏è‚É£ –ü–æ—Ä–∞–¥–∏ —Ç–∞ best practices

### 1. –ó–∞–≤–∂–¥–∏ –º–∞—Å—à—Ç–∞–±—É–π –¥–∞–Ω—ñ

```python
# –ü–û–ì–ê–ù–û
gmm = GaussianMixture(n_components=3)
gmm.fit(X)

# –î–û–ë–†–ï
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
gmm = GaussianMixture(n_components=3)
gmm.fit(X_scaled)
```

---

### 2. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π BIC –¥–ª—è –≤–∏–±–æ—Ä—É K

```python
# –ó–ê–í–ñ–î–ò –ø–µ—Ä–µ–≤—ñ—Ä—è–π BIC –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö K
bic_scores = [GaussianMixture(n_components=k).fit(X).bic(X) 
              for k in range(1, 11)]
optimal_k = np.argmin(bic_scores) + 1
```

---

### 3. –ü–æ—á–∏–Ω–∞–π –∑ 'full' covariance

```python
# –ó–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π 'full' (–Ω–∞–π–≥–Ω—É—á–∫—ñ—à–µ)
gmm = GaussianMixture(n_components=3, covariance_type='full')

# –Ø–∫—â–æ –ø–æ–≤—ñ–ª—å–Ω–æ –∞–±–æ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–Ω—è, —Å–ø—Ä–æ–±—É–π 'tied' –∞–±–æ 'diag'
```

---

### 4. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π n_init > 1 –¥–ª—è —Å—Ç–∞–±—ñ–ª—å–Ω–æ—Å—Ç—ñ

```python
# –ó–∞–ø—É—Å—Ç–∏—Ç–∏ EM –∫—ñ–ª—å–∫–∞ —Ä–∞–∑—ñ–≤
gmm = GaussianMixture(n_components=3, n_init=10, random_state=42)
```

---

### 5. Regularization –¥–ª—è —É–Ω–∏–∫–Ω–µ–Ω–Ω—è —Å–∏–Ω–≥—É–ª—è—Ä–Ω–æ—Å—Ç–µ–π

```python
# –î–æ–¥–∞—Ç–∏ reg_covar –¥–ª—è —Å—Ç–∞–±—ñ–ª—å–Ω–æ—Å—Ç—ñ
gmm = GaussianMixture(
    n_components=3,
    reg_covar=1e-6  # –Ω–µ–≤–µ–ª–∏–∫–∞ –∫–æ–Ω—Å—Ç–∞–Ω—Ç–∞
)
```

---

### 6. –ü–µ—Ä–µ–≤—ñ—Ä—è–π –∑–±—ñ–∂–Ω—ñ—Å—Ç—å

```python
gmm.fit(X)
if not gmm.converged_:
    print("‚ö†Ô∏è EM –Ω–µ –∑—ñ–π—à–æ–≤—Å—è! –ó–±—ñ–ª—å—à max_iter")
```

---

### 7. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π soft clustering –¥–ª—è uncertainty

```python
# –ó–∞–º—ñ—Å—Ç—å hard labels
labels = gmm.predict(X)

# –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ
probs = gmm.predict_proba(X)

# –ó–Ω–∞–π–¥–∏ –Ω–µ–≤–ø–µ–≤–Ω–µ–Ω—ñ —Ç–æ—á–∫–∏
from scipy.stats import entropy
uncertainty = entropy(probs.T)
uncertain_points = X[uncertainty > 1.0]  # –≤–∏—Å–æ–∫–∏–π –µ–Ω—Ç—Ä–æ–ø—ñ—è
```

---

## –ß–µ–∫-–ª–∏—Å—Ç –¥–ª—è GMM

```python
# ‚úÖ 1. –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –¥–∞–Ω—ñ
X = load_data()

# ‚úÖ 2. EDA
print(X.shape)
print(pd.DataFrame(X).describe())

# ‚úÖ 3. –ú–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è (–û–ë–û–í'–Ø–ó–ö–û–í–û!)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# ‚úÖ 4. –í–∏–±—ñ—Ä K —á–µ—Ä–µ–∑ BIC
bic_scores = []
for k in range(1, 11):
    gmm = GaussianMixture(n_components=k, random_state=42)
    gmm.fit(X_scaled)
    bic_scores.append(gmm.bic(X_scaled))

optimal_k = np.argmin(bic_scores) + 1
print(f"Optimal K: {optimal_k}")

# ‚úÖ 5. –ù–∞–≤—á–∞–Ω–Ω—è GMM
gmm = GaussianMixture(
    n_components=optimal_k,
    covariance_type='full',
    n_init=10,
    random_state=42
)
gmm.fit(X_scaled)

# ‚úÖ 6. –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –∑–±—ñ–∂–Ω–æ—Å—Ç—ñ
if not gmm.converged_:
    print("‚ö†Ô∏è –ù–µ –∑—ñ–π—à–æ–≤—Å—è!")

# ‚úÖ 7. Hard —Ç–∞ soft clustering
labels = gmm.predict(X_scaled)
probs = gmm.predict_proba(X_scaled)

# ‚úÖ 8. –û—Ü—ñ–Ω–∫–∞ —è–∫–æ—Å—Ç—ñ
evaluate_gmm(X_scaled, gmm)

# ‚úÖ 9. –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
visualize_gmm(X, labels, probs, gmm)

# ‚úÖ 10. –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è
joblib.dump({'gmm': gmm, 'scaler': scaler}, 'gmm_model.pkl')
```

---

## –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è GMM –∑ —ñ–Ω—à–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏

```python
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.mixture import GaussianMixture
import time

# –î–∞–Ω—ñ
X, y_true = make_blobs(n_samples=500, centers=3, random_state=42)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

methods = {
    'K-Means': KMeans(n_clusters=3, random_state=42),
    'GMM': GaussianMixture(n_components=3, random_state=42),
    'DBSCAN': DBSCAN(eps=0.5, min_samples=5),
    'Hierarchical': AgglomerativeClustering(n_clusters=3)
}

results = []

for name, model in methods.items():
    start = time.time()
    labels = model.fit_predict(X_scaled)
    elapsed = time.time() - start
    
    # –ú–µ—Ç—Ä–∏–∫–∏
    if name == 'DBSCAN':
        mask = labels != -1
        if sum(mask) > 0 and len(set(labels[mask])) > 1:
            sil = silhouette_score(X_scaled[mask], labels[mask])
        else:
            sil = -1
    else:
        sil = silhouette_score(X_scaled, labels)
    
    ari = adjusted_rand_score(y_true, labels)
    
    results.append({
        'Method': name,
        'Silhouette': sil,
        'ARI': ari,
        'Time': elapsed
    })

df_results = pd.DataFrame(results)
print(df_results.to_string(index=False))
```

---

## –ö–æ—Ä–∏—Å–Ω—ñ –ø–æ—Å–∏–ª–∞–Ω–Ω—è

- [sklearn GaussianMixture](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html)
- [sklearn BayesianGaussianMixture](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.BayesianGaussianMixture.html)
- [GMM Tutorial](https://scikit-learn.org/stable/modules/mixture.html)

---

**–°—Ç–≤–æ—Ä–µ–Ω–æ –¥–ª—è –ø—Ä–∞–∫—Ç–∏—á–Ω–æ–≥–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è GMM –≤ –ø—Ä–æ—î–∫—Ç–∞—Ö** üöÄ
