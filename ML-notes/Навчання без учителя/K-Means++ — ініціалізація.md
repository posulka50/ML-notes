
**Проблема:** K-Means дуже чутливий до випадкового вибору початкових центроїдів. Погана ініціалізація → погані результати.

**Рішення:** K-Means++ — розумний спосіб вибору початкових центрів, який збільшує шанси знайти глобальний оптимум.

---

## Проблема випадкової ініціалізації

### Що може піти не так?

**Приклад 1: Всі центри в одному кластері**

```
Дані: 3 чіткі групи точок

Випадкова ініціалізація:
  μ₁ ●     Група 1          Група 2          Група 3
  μ₂ ●       ●●●              ●●●              ●●●
  μ₃ ●       ●●●              ●●●              ●●●
             ●●●              ●●●              ●●●

Всі 3 центри попали в Групу 1!
→ Алгоритм може не знайти справжні кластери
```

**Приклад 2: Локальний мінімум**

```
Погана ініціалізація → локальний оптимум:
   μ₁        μ₂        μ₃
    ●         ●         ●
  ●● ●●     ●● ●●     ●● ●●
  Inertia = 150

Хороша ініціалізація → глобальний оптимум:
  μ₁       μ₂       μ₃
  ●         ●         ●
 ●●        ●●        ●●
  Inertia = 50  ← значно краще!
```

### Чому це проблема?

K-Means **гарантовано знаходить локальний оптимум**, але не завжди глобальний!

```
Випадкова ініціалізація:
├─ Запуск 1: Inertia = 523 (погано)
├─ Запуск 2: Inertia = 142 (добре)
├─ Запуск 3: Inertia = 489 (погано)
└─ Запуск 4: Inertia = 145 (добре)

Результати нестабільні! ⚠️
```

---

## K-Means++ — рішення

### Основна ідея

**Вибирати початкові центри так, щоб вони були якомога далі один від одного.**

Це збільшує ймовірність, що центри потраплять у різні справжні кластери.

---

## Алгоритм K-Means++

### Крок 1: Перший центр

Випадково вибираємо **одну** точку як перший центроїд μ₁.

```python
μ₁ = random_choice(X)
```

### Крок 2: Наступні центри (k = 2, 3, ..., K)

Для кожної точки xᵢ рахуємо:

- **D(xᵢ)** — відстань до **найближчого вже вибраного** центроїда

Вибираємо наступний центр з ймовірністю пропорційною **D²(xᵢ)**:

$$P(x_i) = \frac{D^2(x_i)}{\sum_{j} D^2(x_j)}$$

**Інтуїція:** Чим далі точка від усіх вже вибраних центрів, тим більша ймовірність, що вона стане наступним центром.

### Крок 3: Запуск звичайного K-Means

Після вибору всіх K центрів, запускаємо стандартний K-Means з цими центрами.

---

## Приклад (покроково)

**Дані:** 9 точок, K=3

```
   1  2  3  4  5  6  7  8  9 10
10 ●                       ●  
9                             ●
8
7
6
5
4
3
2     ●
1  ●     ●
   x₁ x₂ x₃             x₇ x₈ x₉
```

Координати:

```
Група 1: x₁=[1,1], x₂=[2,2], x₃=[3,1]
Група 2: x₇=[8,10], x₈=[9,9], x₉=[10,10]
```

---

### Крок 1: Перший центр

Випадково вибрали **x₂ = [2, 2]**

```
μ₁ = [2, 2]  ●
```

---

### Крок 2: Другий центр

**Обчислюємо D(xᵢ)** — відстань до μ₁ для всіх точок:

```
D(x₁) = √((1-2)² + (1-2)²) = √2 ≈ 1.41
D(x₂) = 0  (це μ₁)
D(x₃) = √((3-2)² + (1-2)²) = √2 ≈ 1.41
D(x₇) = √((8-2)² + (10-2)²) = √100 = 10
D(x₈) = √((9-2)² + (9-2)²) = √98 ≈ 9.9
D(x₉) = √((10-2)² + (10-2)²) = √128 ≈ 11.3
```

**Обчислюємо ймовірності вибору** (пропорційно D²):

```
D²(x₁) = 2        → P ≈ 0.01
D²(x₂) = 0        → P = 0
D²(x₃) = 2        → P ≈ 0.01
D²(x₇) = 100      → P ≈ 0.41
D²(x₈) = 98       → P ≈ 0.40
D²(x₉) = 128      → P ≈ 0.53  ← найбільша!

Сума D² = 330
```

**Вибираємо випадково з цими ймовірностями.**

Припустимо, вибрали **x₉ = [10, 10]** (має найбільшу ймовірність).

```
μ₁ = [2, 2]   ●
μ₂ = [10, 10]                  ●
```

---

### Крок 3: Третій центр

**Обчислюємо D(xᵢ)** — відстань до **найближчого** з {μ₁, μ₂}:

```
Для x₁ = [1,1]:
  d(x₁, μ₁) = 1.41
  d(x₁, μ₂) = 12.7
  D(x₁) = min(1.41, 12.7) = 1.41  ← ближче до μ₁

Для x₃ = [3,1]:
  d(x₃, μ₁) = 1.41
  d(x₃, μ₂) = 12.2
  D(x₃) = 1.41

Для x₇ = [8,10]:
  d(x₇, μ₁) = 10
  d(x₇, μ₂) = 2
  D(x₇) = 2  ← ближче до μ₂

Для x₈ = [9,9]:
  d(x₈, μ₁) = 9.9
  d(x₈, μ₂) = 1.41
  D(x₈) = 1.41
```

**Ймовірності:**

```
D²(x₁) = 2     → P ≈ 0.33
D²(x₃) = 2     → P ≈ 0.33
D²(x₇) = 4     → P ≈ 0.67  ← найбільша!
D²(x₈) = 2     → P ≈ 0.33
```

Вибираємо, наприклад, **x₇ = [8, 10]** (не дуже далеко від μ₂, але це приклад).

Або краще вибрати щось посередині, але для простоти:

```
μ₁ = [2, 2]
μ₂ = [10, 10]
μ₃ = [8, 10]  (або інша точка з високою ймовірністю)
```

---

### Результат

Початкові центри K-Means++:

```
μ₁ = [2, 2]    ← в Групі 1
μ₂ = [10, 10]  ← в Групі 2  
μ₃ = [8, 10]   ← також в Групі 2 (могло б бути краще)
```

**Важливо:** Центри розподілені далеко один від одного, що дає хороші початкові позиції!

---

## Візуальне порівняння

### Випадкова ініціалізація

```
Спроба 1:
μ₁ μ₂ μ₃
 ●  ●  ●         (всі в одній групі)
●●● ●●●         ●●●
  ↓
Погано! Inertia = 450
```

```
Спроба 2:
μ₁       μ₂       μ₃
 ●        ●        ●   (по одному в кожній групі)
●●       ●●       ●●
  ↓
Добре! Inertia = 45
```

### K-Means++ ініціалізація

```
Майже завжди:
μ₁       μ₂       μ₃
 ●        ●        ●   (розподілені розумно)
●●       ●●       ●●
  ↓
Стабільно добре! Inertia ≈ 45-50
```

---

## Математичне обґрунтування

### Чому D²?

**Лінійна залежність (D):**

```
Точка на відстані 10 в 10 разів імовірніша за точку на відстані 1
```

**Квадратична залежність (D²):**

```
Точка на відстані 10 в 100 разів імовірніша за точку на відстані 1
```

**Це дає сильну перевагу далеким точкам** → центри розподіляються краще.

### Теорема (Arthur & Vassilvitskii, 2007)

K-Means++ гарантує:

$$E[\text{Inertia}_{K\text{-Means++}}] \leq 8(\ln K + 2) \cdot \text{Inertia}_{\text{optimal}}$$

**Простими словами:** Очікуваний результат K-Means++ максимум в O(log K) разів гірший за оптимум.

Для випадкової ініціалізації ця гарантія **відсутня** — може бути довільно погано!

---

## Переваги K-Means++

### ✅ Стабільність

```
10 запусків випадкової ініціалізації:
Inertia: [523, 142, 489, 145, 512, 138, 501, 149, 485, 141]
Std Dev: 187  ← велика варіативність

10 запусків K-Means++:
Inertia: [145, 142, 148, 143, 146, 141, 147, 144, 142, 145]
Std Dev: 2.3  ← стабільно! ✓
```

### ✅ Швидша збіжність

```
Випадкова ініціалізація: 45 ітерацій до збіжності
K-Means++: 12 ітерацій  ← швидше у ~4 рази
```

Хороші початкові центри → менше переміщень.

### ✅ Кращі результати

```
100 експериментів на різних даних:
Випадкова: середній Silhouette = 0.52
K-Means++: середній Silhouette = 0.68  ✓
```

---

## Недоліки K-Means++

### ❌ Повільніша ініціалізація

```
Випадкова: O(K)  — просто вибрати K точок
K-Means++: O(n·K)  — треба рахувати відстані для всіх точок

Для n=100,000, K=10:
Випадкова: ~0.001 сек
K-Means++: ~0.05 сек
```

Але це компенсується швидшою збіжністю!

### ❌ Все ще недетерміновано

K-Means++ використовує випадковість у виборі точок (хоч і розумно).

**Рішення:** зафіксувати `random_state`:

```python
kmeans = KMeans(n_clusters=3, init='k-means++', random_state=42)
```

---

## Альтернативні методи ініціалізації

### 1. Random (випадкова)

```python
kmeans = KMeans(init='random')
```

- Просто випадково вибирає K точок
- Швидко, але нестабільно

### 2. K-Means++ (за замовчуванням)

```python
kmeans = KMeans(init='k-means++')  # default
```

- Розумний вибір
- Золотий стандарт

### 3. Задати вручну

```python
initial_centers = np.array([[1, 2], [8, 9], [5, 5]])
kmeans = KMeans(n_clusters=3, init=initial_centers, n_init=1)
```

- Якщо знаєш, де мають бути центри
- Наприклад, з попереднього запуску

### 4. K-Means|| (паралельна версія)

- Для дуже великих даних (мільйони точок)
- Не в sklearn, але є в Apache Spark MLlib

---

## Практичні поради

### 1. Завжди використовуй K-Means++

```python
# ПОГАНО
kmeans = KMeans(n_clusters=3, init='random')

# ДОБРЕ (за замовчуванням)
kmeans = KMeans(n_clusters=3)  # init='k-means++' автоматично
```

### 2. Для критичних задач: n_init > 1

```python
# Запустити K-Means++ 10 разів і вибрати найкраще
kmeans = KMeans(n_clusters=3, n_init=10, random_state=42)
```

Це дає **10 різних ініціалізацій K-Means++** і вибирає найкращу.

### 3. Фіксуй random_state для відтворюваності

```python
kmeans = KMeans(n_clusters=3, random_state=42)
```

### 4. Для великих даних: MiniBatchKMeans

```python
from sklearn.cluster import MiniBatchKMeans

# K-Means++ працює і тут
kmeans = MiniBatchKMeans(n_clusters=3, init='k-means++')
```

---

## Експеримент: порівняння

```python
import numpy as np
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt

# Генеруємо 3 чіткі групи
np.random.seed(42)
X1 = np.random.randn(100, 2) + [0, 0]
X2 = np.random.randn(100, 2) + [10, 10]
X3 = np.random.randn(100, 2) + [0, 10]
X = np.vstack([X1, X2, X3])

# Тест 1: Випадкова ініціалізація
random_scores = []
for i in range(50):
    kmeans = KMeans(n_clusters=3, init='random', n_init=1, random_state=i)
    labels = kmeans.fit_predict(X)
    score = silhouette_score(X, labels)
    random_scores.append(score)

# Тест 2: K-Means++
kmpp_scores = []
for i in range(50):
    kmeans = KMeans(n_clusters=3, init='k-means++', n_init=1, random_state=i)
    labels = kmeans.fit_predict(X)
    score = silhouette_score(X, labels)
    kmpp_scores.append(score)

# Результати
print(f"Random init:")
print(f"  Mean: {np.mean(random_scores):.3f}")
print(f"  Std:  {np.std(random_scores):.3f}")
print(f"  Min:  {np.min(random_scores):.3f}")
print(f"  Max:  {np.max(random_scores):.3f}")

print(f"\nK-Means++ init:")
print(f"  Mean: {np.mean(kmpp_scores):.3f}")
print(f"  Std:  {np.std(kmpp_scores):.3f}")
print(f"  Min:  {np.min(kmpp_scores):.3f}")
print(f"  Max:  {np.max(kmpp_scores):.3f}")

# Візуалізація
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.hist(random_scores, bins=20, alpha=0.7, label='Random')
plt.axvline(np.mean(random_scores), color='red', linestyle='--')
plt.xlabel('Silhouette Score')
plt.ylabel('Frequency')
plt.title('Random Initialization')
plt.legend()

plt.subplot(1, 2, 2)
plt.hist(kmpp_scores, bins=20, alpha=0.7, label='K-Means++', color='green')
plt.axvline(np.mean(kmpp_scores), color='red', linestyle='--')
plt.xlabel('Silhouette Score')
plt.title('K-Means++ Initialization')
plt.legend()
plt.tight_layout()
plt.show()
```

**Очікувані результати:**

```
Random init:
  Mean: 0.623
  Std:  0.156  ← велика варіативність!
  Min:  0.312
  Max:  0.784

K-Means++ init:
  Mean: 0.762
  Std:  0.023  ← стабільно!
  Min:  0.715
  Max:  0.789
```

---

## Історична довідка

**2006:** David Arthur і Sergei Vassilvitskii публікують K-Means++

**2007:** Теоретичне обґрунтування (гарантія O(log K))

**2008:** Включено в sklearn як метод за замовчуванням

**Сьогодні:** Індустріальний стандарт для K-Means

---

## Коли НЕ потрібен K-Means++?

### 1. Дані вже попередньо кластеризовані

```python
# Якщо точно знаєш, де центри
centers = previous_kmeans.cluster_centers_
kmeans = KMeans(init=centers, n_init=1)
```

### 2. Онлайн-навчання (Mini-Batch)

Для потокових даних використовується інша стратегія.

### 3. Дуже малі дані (n < 100)

Різниця несуттєва, можна використовувати будь-що.

---

## Ключові висновки

> K-Means++ — це розумна ініціалізація, яка вибирає початкові центри далеко один від одного, що значно покращує стабільність та якість кластеризації.

**Головні переваги:**

- ✅ Стабільніші результати (менша variance)
- ✅ Швидша збіжність (менше ітерацій)
- ✅ Кращі фінальні кластери (вища якість)
- ✅ Теоретичні гарантії (O(log K) від оптимуму)

**Практичний висновок:**

```python
# Завжди використовуй (це за замовчуванням в sklearn):
kmeans = KMeans(n_clusters=3, init='k-means++', n_init=10, random_state=42)
```

**Формула успіху:** $$\text{K-Means++} + \text{n_init=10} + \text{random_state} = \text{Стабільні результати} ✓$$