
–ü–æ–≤–Ω–∏–π –ø—Ä–∞–∫—Ç–∏—á–Ω–∏–π –≥–∞–π–¥ –ø–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—é Hierarchical Clustering –≤ scikit-learn —Ç–∞ scipy –∑ –ø—Ä–∏–∫–ª–∞–¥–∞–º–∏ –∫–æ–¥—É.

---

## üì¶ –û—Å–Ω–æ–≤–Ω—ñ —ñ–º–ø–æ—Ä—Ç–∏

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Hierarchical Clustering
from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram, linkage, fcluster
from scipy.spatial.distance import pdist, squareform

# Metrics
from sklearn.metrics import (
    silhouette_score,
    davies_bouldin_score,
    calinski_harabasz_score
)

# Preprocessing
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# Data
from sklearn.datasets import make_blobs, load_iris
```

---

## 1Ô∏è‚É£ AgglomerativeClustering ‚Äî –æ—Å–Ω–æ–≤–Ω–∏–π –∫–ª–∞—Å (sklearn)

### –ü–∞—Ä–∞–º–µ—Ç—Ä–∏

```python
AgglomerativeClustering(
    n_clusters=2,              # –ö—ñ–ª—å–∫—ñ—Å—Ç—å –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤ (–∞–±–æ None –¥–ª—è distance_threshold)
    affinity='euclidean',      # –ú–µ—Ç—Ä–∏–∫–∞ –≤—ñ–¥—Å—Ç–∞–Ω—ñ: 'euclidean', 'manhattan', 'cosine', etc.
    memory=None,               # –ö–µ—à—É–≤–∞–Ω–Ω—è –¥–ª—è –ø—Ä–∏—Å–∫–æ—Ä–µ–Ω–Ω—è
    connectivity=None,         # –ú–∞—Ç—Ä–∏—Ü—è –∑–≤'—è–∑–∞–Ω–æ—Å—Ç—ñ (–æ–±–º–µ–∂–µ–Ω–Ω—è –Ω–∞ –æ–±'—î–¥–Ω–∞–Ω–Ω—è)
    compute_full_tree='auto',  # –ë—É–¥—É–≤–∞—Ç–∏ –ø–æ–≤–Ω–µ –¥–µ—Ä–µ–≤–æ
    linkage='ward',            # –ú–µ—Ç–æ–¥ linkage: 'ward', 'complete', 'average', 'single'
    distance_threshold=None,   # –ü–æ—Ä—ñ–≥ –≤—ñ–¥—Å—Ç–∞–Ω—ñ (—è–∫—â–æ None, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ n_clusters)
    compute_distances=False    # –ó–±–µ—Ä—ñ–≥–∞—Ç–∏ –≤—ñ–¥—Å—Ç–∞–Ω—ñ –º—ñ–∂ –∫–ª–∞—Å—Ç–µ—Ä–∞–º–∏
)
```

### –ê—Ç—Ä–∏–±—É—Ç–∏ –ø—ñ—Å–ª—è fit

```python
model = AgglomerativeClustering(n_clusters=3, linkage='ward')
model.fit(X)

# –î–æ—Å—Ç—É–ø–Ω—ñ –∞—Ç—Ä–∏–±—É—Ç–∏:
model.labels_              # –ú—ñ—Ç–∫–∏ –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤ (n_samples,)
model.n_clusters_          # –ö—ñ–ª—å–∫—ñ—Å—Ç—å –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤
model.n_leaves_            # –ö—ñ–ª—å–∫—ñ—Å—Ç—å –ª–∏—Å—Ç–∫—ñ–≤ —É –¥–µ—Ä–µ–≤—ñ
model.n_connected_components_  # –ö—ñ–ª—å–∫—ñ—Å—Ç—å –∑–≤'—è–∑–∞–Ω–∏—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç
model.children_            # –Ü—Å—Ç–æ—Ä—ñ—è –æ–±'—î–¥–Ω–∞–Ω—å (n_samples-1, 2)
```

### –ú–µ—Ç–æ–¥–∏

```python
# –ù–∞–≤—á–∞–Ω–Ω—è
model.fit(X)

# –ù–∞–≤—á–∞–Ω–Ω—è + –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
labels = model.fit_predict(X)

# –í–ê–ñ–õ–ò–í–û: AgglomerativeClustering –ù–ï –º–∞—î –º–µ—Ç–æ–¥—É predict()!
# –ù–µ –º–æ–∂–Ω–∞ –ø–µ—Ä–µ–¥–±–∞—á–∏—Ç–∏ –¥–ª—è –Ω–æ–≤–∏—Ö —Ç–æ—á–æ–∫
```

---

## 2Ô∏è‚É£ –ë–∞–∑–æ–≤–∏–π –ø—Ä–∏–∫–ª–∞–¥

```python
import numpy as np
from sklearn.cluster import AgglomerativeClustering
from sklearn.datasets import make_blobs
import matplotlib.pyplot as plt

# 1. –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –¥–∞–Ω–∏—Ö
X, y_true = make_blobs(n_samples=150, centers=3, cluster_std=0.5, random_state=42)

# 2. Hierarchical Clustering
hc = AgglomerativeClustering(n_clusters=3, linkage='ward')
labels = hc.fit_predict(X)

# 3. –†–µ–∑—É–ª—å—Ç–∞—Ç–∏
print(f"–ö—ñ–ª—å–∫—ñ—Å—Ç—å –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤: {hc.n_clusters_}")
print(f"–ö—ñ–ª—å–∫—ñ—Å—Ç—å –ª–∏—Å—Ç–∫—ñ–≤: {hc.n_leaves_}")

# 4. –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
plt.figure(figsize=(10, 6))

plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=50, alpha=0.6)
plt.title('Hierarchical Clustering (Ward linkage)')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.colorbar(label='Cluster')
plt.grid(True, alpha=0.3)
plt.show()
```

---

## 3Ô∏è‚É£ –î–µ–Ω–¥—Ä–æ–≥—Ä–∞–º–∞ –∑ scipy

### –ë–∞–∑–æ–≤–∞ –¥–µ–Ω–¥—Ä–æ–≥—Ä–∞–º–∞

```python
from scipy.cluster.hierarchy import dendrogram, linkage
import matplotlib.pyplot as plt

# 1. –ü–æ–±—É–¥—É–≤–∞—Ç–∏ –¥–µ—Ä–µ–≤–æ (linkage matrix)
Z = linkage(X, method='ward')  # method: 'ward', 'single', 'complete', 'average'

# 2. –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –¥–µ–Ω–¥—Ä–æ–≥—Ä–∞–º–∏
plt.figure(figsize=(12, 5))
dendrogram(Z)
plt.title('Dendrogram (Ward linkage)')
plt.xlabel('Sample index')
plt.ylabel('Distance')
plt.axhline(y=10, color='r', linestyle='--', label='Cut at height=10')
plt.legend()
plt.show()

# Linkage matrix Z:
# Z[i] = [cluster_1, cluster_2, distance, sample_count]
# - –ø–µ—Ä—à—ñ 2 –∫–æ–ª–æ–Ω–∫–∏: —è–∫—ñ –∫–ª–∞—Å—Ç–µ—Ä–∏ –æ–±'—î–¥–Ω–∞–ª–∏—Å—è
# - 3-—è –∫–æ–ª–æ–Ω–∫–∞: –≤—ñ–¥—Å—Ç–∞–Ω—å –æ–±'—î–¥–Ω–∞–Ω–Ω—è
# - 4-–∞ –∫–æ–ª–æ–Ω–∫–∞: –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ç–æ—á–æ–∫ —É –Ω–æ–≤–æ–º—É –∫–ª–∞—Å—Ç–µ—Ä—ñ
```

---

### –ü–æ–∫—Ä–∞—â–µ–Ω–∞ –¥–µ–Ω–¥—Ä–æ–≥—Ä–∞–º–∞

```python
def plot_dendrogram(X, method='ward', truncate_mode=None, p=30):
    """
    –ö—Ä–∞—Å–∏–≤–∞ –¥–µ–Ω–¥—Ä–æ–≥—Ä–∞–º–∞ –∑ –¥–æ–¥–∞—Ç–∫–æ–≤–æ—é —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—î—é
    """
    # Linkage matrix
    Z = linkage(X, method=method)
    
    # –ì—Ä–∞—Ñ—ñ–∫
    plt.figure(figsize=(15, 7))
    
    # –î–µ–Ω–¥—Ä–æ–≥—Ä–∞–º–∞
    dendrogram(
        Z,
        truncate_mode=truncate_mode,  # None, 'lastp', 'level'
        p=p,                          # –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤ –¥–ª—è –ø–æ–∫–∞–∑—É (—è–∫—â–æ truncate)
        leaf_font_size=10,
        show_contracted=True          # –ø–æ–∫–∞–∑–∞—Ç–∏ —Å–∫–æ—Ä–æ—á–µ–Ω—ñ –∫–ª–∞—Å—Ç–µ—Ä–∏
    )
    
    plt.title(f'Dendrogram ({method} linkage)', fontsize=16)
    plt.xlabel('Sample index (or cluster size)', fontsize=12)
    plt.ylabel('Distance', fontsize=12)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()
    
    return Z

# –ü—Ä–∏–∫–ª–∞–¥
Z = plot_dendrogram(X, method='ward')
```

---

### Truncated dendrogram (–¥–ª—è –≤–µ–ª–∏–∫–∏—Ö –¥–∞–Ω–∏—Ö)

```python
# –î–ª—è –≤–µ–ª–∏–∫–∏—Ö –¥–∞–Ω–∏—Ö: –ø–æ–∫–∞–∑–∞—Ç–∏ —Ç—ñ–ª—å–∫–∏ top 30 –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤
plt.figure(figsize=(15, 7))

dendrogram(
    Z,
    truncate_mode='lastp',  # –ø–æ–∫–∞–∑–∞—Ç–∏ —Ç—ñ–ª—å–∫–∏ –æ—Å—Ç–∞–Ω–Ω—ñ p –æ–±'—î–¥–Ω–∞–Ω—å
    p=30,                   # –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤
    show_leaf_counts=True,  # –ø–æ–∫–∞–∑–∞—Ç–∏ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ç–æ—á–æ–∫
    leaf_font_size=10
)

plt.title('Truncated Dendrogram (last 30 merges)')
plt.xlabel('Cluster size')
plt.ylabel('Distance')
plt.show()
```

---

## 4Ô∏è‚É£ –í–∏–±—ñ—Ä –∫—ñ–ª—å–∫–æ—Å—Ç—ñ –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤

### –ú–µ—Ç–æ–¥ 1: –í—ñ–∑—É–∞–ª—å–Ω–æ –∑ –¥–µ–Ω–¥—Ä–æ–≥—Ä–∞–º–∏

```python
from scipy.cluster.hierarchy import dendrogram, linkage

# –ü–æ–±—É–¥—É–≤–∞—Ç–∏ –¥–µ–Ω–¥—Ä–æ–≥—Ä–∞–º—É
Z = linkage(X, method='ward')

plt.figure(figsize=(12, 5))
dendrogram(Z)
plt.title('Find the largest vertical gap')
plt.xlabel('Sample')
plt.ylabel('Distance')

# –î–æ–¥–∞—Ç–∏ –º–æ–∂–ª–∏–≤—ñ —Ä–æ–∑—Ä—ñ–∑–∏
for height in [5, 10, 15]:
    plt.axhline(y=height, color='r', linestyle='--', alpha=0.5, 
                label=f'Cut at {height}')
plt.legend()
plt.show()

# –®—É–∫–∞–π –Ω–∞–π–±—ñ–ª—å—à–∏–π –≤–µ—Ä—Ç–∏–∫–∞–ª—å–Ω–∏–π —Ä–æ–∑—Ä–∏–≤!
```

---

### –ú–µ—Ç–æ–¥ 2: Elbow –Ω–∞ –≤—ñ–¥—Å—Ç–∞–Ω—è—Ö –æ–±'—î–¥–Ω–∞–Ω–Ω—è

```python
# –í—ñ–¥—Å—Ç–∞–Ω—ñ –º—ñ–∂ –æ–±'—î–¥–Ω–∞–Ω–Ω—è–º–∏
distances = Z[:, 2]

# –ì—Ä–∞—Ñ—ñ–∫
plt.figure(figsize=(10, 6))
plt.plot(range(1, len(distances)+1), distances, 'o-')
plt.xlabel('Merge step')
plt.ylabel('Distance')
plt.title('Distances in hierarchical clustering')
plt.grid(True, alpha=0.3)
plt.show()

# –®—É–∫–∞–π —Ä—ñ–∑–∫–µ –∑–±—ñ–ª—å—à–µ–Ω–Ω—è –≤—ñ–¥—Å—Ç–∞–Ω—ñ (elbow)
```

---

### –ú–µ—Ç–æ–¥ 3: Silhouette Score –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö K

```python
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics import silhouette_score

silhouette_scores = []
K_range = range(2, 11)

for k in K_range:
    hc = AgglomerativeClustering(n_clusters=k, linkage='ward')
    labels = hc.fit_predict(X)
    score = silhouette_score(X, labels)
    silhouette_scores.append(score)
    print(f"K={k}: Silhouette={score:.3f}")

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
plt.figure(figsize=(10, 6))
plt.plot(K_range, silhouette_scores, 'o-', linewidth=2, markersize=8)
plt.xlabel('Number of clusters (K)')
plt.ylabel('Silhouette Score')
plt.title('Silhouette Score vs K')
plt.grid(True, alpha=0.3)
plt.show()

# –û–ø—Ç–∏–º–∞–ª—å–Ω–∏–π K
optimal_k = K_range[np.argmax(silhouette_scores)]
print(f"\nOptimal K: {optimal_k}")
```

---

### –ú–µ—Ç–æ–¥ 4: Distance threshold (–∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∏–π –≤–∏–±—ñ—Ä)

```python
from scipy.cluster.hierarchy import fcluster

# –†–æ–∑—Ä—ñ–∑–∞—Ç–∏ –Ω–∞ –ø–µ–≤–Ω—ñ–π –≤–∏—Å–æ—Ç—ñ
height_threshold = 10
clusters = fcluster(Z, height_threshold, criterion='distance')

print(f"–ö—ñ–ª—å–∫—ñ—Å—Ç—å –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤ –ø—Ä–∏ height={height_threshold}: {len(set(clusters))}")

# –†—ñ–∑–Ω—ñ –≤–∏—Å–æ—Ç–∏
for height in [5, 10, 15, 20]:
    clusters = fcluster(Z, height, criterion='distance')
    n_clusters = len(set(clusters))
    print(f"Height {height}: {n_clusters} clusters")
```

---

## 5Ô∏è‚É£ –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è linkage –º–µ—Ç–æ–¥—ñ–≤

```python
from sklearn.datasets import make_moons

# –î–∞–Ω—ñ –∑ –Ω–µ–ª—ñ–Ω—ñ–π–Ω–æ—é —Å—Ç—Ä—É–∫—Ç—É—Ä–æ—é (–¥–≤–∞ "–ø—ñ–≤–º—ñ—Å—è—Ü—ñ")
X, _ = make_moons(n_samples=200, noise=0.05, random_state=42)

# –ú–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# –†—ñ–∑–Ω—ñ linkage –º–µ—Ç–æ–¥–∏
linkage_methods = ['single', 'complete', 'average', 'ward']

fig, axes = plt.subplots(2, 2, figsize=(15, 12))
axes = axes.ravel()

for idx, method in enumerate(linkage_methods):
    # Hierarchical clustering
    hc = AgglomerativeClustering(n_clusters=2, linkage=method)
    labels = hc.fit_predict(X_scaled)
    
    # Silhouette
    sil_score = silhouette_score(X_scaled, labels)
    
    # –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
    axes[idx].scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=50)
    axes[idx].set_title(f'{method.capitalize()} Linkage\nSilhouette: {sil_score:.3f}')
    axes[idx].set_xlabel('Feature 1')
    axes[idx].set_ylabel('Feature 2')
    axes[idx].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

**–û—á—ñ–∫—É–≤–∞–Ω—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏:**

- **Single:** –ó–Ω–∞–π–¥–µ –ø—ñ–≤–º—ñ—Å—è—Ü—ñ ‚úì (–∞–ª–µ chaining –Ω–∞ —Å–∫–ª–∞–¥–Ω–∏—Ö –¥–∞–Ω–∏—Ö)
- **Complete:** –†–æ–∑–¥—ñ–ª–∏—Ç—å –≤–µ—Ä—Ç–∏–∫–∞–ª—å–Ω–æ ‚úó
- **Average:** –ë–∞–ª–∞–Ω—Å
- **Ward:** –†–æ–∑–¥—ñ–ª–∏—Ç—å –≤–µ—Ä—Ç–∏–∫–∞–ª—å–Ω–æ ‚úó (–ø–µ—Ä–µ–¥–±–∞—á–∞—î —Å—Ñ–µ—Ä–∏—á–Ω—ñ –∫–ª–∞—Å—Ç–µ—Ä–∏)

---

## 6Ô∏è‚É£ –ü—Ä–∞–∫—Ç–∏—á–Ω—ñ –ø—Ä–∏–∫–ª–∞–¥–∏

### –ü—Ä–∏–∫–ª–∞–¥ 1: –°–µ–≥–º–µ–Ω—Ç–∞—Ü—ñ—è –∫–ª—ñ—î–Ω—Ç—ñ–≤

```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from scipy.cluster.hierarchy import dendrogram, linkage, fcluster

# 1. –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –¥–∞–Ω–∏—Ö –∫–ª—ñ—î–Ω—Ç—ñ–≤
np.random.seed(42)
n_customers = 200

data = {
    'Age': np.random.randint(18, 70, n_customers),
    'Income': np.random.randint(20000, 150000, n_customers),
    'SpendingScore': np.random.randint(1, 100, n_customers)
}

df = pd.DataFrame(data)
print(df.head())

# 2. Preprocessing
scaler = StandardScaler()
X_scaled = scaler.fit_transform(df)

# 3. –ü–æ–±—É–¥–æ–≤–∞ –¥–µ–Ω–¥—Ä–æ–≥—Ä–∞–º–∏
Z = linkage(X_scaled, method='ward')

plt.figure(figsize=(15, 7))
dendrogram(Z, truncate_mode='lastp', p=20)
plt.title('Customer Segmentation Dendrogram')
plt.xlabel('Cluster size')
plt.ylabel('Distance')
plt.axhline(y=8, color='r', linestyle='--', label='Cut at height=8')
plt.legend()
plt.show()

# 4. –í–∏–±—ñ—Ä –∫—ñ–ª—å–∫–æ—Å—Ç—ñ –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤
optimal_k = 4

# 5. –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—è
hc = AgglomerativeClustering(n_clusters=optimal_k, linkage='ward')
df['Cluster'] = hc.fit_predict(X_scaled)

# 6. –ê–Ω–∞–ª—ñ–∑ –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤
print("\n=== Cluster Analysis ===")
cluster_summary = df.groupby('Cluster').agg({
    'Age': ['mean', 'std'],
    'Income': ['mean', 'std'],
    'SpendingScore': ['mean', 'std']
}).round(2)
print(cluster_summary)

# –ö—ñ–ª—å–∫—ñ—Å—Ç—å –∫–ª—ñ—î–Ω—Ç—ñ–≤ —É –∫–æ–∂–Ω–æ–º—É –∫–ª–∞—Å—Ç–µ—Ä—ñ
print("\nCluster sizes:")
print(df['Cluster'].value_counts().sort_index())

# 7. –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
fig, axes = plt.subplots(1, 2, figsize=(15, 6))

# Income vs Spending Score
axes[0].scatter(df['Income'], df['SpendingScore'], 
                c=df['Cluster'], cmap='viridis', s=50, alpha=0.6)
axes[0].set_xlabel('Income')
axes[0].set_ylabel('Spending Score')
axes[0].set_title('Income vs Spending Score')
axes[0].grid(True, alpha=0.3)

# Age vs Income
axes[1].scatter(df['Age'], df['Income'], 
                c=df['Cluster'], cmap='viridis', s=50, alpha=0.6)
axes[1].set_xlabel('Age')
axes[1].set_ylabel('Income')
axes[1].set_title('Age vs Income')
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

---

### –ü—Ä–∏–∫–ª–∞–¥ 2: –Ü—î—Ä–∞—Ä—Ö—ñ—è –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤ (text clustering)

```python
from sklearn.feature_extraction.text import TfidfVectorizer
from scipy.cluster.hierarchy import dendrogram, linkage

# 1. –î–∞–Ω—ñ (–¥–æ–∫—É–º–µ–Ω—Ç–∏)
documents = [
    "Machine learning is great for data science",
    "Deep learning neural networks are powerful",
    "Python is perfect for machine learning",
    "The cat sat on the mat",
    "Dogs are friendly animals",
    "My cat loves playing with toys",
    "Data science requires statistics knowledge",
    "Neural networks learn from data",
]

# 2. TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü—ñ—è
vectorizer = TfidfVectorizer(stop_words='english')
X_tfidf = vectorizer.fit_transform(documents).toarray()

# 3. Hierarchical clustering
Z = linkage(X_tfidf, method='average')

# 4. –î–µ–Ω–¥—Ä–æ–≥—Ä–∞–º–∞
plt.figure(figsize=(12, 6))
dendrogram(
    Z,
    labels=[f"Doc {i+1}" for i in range(len(documents))],
    leaf_font_size=10
)
plt.title('Document Hierarchy')
plt.xlabel('Document')
plt.ylabel('Distance')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# 5. –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—è
n_clusters = 3
hc = AgglomerativeClustering(n_clusters=n_clusters, linkage='average')
labels = hc.fit_predict(X_tfidf)

# 6. –†–µ–∑—É–ª—å—Ç–∞—Ç–∏
for cluster_id in range(n_clusters):
    print(f"\n=== Cluster {cluster_id} ===")
    cluster_docs = [doc for doc, label in zip(documents, labels) if label == cluster_id]
    for doc in cluster_docs:
        print(f"  - {doc}")
```

**–û—á—ñ–∫—É–≤–∞–Ω—ñ –∫–ª–∞—Å—Ç–µ—Ä–∏:**

- Cluster 0: ML/Data Science –¥–æ–∫—É–º–µ–Ω—Ç–∏
- Cluster 1: –¢–≤–∞—Ä–∏–Ω–∏ (–∫—ñ—à–∫–∏/—Å–æ–±–∞–∫–∏)
- Cluster 2: –Ü–Ω—à–µ

---

### –ü—Ä–∏–∫–ª–∞–¥ 3: –Ü—î—Ä–∞—Ä—Ö—ñ—á–Ω–∞ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü—ñ—è –∑–æ–±—Ä–∞–∂–µ–Ω—å (–∫–æ–ª—ñ—Ä)

```python
import numpy as np
from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram, linkage
import matplotlib.pyplot as plt

# 1. –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –∫–æ–ª—å–æ—Ä–æ–≤–∏—Ö —Ç–æ—á–æ–∫ (RGB)
np.random.seed(42)

# 3 –≥—Ä—É–ø–∏ –∫–æ–ª—å–æ—Ä—ñ–≤: —á–µ—Ä–≤–æ–Ω—ñ, —Å–∏–Ω—ñ, –∑–µ–ª–µ–Ω—ñ
colors_red = np.random.rand(50, 3) * [1, 0.3, 0.3] + [0, 0, 0]
colors_blue = np.random.rand(50, 3) * [0.3, 0.3, 1] + [0, 0, 0]
colors_green = np.random.rand(50, 3) * [0.3, 1, 0.3] + [0, 0, 0]

X_colors = np.vstack([colors_red, colors_blue, colors_green])
X_colors = np.clip(X_colors, 0, 1)  # –æ–±–º–µ–∂–∏—Ç–∏ [0, 1]

# 2. Hierarchical clustering
hc = AgglomerativeClustering(n_clusters=3, linkage='ward')
labels = hc.fit_predict(X_colors)

# 3. –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
fig, axes = plt.subplots(1, 2, figsize=(15, 6))

# –û—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω—ñ –∫–æ–ª—å–æ—Ä–∏
axes[0].scatter(range(len(X_colors)), [0]*len(X_colors), 
                c=X_colors, s=100, marker='s')
axes[0].set_title('Original Colors')
axes[0].set_yticks([])
axes[0].set_xlabel('Color index')

# –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–æ–≤–∞–Ω—ñ (–≤—ñ–¥—Å–æ—Ä—Ç–æ–≤–∞–Ω—ñ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞—Ö)
sorted_idx = np.argsort(labels)
axes[1].scatter(range(len(X_colors)), [0]*len(X_colors),
                c=X_colors[sorted_idx], s=100, marker='s')
axes[1].set_title('Clustered Colors (sorted by cluster)')
axes[1].set_yticks([])
axes[1].set_xlabel('Color index')

plt.tight_layout()
plt.show()

# 4. –î–µ–Ω–¥—Ä–æ–≥—Ä–∞–º–∞
Z = linkage(X_colors, method='ward')

plt.figure(figsize=(12, 5))
dendrogram(Z, truncate_mode='lastp', p=20, color_threshold=0)
plt.title('Color Hierarchy')
plt.xlabel('Sample')
plt.ylabel('Distance')
plt.show()
```

---

## 7Ô∏è‚É£ –û—Ü—ñ–Ω–∫–∞ —è–∫–æ—Å—Ç—ñ

```python
from sklearn.metrics import (
    silhouette_score, 
    davies_bouldin_score,
    calinski_harabasz_score
)

def evaluate_hierarchical(X, labels):
    """
    –û—Ü—ñ–Ω–∏—Ç–∏ —è–∫—ñ—Å—Ç—å Hierarchical Clustering
    """
    n_clusters = len(set(labels))
    n_samples = len(X)
    
    print("=== Hierarchical Clustering Results ===")
    print(f"–ö—ñ–ª—å–∫—ñ—Å—Ç—å –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤: {n_clusters}")
    
    # –ú–µ—Ç—Ä–∏–∫–∏
    sil_score = silhouette_score(X, labels)
    db_score = davies_bouldin_score(X, labels)
    ch_score = calinski_harabasz_score(X, labels)
    
    print(f"\n=== Metrics ===")
    print(f"Silhouette Score: {sil_score:.3f}")
    print(f"Davies-Bouldin Index: {db_score:.3f}")
    print(f"Calinski-Harabasz Score: {ch_score:.1f}")
    
    # –Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü—ñ—è
    print(f"\n–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü—ñ—è:")
    if sil_score > 0.7:
        print("  ‚úì –í—ñ–¥–º—ñ–Ω–Ω–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—è")
    elif sil_score > 0.5:
        print("  ‚úì –î–æ–±—Ä–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—è")
    elif sil_score > 0.25:
        print("  ‚ö† –°–ª–∞–±–∫–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—è")
    else:
        print("  ‚úó –ü–æ–≥–∞–Ω–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—è")
    
    # –†–æ–∑–ø–æ–¥—ñ–ª –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞—Ö
    print(f"\n=== Cluster Sizes ===")
    unique, counts = np.unique(labels, return_counts=True)
    for cluster_id, count in zip(unique, counts):
        print(f"Cluster {cluster_id}: {count} points ({count/n_samples*100:.1f}%)")

# –ü—Ä–∏–∫–ª–∞–¥
evaluate_hierarchical(X_scaled, labels)
```

---

## 8Ô∏è‚É£ Distance threshold (–±–µ–∑ —Ñ—ñ–∫—Å–æ–≤–∞–Ω–æ–≥–æ K)

```python
from sklearn.cluster import AgglomerativeClustering

# –ó–∞–º—ñ—Å—Ç—å n_clusters –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ distance_threshold
hc = AgglomerativeClustering(
    n_clusters=None,           # None!
    distance_threshold=10,     # –ø–æ—Ä—ñ–≥ –≤—ñ–¥—Å—Ç–∞–Ω—ñ
    linkage='ward'
)

labels = hc.fit_predict(X_scaled)

print(f"–ó–Ω–∞–π–¥–µ–Ω–æ –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤: {hc.n_clusters_}")
print(f"–ö—ñ–ª—å–∫—ñ—Å—Ç—å –ª–∏—Å—Ç–∫—ñ–≤: {hc.n_leaves_}")

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
plt.figure(figsize=(10, 6))
plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=50)
plt.title(f'HC with distance_threshold=10 ‚Üí {hc.n_clusters_} clusters')
plt.colorbar(label='Cluster')
plt.show()
```

---

## 9Ô∏è‚É£ Connectivity constraints (–æ–±–º–µ–∂–µ–Ω–Ω—è –Ω–∞ —Å—É—Å—ñ–¥—ñ–≤)

```python
from sklearn.neighbors import kneighbors_graph

# –ü–æ–±—É–¥—É–≤–∞—Ç–∏ –≥—Ä–∞—Ñ k –Ω–∞–π–±–ª–∏–∂—á–∏—Ö —Å—É—Å—ñ–¥—ñ–≤
connectivity = kneighbors_graph(X, n_neighbors=10, include_self=False)

# Hierarchical clustering –∑ –æ–±–º–µ–∂–µ–Ω–Ω—è–º:
# –º–æ–∂–Ω–∞ –æ–±'—î–¥–Ω—É–≤–∞—Ç–∏ —Ç—ñ–ª—å–∫–∏ —Å—É—Å—ñ–¥—ñ–≤ –ø–æ –≥—Ä–∞—Ñ—É
hc_constrained = AgglomerativeClustering(
    n_clusters=3,
    connectivity=connectivity,
    linkage='ward'
)

labels_constrained = hc_constrained.fit_predict(X)

# –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è
fig, axes = plt.subplots(1, 2, figsize=(15, 6))

# –ë–µ–∑ –æ–±–º–µ–∂–µ–Ω—å
hc_normal = AgglomerativeClustering(n_clusters=3, linkage='ward')
labels_normal = hc_normal.fit_predict(X)

axes[0].scatter(X[:, 0], X[:, 1], c=labels_normal, cmap='viridis', s=50)
axes[0].set_title('Normal HC')
axes[0].grid(True, alpha=0.3)

# –ó –æ–±–º–µ–∂–µ–Ω–Ω—è–º–∏
axes[1].scatter(X[:, 0], X[:, 1], c=labels_constrained, cmap='viridis', s=50)
axes[1].set_title('HC with connectivity constraints')
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

**–í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è:** –≥–µ–æ–≥—Ä–∞—Ñ—ñ—á–Ω—ñ –¥–∞–Ω—ñ, –¥–µ –º–æ–∂–Ω–∞ –æ–±'—î–¥–Ω—É–≤–∞—Ç–∏ —Ç—ñ–ª—å–∫–∏ —Å—É—Å—ñ–¥–Ω—ñ —Ä–µ–≥—ñ–æ–Ω–∏.

---

## üîü –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ç–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è

```python
import joblib
from scipy.cluster.hierarchy import linkage

# 1. –ù–∞–≤—á–∞–Ω–Ω—è
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

hc = AgglomerativeClustering(n_clusters=3, linkage='ward')
labels = hc.fit_predict(X_scaled)

# –¢–∞–∫–æ–∂ –∑–±–µ—Ä—ñ–≥–∞—î–º–æ linkage matrix –¥–ª—è –¥–µ–Ω–¥—Ä–æ–≥—Ä–∞–º–∏
Z = linkage(X_scaled, method='ward')

# 2. –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è
model_data = {
    'hc': hc,
    'scaler': scaler,
    'linkage_matrix': Z,
    'labels_train': labels,
    'X_train_scaled': X_scaled
}

joblib.dump(model_data, 'hierarchical_model.pkl')

# 3. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è
loaded_data = joblib.load('hierarchical_model.pkl')
loaded_hc = loaded_data['hc']
loaded_scaler = loaded_data['scaler']
loaded_Z = loaded_data['linkage_matrix']

print(f"–ö–ª–∞—Å—Ç–µ—Ä—ñ–≤: {loaded_hc.n_clusters_}")

# 4. –î–µ–Ω–¥—Ä–æ–≥—Ä–∞–º–∞ –∑—ñ –∑–±–µ—Ä–µ–∂–µ–Ω–∏—Ö –¥–∞–Ω–∏—Ö
plt.figure(figsize=(12, 5))
dendrogram(loaded_Z)
plt.title('Loaded Dendrogram')
plt.show()
```

**–í–ê–ñ–õ–ò–í–û:** Hierarchical Clustering **–ù–ï –º–∞—î** –º–µ—Ç–æ–¥—É `predict()` –¥–ª—è –Ω–æ–≤–∏—Ö —Ç–æ—á–æ–∫!

### –Ø–∫ –∫–ª–∞—Å–∏—Ñ—ñ–∫—É–≤–∞—Ç–∏ –Ω–æ–≤—ñ —Ç–æ—á–∫–∏?

```python
from sklearn.neighbors import KNeighborsClassifier

def predict_hierarchical(new_X, hc, X_train, scaler):
    """
    –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –¥–ª—è –Ω–æ–≤–∏—Ö —Ç–æ—á–æ–∫ —á–µ—Ä–µ–∑ KNN
    
    –õ–æ–≥—ñ–∫–∞: –∑–Ω–∞–π—Ç–∏ –Ω–∞–π–±–ª–∏–∂—á–∏—Ö —Å—É—Å—ñ–¥—ñ–≤ –∑ —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö
            —ñ –ø—Ä–∏—Å–≤–æ—ó—Ç–∏ —ó—Ö –∫–ª–∞—Å—Ç–µ—Ä
    """
    # –ú–∞—Å—à—Ç–∞–±—É–≤–∞—Ç–∏ –Ω–æ–≤—ñ –¥–∞–Ω—ñ
    new_X_scaled = scaler.transform(new_X)
    
    # KNN –Ω–∞ —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö
    knn = KNeighborsClassifier(n_neighbors=5)
    knn.fit(X_train, hc.labels_)
    
    # –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
    predictions = knn.predict(new_X_scaled)
    
    return predictions

# –ü—Ä–∏–∫–ª–∞–¥
new_data = np.array([[1.5, 2.5], [8.0, 8.0]])
new_labels = predict_hierarchical(
    new_data, 
    loaded_hc, 
    loaded_data['X_train_scaled'],
    loaded_scaler
)

print(f"–ù–æ–≤—ñ —Ç–æ—á–∫–∏ –Ω–∞–ª–µ–∂–∞—Ç—å –¥–æ –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤: {new_labels}")
```

---

## 1Ô∏è‚É£1Ô∏è‚É£ BIRCH ‚Äî –¥–ª—è –≤–µ–ª–∏–∫–∏—Ö –¥–∞–Ω–∏—Ö

```python
from sklearn.cluster import Birch

# BIRCH: Balanced Iterative Reducing and Clustering using Hierarchies
# –î–æ–±—Ä–µ –º–∞—Å—à—Ç–∞–±—É—î—Ç—å—Å—è –Ω–∞ –≤–µ–ª–∏–∫—ñ –¥–∞–Ω—ñ

birch = Birch(
    n_clusters=3,              # –∞–±–æ None –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ–≥–æ –≤–∏–±–æ—Ä—É
    threshold=0.5,             # –ø–æ—Ä—ñ–≥ –¥–ª—è CFTree
    branching_factor=50        # –∫—ñ–ª—å–∫—ñ—Å—Ç—å –ø—ñ–¥–∫–ª–∞—Å—Ç–µ—Ä—ñ–≤ —É –≤—É–∑–ª—ñ
)

labels = birch.fit_predict(X_scaled)

print(f"–ö–ª–∞—Å—Ç–µ—Ä—ñ–≤: {birch.n_features_in_}")

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
plt.figure(figsize=(10, 6))
plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=50)
plt.title('BIRCH Clustering')
plt.colorbar(label='Cluster')
plt.show()
```

**–ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ BIRCH:**

- –î–∞–Ω—ñ > 10,000 —Ç–æ—á–æ–∫
- –ü–æ—Ç—Ä—ñ–±–Ω–∞ —ñ—î—Ä–∞—Ä—Ö—ñ—è, –∞–ª–µ –∑–≤–∏—á–∞–π–Ω–∏–π HC –∑–∞–Ω–∞–¥—Ç–æ –ø–æ–≤—ñ–ª—å–Ω–∏–π
- –ú–æ–∂–Ω–∞ –ø–æ–∂–µ—Ä—Ç–≤—É–≤–∞—Ç–∏ —Ç—Ä–æ—Ö–∏ —Ç–æ—á–Ω–æ—Å—Ç—ñ –∑–∞—Ä–∞–¥–∏ —à–≤–∏–¥–∫–æ—Å—Ç—ñ

---

## 1Ô∏è‚É£2Ô∏è‚É£ –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –¥–ª—è –≤–∏—Å–æ–∫–æ–≤–∏–º—ñ—Ä–Ω–∏—Ö –¥–∞–Ω–∏—Ö (PCA)

```python
from sklearn.decomposition import PCA
from sklearn.datasets import load_iris

# 1. –î–∞–Ω—ñ (4D)
iris = load_iris()
X_iris = iris.data

# 2. –ú–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_iris)

# 3. Hierarchical clustering
hc = AgglomerativeClustering(n_clusters=3, linkage='ward')
labels = hc.fit_predict(X_scaled)

# 4. PCA –¥–ª—è –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

# 5. –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
fig, axes = plt.subplots(1, 2, figsize=(15, 6))

# HC —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
axes[0].scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='viridis', s=50)
axes[0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')
axes[0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')
axes[0].set_title('Hierarchical Clustering (PCA projection)')
axes[0].grid(True, alpha=0.3)

# –°–ø—Ä–∞–≤–∂–Ω—ñ –º—ñ—Ç–∫–∏
axes[1].scatter(X_pca[:, 0], X_pca[:, 1], c=iris.target, cmap='viridis', s=50)
axes[1].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')
axes[1].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')
axes[1].set_title('True Labels (PCA projection)')
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Silhouette
from sklearn.metrics import silhouette_score
print(f"Silhouette Score: {silhouette_score(X_scaled, labels):.3f}")
```

---

## 1Ô∏è‚É£3Ô∏è‚É£ –ü–æ—Ä–∞–¥–∏ —Ç–∞ best practices

### 1. –ó–∞–≤–∂–¥–∏ –º–∞—Å—à—Ç–∞–±—É–π –¥–∞–Ω—ñ

```python
# –ü–û–ì–ê–ù–û
hc = AgglomerativeClustering(n_clusters=3)
hc.fit(X)

# –î–û–ë–†–ï
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
hc = AgglomerativeClustering(n_clusters=3)
hc.fit(X_scaled)
```

---

### 2. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π Ward –¥–ª—è –∑–∞–≥–∞–ª—å–Ω–∏—Ö –≤–∏–ø–∞–¥–∫—ñ–≤

```python
# –ó–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º
hc = AgglomerativeClustering(n_clusters=3, linkage='ward')
```

–Ø–∫—â–æ –Ω–µ –ø—Ä–∞—Ü—é—î, —Å–ø—Ä–æ–±—É–π `'average'` –∞–±–æ `'complete'`.

---

### 3. –í—ñ–∑—É–∞–ª—ñ–∑—É–π –¥–µ–Ω–¥—Ä–æ–≥—Ä–∞–º—É –ø–µ—Ä–µ–¥ –≤–∏–±–æ—Ä–æ–º K

```python
# –ó–ê–í–ñ–î–ò –±—É–¥—É–π –¥–µ–Ω–¥—Ä–æ–≥—Ä–∞–º—É –ø–µ—Ä–µ–¥ —Ñ—ñ–∫—Å–∞—Ü—ñ—î—é K!
from scipy.cluster.hierarchy import dendrogram, linkage

Z = linkage(X_scaled, method='ward')
plt.figure(figsize=(12, 5))
dendrogram(Z)
plt.show()

# –¢—ñ–ª—å–∫–∏ –ø–æ—Ç—ñ–º –≤–∏–±–∏—Ä–∞–π K
```

---

### 4. –î–ª—è –≤–µ–ª–∏–∫–∏—Ö –¥–∞–Ω–∏—Ö: sampling –∞–±–æ BIRCH

```python
# –Ø–∫—â–æ n > 10,000
if len(X) > 10000:
    # –í–∞—Ä—ñ–∞–Ω—Ç 1: Sampling
    sample_idx = np.random.choice(len(X), size=5000, replace=False)
    X_sample = X[sample_idx]
    Z = linkage(X_sample, method='ward')
    
    # –í–∞—Ä—ñ–∞–Ω—Ç 2: BIRCH
    from sklearn.cluster import Birch
    birch = Birch(n_clusters=3)
    labels = birch.fit_predict(X)
```

---

### 5. –ü–µ—Ä–µ–≤—ñ—Ä—è–π —Ä—ñ–∑–Ω—ñ linkage

```python
for method in ['ward', 'average', 'complete']:
    hc = AgglomerativeClustering(n_clusters=3, linkage=method)
    labels = hc.fit_predict(X_scaled)
    score = silhouette_score(X_scaled, labels)
    print(f"{method}: Silhouette={score:.3f}")
```

---

### 6. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π distance_threshold –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ–≥–æ K

```python
# –ó–∞–º—ñ—Å—Ç—å –≥–∞–¥–∞–Ω–Ω—è K, –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–π distance_threshold
hc = AgglomerativeClustering(
    n_clusters=None,
    distance_threshold=10,
    linkage='ward'
)
labels = hc.fit_predict(X_scaled)
print(f"–ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –∑–Ω–∞–π–¥–µ–Ω–æ {hc.n_clusters_} –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤")
```

---

## –ß–µ–∫-–ª–∏—Å—Ç –¥–ª—è Hierarchical Clustering

```python
# ‚úÖ 1. –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –¥–∞–Ω—ñ
X = load_data()

# ‚úÖ 2. EDA
print(X.shape)
print(pd.DataFrame(X).describe())

# ‚úÖ 3. –ú–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è (–û–ë–û–í'–Ø–ó–ö–û–í–û!)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# ‚úÖ 4. –ü–æ–±—É–¥—É–≤–∞—Ç–∏ –¥–µ–Ω–¥—Ä–æ–≥—Ä–∞–º—É
from scipy.cluster.hierarchy import dendrogram, linkage
Z = linkage(X_scaled, method='ward')
plt.figure(figsize=(12, 5))
dendrogram(Z)
plt.show()

# ‚úÖ 5. –í–∏–±—Ä–∞—Ç–∏ K (–≤—ñ–∑—É–∞–ª—å–Ω–æ –∞–±–æ —á–µ—Ä–µ–∑ –º–µ—Ç—Ä–∏–∫–∏)
optimal_k = 3  # –∑ –¥–µ–Ω–¥—Ä–æ–≥—Ä–∞–º–∏ –∞–±–æ silhouette

# ‚úÖ 6. –ù–∞–≤—á–∞–Ω–Ω—è
hc = AgglomerativeClustering(n_clusters=optimal_k, linkage='ward')
labels = hc.fit_predict(X_scaled)

# ‚úÖ 7. –û—Ü—ñ–Ω–∫–∞ —è–∫–æ—Å—Ç—ñ
evaluate_hierarchical(X_scaled, labels)

# ‚úÖ 8. –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
visualize_clusters(X, labels)

# ‚úÖ 9. –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è
joblib.dump({'hc': hc, 'scaler': scaler, 'Z': Z}, 'model.pkl')
```

---

## –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –∑ K-Means —ñ DBSCAN

```python
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.datasets import make_moons
import time

# –î–∞–Ω—ñ –∑ –Ω–µ–ª—ñ–Ω—ñ–π–Ω–æ—é —Å—Ç—Ä—É–∫—Ç—É—Ä–æ—é
X, _ = make_moons(n_samples=300, noise=0.05, random_state=42)

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# K-Means
start = time.time()
kmeans = KMeans(n_clusters=2, random_state=42)
labels_km = kmeans.fit_predict(X_scaled)
time_km = time.time() - start

# DBSCAN
start = time.time()
dbscan = DBSCAN(eps=0.3, min_samples=5)
labels_db = dbscan.fit_predict(X_scaled)
time_db = time.time() - start

# Hierarchical
start = time.time()
hc = AgglomerativeClustering(n_clusters=2, linkage='single')
labels_hc = hc.fit_predict(X_scaled)
time_hc = time.time() - start

# –†–µ–∑—É–ª—å—Ç–∞—Ç–∏
from sklearn.metrics import silhouette_score

print("=== Comparison ===")
print(f"K-Means:      Silhouette={silhouette_score(X_scaled, labels_km):.3f}, Time={time_km:.4f}s")
print(f"DBSCAN:       Silhouette={silhouette_score(X_scaled[labels_db!=-1], labels_db[labels_db!=-1]):.3f}, Time={time_db:.4f}s")
print(f"Hierarchical: Silhouette={silhouette_score(X_scaled, labels_hc):.3f}, Time={time_hc:.4f}s")

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

axes[0].scatter(X[:, 0], X[:, 1], c=labels_km, cmap='viridis', s=50)
axes[0].set_title(f'K-Means (Sil={silhouette_score(X_scaled, labels_km):.2f})')

axes[1].scatter(X[:, 0], X[:, 1], c=labels_db, cmap='viridis', s=50)
axes[1].set_title(f'DBSCAN (Sil={silhouette_score(X_scaled[labels_db!=-1], labels_db[labels_db!=-1]):.2f})')

axes[2].scatter(X[:, 0], X[:, 1], c=labels_hc, cmap='viridis', s=50)
axes[2].set_title(f'Hierarchical-Single (Sil={silhouette_score(X_scaled, labels_hc):.2f})')

plt.tight_layout()
plt.show()
```

**–í–∏—Å–Ω–æ–≤–æ–∫:**

- **K-Means:** —à–≤–∏–¥–∫–∏–π, –∞–ª–µ –ø–æ–≥–∞–Ω–æ –Ω–∞ —Å–∫–ª–∞–¥–Ω–∏—Ö —Ñ–æ—Ä–º–∞—Ö
- **DBSCAN:** –¥–æ–±—Ä–µ –Ω–∞ —Å–∫–ª–∞–¥–Ω–∏—Ö —Ñ–æ—Ä–º–∞—Ö, –º–æ–∂–µ –∑–Ω–∞–π—Ç–∏ noise
- **Hierarchical (single):** –¥–æ–±—Ä–µ –Ω–∞ —Å–∫–ª–∞–¥–Ω–∏—Ö —Ñ–æ—Ä–º–∞—Ö, –ø–æ–∫–∞–∑—É—î —ñ—î—Ä–∞—Ä—Ö—ñ—é

---

## –ö–æ—Ä–∏—Å–Ω—ñ –ø–æ—Å–∏–ª–∞–Ω–Ω—è

- [sklearn AgglomerativeClustering](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html)
- [scipy.cluster.hierarchy](https://docs.scipy.org/doc/scipy/reference/cluster.hierarchy.html)
- [sklearn Clustering Guide](https://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering)

---

**–°—Ç–≤–æ—Ä–µ–Ω–æ –¥–ª—è –ø—Ä–∞–∫—Ç–∏—á–Ω–æ–≥–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è Hierarchical Clustering –≤ –ø—Ä–æ—î–∫—Ç–∞—Ö** üöÄ