# Hierarchical Clustering — Ієрархічна кластеризація

**Hierarchical Clustering** — алгоритм кластеризації, який будує **ієрархію** (дерево) кластерів замість того, щоб одразу розділити дані на K груп.

**Головна ідея:** Послідовно об'єднувати (або розділяти) точки/кластери, створюючи деревоподібну структуру кластерів.

---

## Що це

Hierarchical Clustering — це алгоритм, який:

- **Не потребує заздалегідь знати кількість кластерів K** ✓
- **Будує дендрограму** (дерево кластерів) ✓
- **Можна "розрізати" на будь-якому рівні** для отримання K кластерів ✓
- **Показує структуру даних** та відносини між кластерами ✓

---

## Навіщо

### Проблеми K-Means і DBSCAN

```
K-Means:
  ✗ Потрібно знати K заздалегідь
  ✗ Не показує ієрархію (чи можна об'єднати кластери)
  
DBSCAN:
  ✗ Складно підібрати eps
  ✗ Не показує ієрархічну структуру

Hierarchical Clustering:
  ✓ K визначаєш ПІСЛЯ побудови дерева
  ✓ Бачиш всю ієрархію зв'язків
  ✓ Легко інтерпретувати через дендрограму
```

### Приклади задач

- Таксономія (класифікація видів)
- Ієрархія документів (папки → підпапки → файли)
- Організаційна структура компанії
- Філогенетичні дерева (еволюційні зв'язки)
- Сегментація ринку з підгрупами

---

## Два підходи

### 1. Agglomerative (Знизу вгору) ⬆

**Ідея:** Починаємо з кожної точки як окремого кластера, потім **об'єднуємо** найближчі кластери.

```
Крок 0: Кожна точка — окремий кластер
●  ●  ●  ●  ●  ●  (6 кластерів)

Крок 1: Об'єднуємо найближчі
(●●)  ●  (●●)  ●  (4 кластери)

Крок 2: Продовжуємо об'єднувати
((●●)●)  ((●●)●)  (2 кластери)

Крок 3: Фінальне об'єднання
(((●●)●) ((●●)●)) (1 кластер)
```

**Це найпопулярніший підхід!** Використовується в sklearn.

---

### 2. Divisive (Згори вниз) ⬇

**Ідея:** Починаємо з усіх точок в одному кластері, потім **розділяємо** на підгрупи.

```
Крок 0: Всі точки в одному кластері
(●●●●●●)  (1 кластер)

Крок 1: Розділяємо на 2
(●●●) (●●●)  (2 кластери)

Крок 2: Кожен розділяємо далі
(●●)(●) (●●)(●)  (4 кластери)

Крок 3: До окремих точок
● ● ● ● ● ●  (6 кластерів)
```

**Рідше використовується** (складніший, повільніший).

---

## Agglomerative — покроковий алгоритм

**Вхід:** Точки даних X = {x₁, x₂, ..., xₙ}

### Крок 1: Ініціалізація

Кожна точка — окремий кластер:

```
C₁ = {x₁}
C₂ = {x₂}
...
Cₙ = {xₙ}

Маємо n кластерів
```

---

### Крок 2: Знайти найближчі кластери

Рахуємо відстань між **всіма парами** кластерів:

```
d(C₁, C₂) = ?
d(C₁, C₃) = ?
d(C₂, C₃) = ?
...

Знаходимо мінімальну: min d(Cᵢ, Cⱼ)
```

**Як рахувати відстань між кластерами?** → див. **Linkage методи** нижче

---

### Крок 3: Об'єднати найближчі кластери

```
Було: C₁ = {x₁}, C₂ = {x₂}
       ↓ (об'єднуємо)
Стало: C₁₂ = {x₁, x₂}

Кількість кластерів: n → n-1
```

---

### Крок 4: Повторити кроки 2-3

Повторюємо, поки не залишиться **1 кластер** (всі точки).

```
n кластерів → n-1 → n-2 → ... → 2 → 1
```

Зберігаємо **історію об'єднань** → це і є дендрограма!

---

### Псевдокод

```python
def agglomerative_clustering(X, linkage='single'):
    # 1. Кожна точка — кластер
    clusters = [{i} for i in range(len(X))]
    
    # Історія об'єднань (для дендрограми)
    history = []
    
    # 2. Поки не залишиться 1 кластер
    while len(clusters) > 1:
        # Знайти пару найближчих кластерів
        min_dist = float('inf')
        merge_i, merge_j = 0, 1
        
        for i in range(len(clusters)):
            for j in range(i+1, len(clusters)):
                dist = cluster_distance(clusters[i], clusters[j], X, linkage)
                if dist < min_dist:
                    min_dist = dist
                    merge_i, merge_j = i, j
        
        # Об'єднати кластери
        new_cluster = clusters[merge_i] | clusters[merge_j]
        
        # Зберегти в історію
        history.append((merge_i, merge_j, min_dist))
        
        # Видалити старі, додати новий
        clusters = [c for idx, c in enumerate(clusters) 
                   if idx not in [merge_i, merge_j]]
        clusters.append(new_cluster)
    
    return history
```

---

## Linkage методи (способи рахувати відстань між кластерами)

**Проблема:** Як рахувати відстань між двома **кластерами** (групами точок)?

```
Кластер A: {x₁, x₂, x₃}
Кластер B: {x₄, x₅}

d(A, B) = ?
```

Існує кілька методів:

---

### 1. Single Linkage (Мінімальна відстань)

**Ідея:** Відстань = **найменша** відстань між будь-якими точками з двох кластерів.

```
d(A, B) = min { d(xᵢ, xⱼ) | xᵢ ∈ A, xⱼ ∈ B }
```

**Візуально:**

```
Кластер A:  ●  ●  ●
              ↘  
                ↘ найближча пара
Кластер B:        ●  ●
                  
d(A, B) = відстань між найближчою парою
```

**Приклад:**

```
A = {[1,1], [2,2], [3,1]}
B = {[8,8], [9,9]}

d([3,1], [8,8]) = 10.6  ← найменша
d([1,1], [8,8]) = 12.7
d([2,2], [9,9]) = 11.4

d(A, B) = 10.6
```

**Переваги:** Знаходить "ланцюжки" кластерів (elongated clusters)

**Недоліки:** Схильний до **chaining effect** (може об'єднати далекі кластери через ланцюжок точок)

---

### 2. Complete Linkage (Максимальна відстань)

**Ідея:** Відстань = **найбільша** відстань між будь-якими точками.

```
d(A, B) = max { d(xᵢ, xⱼ) | xᵢ ∈ A, xⱼ ∈ B }
```

**Візуально:**

```
Кластер A:  ●  ●  ●
            ↗       
          ↗ найдальша пара
Кластер B:  ●  ●
```

**Приклад:**

```
A = {[1,1], [2,2], [3,1]}
B = {[8,8], [9,9]}

d([1,1], [9,9]) = 13.4  ← найбільша
d([3,1], [8,8]) = 10.6

d(A, B) = 13.4
```

**Переваги:** Створює **компактні** кластери (всі точки близькі один до одного)

**Недоліки:** Може розірвати "природні" кластери складної форми

---

### 3. Average Linkage (Середня відстань)

**Ідея:** Відстань = **середня** відстань між усіма парами точок.

```
d(A, B) = (1 / |A|×|B|) × Σ d(xᵢ, xⱼ)
          для всіх xᵢ ∈ A, xⱼ ∈ B
```

**Приклад:**

```
A = {[1,1], [2,2]}  (2 точки)
B = {[8,8]}         (1 точка)

d([1,1], [8,8]) = 9.9
d([2,2], [8,8]) = 8.5

d(A, B) = (9.9 + 8.5) / (2×1) = 9.2
```

**Переваги:** Баланс між single і complete, стабільніший

**Недоліки:** Повільніший обчислення

---

### 4. Ward's Method (Мінімізація дисперсії)

**Ідея:** Об'єднуємо кластери так, щоб **мінімізувати збільшення дисперсії**.

```
d(A, B) = скільки збільшиться загальна дисперсія, 
          якщо об'єднати A і B
```

**Формула:** (спрощено)

```
d(A, B) = (|A|×|B| / (|A|+|B|)) × ||μₐ - μᵦ||²

де μₐ, μᵦ — центри кластерів A і B
```

**Інтуїція:**

- Якщо центри кластерів далеко → велике d(A, B) → не об'єднуємо
- Якщо центри близько → мале d(A, B) → об'єднуємо

**Переваги:**

- Створює **сферичні, рівного розміру** кластери
- **Найпопулярніший метод!** (схожий на K-Means)

**Недоліки:**

- Чутливий до викидів
- Передбачає сферичні кластери

---

### Порівняння linkage методів

|Метод|Форма кластерів|Стійкість до викидів|Швидкість|Коли використовувати|
|---|---|---|---|---|
|**Single**|Ланцюжки, складна|Погана (chaining)|Швидко|Кластери складної форми|
|**Complete**|Компактні, сферичні|Середня|Середньо|Рівномірні кластери|
|**Average**|Середні|Добра|Повільно|Баланс|
|**Ward**|Сферичні|Погана|Середньо|Схожість на K-Means ✓|

**Рекомендація:** Почни з **Ward** (за замовчуванням у багатьох бібліотеках).

---

## Дендрограма — дерево кластерів

**Дендрограма** — це візуалізація ієрархії кластерів.

### Як читати дендрограму

```
Висота
  │
 10│                 ┌─────┐
  9│         ┌───────┤     │
  8│         │       │     │
  7│     ┌───┤       │     │
  6│     │   │       │     │
  5│   ┌─┤   │       │     │
  4│   │ │   │       │     │
  3│ ┌─┤ │   │       │     │
  2│ │ │ │   │       │     │
  1│ │ │ │   │       │     │
  0└─┴─┴─┴───┴───────┴─────┴──
    1 2 3   4       5 6   7 8
         Точки даних
```

**Елементи дендрограми:**

1. **Листя (внизу)** — окремі точки
2. **Вузли (зверху)** — об'єднані кластери
3. **Висота** — відстань, на якій відбулося об'єднання

---

### Приклад читання

```
Висота
  │
  6│         ┌─────┐
  5│     ┌───┤     │
  4│   ┌─┤   │     │
  3│ ┌─┤ │   │     │
  2│ │ │ │   │     │
  1│ │ │ │   │     │
  0└─┴─┴─┴───┴─────┴─
    1 2 3   4     5 6

Інтерпретація:
- На висоті 2: об'єдналися точки 1 і 2 (вони найближчі)
- На висоті 3: точка 3 приєдналася до {1,2}
- На висоті 4: точка 4 окремо
- На висоті 5: {1,2,3} об'єдналися з {4}
- На висоті 6: {1,2,3,4} об'єдналися з {5,6}
```

**Чим вища гілка, тим пізніше (далі) об'єдналися кластери.**

---

### Вибір кількості кластерів

**Метод 1: Розрізати на певній висоті**

```
Висота
  │
  6│         ┌─────┐
  5│     ┌───┤     │ ← розріз тут (висота = 5.5)
  4│   ┌─┤   │     │
  3│ ┌─┤ │   │     │
  2│ │ │ │   │     │
  1│ │ │ │   │     │
  0└─┴─┴─┴───┴─────┴─
    1 2 3   4     5 6

Розріз на висоті 5.5 → 2 кластери:
  - Кластер 1: {1, 2, 3, 4}
  - Кластер 2: {5, 6}
```

**Метод 2: Задати кількість кластерів**

```python
# Хочемо K=3 кластери
# Алгоритм автоматично знайде висоту розрізу

Висота
  │
  6│         ┌─────┐
  5│     ┌───┤     │
  4│   ┌─┤   │     │ ← розріз тут для K=3
  3│ ┌─┤ │   │     │
  2│ │ │ │   │     │
  1│ │ │ │   │     │
  0└─┴─┴─┴───┴─────┴─

K=3 → кластери:
  - {1, 2, 3}
  - {4}
  - {5, 6}
```

**Метод 3: Візуально — шукати "великий розрив"**

```
Висота
  │
 10│                 ┌─────┐
  9│         ┌───────┤     │
  8│         │       │     │  
  7│     ┌───┤       │     │ ← великий розрив (7→9)
  6│     │   │       │     │    тут варто розрізати!
  5│   ┌─┤   │       │     │
  ...

Великий вертикальний розрив = природна точка розділення
```

---

## Простий приклад (вручну)

**Дані:** 5 точок

```
x₁ = [1, 1]
x₂ = [2, 1]
x₃ = [5, 5]
x₄ = [6, 5]
x₅ = [10, 10]
```

**Візуально:**

```
10│              ● x₅
  │
 5│        ● ●
  │        x₃ x₄
  │
 1│  ● ●
  │  x₁ x₂
  └──────────────
    1  2  5 6  10
```

**Linkage:** Single (мінімальна відстань)

---

### Ітерація 1

**Кластери:** {x₁}, {x₂}, {x₃}, {x₄}, {x₅}

**Матриця відстаней між точками:**

```
     x₁   x₂   x₃   x₄   x₅
x₁   0   1.0  5.7  7.1  12.7
x₂  1.0  0   5.0  6.4  12.0
x₃  5.7 5.0  0   1.0  7.1
x₄  7.1 6.4  1.0  0   7.1
x₅ 12.7 12.0 7.1 7.1  0
```

**Мінімальна відстань:** d(x₁, x₂) = 1.0

**Об'єднуємо:** {x₁, x₂}

**Історія:** (x₁, x₂, висота=1.0)

**Нові кластери:** {x₁, x₂}, {x₃}, {x₄}, {x₅}

---

### Ітерація 2

**Кластери:** {x₁, x₂}, {x₃}, {x₄}, {x₅}

**Відстані між кластерами (Single linkage):**

```
           {x₁,x₂}  x₃   x₄   x₅
{x₁,x₂}       0    5.0  6.4  12.0
x₃          5.0     0   1.0  7.1
x₄          6.4    1.0   0   7.1
x₅         12.0    7.1  7.1   0
```

Для {x₁,x₂} і x₃:

```
d({x₁,x₂}, x₃) = min(d(x₁,x₃), d(x₂,x₃))
               = min(5.7, 5.0) 
               = 5.0
```

**Мінімальна відстань:** d(x₃, x₄) = 1.0

**Об'єднуємо:** {x₃, x₄}

**Історія:** (x₃, x₄, висота=1.0)

**Нові кластери:** {x₁, x₂}, {x₃, x₄}, {x₅}

---

### Ітерація 3

**Кластери:** {x₁, x₂}, {x₃, x₄}, {x₅}

**Відстані:**

```
           {x₁,x₂}  {x₃,x₄}  x₅
{x₁,x₂}       0      5.0    12.0
{x₃,x₄}      5.0      0     7.1
x₅          12.0     7.1     0
```

**Мінімальна відстань:** d({x₁,x₂}, {x₃,x₄}) = 5.0

**Об'єднуємо:** {x₁, x₂, x₃, x₄}

**Історія:** ({x₁,x₂}, {x₃,x₄}, висота=5.0)

**Нові кластери:** {x₁, x₂, x₃, x₄}, {x₅}

---

### Ітерація 4 (фінальна)

**Кластери:** {x₁, x₂, x₃, x₄}, {x₅}

**Відстань:** d({x₁,x₂,x₃,x₄}, {x₅}) = 7.1

**Об'єднуємо:** {x₁, x₂, x₃, x₄, x₅}

**Історія:** ({x₁,x₂,x₃,x₄}, x₅, висота=7.1)

---

### Дендрограма результату

```
Висота
  │
  7│               ┌───────────┐
  6│               │           │
  5│       ┌───────┤           │
  4│       │       │           │
  3│       │       │           │
  2│       │       │           │
  1│   ┌───┤   ┌───┤           │
  0└───┴───┴───┴───┴───────────┴───
      x₁  x₂  x₃  x₄          x₅
```

**Інтерпретація:**

- x₁ і x₂ об'єдналися на висоті 1.0 (близькі)
- x₃ і x₄ об'єдналися на висоті 1.0 (близькі)
- Ці дві групи об'єдналися на висоті 5.0 (середня відстань)
- x₅ приєдналася на висоті 7.1 (далека точка)

**Вибір K:**

- **K=2:** розріз на висоті 6 → {x₁,x₂,x₃,x₄}, {x₅}
- **K=3:** розріз на висоті 2 → {x₁,x₂}, {x₃,x₄}, {x₅}

---

## Переваги та недоліки

### ✅ Переваги

|Перевага|Пояснення|
|---|---|
|**Не потрібно задавати K**|Будуєш дерево один раз, вибираєш K потім|
|**Візуалізація структури**|Дендрограма показує всю ієрархію|
|**Детермінованість**|Завжди однаковий результат (без random)|
|**Інтерпретованість**|Легко пояснити бізнесу|
|**Гнучкість**|Різні linkage → різні форми кластерів|

---

### ❌ Недоліки

|Недолік|Пояснення|
|---|---|
|**Повільний**|O(n²log n) або O(n³) — погано для великих даних|
|**Не можна змінити**|Після об'єднання не можна "роз'єднати"|
|**Чутливість до шуму**|Викиди можуть зіпсувати дерево|
|**Масштабування**|Погано працює з > 10,000 точок|

---

## Порівняння з K-Means і DBSCAN

|Аспект|K-Means|DBSCAN|Hierarchical|
|---|---|---|---|
|**Потрібно K**|Так ✗|Ні ✓|Ні ✓|
|**Форма кластерів**|Сферичні|Довільна|Залежить від linkage|
|**Викиди**|Погано|Добре ✓|Погано|
|**Швидкість**|Швидко ✓|Середньо|Повільно ✗|
|**Детермінованість**|Ні (init)|Так ✓|Так ✓|
|**Візуалізація**|Scatter plot|Scatter plot|Дендрограма ✓|
|**Великі дані**|Добре ✓|Середньо|Погано ✗|

---

## Коли використовувати Hierarchical Clustering

### ✅ Ідеально підходить

1. **Малі/середні дані** (< 10,000 точок)
2. **Потрібна ієрархія** (дерево кластерів)
3. **Невідомий K** (explorative analysis)
4. **Інтерпретація важлива** (для бізнесу/науки)
5. **Таксономія** (біологія, документи)

**Приклади:**

- Класифікація видів тварин
- Організація бібліотеки документів
- Сегментація клієнтів з підгрупами
- Генеалогічні дерева

---

### ❌ Краще використати інше

1. **Великі дані** (> 50,000 точок)
    - Альтернатива: K-Means, MiniBatchKMeans
2. **Потрібна швидкість**
    - Альтернатива: K-Means
3. **Кластери складної форми + великі дані**
    - Альтернатива: DBSCAN, HDBSCAN
4. **Багато викидів**
    - Альтернатива: DBSCAN

---

## Варіації та оптимізації

### 1. BIRCH (для великих даних)

**Ідея:** Summarize data first, then cluster summaries

- Працює з мільйонами точок
- Створює CFTree (Clustering Feature Tree)
- В sklearn: `sklearn.cluster.Birch`

---

### 2. CURE (Clustering Using REpresentatives)

**Ідея:** Використовує кілька представників кластера (не тільки центр)

- Краще для не-сферичних кластерів
- Стійкіший до викидів

---

### 3. CHAMELEON

**Ідея:** Враховує як близькість, так і зв'язаність кластерів

- Краще для складних форм
- Дуже повільний

---

## Складність алгоритму

### Час виконання

**Наївна реалізація:**

- O(n³) — для кожного кроку рахуємо всі відстані

**З оптимізаціями (priority queue):**

- O(n² log n) — для Single/Complete linkage
- O(n³) — для Ward (більше обчислень)

### Пам'ять

- O(n²) — зберігаємо матрицю відстаней

**Порівняння:**

|Метод|Час|Пам'ять|
|---|---|---|
|K-Means|O(nKdt)|O(n)|
|DBSCAN|O(n log n)|O(n)|
|Hierarchical|O(n² log n)|O(n²)|

де n — точок, K — кластерів, d — розмірність, t — ітерації

---

## Практичні поради

### 1. Завжди масштабуй дані

```python
from sklearn.preprocessing import StandardScaler

# ПОГАНО
linkage_matrix = linkage(X, method='ward')

# ДОБРЕ
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
linkage_matrix = linkage(X_scaled, method='ward')
```

---

### 2. Почни з Ward linkage

```python
# За замовчуванням у більшості випадків:
method = 'ward'  # схожий на K-Means
```

Якщо не працює, спробуй:

- `'average'` — більш збалансований
- `'complete'` — компактні кластери
- `'single'` — складні форми (але обережно — chaining!)

---

### 3. Візуалізуй дендрограму

```python
from scipy.cluster.hierarchy import dendrogram, linkage
import matplotlib.pyplot as plt

# Побудувати дерево
Z = linkage(X_scaled, method='ward')

# Дендрограма
plt.figure(figsize=(12, 5))
dendrogram(Z)
plt.title('Dendrogram')
plt.xlabel('Sample index')
plt.ylabel('Distance')
plt.show()
```

**Шукай:**

- Великі вертикальні розриви (хороші місця для розрізу)
- Симетричність дерева
- Чи є викиди (окремі довгі гілки)

---

### 4. Експериментуй з висотою розрізу

```python
from scipy.cluster.hierarchy import fcluster

# Різні висоти
for height in [5, 10, 15, 20]:
    clusters = fcluster(Z, height, criterion='distance')
    print(f"Height {height}: {len(set(clusters))} clusters")
    
# Height 5: 8 clusters
# Height 10: 4 clusters  ← оптимально?
# Height 15: 2 clusters
```

---

### 5. Перевір метриками

```python
from sklearn.metrics import silhouette_score

for k in range(2, 10):
    clusters = fcluster(Z, k, criterion='maxclust')
    score = silhouette_score(X_scaled, clusters)
    print(f"K={k}: Silhouette={score:.3f}")
```

---

### 6. Для великих даних: застосуй sampling

```python
# Якщо n > 10,000
if len(X) > 10000:
    # Вибери випадкову вибірку
    sample_idx = np.random.choice(len(X), size=5000, replace=False)
    X_sample = X[sample_idx]
    
    # Побудуй дерево на вибірці
    Z = linkage(X_sample, method='ward')
    
    # Або використай BIRCH
    from sklearn.cluster import Birch
    birch = Birch(n_clusters=None)  # автоматично
    birch.fit(X)
```

---

## Ключові висновки

> Hierarchical Clustering — потужний метод для explorative analysis та побудови таксономій, але погано масштабується на великі дані.

**Основні принципи:**

- **Agglomerative:** об'єднуємо знизу вгору (популярно)
- **Divisive:** розділяємо згори вниз (рідко)
- **Linkage:** спосіб рахувати відстань між кластерами
    - **Ward** — за замовчуванням (сферичні кластери)
    - **Average** — баланс
    - **Single** — складні форми (але chaining!)
    - **Complete** — компактні кластери

**Дендрограма:**

- Візуалізує всю ієрархію
- Можна "розрізати" на будь-якій висоті
- Великі розриви = природні групи

**Коли використовувати:**

- Малі дані (< 10,000) ✓
- Потрібна ієрархія ✓
- Невідомий K ✓
- Інтерпретація важлива ✓

**Коли НЕ використовувати:**

- Великі дані (> 50,000) → K-Means
- Складні форми + швидкість → DBSCAN
- Багато викидів → DBSCAN

---

## Наступні кроки

1. **BIRCH** — hierarchical для великих даних
2. **Gaussian Mixture Models (GMM)** — probabilistic clustering
3. **HDBSCAN** — density-based з ієрархією
4. **Spectral Clustering** — через similarity graphs