
**Проблема:** Як зрозуміти, чи добре працює K-Means? Як вибрати правильну кількість кластерів K?

**Рішення:** Використовуємо метрики якості кластеризації.

---

## Основні питання

1. **Скільки кластерів?** (Вибір K)
2. **Наскільки добре розділені кластери?** (Якість)
3. **Чи компактні кластери?** (Щільність)

---

## 1️⃣ Elbow Method (Метод ліктя)

### Ідея

Будуємо графік **Inertia** (сума квадратів відстаней до центрів) від **K**.

Шукаємо точку, де зменшення Inertia сповільнюється — це "лікоть".

### Інтуїція

- **K = 1:** всі точки в одному кластері → велика Inertia
- **K = n:** кожна точка — окремий кластер → Inertia = 0
- **Оптимальний K:** де графік "зламується"

### Візуально

```
Inertia
  │
  │ ●
1000│  ●
  │   ●
 500│    ●
  │      ●──●──●──●  ← після K=3 покращення мінімальні
  │                 
    └─────────────────── K
      1  2  3  4  5  6
           ↑
         "лікоть"
```

### Інтерпретація

- **До ліктя:** кожен новий кластер дає значне покращення
- **Після ліктя:** нові кластери дають мало користі
- **Вибір:** K в точці ліктя

### Обмеження

❌ Не завжди є чіткий "лікоть"  
❌ Суб'єктивна інтерпретація  
❌ Працює тільки для K-Means (залежить від Inertia)

---

## 2️⃣ Silhouette Score (Коефіцієнт силуету)

### Ідея

Для кожної точки оцінюємо:

- Наскільки вона близька до свого кластера
- Наскільки далека від сусіднього кластера

### Формула для однієї точки

$$s_i = \frac{b_i - a_i}{\max(a_i, b_i)}$$

де:

- $a_i$ — **середня відстань** до інших точок у **своєму** кластері
- $b_i$ — **середня відстань** до точок у **найближчому сусідньому** кластері

### Що означають значення?

$$s_i \in [-1, 1]$$

|Значення|Інтерпретація|
|---|---|
|**+1**|Точка ідеально підходить до свого кластера|
|**0**|Точка на межі між кластерами|
|**-1**|Точка в неправильному кластері|

### Середній Silhouette Score

Для всіх точок: $$\text{Silhouette} = \frac{1}{n} \sum_{i=1}^{n} s_i$$

**Інтерпретація загального score:**

|Score|Якість кластеризації|
|---|---|
|0.7 - 1.0|Сильна структура ✓✓✓|
|0.5 - 0.7|Розумна структура ✓✓|
|0.25 - 0.5|Слабка структура ✓|
|< 0.25|Немає суттєвої структури ✗|

### Приклад обчислення

**Дані:**

```
Кластер 1: {x₁=[1,2], x₂=[2,1], x₃=[1.5,1.5]}
Кластер 2: {x₄=[8,8], x₅=[9,10]}
```

**Для точки x₁ = [1, 2]:**

**Крок 1:** Відстані до своїх (Кластер 1):

```
d(x₁, x₂) = √((1-2)² + (2-1)²) = √2 ≈ 1.41
d(x₁, x₃) = √((1-1.5)² + (2-1.5)²) = √0.5 ≈ 0.71

a₁ = (1.41 + 0.71) / 2 = 1.06
```

**Крок 2:** Відстані до найближчого сусіднього (Кластер 2):

```
d(x₁, x₄) = √((1-8)² + (2-8)²) = √85 ≈ 9.22
d(x₁, x₅) = √((1-9)² + (2-10)²) = √128 ≈ 11.31

b₁ = (9.22 + 11.31) / 2 = 10.27
```

**Крок 3:** Silhouette для x₁:

```
s₁ = (b₁ - a₁) / max(a₁, b₁)
   = (10.27 - 1.06) / 10.27
   = 0.90  ← дуже добре! ✓
```

### Використання для вибору K

Рахуємо Silhouette Score для різних K:

```python
K = 2: Silhouette = 0.65
K = 3: Silhouette = 0.72  ← найкраще!
K = 4: Silhouette = 0.58
K = 5: Silhouette = 0.45
```

Обираємо K=3 (максимальний Silhouette).

### Переваги та недоліки

✅ **Переваги:**

- Зрозуміла інтерпретація (-1 до 1)
- Не залежить від Inertia
- Працює для будь-якого алгоритму кластеризації
- Можна візуалізувати Silhouette Plot

❌ **Недоліки:**

- Повільніше обчислення O(n²)
- Передбачає сферичні кластери
- Може бути не точним для складних форм

---

## 3️⃣ Davies-Bouldin Index (Індекс Девіса-Болдіна)

### Ідея

Для кожного кластера оцінюємо відношення:

- Внутрішня дисперсія (наскільки розкидані точки всередині)
- Відстань до найближчого кластера

### Формула

$$DB = \frac{1}{K} \sum_{i=1}^{K} \max_{i \neq j} \left( \frac{s_i + s_j}{d_{ij}} \right)$$

де:

- $s_i$ — середня відстань точок до центра кластера $i$
- $d_{ij}$ — відстань між центрами кластерів $i$ і $j$

### Що означають значення?

$$DB \in [0, +\infty)$$

- **Менше = краще**
- **DB = 0:** ідеальна кластеризація (недосяжно на практиці)
- **DB < 1:** добра кластеризація
- **DB > 1:** погана кластеризація

### Інтуїція

Уяви 2 кластери:

- **Компактні** (точки близько до центрів) → мале $s_i$ → добре
- **Далекі один від одного** (великий $d_{ij}$) → добре
- **Відношення мале** → кластери добре розділені ✓

### Приклад обчислення

**Кластер 1:** центр μ₁ = [1.5, 1.5]

```
Точки: {[1,2], [2,1], [1.5,1.5]}
Відстані до центру: {0.71, 0.71, 0}
s₁ = (0.71 + 0.71 + 0) / 3 = 0.47
```

**Кластер 2:** центр μ₂ = [8.5, 9]

```
Точки: {[8,8], [9,10]}
Відстані до центру: {1.12, 1.12}
s₂ = (1.12 + 1.12) / 2 = 1.12
```

**Відстань між центрами:**

```
d₁₂ = √((8.5-1.5)² + (9-1.5)²) = √105.25 ≈ 10.26
```

**Davies-Bouldin Index:**

```
DB = (s₁ + s₂) / d₁₂
   = (0.47 + 1.12) / 10.26
   = 0.155  ← дуже добре! (< 1)
```

### Використання для вибору K

```python
K = 2: DB = 0.45  ← найкраще!
K = 3: DB = 0.52
K = 4: DB = 0.68
K = 5: DB = 0.89
```

Обираємо K=2 (мінімальний DB).

### Переваги та недоліки

✅ **Переваги:**

- Швидке обчислення
- Інтуїтивне (компактність vs відділеність)
- Працює для різних алгоритмів

❌ **Недоліки:**

- Передбачає сферичні кластери
- Чутливий до викидів
- Може давати різні результати з Silhouette

---

## 4️⃣ Calinski-Harabasz Index (Variance Ratio Criterion)

### Ідея

Відношення:

- **Between-cluster variance** (дисперсія між кластерами)
- **Within-cluster variance** (дисперсія всередині кластерів)

### Формула

$$CH = \frac{\text{tr}(B_K)}{\text{tr}(W_K)} \times \frac{n - K}{K - 1}$$

де:

- $B_K$ — between-cluster scatter matrix
- $W_K$ — within-cluster scatter matrix
- $n$ — кількість точок
- $K$ — кількість кластерів

### Спрощена формула

$$CH = \frac{SS_{between} / (K-1)}{SS_{within} / (n-K)}$$

де:

- $SS_{between}$ — сума квадратів відстаней між кластерами
- $SS_{within}$ — сума квадратів відстаней всередині кластерів

### Що означають значення?

$$CH \in [0, +\infty)$$

- **Більше = краще**
- **CH > 100:** дуже добра кластеризація
- **CH > 10:** прийнятна кластеризація
- **CH < 10:** погана кластеризація

### Інтуїція

Уяви дисперсію:

- **Між кластерами велика** → кластери далеко один від одного → добре ✓
- **Всередині кластерів мала** → точки близькі до свого центру → добре ✓
- **Відношення велике** → кластери добре розділені

### Приклад

```python
K = 2: CH = 156.3  ← найкраще!
K = 3: CH = 98.7
K = 4: CH = 67.2
K = 5: CH = 45.1
```

Обираємо K=2 (максимальний CH).

### Переваги та недоліки

✅ **Переваги:**

- Швидке обчислення
- Чітка статистична інтерпретація
- Добре працює на опуклих кластерах

❌ **Недоліки:**

- Передбачає сферичні кластери
- Може давати велике значення для K=2 навіть без справжніх кластерів
- Схильний до переоцінки великих K

---

## 5️⃣ Inertia (Within-Cluster Sum of Squares)

### Формула

$$Inertia = \sum_{k=1}^{K} \sum_{x_i \in C_k} |x_i - \mu_k|^2$$

### Інтуїція

- **Мала Inertia** → точки близькі до своїх центрів → компактні кластери
- **Велика Inertia** → точки розкидані → погано

### Проблема

Inertia **завжди зменшується** зі збільшенням K:

- K=1: велика Inertia
- K=n: Inertia=0 (кожна точка — окремий кластер)

**Тому:** не можна просто вибрати K з мінімальною Inertia!

### Використання

Головне застосування — **Elbow Method** (див. вище).

---

## Порівняння метрик

|Метрика|Оптимум|Діапазон|Швидкість|Коли використовувати|
|---|---|---|---|---|
|**Elbow Method**|Лікоть|[0, ∞)|Швидко|Перший крок, візуальна оцінка|
|**Silhouette**|Максимум|[-1, 1]|Повільно (O(n²))|Точна оцінка якості, порівняння алгоритмів|
|**Davies-Bouldin**|Мінімум|[0, ∞)|Швидко|Компактність + розділеність|
|**Calinski-Harabasz**|Максимум|[0, ∞)|Швидко|Швидка оцінка, великі дані|
|**Inertia**|—|[0, ∞)|Дуже швидко|Тільки для Elbow Method|

---

## Практичний workflow

### Крок 1: Elbow Method (швидкий огляд)

```python
inertias = []
K_range = range(1, 11)

for k in K_range:
    kmeans = KMeans(n_clusters=k)
    kmeans.fit(X)
    inertias.append(kmeans.inertia_)

plt.plot(K_range, inertias, 'o-')
plt.xlabel('K')
plt.ylabel('Inertia')
plt.title('Elbow Method')
```

**Результат:** підозра на K=3 або K=4

### Крок 2: Silhouette Score (точна оцінка)

```python
from sklearn.metrics import silhouette_score

silhouette_scores = []

for k in range(2, 11):
    kmeans = KMeans(n_clusters=k)
    labels = kmeans.fit_predict(X)
    score = silhouette_score(X, labels)
    silhouette_scores.append(score)
    print(f'K={k}: Silhouette={score:.3f}')

# K=2: 0.652
# K=3: 0.721  ← найкраще!
# K=4: 0.588
```

### Крок 3: Перевірка іншими метриками

```python
from sklearn.metrics import davies_bouldin_score, calinski_harabasz_score

for k in range(2, 6):
    kmeans = KMeans(n_clusters=k)
    labels = kmeans.fit_predict(X)
    
    db = davies_bouldin_score(X, labels)
    ch = calinski_harabasz_score(X, labels)
    
    print(f'K={k}: DB={db:.3f}, CH={ch:.1f}')

# K=2: DB=0.523, CH=156.3
# K=3: DB=0.489, CH=142.7  ← DB мінімальний
# K=4: DB=0.612, CH=98.2
```

### Крок 4: Прийняття рішення

```
Метрика           | K=2  | K=3  | K=4
------------------|------|------|------
Elbow             | ?    | ✓    | ✓
Silhouette        | 0.65 | 0.72 | 0.59  ← MAX
Davies-Bouldin    | 0.52 | 0.49 | 0.61  ← MIN
Calinski-Harabasz | 156  | 143  | 98    

Вибір: K=3 (більшість метрик вказує на це)
```

---

## Коли метрики розходяться?

### Ситуація 1: Дані без чіткої структури

```
Silhouette: 0.23 (низький)
Davies-Bouldin: 0.89 (високий)
→ Дані не мають явних кластерів
→ Кластеризація може бути безглуздою
```

### Ситуація 2: Різні форми кластерів

```
K-Means дає: Silhouette=0.45, DB=0.67
DBSCAN дає: Silhouette=0.71, DB=0.38
→ K-Means погано працює на складних формах
```

### Ситуація 3: Шум та викиди

```
До очищення: Silhouette=0.32
Після очищення: Silhouette=0.68
→ Викиди псують метрики
```

---

## Візуалізація Silhouette Plot

### Що це?

Графік Silhouette score для кожної точки в кожному кластері.

### Як читати?

```
Кластер 1: ███████████████▌ 0.75  ← всі точки мають високий score
Кластер 2: █████████████     0.68  ← добре
Кластер 3: ██████▌           0.45  ← слабший кластер
           ────────────────
           -1  0   0.5   1

Середній Silhouette: 0.63
```

**Ознаки доброї кластеризації:**

- Всі кластери вище середнього (пунктирна лінія)
- Приблизно однакова ширина кластерів (однаковий розмір)
- Мало точок з негативним score

**Ознаки поганої кластеризації:**

- Багато точок нижче середнього
- Дуже різні розміри кластерів
- Негативні значення (точки в неправильних кластерах)

---

## Поради при виборі K

### 1. Комбінуй методи

Не покладайся на одну метрику:

```python
# ПОГАНО
if silhouette > 0.5:
    k = k_optimal

# ДОБРЕ  
if (elbow_suggests_k == 3 and 
    silhouette_best == 3 and
    davies_bouldin_best == 3):
    k = 3
```

### 2. Візуалізуй

Якщо дані 2D або можна зробити PCA:

```python
# Подивись очима на кластери
plt.scatter(X[:, 0], X[:, 1], c=labels)
```

Часто очевидно, чи має сенс K.

### 3. Врахуй бізнес-контекст

```
Маркетинг: K=3 (преміум, середній, бюджет)
Анкета: K=5 (strongly agree...strongly disagree)
→ Обери K, що має сенс для задачі
```

### 4. Перевір стабільність

```python
# Запусти K-Means 10 разів з різними random_state
scores = []
for seed in range(10):
    kmeans = KMeans(k=3, random_state=seed)
    score = silhouette_score(X, kmeans.fit_predict(X))
    scores.append(score)

if std(scores) > 0.1:
    print("⚠️ Нестабільна кластеризація!")
```

---

## Приклад: вибір K для сегментації клієнтів

**Дані:** 1000 клієнтів, ознаки (вік, дохід, частота покупок)

### Крок 1: Elbow

```
K  | Inertia
---|--------
1  | 8523
2  | 3421  ← великий спад
3  | 2156  ← спад сповільнюється
4  | 1823
5  | 1654
```

**Висновок:** K=2 або K=3

### Крок 2: Silhouette

```
K  | Silhouette
---|----------
2  | 0.68
3  | 0.71  ← найкраще
4  | 0.59
5  | 0.52
```

**Висновок:** K=3

### Крок 3: Бізнес-сенс

```
K=2: VIP vs звичайні (занадто просто)
K=3: VIP, активні, пасивні (має сенс!) ✓
K=4: надто дрібно
```

### Фінальний вибір: K=3

**Інтерпретація кластерів:**

- Кластер 0: VIP (високий дохід, часті покупки)
- Кластер 1: Активні (середній дохід, регулярні покупки)
- Кластер 2: Пасивні (низький дохід, рідкі покупки)

---

## Ключові висновки

> Не існує "ідеальної" метрики для вибору K. Використовуй комбінацію методів та враховуй контекст задачі.

**Практичний підхід:**

1. **Elbow Method** → грубий діапазон K
2. **Silhouette Score** → точна оцінка якості
3. **Davies-Bouldin** або **Calinski-Harabasz** → підтвердження
4. **Візуалізація** → перевірка очима
5. **Бізнес-логіка** → фінальне рішення

**Золоте правило:**

- Silhouette > 0.5 ✓
- Davies-Bouldin < 1.0 ✓
- Кластери мають сенс для задачі ✓