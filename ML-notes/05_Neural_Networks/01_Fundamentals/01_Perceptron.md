# Perceptron (–ü–µ—Ä—Ü–µ–ø—Ç—Ä–æ–Ω)

## –©–æ —Ü–µ?

**Perceptron** ‚Äî —Ü–µ –Ω–∞–π–ø—Ä–æ—Å—Ç—ñ—à–∞ —à—Ç—É—á–Ω–∞ –Ω–µ–π—Ä–æ–Ω–Ω–∞ –º–µ—Ä–µ–∂–∞ –∑ **–æ–¥–Ω–∏–º –Ω–µ–π—Ä–æ–Ω–æ–º**, —è–∫–∞ –ø—Ä–∏–π–º–∞—î –∫—ñ–ª—å–∫–∞ —á–∏—Å–ª–æ–≤–∏—Ö –≤—Ö–æ–¥—ñ–≤, –∑–≤–∞–∂—É—î —ó—Ö —ñ –≤–∏–¥–∞—î –±—ñ–Ω–∞—Ä–Ω–∏–π –≤–∏—Ö—ñ–¥ (0 –∞–±–æ 1).

**–ì–æ–ª–æ–≤–Ω–∞ —ñ–¥–µ—è:** –∑–º–æ–¥–µ–ª—é–≤–∞—Ç–∏ —Ä–æ–±–æ—Ç—É –±—ñ–æ–ª–æ–≥—ñ—á–Ω–æ–≥–æ –Ω–µ–π—Ä–æ–Ω–∞ ‚Äî –Ω–∞–∫–æ–ø–∏—á–∏—Ç–∏ —Å–∏–≥–Ω–∞–ª–∏ –≤—ñ–¥ –≤—Ö–æ–¥—ñ–≤, —ñ —è–∫—â–æ —Å—É–º–∞ –¥–æ—Å—Ç–∞—Ç–Ω—å–æ –≤–µ–ª–∏–∫–∞, "—Å–ø—Ä–∞—Ü—é–≤–∞—Ç–∏" (–≤–∏–≤–µ—Å—Ç–∏ 1), —ñ–Ω–∞–∫—à–µ ‚Äî "–º–æ–≤—á–∞—Ç–∏" (–≤–∏–≤–µ—Å—Ç–∏ 0).

## –ù–∞–≤—ñ—â–æ –ø–æ—Ç—Ä—ñ–±–µ–Ω?

- üß† **–§—É–Ω–¥–∞–º–µ–Ω—Ç –Ω–µ–π—Ä–æ–Ω–Ω–∏—Ö –º–µ—Ä–µ–∂** ‚Äî –≤—Å—ñ —Å—É—á–∞—Å–Ω—ñ –Ω–µ–π—Ä–æ–º–µ—Ä–µ–∂—ñ —Å–∫–ª–∞–¥–∞—é—Ç—å—Å—è –∑ —Ç–∞–∫–∏—Ö –±–ª–æ–∫—ñ–≤
- üìö **–†–æ–∑—É–º—ñ–Ω–Ω—è —ñ–¥–µ–π** ‚Äî –≤–∞–≥–∏, bias, activation function ‚Äî –≤—Å–µ –∑–≤—ñ–¥—Å–∏
- üéØ **–ë—ñ–Ω–∞—Ä–Ω–∞ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è** ‚Äî —Ä–æ–∑–¥—ñ–ª–∏—Ç–∏ –¥–∞–Ω—ñ –Ω–∞ –¥–≤–∞ –∫–ª–∞—Å–∏ –ø—Ä—è–º–æ—é –ª—ñ–Ω—ñ—î—é
- üî¨ **–ù–∞–≤—á–∞–ª—å–Ω–∞ –º–æ–¥–µ–ª—å** ‚Äî —ñ–¥–µ–∞–ª—å–Ω–æ –¥–ª—è —Ä–æ–∑—É–º—ñ–Ω–Ω—è, —è–∫ –º–∞—à–∏–Ω–∞ "–≤—á–∏—Ç—å—Å—è"

## –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏?

**–ü–æ—Ç—Ä—ñ–±–Ω–æ:**
- –î–∞–Ω—ñ **–ª—ñ–Ω—ñ–π–Ω–æ —Ä–æ–∑–¥—ñ–ª–∏–º—ñ** (–ø—Ä—è–º–∞ –ª—ñ–Ω—ñ—è —Ä–æ–∑–¥—ñ–ª—è—î –¥–≤–∞ –∫–ª–∞—Å–∏)
- –ü–æ—Ç—Ä—ñ–±–Ω–µ **—Ä–æ–∑—É–º—ñ–Ω–Ω—è –æ—Å–Ω–æ–≤** –Ω–µ–π—Ä–æ–Ω–Ω–∏—Ö –º–µ—Ä–µ–∂
- –î—É–∂–µ –ø—Ä–æ—Å—Ç–∞ **baseline** –º–æ–¥–µ–ª—å

**–ù–µ –ø–æ—Ç—Ä—ñ–±–Ω–æ:**
- –î–∞–Ω—ñ **–Ω–µ–ª—ñ–Ω—ñ–π–Ω–æ —Ä–æ–∑–¥—ñ–ª–∏–º—ñ** (XOR, –∫–æ–ª–∞, —Å–ø—ñ—Ä–∞–ª—ñ) ‚Üí MLP, CNN
- –†–µ–≥—Ä–µ—Å—ñ—è ‚Üí Linear Regression
- –°–∫–ª–∞–¥–Ω—ñ –ø–∞—Ç—Ç–µ—Ä–Ω–∏ ‚Üí Deep Learning

---

## –ê–Ω–∞–ª–æ–≥—ñ—è: –Ø–∫ –Ω–µ–π—Ä–æ–Ω –ø—Ä–∏–π–º–∞—î —Ä—ñ—à–µ–Ω–Ω—è

**–£—è–≤—ñ—Ç—å:** –≤–∏ –≤–∏—Ä—ñ—à—É—î—Ç–µ, —á–∏ –π—Ç–∏ –Ω–∞ –≤–µ—á—ñ—Ä–∫—É.

| –§–∞–∫—Ç–æ—Ä | –í–∞–≥–∞ (–≤–∞–∂–ª–∏–≤—ñ—Å—Ç—å) | –í–∞—à–µ –∑–Ω–∞—á–µ–Ω–Ω—è |
|--------|-------------------|---------------|
| –¶—ñ–∫–∞–≤—ñ –ª—é–¥–∏ —Ç–∞–º? | 0.5 (–≤–∞–∂–ª–∏–≤–æ) | 1 (—Ç–∞–∫) |
| –Ñ —É–ª—é–±–ª–µ–Ω–∞ –º—É–∑–∏–∫–∞? | 0.3 (–ø–æ–º—ñ—Ä–Ω–æ) | 0 (–Ω—ñ) |
| –î–∞–ª–µ–∫–æ —ó—Ö–∞—Ç–∏? | -0.4 (–Ω–µ–≥–∞—Ç–∏–≤–Ω–∏–π —Ñ–∞–∫—Ç–æ—Ä) | 1 (—Ç–∞–∫) |

**–ü—ñ–¥—Å—É–º–æ–∫:** 0.5√ó1 + 0.3√ó0 + (-0.4)√ó1 = **0.1**

–Ø–∫—â–æ 0.1 > –ø–æ—Ä–æ–≥—É (threshold = 0) ‚Üí **–ô–¥–µ—à –Ω–∞ –≤–µ—á—ñ—Ä–∫—É!** üéâ

–°–∞–º–µ —Ç–∞–∫ —ñ –ø—Ä–∞—Ü—é—î perceptron ‚Äî –∑–≤–∞–∂–µ–Ω–∞ —Å—É–º–∞ + —Ä—ñ—à–µ–Ω–Ω—è.

---

## –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø–µ—Ä—Ü–µ–ø—Ç—Ä–æ–Ω–∞

```
Inputs              Weights          Sum + Activation    Output
                                        
x‚ÇÅ = 1  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ w‚ÇÅ = 0.5 ‚îÄ‚îÄ‚îê
                           ‚îú‚îÄ‚îÄ‚Üí Œ£(w·µ¢x·µ¢) + b ‚îÄ‚îÄ‚Üí step() ‚îÄ‚îÄ‚Üí ≈∑ (0 –∞–±–æ 1)
x‚ÇÇ = 0  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ w‚ÇÇ = 0.3 ‚îÄ‚îÄ‚î§
                           ‚îÇ
x‚ÇÉ = 1  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ w‚ÇÉ = -0.4 ‚îÄ‚îò
                              ‚Üë
                           b = 0.1 (bias)
```

**–ö–æ–º–ø–æ–Ω–µ–Ω—Ç–∏:**
- **Inputs (x)** ‚Äî –≤—Ö—ñ–¥–Ω—ñ –æ–∑–Ω–∞–∫–∏
- **Weights (w)** ‚Äî –≤–∞–≥–∏ (–∑–Ω–∞—á–∏–º—ñ—Å—Ç—å –∫–æ–∂–Ω–æ–≥–æ –≤—Ö–æ–¥—É)
- **Bias (b)** ‚Äî –∑–º—ñ—â–µ–Ω–Ω—è (threshold –±–µ–∑ –≤—Ö–æ–¥—É)
- **Summation (Œ£)** ‚Äî –∑–≤–∞–∂–µ–Ω–∞ —Å—É–º–∞ –≤—Ö–æ–¥—ñ–≤
- **Activation function** ‚Äî –ø–µ—Ä–µ—Ç–≤–æ—Ä—é—î —Å—É–º—É –Ω–∞ –≤–∏—Ö—ñ–¥ (0 –∞–±–æ 1)
- **Output (≈∑)** ‚Äî –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–∏–π –∫–ª–∞—Å

---

## –ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞ (–º—ñ–Ω—ñ–º—É–º)

### –û–±—á–∏—Å–ª–µ–Ω–Ω—è –≤–∏—Ö–æ–¥—É

$$\text{output} = \text{step}\left(\sum_{i} w_i x_i + b\right)$$

–¥–µ step function:
$$\text{step}(z) = \begin{cases} 1 & \text{—è–∫—â–æ } z \geq 0 \\ 0 & \text{—è–∫—â–æ } z < 0 \end{cases}$$

### –í–µ–∫—Ç–æ—Ä–Ω–∞ —Ñ–æ—Ä–º–∞ (–∫–æ—Ä–æ—Ç–∫–æ)

$$\hat{y} = \text{step}(\mathbf{w} \cdot \mathbf{x} + b)$$

---

## –Ø–∫ –ø–µ—Ä—Ü–µ–ø—Ç—Ä–æ–Ω –Ω–∞–≤—á–∞—î—Ç—å—Å—è?

### Perceptron Learning Rule

**–Ü–¥–µ—è:** –ø–æ—Ä—ñ–≤–Ω—é—î–º–æ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –∑ –ø—Ä–∞–≤–∏–ª—å–Ω–æ—é –≤—ñ–¥–ø–æ–≤—ñ–¥–¥—é —ñ –∫–æ—Ä–∏–≥—É—î–º–æ –≤–∞–≥–∏.

**–ü—Ä–∞–≤–∏–ª–æ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è:**

```
error = y_true - y_pred
w·µ¢ := w·µ¢ + learning_rate √ó error √ó x·µ¢
b  := b  + learning_rate √ó error
```

### –ö—Ä–æ–∫ –∑–∞ –∫—Ä–æ–∫–æ–º

–Ñ –æ–¥–∏–Ω –Ω–∞–≤—á–∞–ª—å–Ω–∏–π –ø—Ä–∏–∫–ª–∞–¥: x = [1, 0], y_true = 1, –ø–æ—Ç–æ—á–Ω—ñ –≤–∞–≥–∏ w = [0.2, -0.5], b = 0.

**–ö—Ä–æ–∫ 1:** –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
$$z = 0.2 \times 1 + (-0.5) \times 0 + 0 = 0.2$$
$$\hat{y} = \text{step}(0.2) = 1 \text{ ‚úì (–ø—Ä–∞–≤–∏–ª—å–Ω–æ)}$$

Error = 1 - 1 = 0 ‚Üí –≤–∞–≥–∏ –Ω–µ –∑–º—ñ–Ω—é—é—Ç—å—Å—è.

---

–Ü–Ω—à–∏–π –ø—Ä–∏–∫–ª–∞–¥: x = [0, 1], y_true = 1.

**–ö—Ä–æ–∫ 1:** –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
$$z = 0.2 \times 0 + (-0.5) \times 1 + 0 = -0.5$$
$$\hat{y} = \text{step}(-0.5) = 0 \text{ ‚úó (–ø–æ–º–∏–ª–∫–∞)}$$

**–ö—Ä–æ–∫ 2:** –û–Ω–æ–≤–ª–µ–Ω–Ω—è –≤–∞–≥ (learning_rate = 0.1)
$$\text{error} = 1 - 0 = 1$$
$$w_1 = 0.2 + 0.1 \times 1 \times 0 = 0.2$$
$$w_2 = -0.5 + 0.1 \times 1 \times 1 = -0.4$$
$$b = 0 + 0.1 \times 1 = 0.1$$

–í–∞–≥–∏ –æ–Ω–æ–≤–∏–ª–∏—Å—å, —â–æ–± –Ω–∞—Å—Ç—É–ø–Ω–æ–≥–æ —Ä–∞–∑—É –ø–µ—Ä–µ–¥–±–∞—á–∏—Ç–∏ –ø—Ä–∞–≤–∏–ª—å–Ω–æ! ‚úì

---

## –ü—Ä–æ—Å—Ç–∏–π –ø—Ä–∏–∫–ª–∞–¥: –ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è AND

### –ó–∞–¥–∞—á–∞

–ù–∞–≤—á–∏—Ç–∏ perceptron –≤—ñ–¥—Ç–≤–æ—Ä—é–≤–∞—Ç–∏ –ª–æ–≥—ñ—á–Ω—É –æ–ø–µ—Ä–∞—Ü—ñ—é AND:

| x‚ÇÅ | x‚ÇÇ | AND |
|----|----|-----|
| 0 | 0 | 0 |
| 0 | 1 | 0 |
| 1 | 0 | 0 |
| 1 | 1 | 1 |

### –ß–æ–º—É AND –ø—ñ–¥—Ö–æ–¥–∏—Ç—å –¥–ª—è perceptron?

```
x‚ÇÇ
 1 | O  X        O = –∫–ª–∞—Å 0, X = –∫–ª–∞—Å 1
   |
 0 | O  O
   |__________
     0   1   x‚ÇÅ

–ú–æ–∂–Ω–∞ –ø—Ä–æ–≤–µ—Å—Ç–∏ –ø—Ä—è–º—É –ª—ñ–Ω—ñ—é –º—ñ–∂ O —Ç–∞ X ‚Üí –ª—ñ–Ω—ñ–π–Ω–æ —Ä–æ–∑–¥—ñ–ª–∏–º–æ ‚úì
```

### –ö–æ–¥

```python
import numpy as np

class Perceptron:
    def __init__(self, learning_rate=0.1, n_iterations=100):
        self.lr = learning_rate
        self.n_iterations = n_iterations
    
    def fit(self, X, y):
        # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –≤–∞–≥ –Ω—É–ª—è–º–∏
        self.weights = np.zeros(X.shape[1])
        self.bias = 0
        
        for epoch in range(self.n_iterations):
            for xi, yi in zip(X, y):
                # –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
                prediction = self.predict_single(xi)
                
                # –û–Ω–æ–≤–ª–µ–Ω–Ω—è –≤–∞–≥
                error = yi - prediction
                self.weights += self.lr * error * xi
                self.bias += self.lr * error
    
    def predict_single(self, x):
        z = np.dot(self.weights, x) + self.bias
        return 1 if z >= 0 else 0
    
    def predict(self, X):
        return np.array([self.predict_single(xi) for xi in X])


# –î–∞–Ω—ñ –¥–ª—è AND
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y = np.array([0, 0, 0, 1])

# –ù–∞–≤—á–∞–Ω–Ω—è
perceptron = Perceptron(learning_rate=0.1, n_iterations=100)
perceptron.fit(X, y)

# –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
y_pred = perceptron.predict(X)

print("=== –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ AND ===")
for xi, yi, pred in zip(X, y, y_pred):
    status = "‚úì" if yi == pred else "‚úó"
    print(f"AND({xi[0]}, {xi[1]}) = {yi}, –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–æ: {pred} {status}")

print(f"\n–í–∞–≥–∏: {perceptron.weights}")
print(f"Bias: {perceptron.bias:.4f}")
print(f"Accuracy: {(y == y_pred).mean():.0%}")
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç:**
```
=== –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ AND ===
AND(0, 0) = 0, –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–æ: 0 ‚úì
AND(0, 1) = 0, –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–æ: 0 ‚úì
AND(1, 0) = 0, –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–æ: 0 ‚úì
AND(1, 1) = 1, –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–æ: 1 ‚úì

–í–∞–≥–∏: [0.2  0.2]
Bias: -0.2000
Accuracy: 100%
```

---

## –°–∫–ª–∞–¥–Ω–∏–π –ø—Ä–∏–∫–ª–∞–¥: XOR ‚Äî –¥–µ perceptron –ø—Ä–æ–≤–∞–ª—é—î—Ç—å—Å—è

### –ó–∞–¥–∞—á–∞ XOR

| x‚ÇÅ | x‚ÇÇ | XOR |
|----|----|-----|
| 0 | 0 | 0 |
| 0 | 1 | 1 |
| 1 | 0 | 1 |
| 1 | 1 | 0 |

### –ß–æ–º—É XOR –Ω–µ–º–æ–∂–ª–∏–≤–∏–π –¥–ª—è perceptron?

```
x‚ÇÇ
 1 | X  O        O = –∫–ª–∞—Å 0, X = –∫–ª–∞—Å 1
   |
 0 | O  X
   |__________
     0   1   x‚ÇÅ

–ù–µ–º–æ–∂–ª–∏–≤–æ –ø—Ä–æ–≤–µ—Å—Ç–∏ –û–î–ù–£ –ø—Ä—è–º—É –ª—ñ–Ω—ñ—é –º—ñ–∂ O —Ç–∞ X ‚Üí –ù–ï –ª—ñ–Ω—ñ–π–Ω–æ —Ä–æ–∑–¥—ñ–ª–∏–º–æ ‚úó
```

### –ö–æ–¥ (–¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü—ñ—è –ø—Ä–æ–≤–∞–ª—É)

```python
# –î–∞–Ω—ñ –¥–ª—è XOR
X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y_xor = np.array([0, 1, 1, 0])

# –ù–∞–≤—á–∞–Ω–Ω—è
perceptron_xor = Perceptron(learning_rate=0.1, n_iterations=1000)
perceptron_xor.fit(X_xor, y_xor)

y_pred_xor = perceptron_xor.predict(X_xor)
accuracy = (y_xor == y_pred_xor).mean()

print(f"XOR Accuracy: {accuracy:.0%}")  # –ù—ñ–∫–æ–ª–∏ –Ω–µ –±—É–¥–µ 100%!
print("Perceptron –Ω–µ –º–æ–∂–µ –Ω–∞–≤—á–∏—Ç–∏—Å—è XOR ‚Äî –ø–æ—Ç—Ä—ñ–±–Ω–∞ MLP!")
```

**–í–∏—Å–Ω–æ–≤–æ–∫:** XOR –ø–æ–∫–∞–∑–∞–≤ **–æ–±–º–µ–∂–µ–Ω–Ω—è perceptron** —ñ —Å–∞–º–µ —Ü–µ —Å–ø–æ–Ω—É–∫–∞–ª–æ –¥–æ —Ä–æ–∑—Ä–æ–±–∫–∏ **Multi-Layer Perceptron (MLP)**.

---

## –ó–±—ñ–∂–Ω—ñ—Å—Ç—å —Ç–∞ –≥–∞—Ä–∞–Ω—Ç—ñ—ó

### –¢–µ–æ—Ä–µ–º–∞ –∑–±—ñ–∂–Ω–æ—Å—Ç—ñ –ø–µ—Ä—Ü–µ–ø—Ç—Ä–æ–Ω–∞

–Ø–∫—â–æ –¥–∞–Ω—ñ **–ª—ñ–Ω—ñ–π–Ω–æ —Ä–æ–∑–¥—ñ–ª–∏–º—ñ**, perceptron **–≥–∞—Ä–∞–Ω—Ç–æ–≤–∞–Ω–æ –∑–Ω–∞–π–¥–µ** —Ä—ñ—à–µ–Ω–Ω—è –∑–∞ —Å–∫—ñ–Ω—á–µ–Ω–Ω—É –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∫—Ä–æ–∫—ñ–≤.

```
Epoch 1: Accuracy = 50%  ‚Üí –≤–∞–≥–∏ –∫–æ—Ä–∏–≥—É—é—Ç—å—Å—è
Epoch 2: Accuracy = 75%  ‚Üí –≤–∞–≥–∏ –∫–æ—Ä–∏–≥—É—é—Ç—å—Å—è
Epoch 3: Accuracy = 100% ‚Üí –≤–∞–≥–∏ –±—ñ–ª—å—à–µ –Ω–µ –∑–º—ñ–Ω—é—é—Ç—å—Å—è! ‚úì
```

### –©–æ –≤—ñ–¥–±—É–≤–∞—î—Ç—å—Å—è –∑ –Ω–µ–ª—ñ–Ω—ñ–π–Ω–∏–º–∏ –¥–∞–Ω–∏–º–∏?

**–í–∞–≥–∏ –Ω—ñ–∫–æ–ª–∏ –Ω–µ –ø–µ—Ä–µ—Å—Ç–∞—é—Ç—å –∑–º—ñ–Ω—é–≤–∞—Ç–∏—Å—è** ‚Äî –∞–ª–≥–æ—Ä–∏—Ç–º "–∫—Ä—É—Ç–∏—Ç—å—Å—è" –±–µ–∑ –∑–±—ñ–∂–Ω–æ—Å—Ç—ñ.

---

## Decision Boundary (–ú–µ–∂–∞ —Ä—ñ—à–µ–Ω–Ω—è)

### –©–æ —Ü–µ?

Perceptron —Ä–æ–∑–¥—ñ–ª—è—î –ø—Ä–æ—Å—Ç—ñ—Ä **–ø—Ä—è–º–æ—é –ª—ñ–Ω—ñ—î—é** (–∞–±–æ –≥—ñ–ø–µ—Ä–ø–ª–æ—â–∏–Ω–æ—é –≤ –±–∞–≥–∞—Ç–æ–≤–∏–º—ñ—Ä–Ω–æ–º—É –ø—Ä–æ—Å—Ç–æ—Ä—ñ).

### –ö–æ–¥ (–≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è)

```python
import matplotlib.pyplot as plt
import numpy as np

def plot_decision_boundary(model, X, y, title="Perceptron Decision Boundary"):
    # –°—ñ—Ç–∫–∞ –¥–ª—è –≤—ñ–¥–æ–±—Ä–∞–∂–µ–Ω–Ω—è –º–µ–∂—ñ
    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5
    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5
    
    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),
                         np.linspace(y_min, y_max, 200))
    
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    
    plt.figure(figsize=(8, 6))
    plt.contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')
    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', 
                edgecolors='k', s=100)
    plt.xlabel('x‚ÇÅ', fontsize=12)
    plt.ylabel('x‚ÇÇ', fontsize=12)
    plt.title(title, fontsize=14, fontweight='bold')
    plt.grid(True, alpha=0.3)
    plt.show()

# AND ‚Äî perceptron —Å–ø—Ä–∞–≤–ª—è—î—Ç—å—Å—è
plot_decision_boundary(perceptron, X, y, "AND ‚Äî Perceptron ‚úì")
```

---

## –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è: Perceptron vs Logistic Regression

| | Perceptron | Logistic Regression |
|---|------------|---------------------|
| **–í–∏—Ö—ñ–¥** | 0 –∞–±–æ 1 (–∂–æ—Ä—Å—Ç–∫–∏–π) | 0.0 –¥–æ 1.0 (–π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å) |
| **Activation** | Step function | Sigmoid function |
| **–ù–∞–≤—á–∞–Ω–Ω—è** | Perceptron rule | Gradient Descent (MSE/Log-loss) |
| **–ó–±—ñ–∂–Ω—ñ—Å—Ç—å** | –¢—ñ–ª—å–∫–∏ —è–∫—â–æ –ª—ñ–Ω—ñ–π–Ω–æ —Ä–æ–∑–¥—ñ–ª–∏–º–æ | –ó–∞–≤–∂–¥–∏ –∑–±—ñ–≥–∞—î—Ç—å—Å—è |
| **–ô–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ** | ‚ùå –ù—ñ | ‚úÖ –¢–∞–∫ |
| **–í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è** | –ù–∞–≤—á–∞–ª—å–Ω–∞ –º–æ–¥–µ–ª—å | Production –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä |

---

## –í—ñ–¥ Perceptron –¥–æ Neural Networks

### –ü—Ä–æ–±–ª–µ–º–∞: –æ–¥–Ω–æ–≥–æ –Ω–µ–π—Ä–æ–Ω–∞ –º–∞–ª–æ

**–û–¥–∏–Ω –Ω–µ–π—Ä–æ–Ω** = –æ–¥–Ω–∞ –ø—Ä—è–º–∞ –ª—ñ–Ω—ñ—è = –ª—ñ–Ω—ñ–π–Ω–∞ –º–µ–∂–∞ —Ä—ñ—à–µ–Ω–Ω—è

**–†—ñ—à–µ–Ω–Ω—è:** –∑'—î–¥–Ω–∞—Ç–∏ –Ω–µ–π—Ä–æ–Ω–∏ –≤ —à–∞—Ä–∏!

```
–û–¥–∏–Ω –ø–µ—Ä—Ü–µ–ø—Ç—Ä–æ–Ω:                Multi-Layer Perceptron:

x‚ÇÅ ‚îÄ‚îÄ‚îê                          x‚ÇÅ ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ [N‚ÇÅ] ‚îÄ‚îÄ‚îê
     ‚îú‚îÄ‚îÄ‚Üí [N] ‚îÄ‚îÄ‚Üí ≈∑             x‚ÇÇ ‚îÄ‚îÄ‚î§   [N‚ÇÇ]  ‚îú‚îÄ‚îÄ‚Üí [N‚ÇÖ] ‚îÄ‚îÄ‚Üí ≈∑
x‚ÇÇ ‚îÄ‚îÄ‚îò                              ‚îÄ‚î§   [N‚ÇÉ] ‚îÄ‚îÄ‚îò
                                x‚ÇÉ ‚îÄ ‚îî‚îÄ‚îÄ [N‚ÇÑ] ‚îÄ‚îÄ‚îò
                                     
  –û–¥–Ω–∞ –ø—Ä—è–º–∞ –ª—ñ–Ω—ñ—è             –°–∫–ª–∞–¥–Ω—ñ –Ω–µ–ª—ñ–Ω—ñ–π–Ω—ñ –º–µ–∂—ñ
```

### –ö–ª—é—á–æ–≤—ñ –∑–º—ñ–Ω–∏ —É MLP

1. **–®–∞—Ä–∏ –Ω–µ–π—Ä–æ–Ω—ñ–≤** ‚Äî –∫—ñ–ª—å–∫–∞ —Ä—ñ–≤–Ω—ñ–≤ –æ–±—á–∏—Å–ª–µ–Ω—å
2. **–ù–µ–ª—ñ–Ω—ñ–π–Ω—ñ –∞–∫—Ç–∏–≤–∞—Ü—ñ—ó** ‚Äî –∑–∞–º—ñ—Å—Ç—å step ‚Üí ReLU, Sigmoid, Tanh
3. **Backpropagation** ‚Äî –µ—Ñ–µ–∫—Ç–∏–≤–Ω–µ –Ω–∞–≤—á–∞–Ω–Ω—è —á–µ—Ä–µ–∑ –≥—Ä–∞–¥—ñ—î–Ω—Ç–∏

---

## –ü–µ—Ä–µ–≤–∞–≥–∏ —Ç–∞ –Ω–µ–¥–æ–ª—ñ–∫–∏

### –ü–µ—Ä–µ–≤–∞–≥–∏ ‚úì

| –ü–µ—Ä–µ–≤–∞–≥–∞ | –ü–æ—è—Å–Ω–µ–Ω–Ω—è |
|----------|-----------|
| **–ü—Ä–æ—Å—Ç–æ—Ç–∞** | –õ–µ–≥–∫–æ –∑—Ä–æ–∑—É–º—ñ—Ç–∏ —Ç–∞ —Ä–µ–∞–ª—ñ–∑—É–≤–∞—Ç–∏ |
| **–®–≤–∏–¥–∫—ñ—Å—Ç—å** | –ú—ñ–Ω—ñ–º–∞–ª—å–Ω—ñ –æ–±—á–∏—Å–ª–µ–Ω–Ω—è |
| **–§—É–Ω–¥–∞–º–µ–Ω—Ç** | –û—Å–Ω–æ–≤–∞ –¥–ª—è —Ä–æ–∑—É–º—ñ–Ω–Ω—è –Ω–µ–π—Ä–æ–Ω–Ω–∏—Ö –º–µ—Ä–µ–∂ |
| **–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å** | –í–∞–≥–∏ –ø–æ–∫–∞–∑—É—é—Ç—å –≤–∞–∂–ª–∏–≤—ñ—Å—Ç—å –æ–∑–Ω–∞–∫ |
| **–ì–∞—Ä–∞–Ω—Ç–æ–≤–∞–Ω–∞ –∑–±—ñ–∂–Ω—ñ—Å—Ç—å** | –ù–∞ –ª—ñ–Ω—ñ–π–Ω–æ —Ä–æ–∑–¥—ñ–ª–∏–º–∏—Ö –¥–∞–Ω–∏—Ö |

### –ù–µ–¥–æ–ª—ñ–∫–∏ ‚úó

| –ù–µ–¥–æ–ª—ñ–∫ | –ü–æ—è—Å–Ω–µ–Ω–Ω—è |
|---------|-----------|
| **–õ—ñ–Ω—ñ–π–Ω—ñ—Å—Ç—å** | –¢—ñ–ª—å–∫–∏ –ª—ñ–Ω—ñ–π–Ω—ñ –º–µ–∂—ñ —Ä—ñ—à–µ–Ω–Ω—è |
| **XOR problem** | –ù–µ –º–æ–∂–µ –≤–∏—É—á–∏—Ç–∏ –Ω–µ–ª—ñ–Ω—ñ–π–Ω—ñ –ø–∞—Ç—Ç–µ—Ä–Ω–∏ |
| **–ù–µ–º–∞—î –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç–µ–π** | –ñ–æ—Ä—Å—Ç–∫–∏–π –≤–∏—Ö—ñ–¥ (0 –∞–±–æ 1) |
| **–ó–∞—Å—Ç–∞—Ä—ñ–ª—ñ—Å—Ç—å** | –ù–∞ –ø—Ä–∞–∫—Ç–∏—Ü—ñ –∑–∞–º—ñ–Ω–µ–Ω–∏–π –∫—Ä–∞—â–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏ |

---

## Scikit-learn —Ä–µ–∞–ª—ñ–∑–∞—Ü—ñ—è

```python
from sklearn.linear_model import Perceptron
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from sklearn.datasets import make_classification

# –õ—ñ–Ω—ñ–π–Ω–æ —Ä–æ–∑–¥—ñ–ª–∏–º—ñ –¥–∞–Ω—ñ
X, y = make_classification(
    n_samples=200,
    n_features=2,
    n_informative=2,
    n_redundant=0,
    random_state=42
)

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Perceptron
model = Perceptron(
    max_iter=100,
    eta0=0.1,        # learning rate
    random_state=42
)

model.fit(X_train, y_train)
y_pred = model.predict(X_test)

print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")
print("\n" + classification_report(y_test, y_pred))

# –í–∞–≥–∏ —Ç–∞ bias
print(f"–í–∞–≥–∏: {model.coef_[0]}")
print(f"Bias: {model.intercept_[0]:.4f}")
```

---

## –ü—Ä–∞–∫—Ç–∏—á–Ω—ñ –ø–æ—Ä–∞–¥–∏ üí°

1. **–ù–æ—Ä–º–∞–ª—ñ–∑—É–π –¥–∞–Ω—ñ** ‚Äî perceptron —á—É—Ç–ª–∏–≤–∏–π –¥–æ –º–∞—Å—à—Ç–∞–±—É –æ–∑–Ω–∞–∫
2. **–ü–µ—Ä–µ–≤—ñ—Ä—è–π –ª—ñ–Ω—ñ–π–Ω—É —Ä–æ–∑–¥—ñ–ª–∏–º—ñ—Å—Ç—å** ‚Äî scatter plot –¥–æ –Ω–∞–≤—á–∞–Ω–Ω—è
3. **–Ø–∫—â–æ –¥–∞–Ω—ñ –Ω–µ —Ä–æ–∑–¥—ñ–ª–∏–º—ñ** ‚Äî –æ–¥—Ä–∞–∑—É –ø–µ—Ä–µ—Ö–æ–¥—å –¥–æ Logistic Regression –∞–±–æ MLP
4. **–ù–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π –≤ production** ‚Äî —Ü–µ –Ω–∞–≤—á–∞–ª—å–Ω–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º
5. **–†–æ–∑—É–º—ñ–π –æ–±—á–∏—Å–ª–µ–Ω–Ω—è –≤—Ä—É—á–Ω—É** ‚Äî –¥–æ–ø–æ–º–∞–≥–∞—î –∑—Ä–æ–∑—É–º—ñ—Ç–∏ –Ω–µ–π—Ä–æ–Ω–Ω—ñ –º–µ—Ä–µ–∂—ñ

---

## –ö–ª—é—á–æ–≤–∞ –∫–æ–Ω—Ü–µ–ø—Ü—ñ—è: –ß–æ–º—É bias –≤–∞–∂–ª–∏–≤–∏–π?

**–ë–µ–∑ bias:**
$$z = w_1 x_1 + w_2 x_2$$

–ú–µ–∂–∞ —Ä—ñ—à–µ–Ω–Ω—è –∑–∞–≤–∂–¥–∏ –ø—Ä–æ—Ö–æ–¥–∏—Ç—å —á–µ—Ä–µ–∑ **–ø–æ—á–∞—Ç–æ–∫ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç** (0, 0). –¶–µ —Å–∏–ª—å–Ω–µ –æ–±–º–µ–∂–µ–Ω–Ω—è!

**–ó bias:**
$$z = w_1 x_1 + w_2 x_2 + b$$

–ú–µ–∂–∞ —Ä—ñ—à–µ–Ω–Ω—è –º–æ–∂–µ –±—É—Ç–∏ **–¥–µ –∑–∞–≤–≥–æ–¥–Ω–æ** —É –ø—Ä–æ—Å—Ç–æ—Ä—ñ. –ù–∞–±–∞–≥–∞—Ç–æ –≥–Ω—É—á–∫—ñ—à–µ.

---

## –ü–æ–≤'—è–∑–∞–Ω—ñ —Ç–µ–º–∏

- [[02_Activation_Functions]] ‚Äî step, sigmoid, ReLU —Ç–∞ —ñ–Ω—à—ñ
- [[03_Backpropagation]] ‚Äî —è–∫ –Ω–∞–≤—á–∞—é—Ç—å—Å—è –≥–ª–∏–±–æ–∫—ñ –º–µ—Ä–µ–∂—ñ
- [[04_Optimizers]] ‚Äî SGD, Adam —Ç–∞ —ñ–Ω—à—ñ –æ–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä–∏
- [[01_MLP]] ‚Äî Multi-Layer Perceptron: perceptron –Ω–∞ —Å—Ç–µ—Ä–æ—ó–¥–∞—Ö
- [[02_Logistic_Regression]] ‚Äî "–º'—è–∫–∞" –≤–µ—Ä—Å—ñ—è perceptron –∑ –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—è–º–∏

## –†–µ—Å—É—Ä—Å–∏

- [StatQuest: Perceptron](https://www.youtube.com/watch?v=4Gac5I64LM4)
- [3Blue1Brown: Neural Networks](https://www.youtube.com/watch?v=aircAruvnKk)
- [Scikit-learn: Perceptron](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html)

---

## –ö–ª—é—á–æ–≤—ñ –≤–∏—Å–Ω–æ–≤–∫–∏

> Perceptron ‚Äî —Ü–µ –æ–¥–∏–Ω —à—Ç—É—á–Ω–∏–π –Ω–µ–π—Ä–æ–Ω, —è–∫–∏–π –Ω–∞–≤—á–∞—î—Ç—å—Å—è —Ä–æ–∑–¥—ñ–ª—è—Ç–∏ –¥–≤–∞ –∫–ª–∞—Å–∏ –ø—Ä—è–º–æ—é –ª—ñ–Ω—ñ—î—é —á–µ—Ä–µ–∑ –∫–æ—Ä–∏–≥—É–≤–∞–Ω–Ω—è –≤–∞–≥ –Ω–∞ –æ—Å–Ω–æ–≤—ñ –ø–æ–º–∏–ª–æ–∫ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è.

**–û—Å–Ω–æ–≤–Ω—ñ –ø—Ä–∏–Ω—Ü–∏–ø–∏:**
- –ó–≤–∞–∂–µ–Ω–∞ —Å—É–º–∞ –≤—Ö–æ–¥—ñ–≤ + bias ‚Üí activation function ‚Üí –≤–∏—Ö—ñ–¥ (0 –∞–±–æ 1)
- –ù–∞–≤—á–∞–Ω–Ω—è: –ø–æ—Ä—ñ–≤–Ω—è–π –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –∑ –ø—Ä–∞–≤–∏–ª—å–Ω–æ—é –≤—ñ–¥–ø–æ–≤—ñ–¥–¥—é, —Å–∫–æ—Ä–∏–≥—É–π –≤–∞–≥–∏
- –ì–∞—Ä–∞–Ω—Ç–æ–≤–∞–Ω–æ –∑–±—ñ–≥–∞—î—Ç—å—Å—è –Ω–∞ –ª—ñ–Ω—ñ–π–Ω–æ —Ä–æ–∑–¥—ñ–ª–∏–º–∏—Ö –¥–∞–Ω–∏—Ö
- **–ù–µ –º–æ–∂–µ** –≤–∏—É—á–∏—Ç–∏ –Ω–µ–ª—ñ–Ω—ñ–π–Ω—ñ –ø–∞—Ç—Ç–µ—Ä–Ω–∏ (XOR) ‚Üí –ø–æ—Ç—Ä—ñ–±–Ω–∞ MLP

**–§–æ—Ä–º—É–ª–∞:**
$$\hat{y} = \text{step}(w_1 x_1 + w_2 x_2 + ... + w_n x_n + b)$$

**–ú—ñ—Å—Ü–µ –≤ –Ω–∞–≤—á–∞–Ω–Ω—ñ:**
- Perceptron = –ø–µ—Ä—à–∏–π –∫—Ä–æ–∫ –¥–æ –Ω–µ–π—Ä–æ–Ω–Ω–∏—Ö –º–µ—Ä–µ–∂
- –†–æ–∑—É–º—ñ–Ω–Ω—è –≤–∞–≥, bias, activation ‚Üí —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç –¥–ª—è –≤—Å—å–æ–≥–æ deep learning

**–ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏:**
- –ù–∞–≤—á–∞–Ω–Ω—è –∫–æ–Ω—Ü–µ–ø—Ü—ñ–π + –ª—ñ–Ω—ñ–π–Ω–æ —Ä–æ–∑–¥—ñ–ª–∏–º—ñ –¥–∞–Ω—ñ = Perceptron ‚úì
- Production = Logistic Regression –∞–±–æ MLP ‚úì

---

#ml #neural-networks #perceptron #binary-classification #fundamentals #deep-learning
