# Activation Functions (–§—É–Ω–∫—Ü—ñ—ó –∞–∫—Ç–∏–≤–∞—Ü—ñ—ó)

## –©–æ —Ü–µ?

**Activation function** ‚Äî —Ü–µ —Ñ—É–Ω–∫—Ü—ñ—è, —è–∫–∞ –∑–∞—Å—Ç–æ—Å–æ–≤—É—î—Ç—å—Å—è –¥–æ –≤–∏—Ö–æ–¥—É –Ω–µ–π—Ä–æ–Ω–∞ –ø—ñ—Å–ª—è –∑–≤–∞–∂–µ–Ω–æ—ó —Å—É–º–∏. –í–æ–Ω–∞ –≤–∏–∑–Ω–∞—á–∞—î, —è–∫–∏–π —Å–∏–≥–Ω–∞–ª –Ω–µ–π—Ä–æ–Ω –ø–µ—Ä–µ–¥–∞—î –¥–∞–ª—ñ.

**–ì–æ–ª–æ–≤–Ω–∞ —ñ–¥–µ—è:** –±–µ–∑ activation function –Ω–µ–π—Ä–æ–Ω–Ω–∞ –º–µ—Ä–µ–∂–∞ ‚Äî —Ü–µ –ø—Ä–æ—Å—Ç–æ –ª—ñ–Ω—ñ–π–Ω–∞ —Ä–µ–≥—Ä–µ—Å—ñ—è, —Å–∫—ñ–ª—å–∫–∏ –± —à–∞—Ä—ñ–≤ —Ç–∏ –Ω–µ –¥–æ–¥–∞–≤. –ù–µ–ª—ñ–Ω—ñ–π–Ω–∞ –∞–∫—Ç–∏–≤–∞—Ü—ñ—è –¥–∞—î –º–µ—Ä–µ–∂—ñ –∑–¥–∞—Ç–Ω—ñ—Å—Ç—å –≤–∏–≤—á–∞—Ç–∏ —Å–∫–ª–∞–¥–Ω—ñ –ø–∞—Ç—Ç–µ—Ä–Ω–∏.

## –ù–∞–≤—ñ—â–æ –ø–æ—Ç—Ä—ñ–±–Ω—ñ?

- üî• **–ù–µ–ª—ñ–Ω—ñ–π–Ω—ñ—Å—Ç—å** ‚Äî –¥–æ–∑–≤–æ–ª—è—î –º–µ—Ä–µ–∂—ñ –∞–ø—Ä–æ–∫—Å–∏–º—É–≤–∞—Ç–∏ –±—É–¥—å-—è–∫—É —Ñ—É–Ω–∫—Ü—ñ—é
- üì° **–ö–æ–Ω—Ç—Ä–æ–ª—å —Å–∏–≥–Ω–∞–ª—É** ‚Äî –æ–±–º–µ–∂—É—î –∞–±–æ –Ω–æ—Ä–º–∞–ª—ñ–∑—É—î –≤–∏—Ö—ñ–¥ –Ω–µ–π—Ä–æ–Ω–∞
- üéØ **–ì—Ä–∞–¥—ñ—î–Ω—Ç–∏** ‚Äî –∑–∞–±–µ–∑–ø–µ—á—É—î –º–æ–∂–ª–∏–≤—ñ—Å—Ç—å backpropagation
- üèóÔ∏è **–†—ñ–∑–Ω—ñ –∑–∞–¥–∞—á—ñ** ‚Äî –≤–∏—Ö—ñ–¥–Ω–∏–π —à–∞—Ä –ø–æ—Ç—Ä–µ–±—É—î —ñ–Ω—à–æ—ó –∞–∫—Ç–∏–≤–∞—Ü—ñ—ó –Ω—ñ–∂ –ø—Ä–∏—Ö–æ–≤–∞–Ω–∏–π

---

## –ß–æ–º—É –Ω–µ–ª—ñ–Ω—ñ–π–Ω—ñ—Å—Ç—å –∫—Ä–∏—Ç–∏—á–Ω–∞?

–Ø–∫—â–æ –∫–æ–∂–µ–Ω –Ω–µ–π—Ä–æ–Ω –ø—Ä–æ—Å—Ç–æ —Ä–∞—Ö—É—î `w¬∑x + b`, —Ç–æ –¥–≤–∞ —à–∞—Ä–∏ —Ä–∞–∑–æ–º ‚Äî —Ü–µ:

```
–®–∞—Ä 1: y‚ÇÅ = W‚ÇÅx + b‚ÇÅ
–®–∞—Ä 2: y‚ÇÇ = W‚ÇÇy‚ÇÅ + b‚ÇÇ = W‚ÇÇ(W‚ÇÅx + b‚ÇÅ) + b‚ÇÇ = (W‚ÇÇW‚ÇÅ)x + (W‚ÇÇb‚ÇÅ + b‚ÇÇ)
```

–¶–µ –∑–Ω–æ–≤—É –ª—ñ–Ω—ñ–π–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è. –•–æ—á 2 —à–∞—Ä–∏, —Ö–æ—á 100 ‚Äî —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Ç–æ–π —Å–∞–º–∏–π. –ù–µ–ª—ñ–Ω—ñ–π–Ω–∞ –∞–∫—Ç–∏–≤–∞—Ü—ñ—è –ª–∞–º–∞—î —Ü—é –∑–∞–∫–æ–Ω–æ–º—ñ—Ä–Ω—ñ—Å—Ç—å.

---

## –û–≥–ª—è–¥ —Ñ—É–Ω–∫—Ü—ñ–π

| –§—É–Ω–∫—Ü—ñ—è | –î—ñ–∞–ø–∞–∑–æ–Ω | –î–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è |
|---------|----------|---------------------|
| **Step** | {0, 1} | –û—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏–π perceptron (–∑–∞—Å—Ç–∞—Ä—ñ–ª–æ) |
| **Sigmoid** | (0, 1) | –í–∏—Ö—ñ–¥–Ω–∏–π —à–∞—Ä, –±—ñ–Ω–∞—Ä–Ω–∞ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è |
| **Tanh** | (-1, 1) | –ü—Ä–∏—Ö–æ–≤–∞–Ω—ñ —à–∞—Ä–∏ RNN (–º–µ–Ω—à –ø–æ–ø—É–ª—è—Ä–Ω–æ) |
| **ReLU** | [0, ‚àû) | –ü—Ä–∏—Ö–æ–≤–∞–Ω—ñ —à–∞—Ä–∏ ‚Äî —Å—Ç–∞–Ω–¥–∞—Ä—Ç de facto |
| **Leaky ReLU** | (-‚àû, ‚àû) | –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ ReLU –ø—Ä–∏ dying neurons |
| **ELU** | (-1, ‚àû) | –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ ReLU, –ø–ª–∞–≤–Ω—ñ—à–∞ |
| **Softmax** | (0, 1), —Å—É–º–∞=1 | –í–∏—Ö—ñ–¥–Ω–∏–π —à–∞—Ä, –±–∞–≥–∞—Ç–æ–∫–ª–∞—Å–æ–≤–∞ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è |
| **Linear** | (-‚àû, ‚àû) | –í–∏—Ö—ñ–¥–Ω–∏–π —à–∞—Ä, —Ä–µ–≥—Ä–µ—Å—ñ—è |

---

## 1. Step Function

–¶–µ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∞ –∞–∫—Ç–∏–≤–∞—Ü—ñ—è perceptron. –ñ–æ—Ä—Å—Ç–∫–µ 0 –∞–±–æ 1.

**–ü—Ä–æ–±–ª–µ–º–∞:** –ø–æ—Ö—ñ–¥–Ω–∞ = 0 –º–∞–π–∂–µ —Å–∫—Ä—ñ–∑—å ‚Üí backpropagation –Ω–µ –ø—Ä–∞—Ü—é—î. –¢–æ–º—É –Ω–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –≤ –Ω–∞–≤—á–∞–Ω–Ω—ñ –º–µ—Ä–µ–∂.

```python
import numpy as np
import matplotlib.pyplot as plt

def step(z):
    return np.where(z >= 0, 1, 0)

z = np.linspace(-5, 5, 300)

plt.figure(figsize=(8, 4))
plt.plot(z, step(z), linewidth=2)
plt.title("Step Function")
plt.xlabel("z"); plt.ylabel("output")
plt.axhline(0, color='k', linewidth=0.5)
plt.axvline(0, color='k', linewidth=0.5)
plt.grid(True, alpha=0.3)
plt.show()
```

---

## 2. Sigmoid (Logistic)

$$\sigma(z) = \frac{1}{1 + e^{-z}}$$

–ü–ª–∞–≤–Ω–æ —Å—Ç–∏—Å–∫–∞—î –±—É–¥—å-—è–∫–µ —á–∏—Å–ª–æ –≤ –¥—ñ–∞–ø–∞–∑–æ–Ω (0, 1). –Ü–¥–µ–∞–ª—å–Ω–æ —ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç—É—î—Ç—å—Å—è —è–∫ –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å.

**–ü–µ—Ä–µ–≤–∞–≥–∏:**
- –ì–ª–∞–¥–∫–∞, –¥–∏—Ñ–µ—Ä–µ–Ω—Ü—ñ–π–æ–≤–Ω–∞ —Å–∫—Ä—ñ–∑—å
- –í–∏—Ö—ñ–¥ —è–∫ –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å

**–ü—Ä–æ–±–ª–µ–º–∏:**
- **Vanishing gradient** ‚Äî –ø—Ä–∏ –≤–µ–ª–∏–∫–∏—Ö |z| –ø–æ—Ö—ñ–¥–Ω–∞ ‚âà 0, –≥—Ä–∞–¥—ñ—î–Ω—Ç–∏ "–∑–Ω–∏–∫–∞—é—Ç—å" —É –≥–ª–∏–±–æ–∫–∏—Ö –º–µ—Ä–µ–∂–∞—Ö
- **Not zero-centered** ‚Äî –≤–∏—Ö—ñ–¥ –∑–∞–≤–∂–¥–∏ –ø–æ–∑–∏—Ç–∏–≤–Ω–∏–π, —â–æ —Å–ø–æ–≤—ñ–ª—å–Ω—é—î –Ω–∞–≤—á–∞–Ω–Ω—è
- –ü–æ–≤—ñ–ª—å–Ω–µ –æ–±—á–∏—Å–ª–µ–Ω–Ω—è (exponent)

**–î–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏:** –≤–∏—Ö—ñ–¥–Ω–∏–π —à–∞—Ä –¥–ª—è –±—ñ–Ω–∞—Ä–Ω–æ—ó –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó. –£ –ø—Ä–∏—Ö–æ–≤–∞–Ω–∏—Ö —à–∞—Ä–∞—Ö ‚Äî —É–Ω–∏–∫–∞—Ç–∏.

```python
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def sigmoid_derivative(z):
    s = sigmoid(z)
    return s * (1 - s)

z = np.linspace(-6, 6, 300)

fig, axes = plt.subplots(1, 2, figsize=(12, 4))
axes[0].plot(z, sigmoid(z), linewidth=2, color='blue')
axes[0].set_title("Sigmoid"); axes[0].set_xlabel("z"); axes[0].set_ylabel("œÉ(z)")
axes[0].grid(True, alpha=0.3)

axes[1].plot(z, sigmoid_derivative(z), linewidth=2, color='red')
axes[1].set_title("Sigmoid ‚Äî –ü–æ—Ö—ñ–¥–Ω–∞"); axes[1].set_xlabel("z"); axes[1].set_ylabel("œÉ'(z)")
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–µ –∑–Ω–∞—á–µ–Ω–Ω—è –ø–æ—Ö—ñ–¥–Ω–æ—ó
print(f"–ú–∞–∫—Å. –ø–æ—Ö—ñ–¥–Ω–∞ sigmoid: {sigmoid_derivative(0):.4f}")  # 0.25
```

**–ó–≤–µ—Ä–Ω–∏ —É–≤–∞–≥—É:** –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –ø–æ—Ö—ñ–¥–Ω–∞ sigmoid = 0.25. –£ 10-—à–∞—Ä–æ–≤—ñ–π –º–µ—Ä–µ–∂—ñ –≥—Ä–∞–¥—ñ—î–Ω—Ç –∑–º–µ–Ω—à—É—î—Ç—å—Å—è —è–∫ 0.25¬π‚Å∞ ‚âà 0.000001. –¶–µ —ñ —î vanishing gradient.

---

## 3. Tanh (Hyperbolic Tangent)

$$\tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}$$

–°—Ö–æ–∂–∞ –Ω–∞ sigmoid, –∞–ª–µ zero-centered ‚Äî –≤–∏—Ö—ñ–¥ –≤—ñ–¥ -1 –¥–æ 1.

**–ü–µ—Ä–µ–≤–∞–≥–∏ –Ω–∞–¥ sigmoid:**
- Zero-centered ‚Üí –≥—Ä–∞–¥—ñ—î–Ω—Ç–∏ –º–æ–∂—É—Ç—å –±—É—Ç–∏ —ñ –ø–æ–∑–∏—Ç–∏–≤–Ω—ñ, —ñ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ñ ‚Üí –Ω–∞–≤—á–∞–Ω–Ω—è —Å—Ç–∞–±—ñ–ª—å–Ω—ñ—à–µ

**–¢–∞ —Å–∞–º–∞ –ø—Ä–æ–±–ª–µ–º–∞:**
- Vanishing gradient –ø—Ä–∏ –Ω–∞—Å–∏—á–µ–Ω–Ω—ñ (–≤–µ–ª–∏–∫–∏—Ö |z|)

**–î–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏:** –ø—Ä–∏—Ö–æ–≤–∞–Ω—ñ —à–∞—Ä–∏ –≤ RNN/LSTM ‚Äî —Ç–∞–º tanh —â–µ –∞–∫—Ç—É–∞–ª—å–Ω–∞. –£ –∑–≤–∏—á–∞–π–Ω–∏—Ö –º–µ—Ä–µ–∂–∞—Ö –∑–∞–º—ñ–Ω–µ–Ω–∞ ReLU.

```python
def tanh(z):
    return np.tanh(z)

def tanh_derivative(z):
    return 1 - np.tanh(z)**2

z = np.linspace(-4, 4, 300)

fig, axes = plt.subplots(1, 2, figsize=(12, 4))
axes[0].plot(z, tanh(z), linewidth=2, color='green')
axes[0].set_title("Tanh"); axes[0].set_xlabel("z"); axes[0].set_ylabel("tanh(z)")
axes[0].axhline(0, color='k', linewidth=0.5)
axes[0].grid(True, alpha=0.3)

axes[1].plot(z, tanh_derivative(z), linewidth=2, color='orange')
axes[1].set_title("Tanh ‚Äî –ü–æ—Ö—ñ–¥–Ω–∞"); axes[1].set_xlabel("z"); axes[1].set_ylabel("tanh'(z)")
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

---

## 4. ReLU (Rectified Linear Unit)

$$\text{ReLU}(z) = \max(0, z)$$

–ù–∞–π–ø—Ä–æ—Å—Ç—ñ—à–∞ —ñ –Ω–∞–π–ø–æ–ø—É–ª—è—Ä–Ω—ñ—à–∞ –∞–∫—Ç–∏–≤–∞—Ü—ñ—è –¥–ª—è –ø—Ä–∏—Ö–æ–≤–∞–Ω–∏—Ö —à–∞—Ä—ñ–≤. –í—ñ–¥'—î–º–Ω–µ ‚Üí 0, –ø–æ–∑–∏—Ç–∏–≤–Ω–µ ‚Üí –±–µ–∑ –∑–º—ñ–Ω.

**–ü–µ—Ä–µ–≤–∞–≥–∏:**
- –û–±—á–∏—Å–ª–µ–Ω–Ω—è: –ø—Ä–æ—Å—Ç–æ `max(0, z)` ‚Äî –¥—É–∂–µ —à–≤–∏–¥–∫–æ
- **–ù–µ–º–∞—î vanishing gradient** –¥–ª—è –ø–æ–∑–∏—Ç–∏–≤–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω—å (–ø–æ—Ö—ñ–¥–Ω–∞ = 1)
- Sparse activation ‚Äî –±–∞–≥–∞—Ç–æ –Ω–µ–π—Ä–æ–Ω—ñ–≤ = 0, –º–µ—Ä–µ–∂–∞ –±—ñ–ª—å—à –µ—Ñ–µ–∫—Ç–∏–≤–Ω–∞

**–ü—Ä–æ–±–ª–µ–º–∞: Dying ReLU**
–Ø–∫—â–æ –Ω–µ–π—Ä–æ–Ω –ø–æ—Ç—Ä–∞–ø–ª—è—î –≤ –∑–æ–Ω—É z < 0 —ñ —Ç–∞–º –∑–∞–ª–∏—à–∞—î—Ç—å—Å—è (–≤–∞–≥–∏ –æ–Ω–æ–≤–ª—é—é—Ç—å—Å—è —Ç–∞–∫, —â–æ z –∑–∞–≤–∂–¥–∏ –≤—ñ–¥'—î–º–Ω–µ), –Ω–µ–π—Ä–æ–Ω "–ø–æ–º–∏—Ä–∞—î" ‚Äî –∑–∞–≤–∂–¥–∏ –≤–∏–¥–∞—î 0, –ø–æ—Ö—ñ–¥–Ω–∞ = 0, –≤–∞–≥–∏ –±—ñ–ª—å—à–µ –Ω–µ –æ–Ω–æ–≤–ª—é—é—Ç—å—Å—è.

**–î–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏:** –ø—Ä–∏—Ö–æ–≤–∞–Ω—ñ —à–∞—Ä–∏ —É MLP, CNN ‚Äî –º–∞–π–∂–µ –∑–∞–≤–∂–¥–∏ ReLU –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º.

```python
def relu(z):
    return np.maximum(0, z)

def relu_derivative(z):
    return np.where(z > 0, 1, 0)

z = np.linspace(-4, 4, 300)

fig, axes = plt.subplots(1, 2, figsize=(12, 4))
axes[0].plot(z, relu(z), linewidth=2, color='purple')
axes[0].set_title("ReLU"); axes[0].set_xlabel("z"); axes[0].set_ylabel("ReLU(z)")
axes[0].grid(True, alpha=0.3)

axes[1].plot(z, relu_derivative(z), linewidth=2, color='brown')
axes[1].set_title("ReLU ‚Äî –ü–æ—Ö—ñ–¥–Ω–∞"); axes[1].set_xlabel("z"); axes[1].set_ylabel("ReLU'(z)")
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

---

## 5. Leaky ReLU

$$\text{LeakyReLU}(z) = \begin{cases} z & \text{—è–∫—â–æ } z > 0 \\ \alpha z & \text{—è–∫—â–æ } z \leq 0 \end{cases}$$

–¥–µ Œ± ‚Äî –º–∞–ª–∏–π –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç (–∑–∞–∑–≤–∏—á–∞–π 0.01).

–í–∏—Ä—ñ—à—É—î –ø—Ä–æ–±–ª–µ–º—É dying ReLU: –∑–∞–º—ñ—Å—Ç—å –Ω—É–ª—è –¥–ª—è –≤—ñ–¥'—î–º–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω—å –¥–∞—î –º–∞–ª–µ–Ω—å–∫–∏–π –Ω–µ–≥–∞—Ç–∏–≤–Ω–∏–π –Ω–∞—Ö–∏–ª.

**–î–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏:** —è–∫—â–æ –º–∞—î—à –ø—Ä–æ–±–ª–µ–º—É dying neurons ‚Äî —Å–ø—Ä–æ–±—É–π Leaky ReLU –∑–∞–º—ñ—Å—Ç—å ReLU.

```python
def leaky_relu(z, alpha=0.01):
    return np.where(z > 0, z, alpha * z)

def leaky_relu_derivative(z, alpha=0.01):
    return np.where(z > 0, 1, alpha)

z = np.linspace(-4, 4, 300)

fig, axes = plt.subplots(1, 2, figsize=(12, 4))
axes[0].plot(z, relu(z), linewidth=2, label='ReLU', color='purple')
axes[0].plot(z, leaky_relu(z), linewidth=2, label='Leaky ReLU (Œ±=0.01)', 
             color='magenta', linestyle='--')
axes[0].set_title("ReLU vs Leaky ReLU"); axes[0].set_xlabel("z")
axes[0].legend(); axes[0].grid(True, alpha=0.3)

axes[1].plot(z, leaky_relu_derivative(z), linewidth=2, color='magenta')
axes[1].set_title("Leaky ReLU ‚Äî –ü–æ—Ö—ñ–¥–Ω–∞"); axes[1].set_xlabel("z")
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

---

## 6. ELU (Exponential Linear Unit)

$$\text{ELU}(z) = \begin{cases} z & \text{—è–∫—â–æ } z > 0 \\ \alpha(e^z - 1) & \text{—è–∫—â–æ } z \leq 0 \end{cases}$$

–ü–ª–∞–≤–Ω–∞ –≤–µ—Ä—Å—ñ—è Leaky ReLU. –í—ñ–¥'—î–º–Ω–∞ —á–∞—Å—Ç–∏–Ω–∞ ‚Äî –ø–ª–∞–≤–Ω–∞ –∫—Ä–∏–≤–∞, –∞ –Ω–µ –ª—ñ–Ω—ñ—è.

**–ü–µ—Ä–µ–≤–∞–≥–∏ –Ω–∞–¥ ReLU:**
- –ü–ª–∞–≤–Ω—ñ—à–∞, —â–æ —ñ–Ω–æ–¥—ñ –ø—Ä–∏—Å–∫–æ—Ä—é—î –∑–±—ñ–∂–Ω—ñ—Å—Ç—å
- Zero-centered —É —Å–µ—Ä–µ–¥–Ω—å–æ–º—É

**–ù–µ–¥–æ–ª—ñ–∫:** –ø–æ–≤—ñ–ª—å–Ω—ñ—à–µ –æ–±—á–∏—Å–ª–µ–Ω–Ω—è (exponent).

```python
def elu(z, alpha=1.0):
    return np.where(z > 0, z, alpha * (np.exp(z) - 1))

z = np.linspace(-4, 4, 300)

plt.figure(figsize=(8, 4))
plt.plot(z, relu(z), linewidth=2, label='ReLU', color='purple')
plt.plot(z, elu(z), linewidth=2, label='ELU (Œ±=1)', color='teal', linestyle='--')
plt.title("ReLU vs ELU")
plt.xlabel("z"); plt.legend(); plt.grid(True, alpha=0.3)
plt.show()
```

---

## 7. Softmax

$$\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j} e^{z_j}}$$

–ü–µ—Ä–µ—Ç–≤–æ—Ä—é—î –≤–µ–∫—Ç–æ—Ä —á–∏—Å–µ–ª —É –≤–µ–∫—Ç–æ—Ä –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç–µ–π ‚Äî –≤—Å—ñ –∑–Ω–∞—á–µ–Ω–Ω—è –º—ñ–∂ 0 —ñ 1, —Å—É–º–∞ = 1.

**–¶–µ –Ω–µ —Ñ—É–Ω–∫—Ü—ñ—è –¥–ª—è –æ–¥–Ω–æ–≥–æ –Ω–µ–π—Ä–æ–Ω–∞ ‚Äî –≤–æ–Ω–∞ –∑–∞—Å—Ç–æ—Å–æ–≤—É—î—Ç—å—Å—è –¥–æ —Ü—ñ–ª–æ–≥–æ —à–∞—Ä—É.**

**–î–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏:** –≤–∏–∫–ª—é—á–Ω–æ –≤–∏—Ö—ñ–¥–Ω–∏–π —à–∞—Ä –¥–ª—è –±–∞–≥–∞—Ç–æ–∫–ª–∞—Å–æ–≤–æ—ó –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó.

```python
def softmax(z):
    # –°—Ç–∞–±—ñ–ª—å–Ω–∞ –≤–µ—Ä—Å—ñ—è (–≤—ñ–¥–Ω—ñ–º–∞–Ω–Ω—è–º –º–∞–∫—Å–∏–º—É–º—É)
    exp_z = np.exp(z - np.max(z))
    return exp_z / exp_z.sum()

# –ü—Ä–∏–∫–ª–∞–¥
z = np.array([2.0, 1.0, 0.1])
probs = softmax(z)

print("–í—Ö—ñ–¥ (logits):", z)
print("–í–∏—Ö—ñ–¥ (probabilities):", np.round(probs, 4))
print(f"–°—É–º–∞: {probs.sum():.4f}")  # –∑–∞–≤–∂–¥–∏ = 1.0

# –ö–ª–∞—Å –∑ –Ω–∞–π–≤–∏—â–æ—é –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—é
print(f"–ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–∏–π –∫–ª–∞—Å: {np.argmax(probs)}")
```

**–í–∏—Ö—ñ–¥:**
```
–í—Ö—ñ–¥ (logits): [2.  1.  0.1]
–í–∏—Ö—ñ–¥ (probabilities): [0.6590 0.2424 0.0986]
–°—É–º–∞: 1.0000
–ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–∏–π –∫–ª–∞—Å: 0
```

---

## 8. Linear (–±–µ–∑ –∞–∫—Ç–∏–≤–∞—Ü—ñ—ó)

$$f(z) = z$$

–¢–æ–±—Ç–æ ‚Äî –Ω—ñ—á–æ–≥–æ –Ω–µ —Ä–æ–±–∏—Ç–∏. –ù–µ–π—Ä–æ–Ω –ø—Ä–æ—Å—Ç–æ –ø–µ—Ä–µ–¥–∞—î –∑–≤–∞–∂–µ–Ω—É —Å—É–º—É –¥–∞–ª—ñ.

**–î–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏:** –≤–∏—Ö—ñ–¥–Ω–∏–π —à–∞—Ä –¥–ª—è **—Ä–µ–≥—Ä–µ—Å—ñ—ó**, –∫–æ–ª–∏ –ø–æ—Ç—Ä—ñ–±–Ω–æ –ø–µ—Ä–µ–¥–±–∞—á–∏—Ç–∏ –¥–æ–≤—ñ–ª—å–Ω–µ —á–∏—Å–ª–æ.

---

## –Ø–∫—É –∞–∫—Ç–∏–≤–∞—Ü—ñ—é –≤–∏–±—Ä–∞—Ç–∏?

### –î–ª—è –ø—Ä–∏—Ö–æ–≤–∞–Ω–∏—Ö —à–∞—Ä—ñ–≤

```python
# –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∏–π –≤–∏–±—ñ—Ä ‚Äî ReLU
model = keras.Sequential([
    keras.layers.Dense(128, activation='relu'),
    keras.layers.Dense(64, activation='relu'),
])
```

–Ø–∫—â–æ –º–∞—î—à dying neurons ‚Üí —Å–ø—Ä–æ–±—É–π `leaky_relu` –∞–±–æ `elu`.

### –î–ª—è –≤–∏—Ö—ñ–¥–Ω–æ–≥–æ —à–∞—Ä—É

```python
# –ë—ñ–Ω–∞—Ä–Ω–∞ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è (0 –∞–±–æ 1)
keras.layers.Dense(1, activation='sigmoid')

# –ë–∞–≥–∞—Ç–æ–∫–ª–∞—Å–æ–≤–∞ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è (3+ –∫–ª–∞—Å–∏)
keras.layers.Dense(10, activation='softmax')

# –†–µ–≥—Ä–µ—Å—ñ—è (–¥–æ–≤—ñ–ª—å–Ω–µ —á–∏—Å–ª–æ)
keras.layers.Dense(1, activation='linear')  # –∞–±–æ –ø—Ä–æ—Å—Ç–æ –±–µ–∑ activation
```

### –¢–∞–±–ª–∏—Ü—è –≤–∏–±–æ—Ä—É

| –ó–∞–¥–∞—á–∞ | –í–∏—Ö—ñ–¥–Ω–∏–π —à–∞—Ä | Loss function |
|--------|-------------|---------------|
| –ë—ñ–Ω–∞—Ä–Ω–∞ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è | Sigmoid (1 –Ω–µ–π—Ä–æ–Ω) | Binary Crossentropy |
| –ë–∞–≥–∞—Ç–æ–∫–ª–∞—Å–æ–≤–∞ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è | Softmax (K –Ω–µ–π—Ä–æ–Ω—ñ–≤) | Categorical Crossentropy |
| –†–µ–≥—Ä–µ—Å—ñ—è | Linear (1 –Ω–µ–π—Ä–æ–Ω) | MSE / MAE |
| –ü—Ä–∏—Ö–æ–≤–∞–Ω—ñ —à–∞—Ä–∏ | ReLU | ‚Äî |

---

## Vanishing Gradient ‚Äî –≥–æ–ª–æ–≤–Ω–∞ –ø—Ä–æ–±–ª–µ–º–∞

–¶–µ –Ω–∞—Å—Ç—ñ–ª—å–∫–∏ –≤–∞–∂–ª–∏–≤–æ, —â–æ –≤–∞—Ä—Ç–æ –∑—Ä–æ–∑—É–º—ñ—Ç–∏ –æ–∫—Ä–µ–º–æ.

**–©–æ –≤—ñ–¥–±—É–≤–∞—î—Ç—å—Å—è –ø—ñ–¥ —á–∞—Å backpropagation:**
- –ì—Ä–∞–¥—ñ—î–Ω—Ç–∏ –ø–µ—Ä–µ–º–Ω–æ–∂—É—é—Ç—å—Å—è —à–∞—Ä –∑–∞ —à–∞—Ä–æ–º –≤ –Ω–∞–ø—Ä—è–º–∫—É –≤—ñ–¥ –≤–∏—Ö–æ–¥—É –¥–æ –≤—Ö–æ–¥—É
- –Ø–∫—â–æ –∫–æ–∂–Ω–∞ –ø–æ—Ö—ñ–¥–Ω–∞ < 1, –≥—Ä–∞–¥—ñ—î–Ω—Ç –∑–º–µ–Ω—à—É—î—Ç—å—Å—è –µ–∫—Å–ø–æ–Ω–µ–Ω—Ü—ñ–π–Ω–æ
- –†–∞–Ω–Ω—ñ —à–∞—Ä–∏ –ø—Ä–∞–∫—Ç–∏—á–Ω–æ –Ω–µ –Ω–∞–≤—á–∞—é—Ç—å—Å—è

**–ù–∞–ø—Ä–∏–∫–ª–∞–¥ ‚Äî sigmoid —É 10-—à–∞—Ä–æ–≤—ñ–π –º–µ—Ä–µ–∂—ñ:**

```python
# –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –ø–æ—Ö—ñ–¥–Ω–∞ sigmoid = 0.25
max_grad_sigmoid = 0.25
layers = 10

gradient_after_backprop = max_grad_sigmoid ** layers
print(f"–ì—Ä–∞–¥—ñ—î–Ω—Ç –ø—ñ—Å–ª—è {layers} —à–∞—Ä—ñ–≤: {gradient_after_backprop:.10f}")
# ‚Üí 0.0000000954 ‚Äî –ø—Ä–∞–∫—Ç–∏—á–Ω–æ –Ω—É–ª—å
```

**–ß–æ–º—É ReLU –≤–∏—Ä—ñ—à—É—î –ø—Ä–æ–±–ª–µ–º—É:**

```python
# –ü–æ—Ö—ñ–¥–Ω–∞ ReLU –¥–ª—è –ø–æ–∑–∏—Ç–∏–≤–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω—å = 1
# –ì—Ä–∞–¥—ñ—î–Ω—Ç –Ω–µ "–∑–Ω–∏–∫–∞—î" –ø—Ä–∏ –ø—Ä–æ—Ö–æ–¥–∂–µ–Ω–Ω—ñ —á–µ—Ä–µ–∑ ReLU-–Ω–µ–π—Ä–æ–Ω–∏
max_grad_relu = 1.0
gradient_relu = max_grad_relu ** layers
print(f"–ì—Ä–∞–¥—ñ—î–Ω—Ç ReLU –ø—ñ—Å–ª—è {layers} —à–∞—Ä—ñ–≤: {gradient_relu:.4f}")
# ‚Üí 1.0 ‚Äî –±–µ–∑ –¥–µ–≥—Ä–∞–¥–∞—Ü—ñ—ó
```

---

## –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –≤—Å—ñ—Ö —Ñ—É–Ω–∫—Ü—ñ–π –Ω–∞ –æ–¥–Ω–æ–º—É –≥—Ä–∞—Ñ—ñ–∫—É

```python
import numpy as np
import matplotlib.pyplot as plt

z = np.linspace(-4, 4, 300)

functions = {
    'Sigmoid': 1 / (1 + np.exp(-z)),
    'Tanh': np.tanh(z),
    'ReLU': np.maximum(0, z),
    'Leaky ReLU': np.where(z > 0, z, 0.01 * z),
    'ELU': np.where(z > 0, z, 1.0 * (np.exp(z) - 1)),
}

fig, axes = plt.subplots(2, 3, figsize=(15, 8))
axes = axes.ravel()

for i, (name, values) in enumerate(functions.items()):
    axes[i].plot(z, values, linewidth=2)
    axes[i].set_title(name, fontsize=13, fontweight='bold')
    axes[i].set_xlabel("z"); axes[i].set_ylabel("f(z)")
    axes[i].axhline(0, color='k', linewidth=0.5)
    axes[i].axvline(0, color='k', linewidth=0.5)
    axes[i].grid(True, alpha=0.3)

# –í—Å—ñ –Ω–∞ –æ–¥–Ω–æ–º—É –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è
for name, values in functions.items():
    axes[5].plot(z, values, linewidth=2, label=name)
axes[5].set_title("–ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è", fontsize=13, fontweight='bold')
axes[5].set_xlabel("z"); axes[5].legend(fontsize=9)
axes[5].axhline(0, color='k', linewidth=0.5)
axes[5].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

---

## –ö–æ–¥ (PyTorch / Keras)

### PyTorch

```python
import torch
import torch.nn as nn

# –í–±—É–¥–æ–≤–∞–Ω—ñ activation functions
relu = nn.ReLU()
sigmoid = nn.Sigmoid()
tanh = nn.Tanh()
leaky_relu = nn.LeakyReLU(negative_slope=0.01)
elu = nn.ELU(alpha=1.0)
softmax = nn.Softmax(dim=1)

# –ê–±–æ —Ñ—É–Ω–∫—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ
import torch.nn.functional as F

x = torch.tensor([-2.0, -1.0, 0.0, 1.0, 2.0])
print(F.relu(x))
print(F.sigmoid(x))
print(F.tanh(x))
```

### Keras / TensorFlow

```python
import tensorflow as tf
from tensorflow import keras

# –£ —à–∞—Ä–∞—Ö
model = keras.Sequential([
    keras.layers.Dense(128, activation='relu'),
    keras.layers.Dense(64, activation='relu'),
    keras.layers.Dense(10, activation='softmax'),  # 10 –∫–ª–∞—Å—ñ–≤
])

# –ê–±–æ —è–∫ –æ–∫—Ä–µ–º—ñ —à–∞—Ä–∏
model = keras.Sequential([
    keras.layers.Dense(128),
    keras.layers.ReLU(),
    keras.layers.Dense(64),
    keras.layers.ReLU(),
    keras.layers.Dense(10),
    keras.layers.Softmax(),
])

# Leaky ReLU
keras.layers.Dense(64),
keras.layers.LeakyReLU(alpha=0.01),
```

---

## –ü—Ä–∞–∫—Ç–∏—á–Ω—ñ –ø–æ—Ä–∞–¥–∏ üí°

1. **–ü–æ—á–∏–Ω–∞–π –∑ ReLU** –¥–ª—è –ø—Ä–∏—Ö–æ–≤–∞–Ω–∏—Ö —à–∞—Ä—ñ–≤ ‚Äî —Ü–µ safe default
2. **Sigmoid —Ç—ñ–ª—å–∫–∏ –Ω–∞ –≤–∏—Ö–æ–¥—ñ** –¥–ª—è –±—ñ–Ω–∞—Ä–Ω–æ—ó –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó, –Ω–µ –≤ –ø—Ä–∏—Ö–æ–≤–∞–Ω–∏—Ö —à–∞—Ä–∞—Ö
3. **Softmax —Ç—ñ–ª—å–∫–∏ –Ω–∞ –≤–∏—Ö–æ–¥—ñ** –¥–ª—è –±–∞–≥–∞—Ç–æ–∫–ª–∞—Å–æ–≤–æ—ó –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó
4. **Dying neurons?** ‚Äî —Å–ø—Ä–æ–±—É–π Leaky ReLU –∞–±–æ –∑–º–µ–Ω—à learning rate
5. **RNN/LSTM** ‚Äî —Ç–∞–º tanh —ñ sigmoid –≤–±—É–¥–æ–≤–∞–Ω—ñ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–Ω–æ, –Ω–µ –∑–º—ñ–Ω—é–π
6. **–ù–æ—Ä–º–∞–ª—ñ–∑—É–π –≤—Ö–æ–¥–∏** ‚Äî activation functions —á—É—Ç–ª–∏–≤—ñ –¥–æ –º–∞—Å—à—Ç–∞–±—É –≤—Ö—ñ–¥–Ω–∏—Ö –¥–∞–Ω–∏—Ö
7. **–ù–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π sigmoid/tanh —É –≥–ª–∏–±–æ–∫–∏—Ö –º–µ—Ä–µ–∂–∞—Ö** –±–µ–∑ batch normalization

---

## –ü–æ–≤'—è–∑–∞–Ω—ñ —Ç–µ–º–∏

- [[01_Perceptron]] ‚Äî step function —è–∫ –ø–µ—Ä—à–∞ –∞–∫—Ç–∏–≤–∞—Ü—ñ—è
- [[03_Backpropagation]] ‚Äî —è–∫ –ø–æ—Ö—ñ–¥–Ω–∞ –∞–∫—Ç–∏–≤–∞—Ü—ñ—ó –≤–ø–ª–∏–≤–∞—î –Ω–∞ –Ω–∞–≤—á–∞–Ω–Ω—è
- [[01_Batch_Normalization]] ‚Äî –¥–æ–ø–æ–º–∞–≥–∞—î –ø—Ä–∏ vanishing/exploding gradients
- [[01_MLP]] ‚Äî –¥–µ —ñ —è–∫ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—Ç—å—Å—è —Ü—ñ —Ñ—É–Ω–∫—Ü—ñ—ó –≤ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä—ñ

## –†–µ—Å—É—Ä—Å–∏

- [CS231n: Activation Functions](https://cs231n.github.io/neural-networks-1/#actfun)
- [PyTorch: Non-linear activations](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity)
- [Keras: Activation Functions](https://keras.io/api/layers/activation_layers/)

---

## –ö–ª—é—á–æ–≤—ñ –≤–∏—Å–Ω–æ–≤–∫–∏

> Activation function –¥–æ–¥–∞—î –Ω–µ–ª—ñ–Ω—ñ–π–Ω—ñ—Å—Ç—å, –±–µ–∑ —è–∫–æ—ó –Ω–µ–π—Ä–æ–Ω–Ω–∞ –º–µ—Ä–µ–∂–∞ ‚Äî –ø—Ä–æ—Å—Ç–æ –ª—ñ–Ω—ñ–π–Ω–∞ —Ä–µ–≥—Ä–µ—Å—ñ—è. –í–∏–±—ñ—Ä –∞–∫—Ç–∏–≤–∞—Ü—ñ—ó —Å—É—Ç—Ç—î–≤–æ –≤–ø–ª–∏–≤–∞—î –Ω–∞ —à–≤–∏–¥–∫—ñ—Å—Ç—å —ñ —è–∫—ñ—Å—Ç—å –Ω–∞–≤—á–∞–Ω–Ω—è.

**–ü—Ä–∞–∫—Ç–∏—á–Ω–µ –ø—Ä–∞–≤–∏–ª–æ:**
- –ü—Ä–∏—Ö–æ–≤–∞–Ω—ñ —à–∞—Ä–∏ ‚Üí **ReLU**
- –í–∏—Ö—ñ–¥, –±—ñ–Ω–∞—Ä–Ω–∞ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è ‚Üí **Sigmoid**
- –í–∏—Ö—ñ–¥, –±–∞–≥–∞—Ç–æ–∫–ª–∞—Å–æ–≤–∞ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è ‚Üí **Softmax**
- –í–∏—Ö—ñ–¥, —Ä–µ–≥—Ä–µ—Å—ñ—è ‚Üí **Linear**

**–ì–æ–ª–æ–≤–Ω–∞ –ø—Ä–æ–±–ª–µ–º–∞ sigmoid/tanh:** vanishing gradient —É –≥–ª–∏–±–æ–∫–∏—Ö –º–µ—Ä–µ–∂–∞—Ö ‚Äî —Å–∞–º–µ —Ç–æ–º—É ReLU —Å—Ç–∞–≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–º.

---

#ml #neural-networks #activation-functions #relu #sigmoid #softmax #deep-learning #fundamentals
