# –õ—ñ–Ω—ñ–π–Ω–∞ –ê–ª–≥–µ–±—Ä–∞ –¥–ª—è Data Science: –ü–æ–≤–Ω–∏–π –ì–∞–π–¥

## –ó–º—ñ—Å—Ç

1. [–í–µ–∫—Ç–æ—Ä–∏](#–≤–µ–∫—Ç–æ—Ä–∏)
2. [–ú–∞—Ç—Ä–∏—Ü—ñ](#–º–∞—Ç—Ä–∏—Ü—ñ)
3. [–ú–∞—Ç—Ä–∏—á–Ω—ñ –æ–ø–µ—Ä–∞—Ü—ñ—ó](#–º–∞—Ç—Ä–∏—á–Ω—ñ-–æ–ø–µ—Ä–∞—Ü—ñ—ó)
4. [–°–∏—Å—Ç–µ–º–∏ –ª—ñ–Ω—ñ–π–Ω–∏—Ö —Ä—ñ–≤–Ω—è–Ω—å](#—Å–∏—Å—Ç–µ–º–∏-–ª—ñ–Ω—ñ–π–Ω–∏—Ö-—Ä—ñ–≤–Ω—è–Ω—å)
5. [–í–µ–∫—Ç–æ—Ä–Ω—ñ –ø—Ä–æ—Å—Ç–æ—Ä–∏](#–≤–µ–∫—Ç–æ—Ä–Ω—ñ-–ø—Ä–æ—Å—Ç–æ—Ä–∏)
6. [–í–ª–∞—Å–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è —Ç–∞ –≤–ª–∞—Å–Ω—ñ –≤–µ–∫—Ç–æ—Ä–∏](#–≤–ª–∞—Å–Ω—ñ-–∑–Ω–∞—á–µ–Ω–Ω—è-—Ç–∞-–≤–ª–∞—Å–Ω—ñ-–≤–µ–∫—Ç–æ—Ä–∏)
7. [–°–∏–Ω–≥—É–ª—è—Ä–Ω–µ —Ä–æ–∑–∫–ª–∞–¥–∞–Ω–Ω—è (SVD)](#—Å–∏–Ω–≥—É–ª—è—Ä–Ω–µ-—Ä–æ–∑–∫–ª–∞–¥–∞–Ω–Ω—è-svd)
8. [–ù–æ—Ä–º–∏ —Ç–∞ –≤—ñ–¥—Å—Ç–∞–Ω—ñ](#–Ω–æ—Ä–º–∏-—Ç–∞-–≤—ñ–¥—Å—Ç–∞–Ω—ñ)
9. [–ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è –≤ ML](#–∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è-–≤-ml)

---

# –í–µ–∫—Ç–æ—Ä–∏

## –©–æ —Ü–µ?

**–í–µ–∫—Ç–æ—Ä** ‚Äî —Ü–µ —É–ø–æ—Ä—è–¥–∫–æ–≤–∞–Ω–∏–π –Ω–∞–±—ñ—Ä —á–∏—Å–µ–ª (–∫–æ–æ—Ä–¥–∏–Ω–∞—Ç). –£ Data Science –≤–µ–∫—Ç–æ—Ä –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—î —Ç–æ—á–∫—É –≤ –±–∞–≥–∞—Ç–æ–≤–∏–º—ñ—Ä–Ω–æ–º—É –ø—Ä–æ—Å—Ç–æ—Ä—ñ –∞–±–æ –Ω–∞–±—ñ—Ä —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ (features).

### –ì–µ–æ–º–µ—Ç—Ä–∏—á–Ω–∞ —ñ–Ω—Ç—É—ó—Ü—ñ—è

```
2D –≤–µ–∫—Ç–æ—Ä v = [3, 2]:
    
    y
  4 |
  3 |
  2 |      ‚Ä¢ (3, 2)
  1 |     /
  0 |____/_______ x
    0  1  2  3  4
    
–°—Ç—Ä—ñ–ª–∫–∞ –∑ –ø–æ—á–∞—Ç–∫—É –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç –¥–æ —Ç–æ—á–∫–∏ (3, 2)
```

### –ú–∞—Ç–µ–º–∞—Ç–∏—á–Ω–µ –æ–∑–Ω–∞—á–µ–Ω–Ω—è

**–í–µ–∫—Ç–æ—Ä-—Å—Ç–æ–≤–ø–µ—Ü—å:**
$$\mathbf{v} = \begin{bmatrix} v_1 \\ v_2 \\ \vdots \\ v_n \end{bmatrix}$$

**–í–µ–∫—Ç–æ—Ä-—Ä—è–¥–æ–∫:**
$$\mathbf{v}^T = \begin{bmatrix} v_1 & v_2 & \cdots & v_n \end{bmatrix}$$

### –í Data Science

**–ü—Ä–∏–∫–ª–∞–¥:** –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –±—É–¥–∏–Ω–∫—É
```python
house = [120,    # –ø–ª–æ—â–∞ (–º¬≤)
         3,      # –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∫—ñ–º–Ω–∞—Ç
         2010,   # —Ä—ñ–∫ –ø–æ–±—É–¥–æ–≤–∏
         500000] # —Ü—ñ–Ω–∞ ($)
```

–¶–µ 4-–≤–∏–º—ñ—Ä–Ω–∏–π –≤–µ–∫—Ç–æ—Ä: $\mathbf{x} = \begin{bmatrix} 120 \\ 3 \\ 2010 \\ 500000 \end{bmatrix}$

---

## –û–ø–µ—Ä–∞—Ü—ñ—ó –∑ –≤–µ–∫—Ç–æ—Ä–∞–º–∏

### 1. –î–æ–¥–∞–≤–∞–Ω–Ω—è –≤–µ–∫—Ç–æ—Ä—ñ–≤

**–ì–µ–æ–º–µ—Ç—Ä–∏—á–Ω–æ:** –ü—Ä–∞–≤–∏–ª–æ –ø–∞—Ä–∞–ª–µ–ª–æ–≥—Ä–∞–º–∞

```
a = [2, 1]
b = [1, 2]

    y
  3 |     b+a
  2 |    ‚Ä¢
  1 |   /|\
  0 |__/_|_\__ x
    0  1  2  3
    
a + b = [2+1, 1+2] = [3, 3]
```

**–ú–∞—Ç–µ–º–∞—Ç–∏—á–Ω–æ:**
$$\mathbf{a} + \mathbf{b} = \begin{bmatrix} a_1 \\ a_2 \\ \vdots \\ a_n \end{bmatrix} + \begin{bmatrix} b_1 \\ b_2 \\ \vdots \\ b_n \end{bmatrix} = \begin{bmatrix} a_1 + b_1 \\ a_2 + b_2 \\ \vdots \\ a_n + b_n \end{bmatrix}$$

**–ö–æ–¥:**
```python
import numpy as np

a = np.array([2, 1])
b = np.array([1, 2])
c = a + b

print(f"a + b = {c}")  # [3 3]
```

### 2. –ú–Ω–æ–∂–µ–Ω–Ω—è –Ω–∞ —Å–∫–∞–ª—è—Ä

**–ì–µ–æ–º–µ—Ç—Ä–∏—á–Ω–æ:** –†–æ–∑—Ç—è–≥—É–≤–∞–Ω–Ω—è/—Å—Ç–∏—Å–Ω–µ–Ω–Ω—è

```
v = [2, 1]
2v = [4, 2]

    y
  2 |    2v
  1 | v ‚Ä¢
  0 |__‚Ä¢______ x
    0  2  4
    
–ú–Ω–æ–∂–µ–Ω–Ω—è –Ω–∞ 2 ‚Üí –ø–æ–¥–≤–æ—é—î –¥–æ–≤–∂–∏–Ω—É
```

**–ú–∞—Ç–µ–º–∞—Ç–∏—á–Ω–æ:**
$$\alpha \mathbf{v} = \alpha \begin{bmatrix} v_1 \\ v_2 \\ \vdots \\ v_n \end{bmatrix} = \begin{bmatrix} \alpha v_1 \\ \alpha v_2 \\ \vdots \\ \alpha v_n \end{bmatrix}$$

**–ö–æ–¥:**
```python
v = np.array([2, 1])
scaled = 2 * v

print(f"2v = {scaled}")  # [4 2]
```

### 3. –°–∫–∞–ª—è—Ä–Ω–∏–π –¥–æ–±—É—Ç–æ–∫ (Dot Product)

**–©–æ —Ü–µ:** –ú—ñ—Ä–∞ "—Å—Ö–æ–∂–æ—Å—Ç—ñ" –Ω–∞–ø—Ä—è–º–∫—ñ–≤ –≤–µ–∫—Ç–æ—Ä—ñ–≤.

**–ú–∞—Ç–µ–º–∞—Ç–∏—á–Ω–æ:**
$$\mathbf{a} \cdot \mathbf{b} = a_1 b_1 + a_2 b_2 + \cdots + a_n b_n = \sum_{i=1}^{n} a_i b_i$$

**–ì–µ–æ–º–µ—Ç—Ä–∏—á–Ω–æ:**
$$\mathbf{a} \cdot \mathbf{b} = \|\mathbf{a}\| \|\mathbf{b}\| \cos(\theta)$$

–¥–µ $\theta$ ‚Äî –∫—É—Ç –º—ñ–∂ –≤–µ–∫—Ç–æ—Ä–∞–º–∏.

**–Ü–Ω—Ç—É—ó—Ü—ñ—è:**
```
–Ø–∫—â–æ a ¬∑ b > 0  ‚Üí  –í–µ–∫—Ç–æ—Ä–∏ –≤ –æ–¥–Ω–æ–º—É –Ω–∞–ø—Ä—è–º–∫—É
–Ø–∫—â–æ a ¬∑ b = 0  ‚Üí  –í–µ–∫—Ç–æ—Ä–∏ –ø–µ—Ä–ø–µ–Ω–¥–∏–∫—É–ª—è—Ä–Ω—ñ (–æ—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω—ñ)
–Ø–∫—â–æ a ¬∑ b < 0  ‚Üí  –í–µ–∫—Ç–æ—Ä–∏ –≤ –ø—Ä–æ—Ç–∏–ª–µ–∂–Ω–∏—Ö –Ω–∞–ø—Ä—è–º–∫–∞—Ö
```

**–ü—Ä–∏–∫–ª–∞–¥:**
```python
a = np.array([1, 2, 3])
b = np.array([4, 5, 6])

# –°–∫–∞–ª—è—Ä–Ω–∏–π –¥–æ–±—É—Ç–æ–∫
dot_product = np.dot(a, b)
# –∞–±–æ
dot_product = a @ b
# –∞–±–æ
dot_product = (a * b).sum()

print(f"a ¬∑ b = {dot_product}")  # 1*4 + 2*5 + 3*6 = 32
```

**–ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è –≤ ML:**
- **Similarity:** –ü–æ–¥—ñ–±–Ω—ñ—Å—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤ (cosine similarity)
- **Predictions:** –õ—ñ–Ω—ñ–π–Ω–∞ —Ä–µ–≥—Ä–µ—Å—ñ—è $\hat{y} = \mathbf{w} \cdot \mathbf{x}$
- **Neural Networks:** Weighted sum inputs

### 4. –î–æ–≤–∂–∏–Ω–∞ –≤–µ–∫—Ç–æ—Ä–∞ (–ù–æ—Ä–º–∞)

**–ï–≤–∫–ª—ñ–¥–æ–≤–∞ –Ω–æ—Ä–º–∞ (L2):**
$$\|\mathbf{v}\| = \sqrt{v_1^2 + v_2^2 + \cdots + v_n^2} = \sqrt{\mathbf{v} \cdot \mathbf{v}}$$

**–ì–µ–æ–º–µ—Ç—Ä–∏—á–Ω–æ:** –í—ñ–¥—Å—Ç–∞–Ω—å –≤—ñ–¥ –ø–æ—á–∞—Ç–∫—É –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç –¥–æ —Ç–æ—á–∫–∏.

```python
v = np.array([3, 4])

# –ù–æ—Ä–º–∞
norm = np.linalg.norm(v)
# –∞–±–æ
norm = np.sqrt((v ** 2).sum())

print(f"||v|| = {norm}")  # sqrt(9 + 16) = 5
```

**–ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è (–æ–¥–∏–Ω–∏—á–Ω–∏–π –≤–µ–∫—Ç–æ—Ä):**
$$\hat{\mathbf{v}} = \frac{\mathbf{v}}{\|\mathbf{v}\|}$$

```python
v = np.array([3, 4])
v_normalized = v / np.linalg.norm(v)

print(f"Normalized: {v_normalized}")  # [0.6, 0.8]
print(f"Norm: {np.linalg.norm(v_normalized)}")  # 1.0
```

**–ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è:**
- **Feature scaling:** –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è features
- **Cosine similarity:** –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –≤–µ–∫—Ç–æ—Ä—ñ–≤ –Ω–µ–∑–∞–ª–µ–∂–Ω–æ –≤—ñ–¥ –¥–æ–≤–∂–∏–Ω–∏

### 5. –í—ñ–¥—Å—Ç–∞–Ω—å –º—ñ–∂ –≤–µ–∫—Ç–æ—Ä–∞–º–∏

**–ï–≤–∫–ª—ñ–¥–æ–≤–∞ –≤—ñ–¥—Å—Ç–∞–Ω—å:**
$$d(\mathbf{a}, \mathbf{b}) = \|\mathbf{a} - \mathbf{b}\| = \sqrt{\sum_{i=1}^{n} (a_i - b_i)^2}$$

```python
a = np.array([1, 2, 3])
b = np.array([4, 5, 6])

# –í—ñ–¥—Å—Ç–∞–Ω—å
distance = np.linalg.norm(a - b)
# –∞–±–æ –∑ scipy
from scipy.spatial.distance import euclidean
distance = euclidean(a, b)

print(f"Distance: {distance}")  # sqrt(27) ‚âà 5.196
```

### 6. Cosine Similarity

**–©–æ —Ü–µ:** –ö–æ—Å–∏–Ω—É—Å –∫—É—Ç–∞ –º—ñ–∂ –≤–µ–∫—Ç–æ—Ä–∞–º–∏ (–Ω–µ –∑–∞–ª–µ–∂–∏—Ç—å –≤—ñ–¥ –¥–æ–≤–∂–∏–Ω–∏).

$$\text{cosine\_similarity}(\mathbf{a}, \mathbf{b}) = \frac{\mathbf{a} \cdot \mathbf{b}}{\|\mathbf{a}\| \|\mathbf{b}\|} = \cos(\theta)$$

**–î—ñ–∞–ø–∞–∑–æ–Ω:** [-1, 1]
- 1 ‚Üí –û–¥–Ω–∞–∫–æ–≤–∏–π –Ω–∞–ø—Ä—è–º–æ–∫
- 0 ‚Üí –ü–µ—Ä–ø–µ–Ω–¥–∏–∫—É–ª—è—Ä–Ω—ñ
- -1 ‚Üí –ü—Ä–æ—Ç–∏–ª–µ–∂–Ω—ñ

```python
from sklearn.metrics.pairwise import cosine_similarity

a = np.array([[1, 2, 3]])
b = np.array([[4, 5, 6]])

similarity = cosine_similarity(a, b)[0, 0]

# –∞–±–æ –≤—Ä—É—á–Ω—É
similarity = np.dot(a, b.T) / (np.linalg.norm(a) * np.linalg.norm(b))

print(f"Cosine similarity: {similarity}")  # 0.974
```

**–ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è –≤ NLP:**
```python
# –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤ (TF-IDF vectors)
doc1 = np.array([0.2, 0.5, 0.3, 0.0])  # TF-IDF –≤–µ–∫—Ç–æ—Ä
doc2 = np.array([0.1, 0.6, 0.2, 0.1])

similarity = cosine_similarity([doc1], [doc2])[0, 0]
print(f"Document similarity: {similarity}")
```

---

## –ü—Ä–æ—Å—Ç—ñ—Ä –≤–µ–∫—Ç–æ—Ä—ñ–≤ (Vector Space)

### –õ—ñ–Ω—ñ–π–Ω–∞ –∫–æ–º–±—ñ–Ω–∞—Ü—ñ—è

**–©–æ —Ü–µ:** –°—É–º–∞ –≤–µ–∫—Ç–æ—Ä—ñ–≤, –ø–æ–º–Ω–æ–∂–µ–Ω–∏—Ö –Ω–∞ —Å–∫–∞–ª—è—Ä–∏.

$$\mathbf{v} = \alpha_1 \mathbf{v}_1 + \alpha_2 \mathbf{v}_2 + \cdots + \alpha_n \mathbf{v}_n$$

**–ü—Ä–∏–∫–ª–∞–¥:**
```python
v1 = np.array([1, 0])
v2 = np.array([0, 1])

# –ë—É–¥—å-—è–∫–∏–π –≤–µ–∫—Ç–æ—Ä –≤ 2D –º–æ–∂–Ω–∞ –≤–∏—Ä–∞–∑–∏—Ç–∏ —á–µ—Ä–µ–∑ v1 —Ç–∞ v2
v = 3 * v1 + 2 * v2  # [3, 2]
```

### –õ—ñ–Ω—ñ–π–Ω–∞ –Ω–µ–∑–∞–ª–µ–∂–Ω—ñ—Å—Ç—å

**–í–µ–∫—Ç–æ—Ä–∏ –ª—ñ–Ω—ñ–π–Ω–æ –Ω–µ–∑–∞–ª–µ–∂–Ω—ñ**, —è–∫—â–æ –∂–æ–¥–µ–Ω –Ω–µ –º–æ–∂–Ω–∞ –≤–∏—Ä–∞–∑–∏—Ç–∏ —á–µ—Ä–µ–∑ —ñ–Ω—à—ñ.

```python
# –õ—ñ–Ω—ñ–π–Ω–æ –Ω–µ–∑–∞–ª–µ–∂–Ω—ñ
v1 = np.array([1, 0])
v2 = np.array([0, 1])
# v2 ‚â† Œ± * v1 –¥–ª—è –±—É–¥—å-—è–∫–æ–≥–æ Œ±

# –õ—ñ–Ω—ñ–π–Ω–æ –∑–∞–ª–µ–∂–Ω—ñ
v1 = np.array([1, 2])
v2 = np.array([2, 4])  # v2 = 2 * v1
```

**–ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —á–µ—Ä–µ–∑ –≤–∏–∑–Ω–∞—á–Ω–∏–∫ (–¥–ª—è 2 –≤–µ–∫—Ç–æ—Ä—ñ–≤ –≤ 2D):**
```python
v1 = np.array([1, 2])
v2 = np.array([3, 4])

# –ü–æ–±—É–¥—É–≤–∞—Ç–∏ –º–∞—Ç—Ä–∏—Ü—é –∑ –≤–µ–∫—Ç–æ—Ä—ñ–≤-—Å—Ç–æ–≤–ø—Ü—ñ–≤
A = np.column_stack([v1, v2])

# –í–∏–∑–Ω–∞—á–Ω–∏–∫
det = np.linalg.det(A)

if det != 0:
    print("–õ—ñ–Ω—ñ–π–Ω–æ –Ω–µ–∑–∞–ª–µ–∂–Ω—ñ")
else:
    print("–õ—ñ–Ω—ñ–π–Ω–æ –∑–∞–ª–µ–∂–Ω—ñ")
```

### –ë–∞–∑–∏—Å

**–ë–∞–∑–∏—Å** ‚Äî –º—ñ–Ω—ñ–º–∞–ª—å–Ω–∏–π –Ω–∞–±—ñ—Ä –ª—ñ–Ω—ñ–π–Ω–æ –Ω–µ–∑–∞–ª–µ–∂–Ω–∏—Ö –≤–µ–∫—Ç–æ—Ä—ñ–≤, —á–µ—Ä–µ–∑ —è–∫—ñ –º–æ–∂–Ω–∞ –≤–∏—Ä–∞–∑–∏—Ç–∏ –±—É–¥—å-—è–∫–∏–π –≤–µ–∫—Ç–æ—Ä –ø—Ä–æ—Å—Ç–æ—Ä—É.

**–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∏–π –±–∞–∑–∏—Å R¬≤:**
```python
e1 = np.array([1, 0])
e2 = np.array([0, 1])

# –ë—É–¥—å-—è–∫–∏–π –≤–µ–∫—Ç–æ—Ä v = [a, b] –º–æ–∂–Ω–∞ –≤–∏—Ä–∞–∑–∏—Ç–∏:
# v = a*e1 + b*e2
```

**–†–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å** = –∫—ñ–ª—å–∫—ñ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä—ñ–≤ —É –±–∞–∑–∏—Å—ñ.

---

## –ü—Ä–æ–µ–∫—Ü—ñ—ó

### –ü—Ä–æ–µ–∫—Ü—ñ—è –≤–µ–∫—Ç–æ—Ä–∞ –Ω–∞ –≤–µ–∫—Ç–æ—Ä

**–©–æ —Ü–µ:** "–¢—ñ–Ω—å" –æ–¥–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞ –Ω–∞ —ñ–Ω—à–æ–º—É.

$$\text{proj}_{\mathbf{b}} \mathbf{a} = \frac{\mathbf{a} \cdot \mathbf{b}}{\mathbf{b} \cdot \mathbf{b}} \mathbf{b} = \frac{\mathbf{a} \cdot \mathbf{b}}{\|\mathbf{b}\|^2} \mathbf{b}$$

**–ì–µ–æ–º–µ—Ç—Ä—ñ—è:**
```
        a
       /|
      / |
     /  | (projection)
    /   |
   /    |
  /_____|
     b
```

```python
a = np.array([3, 4])
b = np.array([1, 0])

# –ü—Ä–æ–µ–∫—Ü—ñ—è a –Ω–∞ b
projection = (np.dot(a, b) / np.dot(b, b)) * b

print(f"Projection: {projection}")  # [3, 0]
```

**–ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è:**
- **PCA:** –ü—Ä–æ–µ–∫—Ü—ñ—è –¥–∞–Ω–∏—Ö –Ω–∞ principal components
- **Regression:** Orthogonal projection

---

# –ú–∞—Ç—Ä–∏—Ü—ñ

## –©–æ —Ü–µ?

**–ú–∞—Ç—Ä–∏—Ü—è** ‚Äî —Ü–µ –ø—Ä—è–º–æ–∫—É—Ç–Ω–∞ —Ç–∞–±–ª–∏—Ü—è —á–∏—Å–µ–ª.

$$A = \begin{bmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mn}
\end{bmatrix}$$

**–†–æ–∑–º—ñ—Ä:** $m \times n$ (m —Ä—è–¥–∫—ñ–≤, n —Å—Ç–æ–≤–ø—Ü—ñ–≤)

### –í Data Science

**Dataset —è–∫ –º–∞—Ç—Ä–∏—Ü—è:**
```python
# –ö–æ–∂–µ–Ω —Ä—è–¥–æ–∫ = –∑—Ä–∞–∑–æ–∫ (sample)
# –ö–æ–∂–µ–Ω —Å—Ç–æ–≤–ø–µ—Ü—å = –æ–∑–Ω–∞–∫–∞ (feature)

data = np.array([
    [120, 3, 2010],  # –ë—É–¥–∏–Ω–æ–∫ 1
    [80,  2, 2015],  # –ë—É–¥–∏–Ω–æ–∫ 2
    [150, 4, 2005],  # –ë—É–¥–∏–Ω–æ–∫ 3
])

print(f"Shape: {data.shape}")  # (3, 3) = 3 samples √ó 3 features
```

**–ú–∞—Ç—Ä–∏—Ü—è = —Å—Ç–µ–∫ –≤–µ–∫—Ç–æ—Ä—ñ–≤:**
```python
# –í–µ–∫—Ç–æ—Ä–∏-—Ä—è–¥–∫–∏ (row vectors)
sample1 = data[0]  # [120, 3, 2010]

# –í–µ–∫—Ç–æ—Ä–∏-—Å—Ç–æ–≤–ø—Ü—ñ (column vectors)
feature1 = data[:, 0]  # [120, 80, 150] (–ø–ª–æ—â–∞)
```

---

## –¢–∏–ø–∏ –º–∞—Ç—Ä–∏—Ü—å

### 1. –ö–≤–∞–¥—Ä–∞—Ç–Ω–∞ –º–∞—Ç—Ä–∏—Ü—è

**n √ó n:**
```python
A = np.array([
    [1, 2],
    [3, 4]
])
# 2√ó2 –∫–≤–∞–¥—Ä–∞—Ç–Ω–∞
```

### 2. –î—ñ–∞–≥–æ–Ω–∞–ª—å–Ω–∞ –º–∞—Ç—Ä–∏—Ü—è

**–ù–µ–Ω—É–ª—å–æ–≤—ñ —Ç—ñ–ª—å–∫–∏ –Ω–∞ –¥—ñ–∞–≥–æ–Ω–∞–ª—ñ:**
$$D = \begin{bmatrix}
d_1 & 0 & 0 \\
0 & d_2 & 0 \\
0 & 0 & d_3
\end{bmatrix}$$

```python
D = np.diag([2, 3, 5])
print(D)
# [[2 0 0]
#  [0 3 0]
#  [0 0 5]]
```

### 3. –û–¥–∏–Ω–∏—á–Ω–∞ –º–∞—Ç—Ä–∏—Ü—è (Identity)

**–î—ñ–∞–≥–æ–Ω–∞–ª—å–Ω–∞ –∑ –æ–¥–∏–Ω–∏—Ü—è–º–∏:**
$$I = \begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{bmatrix}$$

**–í–ª–∞—Å—Ç–∏–≤—ñ—Å—Ç—å:** $A \cdot I = I \cdot A = A$

```python
I = np.eye(3)
print(I)
# [[1. 0. 0.]
#  [0. 1. 0.]
#  [0. 0. 1.]]

A = np.array([[1, 2], [3, 4]])
print(A @ np.eye(2))  # Same as A
```

### 4. –°–∏–º–µ—Ç—Ä–∏—á–Ω–∞ –º–∞—Ç—Ä–∏—Ü—è

**$A = A^T$:**
$$A = \begin{bmatrix}
1 & 2 & 3 \\
2 & 4 & 5 \\
3 & 5 & 6
\end{bmatrix}$$

```python
A = np.array([
    [1, 2, 3],
    [2, 4, 5],
    [3, 5, 6]
])

print(np.array_equal(A, A.T))  # True
```

**–ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è:**
- **–ö–æ–≤–∞—Ä—ñ–∞—Ü—ñ–π–Ω—ñ –º–∞—Ç—Ä–∏—Ü—ñ** –∑–∞–≤–∂–¥–∏ —Å–∏–º–µ—Ç—Ä–∏—á–Ω—ñ
- **Kernel matrices** –≤ ML

### 5. –û—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω–∞ –º–∞—Ç—Ä–∏—Ü—è

**$A^T A = A A^T = I$:**

–°—Ç–æ–≤–ø—Ü—ñ (—Ç–∞ —Ä—è–¥–∫–∏) ‚Äî –æ—Ä—Ç–æ–Ω–æ—Ä–º–æ–≤–∞–Ω—ñ –≤–µ–∫—Ç–æ—Ä–∏.

```python
# –ü—Ä–∏–∫–ª–∞–¥: –º–∞—Ç—Ä–∏—Ü—è –æ–±–µ—Ä—Ç–∞–Ω–Ω—è
theta = np.pi / 4  # 45 –≥—Ä–∞–¥—É—Å—ñ–≤
R = np.array([
    [np.cos(theta), -np.sin(theta)],
    [np.sin(theta),  np.cos(theta)]
])

# –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞
print(R @ R.T)  # ‚âà I
```

**–í–ª–∞—Å—Ç–∏–≤–æ—Å—Ç—ñ:**
- –ó–±–µ—Ä—ñ–≥–∞—é—Ç—å –¥–æ–≤–∂–∏–Ω—É –≤–µ–∫—Ç–æ—Ä—ñ–≤
- –ó–±–µ—Ä—ñ–≥–∞—é—Ç—å –∫—É—Ç–∏
- –û–±–µ—Ä—Ç–∞–Ω–Ω—è, –≤—ñ–¥–æ–±—Ä–∞–∂–µ–Ω–Ω—è

---

## –¢—Ä–∞–Ω—Å–ø–æ–Ω—É–≤–∞–Ω–Ω—è

**–©–æ —Ü–µ:** –ü–æ–º—ñ–Ω—è—Ç–∏ —Ä—è–¥–∫–∏ —Ç–∞ —Å—Ç–æ–≤–ø—Ü—ñ –º—ñ—Å—Ü—è–º–∏.

$$A^T_{ij} = A_{ji}$$

```python
A = np.array([
    [1, 2, 3],
    [4, 5, 6]
])

A_T = A.T
print(A_T)
# [[1 4]
#  [2 5]
#  [3 6]]
```

**–í–ª–∞—Å—Ç–∏–≤–æ—Å—Ç—ñ:**
- $(A^T)^T = A$
- $(A + B)^T = A^T + B^T$
- $(AB)^T = B^T A^T$

---

# –ú–∞—Ç—Ä–∏—á–Ω—ñ –æ–ø–µ—Ä–∞—Ü—ñ—ó

## –ú–Ω–æ–∂–µ–Ω–Ω—è –º–∞—Ç—Ä–∏—Ü—å

### –ú–∞—Ç–µ–º–∞—Ç–∏—á–Ω–æ

**$C = AB$:**

$$C_{ij} = \sum_{k=1}^{n} A_{ik} B_{kj}$$

**–†–æ–∑–º—ñ—Ä–∏:**
- $A: m \times n$
- $B: n \times p$
- $C: m \times p$

**–í–∞–∂–ª–∏–≤–æ:** –ö—ñ–ª—å–∫—ñ—Å—Ç—å —Å—Ç–æ–≤–ø—Ü—ñ–≤ $A$ = –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ä—è–¥–∫—ñ–≤ $B$!

### –ì–µ–æ–º–µ—Ç—Ä–∏—á–Ω–∞ —ñ–Ω—Ç—É—ó—Ü—ñ—è

**–ú–∞—Ç—Ä–∏—Ü—è = –ª—ñ–Ω—ñ–π–Ω–µ –ø–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—è:**

```python
# –ú–∞—Ç—Ä–∏—Ü—è —Ä–æ–∑—Ç—è–≥—É–≤–∞–Ω–Ω—è
S = np.array([
    [2, 0],
    [0, 3]
])

v = np.array([1, 1])
v_transformed = S @ v  # [2, 3]

# –†–æ–∑—Ç—è–≥–Ω—É–ª–∏ –ø–æ x –≤ 2 —Ä–∞–∑–∏, –ø–æ y –≤ 3 —Ä–∞–∑–∏
```

### –ö–æ–¥

```python
A = np.array([
    [1, 2],
    [3, 4]
])

B = np.array([
    [5, 6],
    [7, 8]
])

# –ú–Ω–æ–∂–µ–Ω–Ω—è –º–∞—Ç—Ä–∏—Ü—å
C = A @ B
# –∞–±–æ
C = np.dot(A, B)
# –∞–±–æ
C = np.matmul(A, B)

print(C)
# [[19 22]
#  [43 50]]
```

### –ú–∞—Ç—Ä–∏—Ü—è √ó –í–µ–∫—Ç–æ—Ä

**–î—É–∂–µ –≤–∞–∂–ª–∏–≤–æ –≤ ML!**

```python
A = np.array([
    [1, 2, 3],
    [4, 5, 6]
])

x = np.array([1, 2, 3])

# Matrix-vector product
y = A @ x

print(y)  # [14, 32]

# y[0] = 1*1 + 2*2 + 3*3 = 14
# y[1] = 4*1 + 5*2 + 6*3 = 32
```

**–ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è:**
```python
# –õ—ñ–Ω—ñ–π–Ω–∞ —Ä–µ–≥—Ä–µ—Å—ñ—è: y = Xw
X = np.array([
    [1, 2],  # sample 1
    [3, 4],  # sample 2
    [5, 6]   # sample 3
])

w = np.array([0.5, 0.3])  # weights

predictions = X @ w
print(predictions)  # [1.1, 2.7, 4.3]
```

### –í–ª–∞—Å—Ç–∏–≤–æ—Å—Ç—ñ

**–ù–ï –∫–æ–º—É—Ç–∞—Ç–∏–≤–Ω–µ:**
$$AB \neq BA$$

```python
A = np.array([[1, 2], [3, 4]])
B = np.array([[5, 6], [7, 8]])

print(A @ B)
# [[19 22]
#  [43 50]]

print(B @ A)
# [[23 34]
#  [31 46]]

# Different!
```

**–ê—Å–æ—Ü—ñ–∞—Ç–∏–≤–Ω–µ:**
$$(AB)C = A(BC)$$

**–î–∏—Å—Ç—Ä–∏–±—É—Ç–∏–≤–Ω–µ:**
$$A(B + C) = AB + AC$$

---

## –í–∏–∑–Ω–∞—á–Ω–∏–∫ (Determinant)

**–¢—ñ–ª—å–∫–∏ –¥–ª—è –∫–≤–∞–¥—Ä–∞—Ç–Ω–∏—Ö –º–∞—Ç—Ä–∏—Ü—å!**

### –©–æ —Ü–µ?

**–ì–µ–æ–º–µ—Ç—Ä–∏—á–Ω–æ:** –ú–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è –æ–±'—î–º—É –ø—Ä–∏ –ª—ñ–Ω—ñ–π–Ω–æ–º—É –ø–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—ñ.

**–î–ª—è 2√ó2:**
$$\det(A) = \begin{vmatrix} a & b \\ c & d \end{vmatrix} = ad - bc$$

**–î–ª—è 3√ó3:**
$$\det(A) = a_{11}(a_{22}a_{33} - a_{23}a_{32}) - a_{12}(a_{21}a_{33} - a_{23}a_{31}) + a_{13}(a_{21}a_{32} - a_{22}a_{31})$$

### –ö–æ–¥

```python
A = np.array([
    [1, 2],
    [3, 4]
])

det = np.linalg.det(A)
print(f"Determinant: {det}")  # -2.0
```

### –Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü—ñ—è

```python
# –Ø–∫—â–æ det(A) = 0  ‚Üí  –ú–∞—Ç—Ä–∏—Ü—è –≤–∏—Ä–æ–¥–∂–µ–Ω–Ω–∞ (singular)
#                      –ù–µ –º–∞—î –æ–±–µ—Ä–Ω–µ–Ω–æ—ó
#                      –í–µ–∫—Ç–æ—Ä–∏ –ª—ñ–Ω—ñ–π–Ω–æ –∑–∞–ª–µ–∂–Ω—ñ

A = np.array([
    [1, 2],
    [2, 4]  # –¥—Ä—É–≥–∏–π —Ä—è–¥–æ–∫ = 2 √ó –ø–µ—Ä—à–∏–π
])

print(np.linalg.det(A))  # 0.0

# –Ø–∫—â–æ det(A) ‚â† 0  ‚Üí  –ú–∞—Ç—Ä–∏—Ü—è –Ω–µ–≤–∏—Ä–æ–¥–∂–µ–Ω–Ω–∞
#                      –ú–∞—î –æ–±–µ—Ä–Ω–µ–Ω—É
#                      –í–µ–∫—Ç–æ—Ä–∏ –ª—ñ–Ω—ñ–π–Ω–æ –Ω–µ–∑–∞–ª–µ–∂–Ω—ñ
```

**–í–ª–∞—Å—Ç–∏–≤–æ—Å—Ç—ñ:**
- $\det(AB) = \det(A) \det(B)$
- $\det(A^T) = \det(A)$
- $\det(A^{-1}) = 1/\det(A)$

---

## –û–±–µ—Ä–Ω–µ–Ω–∞ –º–∞—Ç—Ä–∏—Ü—è

**–©–æ —Ü–µ:** –ú–∞—Ç—Ä–∏—Ü—è $A^{-1}$ —Ç–∞–∫–∞, —â–æ:

$$A A^{-1} = A^{-1} A = I$$

**–Ü—Å–Ω—É—î —Ç—ñ–ª—å–∫–∏ —è–∫—â–æ** $\det(A) \neq 0$!

### –î–ª—è 2√ó2

$$A^{-1} = \frac{1}{\det(A)} \begin{bmatrix} d & -b \\ -c & a \end{bmatrix}$$

–¥–µ $A = \begin{bmatrix} a & b \\ c & d \end{bmatrix}$

### –ö–æ–¥

```python
A = np.array([
    [1, 2],
    [3, 4]
])

# –û–±–µ—Ä–Ω–µ–Ω–∞ –º–∞—Ç—Ä–∏—Ü—è
A_inv = np.linalg.inv(A)

print("A^-1:")
print(A_inv)

# –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞
print("\nA @ A^-1:")
print(A @ A_inv)  # ‚âà I
```

### –†–æ–∑–≤'—è–∑–∞–Ω–Ω—è —Å–∏—Å—Ç–µ–º —Ä—ñ–≤–Ω—è–Ω—å

**$Ax = b$ ‚Üí $x = A^{-1}b$**

```python
A = np.array([
    [2, 1],
    [1, 3]
])

b = np.array([5, 6])

# –†–æ–∑–≤'—è–∑–æ–∫
x = np.linalg.inv(A) @ b
# –ê–ë–û –∫—Ä–∞—â–µ (–±—ñ–ª—å—à —Å—Ç–∞–±—ñ–ª—å–Ω–æ —á–∏—Å–µ–ª—å–Ω–æ):
x = np.linalg.solve(A, b)

print(f"Solution: {x}")  # [1.8, 1.4]

# –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞
print(f"A @ x = {A @ x}")  # ‚âà b
```

**–ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è:**
- –†–æ–∑–≤'—è–∑–∞–Ω–Ω—è –ª—ñ–Ω—ñ–π–Ω–∏—Ö —Å–∏—Å—Ç–µ–º
- –õ—ñ–Ω—ñ–π–Ω–∞ —Ä–µ–≥—Ä–µ—Å—ñ—è: $w = (X^T X)^{-1} X^T y$

---

## –†–∞–Ω–≥ –º–∞—Ç—Ä–∏—Ü—ñ

**–©–æ —Ü–µ:** –ö—ñ–ª—å–∫—ñ—Å—Ç—å –ª—ñ–Ω—ñ–π–Ω–æ –Ω–µ–∑–∞–ª–µ–∂–Ω–∏—Ö —Ä—è–¥–∫—ñ–≤ (–∞–±–æ —Å—Ç–æ–≤–ø—Ü—ñ–≤).

**–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∏–π —Ä–∞–Ω–≥ = min(m, n)** –¥–ª—è –º–∞—Ç—Ä–∏—Ü—ñ m√ón.

### –ö–æ–¥

```python
A = np.array([
    [1, 2, 3],
    [4, 5, 6],
    [7, 8, 9]  # –ª—ñ–Ω—ñ–π–Ω–æ –∑–∞–ª–µ–∂–Ω–∏–π –≤—ñ–¥ –ø–µ—Ä—à–∏—Ö –¥–≤–æ—Ö
])

rank = np.linalg.matrix_rank(A)
print(f"Rank: {rank}")  # 2 (–∞ –Ω–µ 3!)

# –ü–æ–≤–Ω–∏–π —Ä–∞–Ω–≥
B = np.array([
    [1, 2],
    [3, 4]
])

print(np.linalg.matrix_rank(B))  # 2 (–ø–æ–≤–Ω–∏–π —Ä–∞–Ω–≥)
```

**–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü—ñ—è:**
- **–†–∞–Ω–≥ < min(m,n)** ‚Üí –õ—ñ–Ω—ñ–π–Ω–∞ –∑–∞–ª–µ–∂–Ω—ñ—Å—Ç—å
- **–†–∞–Ω–≥ = min(m,n)** ‚Üí –ü–æ–≤–Ω–∏–π —Ä–∞–Ω–≥

**–ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è:**
- –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –ª—ñ–Ω—ñ–π–Ω–æ—ó –Ω–µ–∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ features
- Dimensionality reduction

---

# –°–∏—Å—Ç–µ–º–∏ –ª—ñ–Ω—ñ–π–Ω–∏—Ö —Ä—ñ–≤–Ω—è–Ω—å

## –ú–∞—Ç—Ä–∏—á–Ω–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—è

**–°–∏—Å—Ç–µ–º–∞:**
$$\begin{cases}
2x + 3y = 8 \\
4x + 5y = 14
\end{cases}$$

**–ú–∞—Ç—Ä–∏—á–Ω–∞ —Ñ–æ—Ä–º–∞: $Ax = b$**

$$\begin{bmatrix} 2 & 3 \\ 4 & 5 \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix} = \begin{bmatrix} 8 \\ 14 \end{bmatrix}$$

### –†–æ–∑–≤'—è–∑–∞–Ω–Ω—è

```python
A = np.array([
    [2, 3],
    [4, 5]
])

b = np.array([8, 14])

# –ú–µ—Ç–æ–¥ 1: –û–±–µ—Ä–Ω–µ–Ω–∞ –º–∞—Ç—Ä–∏—Ü—è (–Ω–µ —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–æ!)
x = np.linalg.inv(A) @ b

# –ú–µ—Ç–æ–¥ 2: np.linalg.solve (–∫—Ä–∞—â–µ!)
x = np.linalg.solve(A, b)

print(f"Solution: x = {x[0]}, y = {x[1]}")  # x=1, y=2

# –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞
print(f"Check: A @ x = {A @ x}")  # [8, 14]
```

## –ü–µ—Ä–µ–æ–±—É–º–æ–≤–ª–µ–Ω—ñ —Å–∏—Å—Ç–µ–º–∏ (Overdetermined)

**–ë—ñ–ª—å—à–µ —Ä—ñ–≤–Ω—è–Ω—å –Ω—ñ–∂ –Ω–µ–≤—ñ–¥–æ–º–∏—Ö** (m > n):

```python
# 3 —Ä—ñ–≤–Ω—è–Ω–Ω—è, 2 –Ω–µ–≤—ñ–¥–æ–º–∏—Ö
A = np.array([
    [1, 1],
    [1, 2],
    [1, 3]
])

b = np.array([2, 3, 5])

# –ù–µ–º–∞—î —Ç–æ—á–Ω–æ–≥–æ —Ä–æ–∑–≤'—è–∑–∫—É!
# –ó–Ω–∞–π—Ç–∏ –Ω–∞–π–∫—Ä–∞—â–µ –Ω–∞–±–ª–∏–∂–µ–Ω–Ω—è (least squares)

x = np.linalg.lstsq(A, b, rcond=None)[0]
print(f"Best fit: {x}")

# –ó–∞–ª–∏—à–æ–∫ (residual)
residual = A @ x - b
print(f"Residual: {residual}")
```

**–¶–µ –æ—Å–Ω–æ–≤–∞ –ª—ñ–Ω—ñ–π–Ω–æ—ó —Ä–µ–≥—Ä–µ—Å—ñ—ó!**

---

# –í–µ–∫—Ç–æ—Ä–Ω—ñ –ø—Ä–æ—Å—Ç–æ—Ä–∏

## –ü—ñ–¥–ø—Ä–æ—Å—Ç—ñ—Ä

**–ü—ñ–¥–ø—Ä–æ—Å—Ç—ñ—Ä** –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç–æ—Ä—É ‚Äî —Ü–µ –ø—ñ–¥–º–Ω–æ–∂–∏–Ω–∞, —è–∫–∞ —Ç–∞–∫–æ–∂ —î –≤–µ–∫—Ç–æ—Ä–Ω–∏–º –ø—Ä–æ—Å—Ç–æ—Ä–æ–º.

**–ü—Ä–∏–∫–ª–∞–¥ –≤ R¬≥:**
- –õ—ñ–Ω—ñ—è —á–µ—Ä–µ–∑ –ø–æ—á–∞—Ç–æ–∫ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç ‚Üí 1D –ø—ñ–¥–ø—Ä–æ—Å—Ç—ñ—Ä
- –ü–ª–æ—â–∏–Ω–∞ —á–µ—Ä–µ–∑ –ø–æ—á–∞—Ç–æ–∫ ‚Üí 2D –ø—ñ–¥–ø—Ä–æ—Å—Ç—ñ—Ä

### Column Space (–ü—Ä–æ—Å—Ç—ñ—Ä —Å—Ç–æ–≤–ø—Ü—ñ–≤)

**–©–æ —Ü–µ:** –í—Å—ñ –º–æ–∂–ª–∏–≤—ñ –ª—ñ–Ω—ñ–π–Ω—ñ –∫–æ–º–±—ñ–Ω–∞—Ü—ñ—ó —Å—Ç–æ–≤–ø—Ü—ñ–≤ –º–∞—Ç—Ä–∏—Ü—ñ.

$$\text{Col}(A) = \{\mathbf{y} : \mathbf{y} = A\mathbf{x} \text{ for some } \mathbf{x}\}$$

**–†–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å = —Ä–∞–Ω–≥ –º–∞—Ç—Ä–∏—Ü—ñ.**

```python
A = np.array([
    [1, 2],
    [3, 4],
    [5, 6]
])

# –ü—Ä–æ—Å—Ç—ñ—Ä —Å—Ç–æ–≤–ø—Ü—ñ–≤ –º–∞—î —Ä–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å 2
# (–≤—Å—ñ –ª—ñ–Ω—ñ–π–Ω—ñ –∫–æ–º–±—ñ–Ω–∞—Ü—ñ—ó 2 —Å—Ç–æ–≤–ø—Ü—ñ–≤ –≤ R¬≥)

rank = np.linalg.matrix_rank(A)
print(f"Dimension of Col(A): {rank}")  # 2
```

### Null Space (–ù—É–ª—å-–ø—Ä–æ—Å—Ç—ñ—Ä)

**–©–æ —Ü–µ:** –í—Å—ñ –≤–µ–∫—Ç–æ—Ä–∏ $\mathbf{x}$ —Ç–∞–∫—ñ, —â–æ $A\mathbf{x} = \mathbf{0}$.

```python
from scipy.linalg import null_space

A = np.array([
    [1, 2, 3],
    [4, 5, 6],
    [7, 8, 9]
])

# –ó–Ω–∞–π—Ç–∏ null space
null = null_space(A)
print("Null space basis:")
print(null)

# –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞
print("\nA @ null ‚âà 0:")
print(A @ null)
```

---

# –í–ª–∞—Å–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è —Ç–∞ –≤–ª–∞—Å–Ω—ñ –≤–µ–∫—Ç–æ—Ä–∏

## –©–æ —Ü–µ?

**–í–ª–∞—Å–Ω–∏–π –≤–µ–∫—Ç–æ—Ä** $\mathbf{v}$ –º–∞—Ç—Ä–∏—Ü—ñ $A$ ‚Äî —Ü–µ –≤–µ–∫—Ç–æ—Ä, —â–æ —Ç—ñ–ª—å–∫–∏ –º–∞—Å—à—Ç–∞–±—É—î—Ç—å—Å—è (–Ω–µ –æ–±–µ—Ä—Ç–∞—î—Ç—å—Å—è) –ø—Ä–∏ –º–Ω–æ–∂–µ–Ω–Ω—ñ –Ω–∞ $A$:

$$A\mathbf{v} = \lambda \mathbf{v}$$

–¥–µ $\lambda$ ‚Äî **–≤–ª–∞—Å–Ω–µ –∑–Ω–∞—á–µ–Ω–Ω—è** (eigenvalue).

### –ì–µ–æ–º–µ—Ç—Ä–∏—á–Ω–∞ —ñ–Ω—Ç—É—ó—Ü—ñ—è

```
–ó–≤–∏—á–∞–π–Ω–∏–π –≤–µ–∫—Ç–æ—Ä:
    A @ u
     ‚Üó
    u ‚Üí A –æ–±–µ—Ä—Ç–∞—î —Ç–∞ –º–∞—Å—à—Ç–∞–±—É—î

–í–ª–∞—Å–Ω–∏–π –≤–µ–∫—Ç–æ—Ä:
    A @ v = Œªv
    ‚Üë
    v ‚Üí A —Ç—ñ–ª—å–∫–∏ –º–∞—Å—à—Ç–∞–±—É—î (–≤ —Ç–æ–º—É –∂ –Ω–∞–ø—Ä—è–º–∫—É)
```

### –ü—Ä–∏–∫–ª–∞–¥

```python
A = np.array([
    [2, 0],
    [0, 3]
])

# –í–ª–∞—Å–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è —Ç–∞ –≤–µ–∫—Ç–æ—Ä–∏
eigenvalues, eigenvectors = np.linalg.eig(A)

print("Eigenvalues:", eigenvalues)     # [2, 3]
print("Eigenvectors:\n", eigenvectors)

# –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –¥–ª—è –ø–µ—Ä—à–æ–≥–æ –≤–ª–∞—Å–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞
v = eigenvectors[:, 0]
lambda_val = eigenvalues[0]

print(f"\nA @ v = {A @ v}")
print(f"Œª * v = {lambda_val * v}")
# –û–¥–Ω–∞–∫–æ–≤—ñ!
```

### –î—ñ–∞–≥–æ–Ω–∞–ª—å–Ω–∞ –º–∞—Ç—Ä–∏—Ü—è

**–î–ª—è –¥—ñ–∞–≥–æ–Ω–∞–ª—å–Ω–æ—ó –º–∞—Ç—Ä–∏—Ü—ñ:**
- –í–ª–∞—Å–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è = –¥—ñ–∞–≥–æ–Ω–∞–ª—å–Ω—ñ –µ–ª–µ–º–µ–Ω—Ç–∏
- –í–ª–∞—Å–Ω—ñ –≤–µ–∫—Ç–æ—Ä–∏ = —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∏–π –±–∞–∑–∏—Å

```python
D = np.array([
    [5, 0, 0],
    [0, 3, 0],
    [0, 0, 7]
])

eigenvalues, eigenvectors = np.linalg.eig(D)

print("Eigenvalues:", eigenvalues)  # [5, 3, 7]
print("Eigenvectors:\n", eigenvectors)
# [[1, 0, 0],
#  [0, 1, 0],
#  [0, 0, 1]]
```

---

## –í–ª–∞—Å–Ω–µ —Ä–æ–∑–∫–ª–∞–¥–∞–Ω–Ω—è (Eigendecomposition)

**–î–ª—è —Å–∏–º–µ—Ç—Ä–∏—á–Ω–æ—ó –º–∞—Ç—Ä–∏—Ü—ñ:**

$$A = Q \Lambda Q^T$$

–¥–µ:
- $Q$ ‚Äî –º–∞—Ç—Ä–∏—Ü—è –≤–ª–∞—Å–Ω–∏—Ö –≤–µ–∫—Ç–æ—Ä—ñ–≤ (–æ—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω–∏—Ö)
- $\Lambda$ ‚Äî –¥—ñ–∞–≥–æ–Ω–∞–ª—å–Ω–∞ –º–∞—Ç—Ä–∏—Ü—è –≤–ª–∞—Å–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω—å

```python
# –°–∏–º–µ—Ç—Ä–∏—á–Ω–∞ –º–∞—Ç—Ä–∏—Ü—è
A = np.array([
    [4, 2],
    [2, 3]
])

# –í–ª–∞—Å–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è —Ç–∞ –≤–µ–∫—Ç–æ—Ä–∏
eigenvalues, eigenvectors = np.linalg.eig(A)

# –°—Ç–≤–æ—Ä–∏—Ç–∏ Œõ (Lambda)
Lambda = np.diag(eigenvalues)

# –†–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—è: A = Q Œõ Q^T
Q = eigenvectors
A_reconstructed = Q @ Lambda @ Q.T

print("Original A:")
print(A)
print("\nReconstructed A:")
print(A_reconstructed)
# –û–¥–Ω–∞–∫–æ–≤—ñ!
```

### –ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è

**1. PCA (Principal Component Analysis):**
```python
from sklearn.decomposition import PCA

# –î–∞–Ω—ñ
X = np.random.randn(100, 5)

# PCA –∑–Ω–∞—Ö–æ–¥–∏—Ç—å –≤–ª–∞—Å–Ω—ñ –≤–µ–∫—Ç–æ—Ä–∏ –∫–æ–≤–∞—Ä—ñ–∞—Ü—ñ–π–Ω–æ—ó –º–∞—Ç—Ä–∏—Ü—ñ
pca = PCA(n_components=2)
X_transformed = pca.fit_transform(X)

# –í–ª–∞—Å–Ω—ñ –≤–µ–∫—Ç–æ—Ä–∏ = principal components
print("Principal components (eigenvectors):")
print(pca.components_)

# –í–ª–∞—Å–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è = explained variance
print("\nExplained variance (eigenvalues):")
print(pca.explained_variance_)
```

**2. –®–≤–∏–¥–∫–µ –æ–±—á–∏—Å–ª–µ–Ω–Ω—è —Å—Ç–µ–ø–µ–Ω—ñ–≤ –º–∞—Ç—Ä–∏—Ü—ñ:**

$$A^n = Q \Lambda^n Q^T$$

```python
# A^100 —á–µ—Ä–µ–∑ –≤–ª–∞—Å–Ω–µ —Ä–æ–∑–∫–ª–∞–¥–∞–Ω–Ω—è
eigenvalues, eigenvectors = np.linalg.eig(A)
Lambda = np.diag(eigenvalues)
Q = eigenvectors

# A^100
Lambda_100 = np.diag(eigenvalues ** 100)
A_100 = Q @ Lambda_100 @ Q.T

# –ü–æ—Ä—ñ–≤–Ω—è—Ç–∏ –∑ –ø—Ä—è–º–∏–º –æ–±—á–∏—Å–ª–µ–Ω–Ω—è–º (–ø–æ–≤—ñ–ª—å–Ω–æ!)
# A_100_direct = np.linalg.matrix_power(A, 100)
```

---

# –°–∏–Ω–≥—É–ª—è—Ä–Ω–µ —Ä–æ–∑–∫–ª–∞–¥–∞–Ω–Ω—è (SVD)

## –©–æ —Ü–µ?

**SVD (Singular Value Decomposition)** ‚Äî —Ä–æ–∑–∫–ª–∞–¥–∞–Ω–Ω—è –±—É–¥—å-—è–∫–æ—ó –º–∞—Ç—Ä–∏—Ü—ñ:

$$A = U \Sigma V^T$$

–¥–µ:
- $A$: $m \times n$ (–≤–∏—Ö—ñ–¥–Ω–∞ –º–∞—Ç—Ä–∏—Ü—è)
- $U$: $m \times m$ (–æ—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω–∞, left singular vectors)
- $\Sigma$: $m \times n$ (–¥—ñ–∞–≥–æ–Ω–∞–ª—å–Ω–∞, singular values)
- $V^T$: $n \times n$ (–æ—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω–∞, right singular vectors)

### –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è

```
A (m√ón)  =  U (m√óm)  √ó  Œ£ (m√ón)  √ó  V^T (n√ón)

[data]  =  [left]   √ó  [scale]  √ó  [right]
```

### –ö–æ–¥

```python
A = np.array([
    [1, 2, 3],
    [4, 5, 6],
    [7, 8, 9]
])

# SVD
U, S, VT = np.linalg.svd(A, full_matrices=True)

print(f"U shape: {U.shape}")    # (3, 3)
print(f"S shape: {S.shape}")    # (3,) - —Ç—ñ–ª—å–∫–∏ –¥—ñ–∞–≥–æ–Ω–∞–ª—å
print(f"VT shape: {VT.shape}")  # (3, 3)

# Singular values
print(f"\nSingular values: {S}")

# –†–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—è
Sigma = np.zeros((3, 3))
Sigma[:3, :3] = np.diag(S)

A_reconstructed = U @ Sigma @ VT

print("\nOriginal A:")
print(A)
print("\nReconstructed A:")
print(A_reconstructed)
```

---

## –ó–≤'—è–∑–æ–∫ –∑ –≤–ª–∞—Å–Ω–∏–º —Ä–æ–∑–∫–ª–∞–¥–∞–Ω–Ω—è–º

**–î–ª—è —Å–∏–º–µ—Ç—Ä–∏—á–Ω–æ—ó –º–∞—Ç—Ä–∏—Ü—ñ:**
- SVD —Ç–∞ eigendecomposition –¥–∞—é—Ç—å —Ç–µ —Å–∞–º–µ
- Singular values = |eigenvalues|

**–ó–∞–≥–∞–ª–æ–º:**
- $A^T A$ –º–∞—î –≤–ª–∞—Å–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è = $\sigma_i^2$ (–∫–≤–∞–¥—Ä–∞—Ç–∏ singular values)
- $V$ = –≤–ª–∞—Å–Ω—ñ –≤–µ–∫—Ç–æ—Ä–∏ $A^T A$
- $U$ = –≤–ª–∞—Å–Ω—ñ –≤–µ–∫—Ç–æ—Ä–∏ $A A^T$

```python
A = np.array([
    [1, 2],
    [3, 4],
    [5, 6]
])

# SVD
U, S, VT = np.linalg.svd(A, full_matrices=False)

# –í–ª–∞—Å–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è A^T A
eigenvalues_ATA = np.linalg.eigvalsh(A.T @ A)

print("Singular values:", S)
print("sqrt(eigenvalues of A^T A):", np.sqrt(eigenvalues_ATA[::-1]))
# –û–¥–Ω–∞–∫–æ–≤—ñ!
```

---

## –ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è SVD

### 1. Dimensionality Reduction

**Truncated SVD** ‚Äî –∑–±–µ—Ä–µ–≥—Ç–∏ —Ç—ñ–ª—å–∫–∏ top k singular values:

```python
from sklearn.decomposition import TruncatedSVD

# –î–∞–Ω—ñ: 100 samples, 50 features
X = np.random.randn(100, 50)

# –ó–º–µ–Ω—à–∏—Ç–∏ –¥–æ 10 –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ñ–≤
svd = TruncatedSVD(n_components=10)
X_reduced = svd.fit_transform(X)

print(f"Original shape: {X.shape}")      # (100, 50)
print(f"Reduced shape: {X_reduced.shape}")  # (100, 10)

# –ü–æ—è—Å–Ω–µ–Ω–∞ variance
print(f"Explained variance: {svd.explained_variance_ratio_.sum():.2%}")
```

### 2. Image Compression

```python
from PIL import Image

# –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è (grayscale)
img = Image.open('image.jpg').convert('L')
img_array = np.array(img, dtype=float)

print(f"Original shape: {img_array.shape}")

# SVD
U, S, VT = np.linalg.svd(img_array, full_matrices=False)

# –°—Ç–∏—Å–∫–∞–Ω–Ω—è: –∑–±–µ—Ä–µ–≥—Ç–∏ —Ç—ñ–ª—å–∫–∏ top k singular values
k = 50

U_k = U[:, :k]
S_k = S[:k]
VT_k = VT[:k, :]

# –†–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—è
img_compressed = U_k @ np.diag(S_k) @ VT_k

# Compression ratio
original_size = img_array.size
compressed_size = U_k.size + S_k.size + VT_k.size
ratio = original_size / compressed_size

print(f"Compression ratio: {ratio:.2f}x")

# –ó–±–µ—Ä–µ–≥—Ç–∏
img_compressed_uint8 = np.clip(img_compressed, 0, 255).astype(np.uint8)
Image.fromarray(img_compressed_uint8).save('compressed.jpg')
```

### 3. Pseudo-inverse (Moore-Penrose)

**–î–ª—è –ø–µ—Ä–µ–æ–±—É–º–æ–≤–ª–µ–Ω–∏—Ö —Å–∏—Å—Ç–µ–º:**

$$A^+ = V \Sigma^+ U^T$$

–¥–µ $\Sigma^+$ ‚Äî pseudo-inverse –¥—ñ–∞–≥–æ–Ω–∞–ª—å–Ω–æ—ó –º–∞—Ç—Ä–∏—Ü—ñ.

```python
A = np.array([
    [1, 2],
    [3, 4],
    [5, 6]
])

# Pseudo-inverse —á–µ—Ä–µ–∑ SVD
U, S, VT = np.linalg.svd(A, full_matrices=False)

# Œ£^+ (–æ–±–µ—Ä–Ω–µ–Ω—ñ –Ω–µ–Ω—É–ª—å–æ–≤—ñ singular values)
S_inv = 1 / S
Sigma_plus = np.diag(S_inv)

A_pinv = VT.T @ Sigma_plus @ U.T

# –ê–±–æ –ø—Ä–æ—Å—Ç–æ:
A_pinv = np.linalg.pinv(A)

print("Pseudo-inverse:")
print(A_pinv)

# –ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è: least squares
b = np.array([1, 2, 3])
x = A_pinv @ b

print(f"\nSolution: {x}")
print(f"A @ x = {A @ x}")  # ‚âà b (best approximation)
```

### 4. Recommender Systems

```python
# User-item matrix (ratings)
R = np.array([
    [5, 3, 0, 1],  # User 1
    [4, 0, 0, 1],  # User 2
    [1, 1, 0, 5],  # User 3
    [1, 0, 0, 4],  # User 4
])

# SVD (replace 0 with mean first in practice)
U, S, VT = np.linalg.svd(R, full_matrices=False)

# –ó–º–µ–Ω—à–∏—Ç–∏ —Ä–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å
k = 2
U_k = U[:, :k]
S_k = S[:k]
VT_k = VT[:k, :]

# –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –≤—Å—ñ—Ö —Ä–µ–π—Ç–∏–Ω–≥—ñ–≤
R_pred = U_k @ np.diag(S_k) @ VT_k

print("Predicted ratings:")
print(R_pred)
# –ú–æ–∂–Ω–∞ –∑–∞–ø–æ–≤–Ω–∏—Ç–∏ –≤—ñ–¥—Å—É—Ç–Ω—ñ —Ä–µ–π—Ç–∏–Ω–≥–∏!
```

---

# –ù–æ—Ä–º–∏ —Ç–∞ –≤—ñ–¥—Å—Ç–∞–Ω—ñ

## –í–µ–∫—Ç–æ—Ä–Ω—ñ –Ω–æ—Ä–º–∏

### L1 norm (Manhattan)

$$\|\mathbf{x}\|_1 = |x_1| + |x_2| + \cdots + |x_n| = \sum_{i=1}^{n} |x_i|$$

```python
x = np.array([3, -4, 5])

l1_norm = np.sum(np.abs(x))
# –∞–±–æ
l1_norm = np.linalg.norm(x, ord=1)

print(f"L1 norm: {l1_norm}")  # 12
```

**–ì–µ–æ–º–µ—Ç—Ä–∏—á–Ω–æ:** –í—ñ–¥—Å—Ç–∞–Ω—å "–º—ñ—Å—å–∫–∏–º–∏ –∫–≤–∞—Ä—Ç–∞–ª–∞–º–∏".

**–ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è:**
- Lasso regression (L1 regularization)
- Sparse models

### L2 norm (Euclidean)

$$\|\mathbf{x}\|_2 = \sqrt{x_1^2 + x_2^2 + \cdots + x_n^2} = \sqrt{\sum_{i=1}^{n} x_i^2}$$

```python
x = np.array([3, 4])

l2_norm = np.sqrt(np.sum(x ** 2))
# –∞–±–æ
l2_norm = np.linalg.norm(x)  # default ord=2

print(f"L2 norm: {l2_norm}")  # 5.0
```

**–ì–µ–æ–º–µ—Ç—Ä–∏—á–Ω–æ:** –ü—Ä—è–º–∞ –≤—ñ–¥—Å—Ç–∞–Ω—å.

**–ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è:**
- Ridge regression (L2 regularization)
- Euclidean distance

### L‚àû norm (Maximum)

$$\|\mathbf{x}\|_\infty = \max(|x_1|, |x_2|, \ldots, |x_n|)$$

```python
x = np.array([3, -7, 2])

linf_norm = np.max(np.abs(x))
# –∞–±–æ
linf_norm = np.linalg.norm(x, ord=np.inf)

print(f"L‚àû norm: {linf_norm}")  # 7
```

### –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –Ω–æ—Ä–º

```python
x = np.array([3, 4])

print(f"L1 norm: {np.linalg.norm(x, 1)}")    # 7
print(f"L2 norm: {np.linalg.norm(x, 2)}")    # 5
print(f"L‚àû norm: {np.linalg.norm(x, np.inf)}")  # 4

# L1 ‚â• L2 ‚â• L‚àû (–¥–ª—è –æ–¥–∏–Ω–∏—á–Ω–∏—Ö –≤–µ–∫—Ç–æ—Ä—ñ–≤)
```

---

## –ú–∞—Ç—Ä–∏—á–Ω—ñ –Ω–æ—Ä–º–∏

### Frobenius norm

$$\|A\|_F = \sqrt{\sum_{i=1}^{m} \sum_{j=1}^{n} a_{ij}^2}$$

**–ê–Ω–∞–ª–æ–≥ L2 norm –¥–ª—è –º–∞—Ç—Ä–∏—Ü—å.**

```python
A = np.array([
    [1, 2],
    [3, 4]
])

frob_norm = np.linalg.norm(A, 'fro')
# –∞–±–æ
frob_norm = np.sqrt(np.sum(A ** 2))

print(f"Frobenius norm: {frob_norm}")  # sqrt(30) ‚âà 5.48
```

### Spectral norm (2-norm)

**–ù–∞–π–±—ñ–ª—å—à–µ singular value:**

$$\|A\|_2 = \sigma_{\max}(A)$$

```python
A = np.array([
    [1, 2],
    [3, 4]
])

spectral_norm = np.linalg.norm(A, 2)

# –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —á–µ—Ä–µ–∑ SVD
_, S, _ = np.linalg.svd(A)
print(f"Spectral norm: {spectral_norm}")  # max singular value
print(f"Max singular value: {S[0]}")
```

---

## –í—ñ–¥—Å—Ç–∞–Ω—ñ –º—ñ–∂ –≤–µ–∫—Ç–æ—Ä–∞–º–∏

### Euclidean distance

$$d(\mathbf{x}, \mathbf{y}) = \|\mathbf{x} - \mathbf{y}\|_2 = \sqrt{\sum_{i=1}^{n} (x_i - y_i)^2}$$

```python
from scipy.spatial.distance import euclidean

x = np.array([1, 2, 3])
y = np.array([4, 5, 6])

dist = euclidean(x, y)
# –∞–±–æ
dist = np.linalg.norm(x - y)

print(f"Euclidean distance: {dist}")  # sqrt(27) ‚âà 5.196
```

### Manhattan distance

$$d(\mathbf{x}, \mathbf{y}) = \|\mathbf{x} - \mathbf{y}\|_1 = \sum_{i=1}^{n} |x_i - y_i|$$

```python
from scipy.spatial.distance import cityblock

dist = cityblock(x, y)
# –∞–±–æ
dist = np.sum(np.abs(x - y))

print(f"Manhattan distance: {dist}")  # 9
```

### Cosine distance

$$d(\mathbf{x}, \mathbf{y}) = 1 - \frac{\mathbf{x} \cdot \mathbf{y}}{\|\mathbf{x}\| \|\mathbf{y}\|}$$

```python
from scipy.spatial.distance import cosine

x = np.array([1, 2, 3])
y = np.array([4, 5, 6])

dist = cosine(x, y)

print(f"Cosine distance: {dist}")  # 1 - cosine_similarity
```

---

# –ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è –≤ Machine Learning

## 1. –õ—ñ–Ω—ñ–π–Ω–∞ —Ä–µ–≥—Ä–µ—Å—ñ—è

**–ú–æ–¥–µ–ª—å:** $y = \mathbf{w}^T \mathbf{x} + b$

**–ú–∞—Ç—Ä–∏—á–Ω–∞ —Ñ–æ—Ä–º–∞:** $\mathbf{y} = X\mathbf{w}$

**–†–æ–∑–≤'—è–∑–æ–∫ (Normal Equation):**
$$\mathbf{w} = (X^T X)^{-1} X^T \mathbf{y}$$

```python
from sklearn.linear_model import LinearRegression

# –î–∞–Ω—ñ
X = np.array([
    [1, 1],
    [1, 2],
    [2, 2],
    [2, 3]
])

y = np.array([1, 2, 2, 3])

# –†—É—á–Ω–∏–π —Ä–æ–∑—Ä–∞—Ö—É–Ω–æ–∫
XTX_inv = np.linalg.inv(X.T @ X)
w = XTX_inv @ X.T @ y

print(f"Weights (manual): {w}")

# –ß–µ—Ä–µ–∑ sklearn
model = LinearRegression(fit_intercept=False)
model.fit(X, y)

print(f"Weights (sklearn): {model.coef_}")
```

**–ß–æ–º—É –ø—Ä–∞—Ü—é—î:** –ú—ñ–Ω—ñ–º—ñ–∑—É—î $\|X\mathbf{w} - \mathbf{y}\|^2$ (L2 loss).

---

## 2. PCA (Principal Component Analysis)

**–©–æ —Ä–æ–±–∏—Ç—å:**
1. –û–±—á–∏—Å–ª—é—î –∫–æ–≤–∞—Ä—ñ–∞—Ü—ñ–π–Ω—É –º–∞—Ç—Ä–∏—Ü—é: $C = \frac{1}{n} X^T X$
2. –ó–Ω–∞—Ö–æ–¥–∏—Ç—å –≤–ª–∞—Å–Ω—ñ –≤–µ–∫—Ç–æ—Ä–∏ $C$
3. –ü—Ä–æ–µ–∫—Ç—É—î –¥–∞–Ω—ñ –Ω–∞ top k –≤–ª–∞—Å–Ω–∏—Ö –≤–µ–∫—Ç–æ—Ä—ñ–≤

```python
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –¥–∞–Ω–∏—Ö
np.random.seed(42)
X = np.random.randn(100, 5)

# Scaling (–≤–∞–∂–ª–∏–≤–æ!)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

print(f"Original shape: {X.shape}")      # (100, 5)
print(f"PCA shape: {X_pca.shape}")       # (100, 2)

# –í–ª–∞—Å–Ω—ñ –≤–µ–∫—Ç–æ—Ä–∏ (principal components)
print("\nPrincipal Components:")
print(pca.components_)

# –í–ª–∞—Å–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è (explained variance)
print("\nExplained variance:")
print(pca.explained_variance_)
print(f"Total: {pca.explained_variance_ratio_.sum():.2%}")
```

**–†—É—á–Ω–∏–π —Ä–æ–∑—Ä–∞—Ö—É–Ω–æ–∫:**
```python
# 1. –¶–µ–Ω—Ç—Ä—É–≤–∞–Ω–Ω—è
X_centered = X_scaled - X_scaled.mean(axis=0)

# 2. –ö–æ–≤–∞—Ä—ñ–∞—Ü—ñ–π–Ω–∞ –º–∞—Ç—Ä–∏—Ü—è
Cov = (X_centered.T @ X_centered) / (len(X) - 1)

# 3. –í–ª–∞—Å–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è —Ç–∞ –≤–µ–∫—Ç–æ—Ä–∏
eigenvalues, eigenvectors = np.linalg.eigh(Cov)

# –í—ñ–¥—Å–æ—Ä—Ç—É–≤–∞—Ç–∏ –∑–∞ —Å–ø–∞–¥–∞–Ω–Ω—è–º
idx = eigenvalues.argsort()[::-1]
eigenvalues = eigenvalues[idx]
eigenvectors = eigenvectors[:, idx]

# 4. –ü—Ä–æ–µ–∫—Ü—ñ—è
X_pca_manual = X_centered @ eigenvectors[:, :2]

print("\nManual PCA:")
print(X_pca_manual[:5])
print("\nsklearn PCA:")
print(X_pca[:5])
# –û–¥–Ω–∞–∫–æ–≤—ñ (–∑ —Ç–æ—á–Ω—ñ—Å—Ç—é –¥–æ –∑–Ω–∞–∫—É)!
```

---

## 3. Cosine Similarity (NLP)

**–ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤:**

```python
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# –î–æ–∫—É–º–µ–Ω—Ç–∏
documents = [
    "machine learning is great",
    "deep learning is amazing",
    "cats and dogs are animals"
]

# TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü—ñ—è
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(documents)

# Cosine similarity
similarity_matrix = cosine_similarity(X)

print("Similarity matrix:")
print(similarity_matrix)
# [[1.   0.49 0.  ]   Doc 0 vs Doc 0, 1, 2
#  [0.49 1.   0.  ]   Doc 1 vs Doc 0, 1, 2
#  [0.   0.   1.  ]]  Doc 2 vs Doc 0, 1, 2

# –î–æ–∫—É–º–µ–Ω—Ç–∏ 0 —Ç–∞ 1 —Å—Ö–æ–∂—ñ (0.49)
# –î–æ–∫—É–º–µ–Ω—Ç 2 –Ω–µ —Å—Ö–æ–∂–∏–π –Ω–∞ —ñ–Ω—à—ñ (0.0)
```

---

## 4. Regularization

### L1 (Lasso)

**–ú—ñ–Ω—ñ–º—ñ–∑—É—î:** $\|X\mathbf{w} - \mathbf{y}\|^2 + \alpha \|\mathbf{w}\|_1$

```python
from sklearn.linear_model import Lasso

model = Lasso(alpha=0.1)
model.fit(X_train, y_train)

# L1 —Å—Ç–≤–æ—Ä—é—î sparse –≤–µ–∫—Ç–æ—Ä w (–±–∞–≥–∞—Ç–æ –Ω—É–ª—ñ–≤)
print(f"Non-zero weights: {np.sum(model.coef_ != 0)}")
```

### L2 (Ridge)

**–ú—ñ–Ω—ñ–º—ñ–∑—É—î:** $\|X\mathbf{w} - \mathbf{y}\|^2 + \alpha \|\mathbf{w}\|_2^2$

```python
from sklearn.linear_model import Ridge

model = Ridge(alpha=1.0)
model.fit(X_train, y_train)

# L2 —Ä–æ–±–∏—Ç—å weights –º–∞–ª–µ–Ω—å–∫–∏–º–∏, –∞–ª–µ –Ω–µ –Ω—É–ª—å–æ–≤–∏–º–∏
```

---

## 5. Distance-based methods

### K-Nearest Neighbors

```python
from sklearn.neighbors import KNeighborsClassifier

# KNN –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î Euclidean distance (–∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º)
knn = KNeighborsClassifier(n_neighbors=5, metric='euclidean')
knn.fit(X_train, y_train)

# –î–ª—è –Ω–æ–≤–æ—ó —Ç–æ—á–∫–∏ –∑–Ω–∞—Ö–æ–¥–∏—Ç—å 5 –Ω–∞–π–±–ª–∏–∂—á–∏—Ö (–∑–∞ L2 norm)
y_pred = knn.predict(X_test)
```

### K-Means Clustering

```python
from sklearn.cluster import KMeans

# K-Means –º—ñ–Ω—ñ–º—ñ–∑—É—î –≤—ñ–¥—Å—Ç–∞–Ω—ñ –¥–æ —Ü–µ–Ω—Ç—Ä–æ—ó–¥—ñ–≤
kmeans = KMeans(n_clusters=3)
kmeans.fit(X)

# –¶–µ–Ω—Ç—Ä–æ—ó–¥–∏
print("Centroids:")
print(kmeans.cluster_centers_)

# –í—ñ–¥—Å—Ç–∞–Ω—å = Euclidean
```

---

## 6. Neural Networks

**Forward pass:**
$$\mathbf{h} = \sigma(W \mathbf{x} + \mathbf{b})$$

**–ú–∞—Ç—Ä–∏—á–Ω–µ –º–Ω–æ–∂–µ–Ω–Ω—è –≤—Å—é–¥–∏!**

```python
import torch
import torch.nn as nn

# –ü—Ä–æ—Å—Ç–∏–π feedforward layer
layer = nn.Linear(in_features=10, out_features=5)

# –í–Ω—É—Ç—Ä—ñ—à–Ω—å–æ: y = W @ x + b
x = torch.randn(32, 10)  # batch size 32
y = layer(x)             # (32, 5)

print(f"Weight matrix shape: {layer.weight.shape}")  # (5, 10)
print(f"Output shape: {y.shape}")  # (32, 5)
```

---

## –ü—Ä–∞–∫—Ç–∏—á–Ω—ñ –ø–æ—Ä–∞–¥–∏ üí°

### 1. –ó–∞–≤–∂–¥–∏ –ø–µ—Ä–µ–≤—ñ—Ä—è–π —Ä–æ–∑–º—ñ—Ä–∏ (shape)

```python
# ‚ùå –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ
A = np.array([[1, 2, 3]])  # (1, 3)
B = np.array([[4, 5, 6]])  # (1, 3)
# C = A @ B  # ValueError! (1,3) @ (1,3) –Ω–µ –ø—Ä–∞—Ü—é—î

# ‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω–æ
C = A @ B.T  # (1, 3) @ (3, 1) = (1, 1)
```

### 2. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π broadcasting —Ä–æ–∑—É–º–Ω–æ

```python
# Broadcasting –¥–æ–∑–≤–æ–ª—è—î –æ–ø–µ—Ä–∞—Ü—ñ—ó –∑ —Ä—ñ–∑–Ω–∏–º–∏ shapes
A = np.array([[1, 2, 3],
              [4, 5, 6]])  # (2, 3)

b = np.array([10, 20, 30])  # (3,)

# –î–æ–¥–∞–≤–∞–Ω–Ω—è: broadcast b –¥–æ –∫–æ–∂–Ω–æ–≥–æ —Ä—è–¥–∫–∞ A
C = A + b  # (2, 3)
print(C)
# [[11 22 33]
#  [14 25 36]]
```

### 3. –£–Ω–∏–∫–∞–π –æ–±–µ—Ä–Ω–µ–Ω–∏—Ö –º–∞—Ç—Ä–∏—Ü—å –∫–æ–ª–∏ –º–æ–∂–ª–∏–≤–æ

```python
# ‚ùå –ü–æ–≤—ñ–ª—å–Ω–æ —Ç–∞ –Ω–µ—Å—Ç–∞–±—ñ–ª—å–Ω–æ
x = np.linalg.inv(A) @ b

# ‚úÖ –ö—Ä–∞—â–µ
x = np.linalg.solve(A, b)
```

### 4. –ü–µ—Ä–µ–≤—ñ—Ä—è–π —É–º–æ–≤–∏ —ñ—Å–Ω—É–≤–∞–Ω–Ω—è

```python
A = np.array([[1, 2], [2, 4]])

# –ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ —á–∏ –º–æ–∂–Ω–∞ —ñ–Ω–≤–µ—Ä—Ç—É–≤–∞—Ç–∏
det = np.linalg.det(A)
if abs(det) < 1e-10:
    print("Matrix is singular! Cannot invert.")
else:
    A_inv = np.linalg.inv(A)
```

### 5. Scaling –¥–ª—è numerical stability

```python
# –î–ª—è –≤–µ–ª–∏–∫–∏—Ö —á–∏—Å–µ–ª
X = np.array([[1000, 2000], [3000, 4000]])

# Scale before operations
X_scaled = X / 1000  # [1-4] range
# Compute
# Then scale back
```

---

## –®–ø–∞—Ä–≥–∞–ª–∫–∞ –æ–ø–µ—Ä–∞—Ü—ñ–π

```python
import numpy as np

# –°—Ç–≤–æ—Ä–µ–Ω–Ω—è
A = np.array([[1, 2], [3, 4]])
I = np.eye(3)                    # –û–¥–∏–Ω–∏—á–Ω–∞ 3√ó3
Z = np.zeros((2, 3))             # –ù—É–ª—ñ 2√ó3
D = np.diag([1, 2, 3])           # –î—ñ–∞–≥–æ–Ω–∞–ª—å–Ω–∞

# –û–ø–µ—Ä–∞—Ü—ñ—ó
A + B                            # –î–æ–¥–∞–≤–∞–Ω–Ω—è
A - B                            # –í—ñ–¥–Ω—ñ–º–∞–Ω–Ω—è
A * B                            # Element-wise –º–Ω–æ–∂–µ–Ω–Ω—è
A @ B                            # –ú–∞—Ç—Ä–∏—á–Ω–µ –º–Ω–æ–∂–µ–Ω–Ω—è
A.T                              # –¢—Ä–∞–Ω—Å–ø–æ–Ω—É–≤–∞–Ω–Ω—è
np.linalg.inv(A)                 # –û–±–µ—Ä–Ω–µ–Ω–∞
np.linalg.det(A)                 # –í–∏–∑–Ω–∞—á–Ω–∏–∫
np.linalg.matrix_rank(A)         # –†–∞–Ω–≥

# –í–µ–∫—Ç–æ—Ä–Ω—ñ –æ–ø–µ—Ä–∞—Ü—ñ—ó
np.dot(a, b)                     # –°–∫–∞–ª—è—Ä–Ω–∏–π –¥–æ–±—É—Ç–æ–∫
np.linalg.norm(v)                # –ù–æ—Ä–º–∞ (L2)
np.linalg.norm(v, 1)             # L1 –Ω–æ—Ä–º–∞
np.linalg.norm(v, np.inf)        # L‚àû –Ω–æ—Ä–º–∞

# –†–æ–∑–∫–ª–∞–¥–∞–Ω–Ω—è
eigenvalues, eigenvectors = np.linalg.eig(A)     # –í–ª–∞—Å–Ω—ñ
U, S, VT = np.linalg.svd(A)                      # SVD

# –†–æ–∑–≤'—è–∑–∞–Ω–Ω—è —Å–∏—Å—Ç–µ–º
x = np.linalg.solve(A, b)        # Ax = b
x = np.linalg.lstsq(A, b)[0]     # Least squares

# –ö–æ—Ä–∏—Å–Ω—ñ —Ñ—É–Ω–∫—Ü—ñ—ó
A.shape                          # –†–æ–∑–º—ñ—Ä
A.reshape(3, 2)                  # Reshape
A.flatten()                      # –í 1D
np.concatenate([A, B])           # –°–∫–ª–µ—ó—Ç–∏
np.column_stack([v1, v2])        # –í–µ–∫—Ç–æ—Ä–∏ –≤ –º–∞—Ç—Ä–∏—Ü—é
```

---

## –†–µ—Å—É—Ä—Å–∏ –¥–ª—è –ø–æ–≥–ª–∏–±–ª–µ–Ω–æ–≥–æ –≤–∏–≤—á–µ–Ω–Ω—è

### –ö–Ω–∏–≥–∏
- **"Linear Algebra and Its Applications" by Gilbert Strang** ‚Äî –∫–ª–∞—Å–∏–∫–∞
- **"Introduction to Linear Algebra" by Gilbert Strang** ‚Äî –±—ñ–ª—å—à –¥–æ—Å—Ç—É–ø–Ω–∞
- **"No Bullshit Guide to Linear Algebra" by Ivan Savov** ‚Äî –ø—Ä–∞–∫—Ç–∏—á–Ω–∞

### –í—ñ–¥–µ–æ –∫—É—Ä—Å–∏
- **3Blue1Brown - Essence of Linear Algebra** (YouTube) ‚Äî –Ω–∞–π–∫—Ä–∞—â–∞ –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è!
- **Gilbert Strang's MIT 18.06** ‚Äî –ø–æ–≤–Ω–∏–π –∫—É—Ä—Å
- **Khan Academy - Linear Algebra** ‚Äî step-by-step

### –Ü–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ñ —Ä–µ—Å—É—Ä—Å–∏
- **Matrix Calculus** (matrixcalculus.org) ‚Äî –æ–±—á–∏—Å–ª–µ–Ω–Ω—è –ø–æ—Ö—ñ–¥–Ω–∏—Ö –º–∞—Ç—Ä–∏—Ü—å
- **Seeing Theory** ‚Äî –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó

---

## –ö–ª—é—á–æ–≤—ñ –≤–∏—Å–Ω–æ–≤–∫–∏

> –õ—ñ–Ω—ñ–π–Ω–∞ –∞–ª–≥–µ–±—Ä–∞ ‚Äî —Ü–µ –º–æ–≤–∞ Data Science. –í—Å–µ –≤ ML –∑–≤–æ–¥–∏—Ç—å—Å—è –¥–æ –æ–ø–µ—Ä–∞—Ü—ñ–π –∑ –≤–µ–∫—Ç–æ—Ä–∞–º–∏ —Ç–∞ –º–∞—Ç—Ä–∏—Ü—è–º–∏.

**–§—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ñ –∫–æ–Ω—Ü–µ–ø—Ü—ñ—ó:**
1. **–í–µ–∫—Ç–æ—Ä–∏** ‚Äî —Ç–æ—á–∫–∏/–Ω–∞–ø—Ä—è–º–∫–∏ –≤ –ø—Ä–æ—Å—Ç–æ—Ä—ñ
2. **–ú–∞—Ç—Ä–∏—Ü—ñ** ‚Äî –ª—ñ–Ω—ñ–π–Ω—ñ –ø–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—è
3. **–°–∫–∞–ª—è—Ä–Ω–∏–π –¥–æ–±—É—Ç–æ–∫** ‚Äî similarity
4. **–ù–æ—Ä–º–∏** ‚Äî –¥–æ–≤–∂–∏–Ω–∞/–≤—ñ–¥—Å—Ç–∞–Ω—å
5. **–í–ª–∞—Å–Ω—ñ –≤–µ–∫—Ç–æ—Ä–∏** ‚Äî –Ω–∞–ø—Ä—è–º–∫–∏, —â–æ –∑–±–µ—Ä—ñ–≥–∞—é—Ç—å—Å—è –ø—Ä–∏ –ø–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—ñ

**–ö–ª—é—á–æ–≤—ñ –æ–ø–µ—Ä–∞—Ü—ñ—ó:**
- –ú–∞—Ç—Ä–∏—á–Ω–µ –º–Ω–æ–∂–µ–Ω–Ω—è: $C = AB$
- –¢—Ä–∞–Ω—Å–ø–æ–Ω—É–≤–∞–Ω–Ω—è: $A^T$
- –û–±–µ—Ä–Ω–µ–Ω–∞: $A^{-1}$ (—è–∫—â–æ —ñ—Å–Ω—É—î)
- –í–∏–∑–Ω–∞—á–Ω–∏–∫: $\det(A)$
- –†–∞–Ω–≥: –ª—ñ–Ω—ñ–π–Ω–∞ –Ω–µ–∑–∞–ª–µ–∂–Ω—ñ—Å—Ç—å

**–†–æ–∑–∫–ª–∞–¥–∞–Ω–Ω—è:**
- **Eigendecomposition:** $A = Q\Lambda Q^T$ (—Å–∏–º–µ—Ç—Ä–∏—á–Ω—ñ –º–∞—Ç—Ä–∏—Ü—ñ)
- **SVD:** $A = U\Sigma V^T$ (–±—É–¥—å-—è–∫—ñ –º–∞—Ç—Ä–∏—Ü—ñ)

**–í Machine Learning:**
- **Regression:** $\mathbf{w} = (X^T X)^{-1} X^T \mathbf{y}$
- **PCA:** –í–ª–∞—Å–Ω—ñ –≤–µ–∫—Ç–æ—Ä–∏ –∫–æ–≤–∞—Ä—ñ–∞—Ü—ñ–π–Ω–æ—ó –º–∞—Ç—Ä–∏—Ü—ñ
- **Neural Networks:** –ú–∞—Ç—Ä–∏—á–Ω—ñ –º–Ω–æ–∂–µ–Ω–Ω—è –≤—Å—é–¥–∏
- **Similarity:** Cosine similarity, distances
- **Regularization:** L1/L2 norms

**–ù–∞–π–≤–∞–∂–ª–∏–≤—ñ—à–µ:**
- –†–æ–∑—É–º—ñ–π –≥–µ–æ–º–µ—Ç—Ä–∏—á–Ω—É —ñ–Ω—Ç—É—ó—Ü—ñ—é
- –ü–µ—Ä–µ–≤—ñ—Ä—è–π —Ä–æ–∑–º—ñ—Ä–∏ –º–∞—Ç—Ä–∏—Ü—å (shape)
- NumPy ‚Äî —Ç–≤—ñ–π –Ω–∞–π–∫—Ä–∞—â–∏–π –¥—Ä—É–≥
- –ü—Ä–∞–∫—Ç–∏–∫—É–π—Å—è –Ω–∞ —Ä–µ–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö

---

#math #linear-algebra #vectors #matrices #eigenvalues #svd #ml-fundamentals #data-science #numpy
