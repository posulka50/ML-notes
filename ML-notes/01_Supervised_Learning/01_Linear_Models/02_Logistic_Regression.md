# Logistic Regression (–õ–æ–≥—ñ—Å—Ç–∏—á–Ω–∞ —Ä–µ–≥—Ä–µ—Å—ñ—è)

## –©–æ —Ü–µ?

**–õ–æ–≥—ñ—Å—Ç–∏—á–Ω–∞ —Ä–µ–≥—Ä–µ—Å—ñ—è** ‚Äî —Ü–µ –∞–ª–≥–æ—Ä–∏—Ç–º supervised learning –¥–ª—è **–±—ñ–Ω–∞—Ä–Ω–æ—ó –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó**, —è–∫–∏–π –ø–µ—Ä–µ–¥–±–∞—á–∞—î **–π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å** –Ω–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ –æ–±'—î–∫—Ç–∞ –¥–æ –ø–µ–≤–Ω–æ–≥–æ –∫–ª–∞—Å—É.

**–ì–æ–ª–æ–≤–Ω–∞ —ñ–¥–µ—è:** –ø–µ—Ä–µ—Ç–≤–æ—Ä–∏—Ç–∏ –ª—ñ–Ω—ñ–π–Ω—É –∫–æ–º–±—ñ–Ω–∞—Ü—ñ—é –æ–∑–Ω–∞–∫ —É –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å $P(y=1|X)$ –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é **—Å–∏–≥–º–æ—ó–¥–Ω–æ—ó —Ñ—É–Ω–∫—Ü—ñ—ó** (–ª–æ–≥—ñ—Å—Ç–∏—á–Ω–æ—ó —Ñ—É–Ω–∫—Ü—ñ—ó).

## –ù–∞–≤—ñ—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–∞?

- üéØ **–ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è** ‚Äî —Å–ø–∞–º/–Ω–µ —Å–ø–∞–º, —Ö–≤–æ—Ä–æ–±–∞/–∑–¥–æ—Ä–æ–≤–∏–π, —Å—Ö–≤–∞–ª–∏—Ç–∏/–≤—ñ–¥—Ö–∏–ª–∏—Ç–∏
- üìä **–ô–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ** ‚Äî –Ω–µ –ø—Ä–æ—Å—Ç–æ –∫–ª–∞—Å, –∞ –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å –Ω–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ
- üîç **–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü—ñ—è** ‚Äî –∑—Ä–æ–∑—É–º—ñ—Ç–∏ –≤–ø–ª–∏–≤ –æ–∑–Ω–∞–∫ —á–µ—Ä–µ–∑ odds ratio
- ‚ö° **–®–≤–∏–¥–∫—ñ—Å—Ç—å** ‚Äî —à–≤–∏–¥–∫–µ –Ω–∞–≤—á–∞–Ω–Ω—è —Ç–∞ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
- üìà **Baseline** ‚Äî —Å—Ç–∞—Ä—Ç–æ–≤–∞ –º–æ–¥–µ–ª—å –¥–ª—è –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó

## –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏?

**–ü–æ—Ç—Ä—ñ–±–Ω–æ:**
- **–ë—ñ–Ω–∞—Ä–Ω–∞ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è** (2 –∫–ª–∞—Å–∏: 0/1, Yes/No)
- –ö–ª–∞—Å–∏ **–ª—ñ–Ω—ñ–π–Ω–æ —Ä–æ–∑–¥—ñ–ª–∏–º—ñ** –∞–±–æ –±–ª–∏–∑—å–∫–æ –¥–æ —Ü—å–æ–≥–æ
- –ü–æ—Ç—Ä—ñ–±–Ω—ñ **–∫–∞–ª—ñ–±—Ä–æ–≤–∞–Ω—ñ –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ** $P(y=1)$
- **–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å** –≤–∞–∂–ª–∏–≤–∞
- –®–≤–∏–¥–∫–µ –Ω–∞–≤—á–∞–Ω–Ω—è –Ω–∞ –≤–µ–ª–∏–∫–∏—Ö –¥–∞–Ω–∏—Ö

**–ù–µ –ø–æ—Ç—Ä—ñ–±–Ω–æ:**
- –ë–∞–≥–∞—Ç–æ–∫–ª–∞—Å–æ–≤–∞ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è ‚Üí Softmax Regression, Tree-based
- –ö–ª–∞—Å–∏ **–Ω–µ–ª—ñ–Ω—ñ–π–Ω–æ —Ä–æ–∑–¥—ñ–ª–∏–º—ñ** ‚Üí SVM –∑ kernel, Neural Networks
- –õ–∏—à–µ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –∫–ª–∞—Å—É –±–µ–∑ –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç–µ–π ‚Üí SVM
- –†–µ–≥—Ä–µ—Å—ñ—è (—á–∏—Å–ª–æ–≤–∞ –∑–º—ñ–Ω–Ω–∞) ‚Üí Linear Regression

---

## –ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞

### –õ—ñ–Ω—ñ–π–Ω–∞ –∫–æ–º–±—ñ–Ω–∞—Ü—ñ—è (logit)

$$z = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_p x_p = \beta^T X$$

–¥–µ:
- $z$ ‚Äî log-odds (–ª–æ–≥–∞—Ä–∏—Ñ–º —à–∞–Ω—Å—ñ–≤)
- $\beta$ ‚Äî –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç–∏ –º–æ–¥–µ–ª—ñ
- $X$ ‚Äî –≤–µ–∫—Ç–æ—Ä –æ–∑–Ω–∞–∫

### –°–∏–≥–º–æ—ó–¥–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è

$$\sigma(z) = \frac{1}{1 + e^{-z}}$$

**–í–ª–∞—Å—Ç–∏–≤–æ—Å—Ç—ñ:**
- –î—ñ–∞–ø–∞–∑–æ–Ω: $(0, 1)$ ‚Äî —ñ–¥–µ–∞–ª—å–Ω–æ –¥–ª—è –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç–µ–π
- –ú–æ–Ω–æ—Ç–æ–Ω–Ω–æ –∑—Ä–æ—Å—Ç–∞—î
- S-–ø–æ–¥—ñ–±–Ω–∞ –∫—Ä–∏–≤–∞
- $\sigma(0) = 0.5$
- $\sigma(-\infty) = 0$, $\sigma(+\infty) = 1$

### –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ

$$P(y=1|X) = \sigma(\beta^T X) = \frac{1}{1 + e^{-\beta^T X}}$$

$$P(y=0|X) = 1 - P(y=1|X) = \frac{e^{-\beta^T X}}{1 + e^{-\beta^T X}}$$

### –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –∫–ª–∞—Å—É (Decision Boundary)

$$\hat{y} = \begin{cases} 
1 & \text{—è–∫—â–æ } P(y=1|X) \geq 0.5 \\
0 & \text{—è–∫—â–æ } P(y=1|X) < 0.5
\end{cases}$$

---

## –ì—Ä–∞—Ñ—ñ–∫ —Å–∏–≥–º–æ—ó–¥–∏

```
P(y=1)
  1 |           ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    |         ‚ï±
0.5 |       ‚ï±     ‚Üê Decision boundary (z=0)
    |     ‚ï±
  0 | ‚îÄ‚îÄ‚îÄ
    |_____________________
       -‚àû    0    +‚àû    z
```

**–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü—ñ—è:**
- $z > 0$ ‚Üí $P(y=1) > 0.5$ ‚Üí –∫–ª–∞—Å 1
- $z = 0$ ‚Üí $P(y=1) = 0.5$ ‚Üí –º–µ–∂–∞ —Ä—ñ—à–µ–Ω–Ω—è
- $z < 0$ ‚Üí $P(y=1) < 0.5$ ‚Üí –∫–ª–∞—Å 0

---

## –ü—Ä–æ—Å—Ç–∏–π –ø—Ä–∏–∫–ª–∞–¥: –°—Ö–≤–∞–ª–µ–Ω–Ω—è –∫—Ä–µ–¥–∏—Ç—É

### –î–∞–Ω—ñ

| –î–æ—Ö—ñ–¥ (—Ç–∏—Å. $) | –ö—Ä–µ–¥–∏—Ç–Ω–∏–π —Ä–µ–π—Ç–∏–Ω–≥ | –°—Ö–≤–∞–ª–µ–Ω–æ |
|----------------|-------------------|----------|
| 30 | 400 | 0 (–ù—ñ) |
| 50 | 600 | 1 (–¢–∞–∫) |
| 40 | 550 | 0 (–ù—ñ) |
| 70 | 750 | 1 (–¢–∞–∫) |
| 60 | 700 | 1 (–¢–∞–∫) |
| 35 | 450 | 0 (–ù—ñ) |
| 80 | 800 | 1 (–¢–∞–∫) |

### –ú–æ–¥–µ–ª—å

–ü—ñ—Å–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è –æ—Ç—Ä–∏–º–∞–ª–∏ –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç–∏:
$$z = -5 + 0.05 \times \text{–î–æ—Ö—ñ–¥} + 0.01 \times \text{–†–µ–π—Ç–∏–Ω–≥}$$

### –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –¥–ª—è –Ω–æ–≤–æ–≥–æ –∫–ª—ñ—î–Ω—Ç–∞

**–ö–ª—ñ—î–Ω—Ç:** –î–æ—Ö—ñ–¥ = 55 —Ç–∏—Å. $, –†–µ–π—Ç–∏–Ω–≥ = 650

**–ö—Ä–æ–∫ 1:** –û–±—á–∏—Å–ª–∏—Ç–∏ log-odds
$$z = -5 + 0.05(55) + 0.01(650)$$
$$z = -5 + 2.75 + 6.5 = 4.25$$

**–ö—Ä–æ–∫ 2:** –ó–∞—Å—Ç–æ—Å—É–≤–∞—Ç–∏ sigmoid
$$P(\text{–°—Ö–≤–∞–ª–µ–Ω–æ}=1) = \frac{1}{1 + e^{-4.25}} = \frac{1}{1 + 0.014} = 0.986$$

**–ö—Ä–æ–∫ 3:** –ü—Ä–∏–π–Ω—è—Ç–∏ —Ä—ñ—à–µ–Ω–Ω—è (threshold = 0.5)
$$P = 0.986 > 0.5 \Rightarrow \text{–°—Ö–≤–∞–ª–µ–Ω–æ!} \checkmark$$

**–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü—ñ—è:** –ô–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å —Å—Ö–≤–∞–ª–µ–Ω–Ω—è –∫—Ä–µ–¥–∏—Ç—É ‚Äî **98.6%**

---

## –°–∫–ª–∞–¥–Ω–∏–π –ø—Ä–∏–∫–ª–∞–¥: –î—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –¥—ñ–∞–±–µ—Ç—É

### –î–∞–Ω—ñ

1000 –ø–∞—Ü—ñ—î–Ω—Ç—ñ–≤:

| –û–∑–Ω–∞–∫–∞ | –î—ñ–∞–ø–∞–∑–æ–Ω | –û–ø–∏—Å |
|--------|----------|------|
| Glucose | 70-200 mg/dL | –†—ñ–≤–µ–Ω—å –≥–ª—é–∫–æ–∑–∏ |
| BMI | 18-45 | –Ü–Ω–¥–µ–∫—Å –º–∞—Å–∏ —Ç—ñ–ª–∞ |
| Age | 21-81 | –í—ñ–∫ |
| BloodPressure | 60-120 | –¢–∏—Å–∫ |
| **Outcome** | 0/1 | –î—ñ–∞–±–µ—Ç (0=–ù—ñ, 1=–¢–∞–∫) |

**–†–æ–∑–ø–æ–¥—ñ–ª:** 65% –∑–¥–æ—Ä–æ–≤—ñ, 35% —Ö–≤–æ—Ä—ñ (–Ω–µ–∑–±–∞–ª–∞–Ω—Å–æ–≤–∞–Ω—ñ –∫–ª–∞—Å–∏)

### –ù–∞–≤—á–µ–Ω–∞ –º–æ–¥–µ–ª—å

$$z = -8 + 0.05 \times \text{Glucose} + 0.1 \times \text{BMI} + 0.02 \times \text{Age} + 0.01 \times \text{BP}$$

### –ü—Ä–∏–∫–ª–∞–¥ 1: –ú–æ–ª–æ–¥–∞ –ª—é–¥–∏–Ω–∞ –∑ –Ω–æ—Ä–º–∞–ª—å–Ω–∏–º–∏ –ø–æ–∫–∞–∑–Ω–∏–∫–∞–º–∏

**–î–∞–Ω—ñ:** Glucose=90, BMI=22, Age=25, BP=80

$$z = -8 + 0.05(90) + 0.1(22) + 0.02(25) + 0.01(80)$$
$$z = -8 + 4.5 + 2.2 + 0.5 + 0.8 = 0$$

$$P(\text{–î—ñ–∞–±–µ—Ç}=1) = \frac{1}{1 + e^{0}} = 0.5$$

**–†—ñ—à–µ–Ω–Ω—è:** **–ì—Ä–∞–Ω–∏—á–Ω–∏–π –≤–∏–ø–∞–¥–æ–∫** (–ø–æ—Ç—Ä—ñ–±–Ω—ñ –¥–æ–¥–∞—Ç–∫–æ–≤—ñ –æ–±—Å—Ç–µ–∂–µ–Ω–Ω—è)

### –ü—Ä–∏–∫–ª–∞–¥ 2: –õ—é–¥–∏–Ω–∞ –∑ –≤–∏—Å–æ–∫–∏–º–∏ –ø–æ–∫–∞–∑–Ω–∏–∫–∞–º–∏

**–î–∞–Ω—ñ:** Glucose=180, BMI=35, Age=55, BP=110

$$z = -8 + 0.05(180) + 0.1(35) + 0.02(55) + 0.01(110)$$
$$z = -8 + 9 + 3.5 + 1.1 + 1.1 = 6.7$$

$$P(\text{–î—ñ–∞–±–µ—Ç}=1) = \frac{1}{1 + e^{-6.7}} = 0.999$$

**–†—ñ—à–µ–Ω–Ω—è:** **–î—ñ–∞–±–µ—Ç** (–π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å 99.9%) ‚ö†Ô∏è

### –ü—Ä–∏–∫–ª–∞–¥ 3: –ü—Ä–æ–º—ñ–∂–Ω–∏–π –≤–∏–ø–∞–¥–æ–∫

**–î–∞–Ω—ñ:** Glucose=120, BMI=28, Age=45, BP=90

$$z = -8 + 0.05(120) + 0.1(28) + 0.02(45) + 0.01(90)$$
$$z = -8 + 6 + 2.8 + 0.9 + 0.9 = 2.6$$

$$P(\text{–î—ñ–∞–±–µ—Ç}=1) = \frac{1}{1 + e^{-2.6}} = 0.931$$

**–†—ñ—à–µ–Ω–Ω—è:** **–í–∏—Å–æ–∫–∞ –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å –¥—ñ–∞–±–µ—Ç—É** (93.1%)

---

## –§—É–Ω–∫—Ü—ñ—è –≤—Ç—Ä–∞—Ç: Log-Loss (Binary Cross-Entropy)

### –î–ª—è –æ–¥–Ω–æ–≥–æ –ø—Ä–∏–∫–ª–∞–¥—É

$$L(y, \hat{p}) = -[y \log(\hat{p}) + (1-y) \log(1-\hat{p})]$$

–¥–µ:
- $y \in \{0, 1\}$ ‚Äî —Ä–µ–∞–ª—å–Ω–∏–π –∫–ª–∞—Å
- $\hat{p} = P(y=1|X)$ ‚Äî –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–∞ –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å

### –î–ª—è –≤—Å—å–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç—É

$$J(\beta) = -\frac{1}{n} \sum_{i=1}^{n} [y_i \log(\hat{p}_i) + (1-y_i) \log(1-\hat{p}_i)]$$

### –Ü–Ω—Ç—É—ó—Ü—ñ—è

**–Ø–∫—â–æ —Ä–µ–∞–ª—å–Ω–∏–π –∫–ª–∞—Å $y=1$:**
- $\hat{p} = 0.9$ ‚Üí $L = -\log(0.9) = 0.105$ (–º–∞–ª–∞ –ø–æ–º–∏–ª–∫–∞) ‚úì
- $\hat{p} = 0.5$ ‚Üí $L = -\log(0.5) = 0.693$ (–ø–æ–º—ñ—Ä–Ω–∞ –ø–æ–º–∏–ª–∫–∞)
- $\hat{p} = 0.1$ ‚Üí $L = -\log(0.1) = 2.303$ (–≤–µ–ª–∏–∫–∞ –ø–æ–º–∏–ª–∫–∞) ‚úó

**–Ø–∫—â–æ —Ä–µ–∞–ª—å–Ω–∏–π –∫–ª–∞—Å $y=0$:**
- $\hat{p} = 0.1$ ‚Üí $L = -\log(0.9) = 0.105$ (–º–∞–ª–∞ –ø–æ–º–∏–ª–∫–∞) ‚úì
- $\hat{p} = 0.5$ ‚Üí $L = -\log(0.5) = 0.693$ (–ø–æ–º—ñ—Ä–Ω–∞ –ø–æ–º–∏–ª–∫–∞)
- $\hat{p} = 0.9$ ‚Üí $L = -\log(0.1) = 2.303$ (–≤–µ–ª–∏–∫–∞ –ø–æ–º–∏–ª–∫–∞) ‚úó

> **Log-loss —Å—É–≤–æ—Ä–æ –∫–∞—Ä–∞—î –≤–ø–µ–≤–Ω–µ–Ω—ñ, –∞–ª–µ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ñ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è!**

---

## –û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è: Gradient Descent

–ù–µ–º–∞—î –∞–Ω–∞–ª—ñ—Ç–∏—á–Ω–æ–≥–æ —Ä—ñ—à–µ–Ω–Ω—è –¥–ª—è Logistic Regression ‚Üí –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –≥—Ä–∞–¥—ñ—î–Ω—Ç–Ω–∏–π —Å–ø—É—Å–∫.

### –ì—Ä–∞–¥—ñ—î–Ω—Ç

$$\frac{\partial J}{\partial \beta_j} = \frac{1}{n} \sum_{i=1}^{n} (\hat{p}_i - y_i) x_{ij}$$

–¥–µ $\hat{p}_i = \sigma(\beta^T X_i)$.

### –û–Ω–æ–≤–ª–µ–Ω–Ω—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤

$$\beta_j := \beta_j - \alpha \frac{\partial J}{\partial \beta_j}$$

–¥–µ $\alpha$ ‚Äî learning rate.

### –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –∑ Linear Regression

| | Linear Regression | Logistic Regression |
|---|---|---|
| **–ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è** | $\hat{y} = \beta^T X$ | $\hat{p} = \sigma(\beta^T X)$ |
| **Loss** | MSE | Log-Loss |
| **–ì—Ä–∞–¥—ñ—î–Ω—Ç** | $\frac{1}{n} \sum (\hat{y}_i - y_i) x_{ij}$ | $\frac{1}{n} \sum (\hat{p}_i - y_i) x_{ij}$ |
| **–†—ñ—à–µ–Ω–Ω—è** | –ê–Ω–∞–ª—ñ—Ç–∏—á–Ω–µ (Normal Eq) | Gradient Descent |

---

## –Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü—ñ—è: Odds Ratio

### –®–∞–Ω—Å–∏ (Odds)

$$\text{Odds} = \frac{P(y=1)}{P(y=0)} = \frac{P(y=1)}{1 - P(y=1)}$$

**–ü—Ä–∏–∫–ª–∞–¥:**
- $P(y=1) = 0.8$ ‚Üí Odds = $\frac{0.8}{0.2} = 4$ (—à–∞–Ω—Å–∏ 4:1 –Ω–∞ –∫–æ—Ä–∏—Å—Ç—å –∫–ª–∞—Å—É 1)

### –õ–æ–≥–∞—Ä–∏—Ñ–º —à–∞–Ω—Å—ñ–≤ (Log-Odds)

$$\log(\text{Odds}) = \log\left(\frac{P(y=1)}{1-P(y=1)}\right) = \beta^T X = z$$

**–í–∏—Å–Ω–æ–≤–æ–∫:** Logistic Regression –º–æ–¥–µ–ª—é—î **log-odds —è–∫ –ª—ñ–Ω—ñ–π–Ω—É —Ñ—É–Ω–∫—Ü—ñ—é –≤—ñ–¥ X**.

### Odds Ratio (OR)

–ü—Ä–∏ –∑–±—ñ–ª—å—à–µ–Ω–Ω—ñ $x_j$ –Ω–∞ 1 –æ–¥–∏–Ω–∏—Ü—é:
$$\text{Odds Ratio} = e^{\beta_j}$$

**–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü—ñ—è:**

| $\beta_j$ | $e^{\beta_j}$ | –ó–Ω–∞—á–µ–Ω–Ω—è |
|-----------|---------------|----------|
| 0.69 | 2.0 | –®–∞–Ω—Å–∏ **–ø–æ–¥–≤–æ—é—é—Ç—å—Å—è** |
| 1.10 | 3.0 | –®–∞–Ω—Å–∏ **–ø–æ—Ç—Ä–æ—é—é—Ç—å—Å—è** |
| -0.69 | 0.5 | –®–∞–Ω—Å–∏ **–∑–º–µ–Ω—à—É—é—Ç—å—Å—è –≤–¥–≤—ñ—á—ñ** |
| 0 | 1.0 | –û–∑–Ω–∞–∫–∞ **–Ω–µ –≤–ø–ª–∏–≤–∞—î** |

### –ü—Ä–∏–∫–ª–∞–¥

–£ –º–æ–¥–µ–ª—ñ –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –¥—ñ–∞–±–µ—Ç—É: $\beta_{\text{Glucose}} = 0.05$

$$\text{OR} = e^{0.05} = 1.051$$

**–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü—ñ—è:** –ü—Ä–∏ –∑–±—ñ–ª—å—à–µ–Ω–Ω—ñ –≥–ª—é–∫–æ–∑–∏ –Ω–∞ 1 mg/dL, —à–∞–Ω—Å–∏ –º–∞—Ç–∏ –¥—ñ–∞–±–µ—Ç –∑—Ä–æ—Å—Ç–∞—é—Ç—å —É **1.051 —Ä–∞–∑–∏** (–∞–±–æ –Ω–∞ 5.1%).

---

## Decision Threshold (–ü–æ—Ä—ñ–≥ —Ä—ñ—à–µ–Ω–Ω—è)

### –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∏–π –ø–æ—Ä—ñ–≥: 0.5

$$\hat{y} = \begin{cases}
1 & \text{—è–∫—â–æ } P(y=1) \geq 0.5 \\
0 & \text{—è–∫—â–æ } P(y=1) < 0.5
\end{cases}$$

### –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –ø–æ—Ä–æ–≥—É

**–ü—Ä–∏–∫–ª–∞–¥: –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ —Ä–∞–∫—É**

| –ü–æ—Ä—ñ–≥ | Precision | Recall | –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è |
|-------|-----------|--------|--------------|
| 0.3 | –ù–∏–∑—å–∫–∞ (60%) | –í–∏—Å–æ–∫–∞ (95%) | –ö—Ä–∏—Ç–∏—á–Ω–æ –Ω–µ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç–∏ —Ö–≤–æ—Ä–æ–≥–æ |
| 0.5 | –°–µ—Ä–µ–¥–Ω—è (75%) | –°–µ—Ä–µ–¥–Ω—ñ–π (80%) | –ë–∞–ª–∞–Ω—Å |
| 0.7 | –í–∏—Å–æ–∫–∞ (90%) | –ù–∏–∑—å–∫–∏–π (70%) | –í–∞–∂–ª–∏–≤–æ —É–Ω–∏–∫–Ω—É—Ç–∏ false positives |

### Trade-off

```
–ü–æ—Ä—ñ–≥ 0.3:
‚úì Recall: 95% ‚Äî –º–∞–π–∂–µ –≤—Å—ñ—Ö —Ö–≤–æ—Ä–∏—Ö –∑–Ω–∞–π—à–ª–∏
‚úó Precision: 60% ‚Äî –±–∞–≥–∞—Ç–æ false positives

–ü–æ—Ä—ñ–≥ 0.7:
‚úó Recall: 70% ‚Äî –ø—Ä–æ–ø—É—Å—Ç–∏–ª–∏ –¥–µ—è–∫–∏—Ö —Ö–≤–æ—Ä–∏—Ö
‚úì Precision: 90% ‚Äî –º–∞–π–∂–µ –≤—Å—ñ –ø–æ–∑–∏—Ç–∏–≤–∏ –ø—Ä–∞–≤–∏–ª—å–Ω—ñ
```

---

## –ú–µ—Ç—Ä–∏–∫–∏ –æ—Ü—ñ–Ω–∫–∏

### Confusion Matrix

```
                –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–æ
                 0       1
–†–µ–∞–ª—å–Ω–æ  0    TN=50   FP=10   
         1    FN=5    TP=35   
```

### –û—Å–Ω–æ–≤–Ω—ñ –º–µ—Ç—Ä–∏–∫–∏

| –ú–µ—Ç—Ä–∏–∫–∞ | –§–æ—Ä–º—É–ª–∞ | –ó–Ω–∞—á–µ–Ω–Ω—è |
|---------|---------|----------|
| **Accuracy** | $\frac{TP + TN}{TP + TN + FP + FN}$ | –ó–∞–≥–∞–ª—å–Ω–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å |
| **Precision** | $\frac{TP}{TP + FP}$ | –°–∫—ñ–ª—å–∫–∏ –∑ –ø–æ–∑–∏—Ç–∏–≤–Ω–∏—Ö –ø—Ä–∞–≤–∏–ª—å–Ω—ñ |
| **Recall (Sensitivity)** | $\frac{TP}{TP + FN}$ | –°–∫—ñ–ª—å–∫–∏ —Ä–µ–∞–ª—å–Ω–∏—Ö –ø–æ–∑–∏—Ç–∏–≤–Ω–∏—Ö –∑–Ω–∞–π—à–ª–∏ |
| **Specificity** | $\frac{TN}{TN + FP}$ | –°–∫—ñ–ª—å–∫–∏ –Ω–µ–≥–∞—Ç–∏–≤–Ω–∏—Ö –ø—Ä–∞–≤–∏–ª—å–Ω–æ –≤–∏–∑–Ω–∞—á–∏–ª–∏ |
| **F1-Score** | $2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}$ | –ì–∞—Ä–º–æ–Ω—ñ–π–Ω–µ —Å–µ—Ä–µ–¥–Ω—î |

### –ü—Ä–∏–∫–ª–∞–¥ —Ä–æ–∑—Ä–∞—Ö—É–Ω–∫—É

–î–ª—è –º–∞—Ç—Ä–∏—Ü—ñ –≤–∏—â–µ:
- **Accuracy** = $\frac{50+35}{100} = 0.85$ (85%)
- **Precision** = $\frac{35}{35+10} = 0.78$ (78%)
- **Recall** = $\frac{35}{35+5} = 0.88$ (88%)
- **F1-Score** = $2 \times \frac{0.78 \times 0.88}{0.78 + 0.88} = 0.82$ (82%)

### ROC Curve —Ç–∞ AUC

**ROC (Receiver Operating Characteristic)** ‚Äî –≥—Ä–∞—Ñ—ñ–∫ TPR vs FPR –ø—Ä–∏ —Ä—ñ–∑–Ω–∏—Ö –ø–æ—Ä–æ–≥–∞—Ö:

```
TPR (Recall)
  1 |    ‚ï±‚îÄ‚îÄ‚îÄ‚îÄ  ‚Üê –Ü–¥–µ–∞–ª—å–Ω–∞ –º–æ–¥–µ–ª—å (AUC=1.0)
    |   ‚ï±
    |  ‚ï±  ‚Üê –î–æ–±—Ä–∞ –º–æ–¥–µ–ª—å (AUC~0.9)
0.5 | ‚ï±
    |‚ï±_____ ‚Üê –í–∏–ø–∞–¥–∫–æ–≤–µ –≥–∞–¥–∞–Ω–Ω—è (AUC=0.5)
  0 |________
    0   0.5  1  FPR
```

**AUC (Area Under Curve):**
- AUC = 1.0 ‚Üí –Ü–¥–µ–∞–ª—å–Ω–∞ –º–æ–¥–µ–ª—å
- AUC = 0.9-1.0 ‚Üí –í—ñ–¥–º—ñ–Ω–Ω–∞
- AUC = 0.8-0.9 ‚Üí –î–æ–±—Ä–∞
- AUC = 0.7-0.8 ‚Üí –ü—Ä–∏–π–Ω—è—Ç–Ω–∞
- AUC = 0.5 ‚Üí –Ø–∫ –ø—ñ–¥–∫–∏–¥–∞–Ω–Ω—è –º–æ–Ω–µ—Ç–∏

---

## –ö–æ–¥ (Python + scikit-learn)

### –ü—Ä–æ—Å—Ç–∏–π –ø—Ä–∏–∫–ª–∞–¥

```python
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

# –î–∞–Ω—ñ
X = np.array([[30, 400], [50, 600], [40, 550], 
              [70, 750], [60, 700], [35, 450], [80, 800]])
y = np.array([0, 1, 0, 1, 1, 0, 1])

# –ú–æ–¥–µ–ª—å
model = LogisticRegression()
model.fit(X, y)

# –ö–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç–∏
print(f"Intercept (Œ≤‚ÇÄ): {model.intercept_[0]:.4f}")
print(f"Coefficients (Œ≤): {model.coef_[0]}")

# –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –∫–ª–∞—Å—É
X_new = np.array([[55, 650]])
y_pred_class = model.predict(X_new)
print(f"Predicted class: {y_pred_class[0]}")  # 1

# –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ
y_pred_proba = model.predict_proba(X_new)
print(f"P(class=0): {y_pred_proba[0][0]:.4f}")
print(f"P(class=1): {y_pred_proba[0][1]:.4f}")  # 0.986

# –û—Ü—ñ–Ω–∫–∞
y_pred = model.predict(X)
print(f"Accuracy: {accuracy_score(y, y_pred):.4f}")
```

### –ü–æ–≤–Ω–∏–π –ø—Ä–∏–∫–ª–∞–¥

```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    classification_report, 
    confusion_matrix,
    roc_auc_score,
    roc_curve
)
import matplotlib.pyplot as plt

# –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –¥–∞–Ω–∏—Ö
from sklearn.datasets import make_classification
X, y = make_classification(
    n_samples=1000,
    n_features=10,
    n_informative=5,
    n_redundant=2,
    n_classes=2,
    weights=[0.65, 0.35],  # –ù–µ–∑–±–∞–ª–∞–Ω—Å–æ–≤–∞–Ω—ñ –∫–ª–∞—Å–∏
    random_state=42
)

# –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

# –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è (–í–ê–ñ–õ–ò–í–û –¥–ª—è LogReg!)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# –ú–æ–¥–µ–ª—å
model = LogisticRegression(
    penalty='l2',          # Ridge regularization
    C=1.0,                 # Inverse of Œª
    solver='lbfgs',        # Optimization algorithm
    max_iter=1000,
    random_state=42
)

model.fit(X_train_scaled, y_train)

# –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
y_pred = model.predict(X_test_scaled)
y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]

# –ú–µ—Ç—Ä–∏–∫–∏
print("=== Classification Report ===")
print(classification_report(y_test, y_pred))

print("\n=== Confusion Matrix ===")
cm = confusion_matrix(y_test, y_pred)
print(cm)

print(f"\nAccuracy: {accuracy_score(y_test, y_pred):.4f}")
print(f"ROC-AUC: {roc_auc_score(y_test, y_pred_proba):.4f}")

# –ö–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç–∏ —Ç–∞ Odds Ratios
print("\n=== Coefficients & Odds Ratios ===")
print(f"Intercept: {model.intercept_[0]:.4f}")
for i, coef in enumerate(model.coef_[0]):
    odds_ratio = np.exp(coef)
    print(f"Feature {i}: Œ≤={coef:.4f}, OR={odds_ratio:.4f}")
```

### ROC Curve

```python
# ROC Curve
fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)
auc = roc_auc_score(y_test, y_pred_proba)

plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, linewidth=2, 
         label=f'ROC Curve (AUC={auc:.3f})')
plt.plot([0, 1], [0, 1], 'k--', linewidth=2, 
         label='Random Guess (AUC=0.5)')
plt.xlabel('False Positive Rate', fontsize=12)
plt.ylabel('True Positive Rate (Recall)', fontsize=12)
plt.title('ROC Curve', fontsize=14, fontweight='bold')
plt.legend(fontsize=11)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

### –í–∏–±—ñ—Ä –ø–æ—Ä–æ–≥—É

```python
# Precision-Recall –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö –ø–æ—Ä–æ–≥—ñ–≤
from sklearn.metrics import precision_recall_curve

precision, recall, thresholds = precision_recall_curve(
    y_test, y_pred_proba
)

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
plt.figure(figsize=(10, 6))
plt.plot(thresholds, precision[:-1], label='Precision')
plt.plot(thresholds, recall[:-1], label='Recall')
plt.axvline(x=0.5, color='r', linestyle='--', 
            label='Default threshold (0.5)')
plt.xlabel('Threshold', fontsize=12)
plt.ylabel('Score', fontsize=12)
plt.title('Precision-Recall vs Threshold', 
          fontsize=14, fontweight='bold')
plt.legend(fontsize=11)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# –í–∏–±—ñ—Ä –∫–∞—Å—Ç–æ–º–Ω–æ–≥–æ –ø–æ—Ä–æ–≥—É
custom_threshold = 0.3
y_pred_custom = (y_pred_proba >= custom_threshold).astype(int)
print(f"\nWith threshold={custom_threshold}:")
print(classification_report(y_test, y_pred_custom))
```

---

## –ë–∞–≥–∞—Ç–æ–∫–ª–∞—Å–æ–≤–∞ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è

### One-vs-Rest (OvR)

–î–ª—è K –∫–ª–∞—Å—ñ–≤ –±—É–¥—É—î–º–æ K –±—ñ–Ω–∞—Ä–Ω–∏—Ö –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä—ñ–≤:
- –ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä 1: "–ö–ª–∞—Å 1" vs "–Ü–Ω—à—ñ"
- –ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä 2: "–ö–ª–∞—Å 2" vs "–Ü–Ω—à—ñ"
- ...

**–ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è:** –∫–ª–∞—Å –∑ –Ω–∞–π–≤–∏—â–æ—é –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—é.

```python
model = LogisticRegression(multi_class='ovr')
model.fit(X_train, y_train)
```

### Softmax Regression (Multinomial)

–£–∑–∞–≥–∞–ª—å–Ω–µ–Ω–Ω—è –¥–ª—è K –∫–ª–∞—Å—ñ–≤:

$$P(y=k|X) = \frac{e^{z_k}}{\sum_{j=1}^{K} e^{z_j}}$$

–¥–µ $z_k = \beta_k^T X$.

```python
model = LogisticRegression(multi_class='multinomial')
model.fit(X_train, y_train)
```

---

## –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—è

### L1 (Lasso)

$$J(\beta) = -\frac{1}{n} \sum \text{log-loss} + \lambda \sum_{j=1}^{p} |\beta_j|$$

- –ó–∞–Ω—É–ª—è—î –Ω–µ–ø–æ—Ç—Ä—ñ–±–Ω—ñ –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç–∏
- **Feature selection**

```python
model = LogisticRegression(penalty='l1', solver='liblinear')
```

### L2 (Ridge)

$$J(\beta) = -\frac{1}{n} \sum \text{log-loss} + \lambda \sum_{j=1}^{p} \beta_j^2$$

- –ó–º–µ–Ω—à—É—î –≤–µ–ª–∏—á–∏–Ω—É –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç—ñ–≤
- –î–æ–ø–æ–º–∞–≥–∞—î –ø—Ä–∏ –º—É–ª—å—Ç–∏–∫–æ–ª—ñ–Ω–µ–∞—Ä–Ω–æ—Å—Ç—ñ

```python
model = LogisticRegression(penalty='l2', C=1.0)  # C = 1/Œª
```

### Elastic Net

$$J(\beta) = \text{log-loss} + \lambda [\alpha \sum |\beta_j| + (1-\alpha) \sum \beta_j^2]$$

```python
model = LogisticRegression(penalty='elasticnet', 
                          solver='saga', 
                          l1_ratio=0.5)
```

---

## –ü–µ—Ä–µ–≤–∞–≥–∏ —Ç–∞ –Ω–µ–¥–æ–ª—ñ–∫–∏

### –ü–µ—Ä–µ–≤–∞–≥–∏ ‚úì

| –ü–µ—Ä–µ–≤–∞–≥–∞ | –ü–æ—è—Å–Ω–µ–Ω–Ω—è |
|----------|-----------|
| **–ô–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ** | –î–∞—î –∫–∞–ª—ñ–±—Ä–æ–≤–∞–Ω—ñ P(y=1), –Ω–µ —Ç—ñ–ª—å–∫–∏ –∫–ª–∞—Å |
| **–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å** | Odds ratio –º–∞—î —á—ñ—Ç–∫–∏–π –∑–º—ñ—Å—Ç |
| **–®–≤–∏–¥–∫—ñ—Å—Ç—å** | –®–≤–∏–¥–∫–µ –Ω–∞–≤—á–∞–Ω–Ω—è —Ç–∞ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è |
| **Regularization** | –õ–µ–≥–∫–æ –¥–æ–¥–∞—Ç–∏ L1/L2 |
| **–ú–∞–ª—ñ –¥–∞–Ω—ñ** | –ü—Ä–∞—Ü—é—î –Ω–∞ –Ω–µ–≤–µ–ª–∏–∫–∏—Ö –≤–∏–±—ñ—Ä–∫–∞—Ö |
| **Baseline** | –í—ñ–¥–º—ñ–Ω–Ω–∞ —Å—Ç–∞—Ä—Ç–æ–≤–∞ –º–æ–¥–µ–ª—å |
| **–ë–∞–≥–∞—Ç–æ–∫–ª–∞—Å–æ–≤—ñ—Å—Ç—å** | –õ–µ–≥–∫–æ —Ä–æ–∑—à–∏—Ä–∏—Ç–∏ (OvR, Softmax) |

### –ù–µ–¥–æ–ª—ñ–∫–∏ ‚úó

| –ù–µ–¥–æ–ª—ñ–∫ | –ü–æ—è—Å–Ω–µ–Ω–Ω—è |
|---------|-----------|
| **–õ—ñ–Ω—ñ–π–Ω–∞ –º–µ–∂–∞** | –ü–µ—Ä–µ–¥–±–∞—á–∞—î –ª—ñ–Ω—ñ–π–Ω—É —Ä–æ–∑–¥—ñ–ª–∏–º—ñ—Å—Ç—å |
| **–ù–µ–ª—ñ–Ω—ñ–π–Ω—ñ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ** | –ü–æ–≥–∞–Ω–æ –∑ —Å–∫–ª–∞–¥–Ω–∏–º–∏ –≤–∑–∞—î–º–æ–¥—ñ—è–º–∏ |
| **–ß—É—Ç–ª–∏–≤—ñ—Å—Ç—å –¥–æ outliers** | –ï–∫—Å—Ç—Ä–µ–º—É–º–∏ –≤–ø–ª–∏–≤–∞—é—Ç—å –Ω–∞ –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç–∏ |
| **–ú—É–ª—å—Ç–∏–∫–æ–ª—ñ–Ω–µ–∞—Ä–Ω—ñ—Å—Ç—å** | –ü—Ä–æ–±–ª–µ–º–∏ –ø—Ä–∏ —Å–∏–ª—å–Ω—ñ–π –∫–æ—Ä–µ–ª—è—Ü—ñ—ó |
| **–ù–µ–∑–±–∞–ª–∞–Ω—Å–æ–≤–∞–Ω—ñ –∫–ª–∞—Å–∏** | –ü–æ—Ç—Ä–µ–±—É—î —Å–ø–µ—Ü—ñ–∞–ª—å–Ω–æ—ó –æ–±—Ä–æ–±–∫–∏ |

---

## –û–±—Ä–æ–±–∫–∞ –Ω–µ–∑–±–∞–ª–∞–Ω—Å–æ–≤–∞–Ω–∏—Ö –∫–ª–∞—Å—ñ–≤

### –ü—Ä–æ–±–ª–µ–º–∞

–Ø–∫—â–æ 95% –∫–ª–∞—Å 0, –º–æ–¥–µ–ª—å –º–æ–∂–µ –∑–∞–≤–∂–¥–∏ –ø–µ—Ä–µ–¥–±–∞—á–∞—Ç–∏ 0 ‚Üí accuracy=95%, –∞–ª–µ –±–µ–∑–∫–æ—Ä–∏—Å–Ω–∞!

### –†—ñ—à–µ–Ω–Ω—è

#### 1. Class weights

```python
model = LogisticRegression(class_weight='balanced')
# –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –ø—ñ–¥–±–∏—Ä–∞—î –≤–∞–≥–∏ –ø—Ä–æ–ø–æ—Ä—Ü—ñ–π–Ω–æ –¥–æ —á–∞—Å—Ç–æ—Ç –∫–ª–∞—Å—ñ–≤
```

#### 2. Custom weights

```python
model = LogisticRegression(class_weight={0: 1, 1: 5})
# –ö–ª–∞—Å 1 –≤ 5 —Ä–∞–∑—ñ–≤ –≤–∞–∂–ª–∏–≤—ñ—à–∏–π
```

#### 3. Resampling

```python
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler

# Oversampling minority class
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

# Undersampling majority class
rus = RandomUnderSampler(random_state=42)
X_resampled, y_resampled = rus.fit_resample(X_train, y_train)
```

#### 4. Adjust threshold

```python
# –ó–Ω–∏–∑–∏—Ç–∏ –ø–æ—Ä—ñ–≥ –¥–ª—è —Ä—ñ–¥–∫—ñ—Å–Ω–æ–≥–æ –∫–ª–∞—Å—É
threshold = 0.3  # –ó–∞–º—ñ—Å—Ç—å 0.5
y_pred = (y_pred_proba >= threshold).astype(int)
```

#### 5. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –ø—Ä–∞–≤–∏–ª—å–Ω—ñ –º–µ—Ç—Ä–∏–∫–∏

```python
# ‚ùå –ù–ï –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏: Accuracy
# ‚úÖ –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏: F1, Precision, Recall, ROC-AUC
```

---

## –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ Logistic Regression

### –Ü–¥–µ–∞–ª—å–Ω–æ –ø—ñ–¥—Ö–æ–¥–∏—Ç—å ‚úì

- **–ë—ñ–Ω–∞—Ä–Ω–∞ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è** –∑ –ª—ñ–Ω—ñ–π–Ω–æ —Ä–æ–∑–¥—ñ–ª–∏–º–∏–º–∏ –∫–ª–∞—Å–∞–º–∏
- –ü–æ—Ç—Ä—ñ–±–Ω—ñ **–π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ**, –Ω–µ —Ç—ñ–ª—å–∫–∏ –∫–ª–∞—Å–∏
- **–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å** –≤–∞–∂–ª–∏–≤–∞ (–º–µ–¥–∏—Ü–∏–Ω–∞, —Ñ—ñ–Ω–∞–Ω—Å–∏)
- **–®–≤–∏–¥–∫–∏–π baseline** –ø–µ—Ä–µ–¥ —Å–∫–ª–∞–¥–Ω—ñ—à–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏
- –ú–∞–ª—ñ/—Å–µ—Ä–µ–¥–Ω—ñ –¥–∞—Ç–∞—Å–µ—Ç–∏

### –ö—Ä–∞—â–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ —ñ–Ω—à–µ ‚úó

- –ö–ª–∞—Å–∏ **–Ω–µ–ª—ñ–Ω—ñ–π–Ω–æ —Ä–æ–∑–¥—ñ–ª–∏–º—ñ** ‚Üí SVM –∑ kernel, Tree-based, Neural Networks
- **–°–∫–ª–∞–¥–Ω—ñ –≤–∑–∞—î–º–æ–¥—ñ—ó** ‚Üí Random Forest, Gradient Boosting
- –ë–∞–≥–∞—Ç–æ **outliers** ‚Üí Robust models
- **–ó–æ–±—Ä–∞–∂–µ–Ω–Ω—è, —Ç–µ–∫—Å—Ç** ‚Üí Deep Learning
- –¢—ñ–ª—å–∫–∏ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è, –±–µ–∑ –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç–µ–π ‚Üí SVM

---

## –ü—Ä–∞–∫—Ç–∏—á–Ω—ñ –ø–æ—Ä–∞–¥–∏ üí°

1. **–ó–∞–≤–∂–¥–∏ –Ω–æ—Ä–º–∞–ª—ñ–∑—É–π –¥–∞–Ω—ñ** ‚Äî StandardScaler –ø–µ—Ä–µ–¥ –Ω–∞–≤—á–∞–Ω–Ω—è–º
2. **–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π stratify** –ø—Ä–∏ —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—ñ –Ω–µ–∑–±–∞–ª–∞–Ω—Å–æ–≤–∞–Ω–∏—Ö –∫–ª–∞—Å—ñ–≤
3. **ROC-AUC –∫—Ä–∞—â–µ –∑–∞ Accuracy** ‚Äî –æ—Å–æ–±–ª–∏–≤–æ –¥–ª—è –Ω–µ–∑–±–∞–ª–∞–Ω—Å–æ–≤–∞–Ω–∏—Ö –¥–∞–Ω–∏—Ö
4. **–ü—ñ–¥–±–∏—Ä–∞–π threshold** ‚Äî 0.5 –Ω–µ –∑–∞–≤–∂–¥–∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω–∏–π
5. **Regularization –∑–∞–≤–∂–¥–∏** ‚Äî L2 –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º, L1 –¥–ª—è feature selection
6. **Class weights** –¥–ª—è –Ω–µ–∑–±–∞–ª–∞–Ω—Å–æ–≤–∞–Ω–∏—Ö –∫–ª–∞—Å—ñ–≤
7. **–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç—É–π —á–µ—Ä–µ–∑ Odds Ratio** ‚Äî exp(Œ≤) –º–∞—î –ø—Ä–∞–∫—Ç–∏—á–Ω–∏–π –∑–º—ñ—Å—Ç
8. **Cross-validation** –¥–ª—è –Ω–∞–¥—ñ–π–Ω–æ—ó –æ—Ü—ñ–Ω–∫–∏
9. **–ü–µ—Ä–µ–≤—ñ—Ä—è–π –Ω–∞ overfitting** ‚Äî train vs test metrics
10. **Feature engineering** ‚Äî polynomial, interactions –º–æ–∂—É—Ç—å –¥–æ–ø–æ–º–æ–≥—Ç–∏

---

## –ü–æ–≤'—è–∑–∞–Ω—ñ —Ç–µ–º–∏

- [[01_Linear_Regression]] ‚Äî –¥–ª—è —Ä–µ–≥—Ä–µ—Å—ñ—ó
- [[03_Regularization]] ‚Äî Ridge, Lasso, Elastic Net
- [[Decision_Trees]] ‚Äî –¥–ª—è –Ω–µ–ª—ñ–Ω—ñ–π–Ω–∏—Ö –º–µ–∂
- [[SVM]] ‚Äî –¥–ª—è —Å–∫–ª–∞–¥–Ω–∏—Ö –º–µ–∂ —Ä—ñ—à–µ–Ω–Ω—è
- [[Cross_Validation]] ‚Äî –æ—Ü—ñ–Ω–∫–∞ –º–æ–¥–µ–ª–µ–π
- [[Feature_Engineering]] ‚Äî –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è –æ–∑–Ω–∞–∫

## –†–µ—Å—É—Ä—Å–∏

- [Scikit-learn: Logistic Regression](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression)
- [StatQuest: Logistic Regression](https://www.youtube.com/watch?v=yIYKR4sgzI8)
- [Andrew Ng: Classification](https://www.coursera.org/learn/machine-learning)

---

## –ö–ª—é—á–æ–≤—ñ –≤–∏—Å–Ω–æ–≤–∫–∏

> Logistic Regression –ø–µ—Ä–µ—Ç–≤–æ—Ä—é—î –ª—ñ–Ω—ñ–π–Ω—É –∫–æ–º–±—ñ–Ω–∞—Ü—ñ—é –æ–∑–Ω–∞–∫ —É –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é —Å–∏–≥–º–æ—ó–¥–Ω–æ—ó —Ñ—É–Ω–∫—Ü—ñ—ó.

**–û—Å–Ω–æ–≤–Ω—ñ –ø—Ä–∏–Ω—Ü–∏–ø–∏:**
- –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î sigmoid: $\sigma(z) = \frac{1}{1 + e^{-z}}$
- –û–ø—Ç–∏–º—ñ–∑—É—î log-loss —Ñ—É–Ω–∫—Ü—ñ—é
- –î–∞—î –∫–∞–ª—ñ–±—Ä–æ–≤–∞–Ω—ñ –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ $P(y=1|X)$
- –ö–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç–∏ —ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç—É—é—Ç—å—Å—è —á–µ—Ä–µ–∑ odds ratio: $\text{OR} = e^{\beta}$
- Threshold –º–æ–∂–Ω–∞ –Ω–∞–ª–∞—à—Ç–æ–≤—É–≤–∞—Ç–∏ –ø—ñ–¥ –∑–∞–¥–∞—á—É

**–§–æ—Ä–º—É–ª–∞ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è:**
$$P(y=1|X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x_1 + ... + \beta_p x_p)}}$$

**–ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏:**
- –ë—ñ–Ω–∞—Ä–Ω–∞ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è + —ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å + –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ = Logistic Regression ‚úì

---

#ml #supervised-learning #classification #logistic-regression #binary-classification
