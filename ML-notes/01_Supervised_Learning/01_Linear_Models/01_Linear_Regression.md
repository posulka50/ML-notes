# Linear Regression (–õ—ñ–Ω—ñ–π–Ω–∞ —Ä–µ–≥—Ä–µ—Å—ñ—è)

## –©–æ —Ü–µ?

**–õ—ñ–Ω—ñ–π–Ω–∞ —Ä–µ–≥—Ä–µ—Å—ñ—è** ‚Äî —Ü–µ –º–µ—Ç–æ–¥ supervised learning –¥–ª—è –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è **—á–∏—Å–ª–æ–≤–æ—ó** (–Ω–µ–ø–µ—Ä–µ—Ä–≤–Ω–æ—ó) –∑–º—ñ–Ω–Ω–æ—ó –Ω–∞ –æ—Å–Ω–æ–≤—ñ –æ–¥–Ω—ñ—î—ó –∞–±–æ –∫—ñ–ª—å–∫–æ—Ö –æ–∑–Ω–∞–∫ –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é –ª—ñ–Ω—ñ–π–Ω–æ—ó –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ.

**–ì–æ–ª–æ–≤–Ω–∞ —ñ–¥–µ—è:** –∑–Ω–∞–π—Ç–∏ –ø—Ä—è–º—É –ª—ñ–Ω—ñ—é (–∞–±–æ –≥—ñ–ø–µ—Ä–ø–ª–æ—â–∏–Ω—É –≤ –±–∞–≥–∞—Ç–æ–≤–∏–º—ñ—Ä–Ω–æ–º—É –ø—Ä–æ—Å—Ç–æ—Ä—ñ), —è–∫–∞ –Ω–∞–π–∫—Ä–∞—â–µ –æ–ø–∏—Å—É—î –∑–∞–ª–µ–∂–Ω—ñ—Å—Ç—å –º—ñ–∂ –≤—Ö—ñ–¥–Ω–∏–º–∏ –æ–∑–Ω–∞–∫–∞–º–∏ (X) —Ç–∞ —Ü—ñ–ª—å–æ–≤–æ—é –∑–º—ñ–Ω–Ω–æ—é (y).

## –ù–∞–≤—ñ—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–∞?

- üìà **–ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è** ‚Äî —Ü—ñ–Ω–∏ –±—É–¥–∏–Ω–∫—ñ–≤, –∑–∞—Ä–ø–ª–∞—Ç–∏, —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∏, –ø—Ä–æ–¥–∞–∂—ñ–≤
- üìä **–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü—ñ—è** ‚Äî –∑—Ä–æ–∑—É–º—ñ—Ç–∏ –≤–ø–ª–∏–≤ –∫–æ–∂–Ω–æ—ó –æ–∑–Ω–∞–∫–∏ –Ω–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç
- üéØ **Baseline** ‚Äî –ø—Ä–æ—Å—Ç–∞ –º–æ–¥–µ–ª—å –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –∑ –±—ñ–ª—å—à —Å–∫–ª–∞–¥–Ω–∏–º–∏
- üî¨ **–î–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è** ‚Äî –≤–∏—è–≤–∏—Ç–∏ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ –º—ñ–∂ –∑–º—ñ–Ω–Ω–∏–º–∏

## –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏?

**–ü–æ—Ç—Ä—ñ–±–Ω–æ:**
- –ó–∞–ª–µ–∂–Ω—ñ—Å—Ç—å –º—ñ–∂ X —Ç–∞ y **–ø—Ä–∏–±–ª–∏–∑–Ω–æ –ª—ñ–Ω—ñ–π–Ω–∞**
- –ü–æ—Ç—Ä—ñ–±–Ω–∞ **—ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å** —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
- –ù–µ–≤–µ–ª–∏–∫–∏–π –Ω–∞–±—ñ—Ä –¥–∞–Ω–∏—Ö
- –®–≤–∏–¥–∫–µ –Ω–∞–≤—á–∞–Ω–Ω—è —Ç–∞ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è

**–ù–µ –ø–æ—Ç—Ä—ñ–±–Ω–æ:**
- –ó–∞–ª–µ–∂–Ω—ñ—Å—Ç—å **–Ω–µ–ª—ñ–Ω—ñ–π–Ω–∞** ‚Üí Polynomial Regression, Tree-based models
- –ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è ‚Üí Logistic Regression
- –°–∫–ª–∞–¥–Ω—ñ –≤–∑–∞—î–º–æ–¥—ñ—ó –º—ñ–∂ –æ–∑–Ω–∞–∫–∞–º–∏ ‚Üí Neural Networks, Random Forest

---

## –ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞

### –ü—Ä–æ—Å—Ç–∞ –ª—ñ–Ω—ñ–π–Ω–∞ —Ä–µ–≥—Ä–µ—Å—ñ—è (–æ–¥–Ω–∞ –æ–∑–Ω–∞–∫–∞)

$$y = \beta_0 + \beta_1 x + \varepsilon$$

–¥–µ:
- $y$ ‚Äî —Ü—ñ–ª—å–æ–≤–∞ –∑–º—ñ–Ω–Ω–∞ (—Ç–µ, —â–æ –ø–µ—Ä–µ–¥–±–∞—á–∞—î–º–æ)
- $x$ ‚Äî –≤—Ö—ñ–¥–Ω–∞ –æ–∑–Ω–∞–∫–∞
- $\beta_0$ ‚Äî intercept (–∑–º—ñ—â–µ–Ω–Ω—è, –∫–æ–Ω—Å—Ç–∞–Ω—Ç–∞)
- $\beta_1$ ‚Äî slope (–Ω–∞—Ö–∏–ª, –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç)
- $\varepsilon$ ‚Äî –ø–æ–º–∏–ª–∫–∞ (noise)

### –ú–Ω–æ–∂–∏–Ω–Ω–∞ –ª—ñ–Ω—ñ–π–Ω–∞ —Ä–µ–≥—Ä–µ—Å—ñ—è (–±–∞–≥–∞—Ç–æ –æ–∑–Ω–∞–∫)

$$y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_p x_p + \varepsilon$$

–ê–±–æ —É –≤–µ–∫—Ç–æ—Ä–Ω—ñ–π —Ñ–æ—Ä–º—ñ:
$$y = \beta^T X + \varepsilon$$

–¥–µ:
- $p$ ‚Äî –∫—ñ–ª—å–∫—ñ—Å—Ç—å –æ–∑–Ω–∞–∫
- $\beta = [\beta_0, \beta_1, ..., \beta_p]^T$ ‚Äî –≤–µ–∫—Ç–æ—Ä –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç—ñ–≤
- $X = [1, x_1, x_2, ..., x_p]^T$ ‚Äî –≤–µ–∫—Ç–æ—Ä –æ–∑–Ω–∞–∫ (1 –¥–ª—è intercept)

---

## –ú–µ—Ç–æ–¥ –Ω–∞–π–º–µ–Ω—à–∏—Ö –∫–≤–∞–¥—Ä–∞—Ç—ñ–≤ (–ú–ù–ö)

### –Ü–¥–µ—è

–ó–Ω–∞–π—Ç–∏ —Ç–∞–∫—ñ –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç–∏ $\beta$, —â–æ–± **–º—ñ–Ω—ñ–º—ñ–∑—É–≤–∞—Ç–∏ —Å—É–º—É –∫–≤–∞–¥—Ä–∞—Ç—ñ–≤ –ø–æ–º–∏–ª–æ–∫** (–≤—ñ–¥—Å—Ç–∞–Ω–µ–π –≤—ñ–¥ —Ç–æ—á–æ–∫ –¥–æ –ª—ñ–Ω—ñ—ó).

### –§—É–Ω–∫—Ü—ñ—è –≤—Ç—Ä–∞—Ç (MSE - Mean Squared Error)

$$J(\beta) = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 = \frac{1}{n} \sum_{i=1}^{n} (y_i - \beta^T X_i)^2$$

–¥–µ:
- $n$ ‚Äî –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Å–ø–æ—Å—Ç–µ—Ä–µ–∂–µ–Ω—å
- $y_i$ ‚Äî —Ä–µ–∞–ª—å–Ω–µ –∑–Ω–∞—á–µ–Ω–Ω—è
- $\hat{y}_i$ ‚Äî –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–µ –∑–Ω–∞—á–µ–Ω–Ω—è

### –ê–Ω–∞–ª—ñ—Ç–∏—á–Ω–µ —Ä—ñ—à–µ–Ω–Ω—è (Normal Equation)

$$\beta = (X^T X)^{-1} X^T y$$

**–ü–µ—Ä–µ–≤–∞–≥–∏:**
- ‚úÖ –¢–æ—á–Ω–µ —Ä—ñ—à–µ–Ω–Ω—è –∑–∞ –æ–¥–Ω—É –æ–ø–µ—Ä–∞—Ü—ñ—é
- ‚úÖ –ù–µ –ø–æ—Ç—Ä—ñ–±–µ–Ω –ø—ñ–¥–±—ñ—Ä –≥—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤

**–ù–µ–¥–æ–ª—ñ–∫–∏:**
- ‚ùå –ü–æ–≤—ñ–ª—å–Ω–æ –¥–ª—è –≤–µ–ª–∏–∫–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç—ñ–≤ (–æ–±–µ—Ä–Ω–µ–Ω–Ω—è –º–∞—Ç—Ä–∏—Ü—ñ O(p¬≥))
- ‚ùå –ú–æ–∂–µ –±—É—Ç–∏ –Ω–µ—Å—Ç–∞–±—ñ–ª—å–Ω–∏–º –ø—Ä–∏ –º—É–ª—å—Ç–∏–∫–æ–ª—ñ–Ω–µ–∞—Ä–Ω–æ—Å—Ç—ñ

---

## –ü—Ä–æ—Å—Ç–∏–π –ø—Ä–∏–∫–ª–∞–¥: –¶—ñ–Ω–∞ vs –ü–ª–æ—â–∞ –∫–≤–∞—Ä—Ç–∏—Ä–∏

### –î–∞–Ω—ñ

| –ü–ª–æ—â–∞ (–º¬≤) | –¶—ñ–Ω–∞ (—Ç–∏—Å. $) |
|------------|---------------|
| 40 | 80 |
| 50 | 95 |
| 60 | 110 |
| 70 | 120 |
| 80 | 140 |

### –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç—ñ–≤ (–ú–ù–ö)

–î–ª—è –ø—Ä–æ—Å—Ç–æ—ó –ª—ñ–Ω—ñ–π–Ω–æ—ó —Ä–µ–≥—Ä–µ—Å—ñ—ó:

$$\beta_1 = \frac{n\sum xy - \sum x \sum y}{n\sum x^2 - (\sum x)^2}$$

$$\beta_0 = \frac{\sum y - \beta_1\sum x}{n}$$

**–û–±—á–∏—Å–ª–µ–Ω–Ω—è:**
- $n = 5$
- $\sum x = 300$
- $\sum y = 545$
- $\sum xy = 34350$
- $\sum x^2 = 19000$

$$\beta_1 = \frac{5 \times 34350 - 300 \times 545}{5 \times 19000 - 300^2} = \frac{8250}{5000} = 1.65$$

$$\beta_0 = \frac{545 - 1.65 \times 300}{5} = \frac{50}{5} = 10$$

### –†–µ–∑—É–ª—å—Ç–∞—Ç

$$\text{–¶—ñ–Ω–∞} = 10 + 1.65 \times \text{–ü–ª–æ—â–∞}$$

**–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü—ñ—è:**
- **$\beta_0 = 10$**: –±–∞–∑–æ–≤–∞ –≤–∞—Ä—Ç—ñ—Å—Ç—å (—É–º–æ–≤–Ω–∞) —Å—Ç–∞–Ω–æ–≤–∏—Ç—å 10 —Ç–∏—Å. $
- **$\beta_1 = 1.65$**: –∫–æ–∂–µ–Ω –¥–æ–¥–∞—Ç–∫–æ–≤–∏–π –º¬≤ –¥–æ–¥–∞—î 1.65 —Ç–∏—Å. $ –¥–æ —Ü—ñ–Ω–∏

### –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è

–î–ª—è –∫–≤–∞—Ä—Ç–∏—Ä–∏ 60 –º¬≤:
$$\hat{y} = 10 + 1.65 \times 60 = 10 + 99 = 109 \text{ —Ç–∏—Å. \$}$$

–†–µ–∞–ª—å–Ω–µ –∑–Ω–∞—á–µ–Ω–Ω—è: 110 —Ç–∏—Å. $ ‚Üí **–ø–æ–º–∏–ª–∫–∞ –ª–∏—à–µ 1 —Ç–∏—Å. $** ‚úì

---

## –°–∫–ª–∞–¥–Ω–∏–π –ø—Ä–∏–∫–ª–∞–¥: –ó–∞—Ä–ø–ª–∞—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤—ñ –±–∞–≥–∞—Ç—å–æ—Ö –æ–∑–Ω–∞–∫

### –î–∞–Ω—ñ

100 –ø—Ä–∞—Ü—ñ–≤–Ω–∏–∫—ñ–≤ –∑ –æ–∑–Ω–∞–∫–∞–º–∏:

| –û–∑–Ω–∞–∫–∞ | –û–ø–∏—Å | –î—ñ–∞–ø–∞–∑–æ–Ω |
|--------|------|----------|
| Years_Experience | –î–æ—Å–≤—ñ–¥ (—Ä–æ–∫–∏) | 0-20 |
| Education_Level | –û—Å–≤—ñ—Ç–∞ (1=—à–∫–æ–ª–∞, 2=–±–∞–∫–∞–ª–∞–≤—Ä, 3=–º–∞–≥—ñ—Å—Ç—Ä, 4=PhD) | 1-4 |
| Age | –í—ñ–∫ | 22-65 |
| Department | –í—ñ–¥–¥—ñ–ª (0=IT, 1=Sales, 2=HR) | 0-2 |
| **Salary** | –ó–∞—Ä–ø–ª–∞—Ç–∞ (—Ç–∏—Å. $) | 30-150 |

### –ú–æ–¥–µ–ª—å

$$\text{Salary} = \beta_0 + \beta_1 \times \text{Experience} + \beta_2 \times \text{Education} + \beta_3 \times \text{Age} + \beta_4 \times \text{Department}$$

### –†–µ–∑—É–ª—å—Ç–∞—Ç –Ω–∞–≤—á–∞–Ω–Ω—è

–ü—ñ—Å–ª—è –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è –ú–ù–ö:

$$\text{Salary} = 25 + 3.2 \times \text{Experience} + 8.5 \times \text{Education} + 0.5 \times \text{Age} - 5 \times \text{Department}$$

**–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü—ñ—è –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç—ñ–≤:**
- **$\beta_0 = 25$**: –±–∞–∑–æ–≤–∞ –∑–∞—Ä–ø–ª–∞—Ç–∞ 25 —Ç–∏—Å. $
- **$\beta_1 = 3.2$**: –∫–æ–∂–µ–Ω —Ä—ñ–∫ –¥–æ—Å–≤—ñ–¥—É –¥–æ–¥–∞—î 3.2 —Ç–∏—Å. $
- **$\beta_2 = 8.5$**: –∫–æ–∂–µ–Ω —Ä—ñ–≤–µ–Ω—å –æ—Å–≤—ñ—Ç–∏ –¥–æ–¥–∞—î 8.5 —Ç–∏—Å. $
- **$\beta_3 = 0.5$**: –∫–æ–∂–µ–Ω —Ä—ñ–∫ –≤—ñ–∫—É –¥–æ–¥–∞—î 0.5 —Ç–∏—Å. $
- **$\beta_4 = -5$**: –≤—ñ–¥–¥—ñ–ª –≤–ø–ª–∏–≤–∞—î (IT=0 –±–∞–∑–æ–≤–∏–π, Sales=-5, HR=-10)

### –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è

**–ü—Ä–∞—Ü—ñ–≤–Ω–∏–∫:** –¥–æ—Å–≤—ñ–¥ 5 —Ä–æ–∫—ñ–≤, –±–∞–∫–∞–ª–∞–≤—Ä (2), –≤—ñ–∫ 30, IT (0)

$$\hat{y} = 25 + 3.2(5) + 8.5(2) + 0.5(30) - 5(0)$$
$$\hat{y} = 25 + 16 + 17 + 15 + 0 = 73 \text{ —Ç–∏—Å. \$}$$

### –ú–µ—Ç—Ä–∏–∫–∏ —è–∫–æ—Å—Ç—ñ

- **R¬≤ = 0.82** ‚Äî –º–æ–¥–µ–ª—å –ø–æ—è—Å–Ω—é—î 82% –≤–∞—Ä—ñ–∞—Ü—ñ—ó –∑–∞—Ä–ø–ª–∞—Ç
- **RMSE = 8.5 —Ç–∏—Å. $** ‚Äî —Å–µ—Ä–µ–¥–Ω—è –ø–æ–º–∏–ª–∫–∞ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
- **MAE = 6.2 —Ç–∏—Å. $** ‚Äî —Å–µ—Ä–µ–¥–Ω—î –∞–±—Å–æ–ª—é—Ç–Ω–µ –≤—ñ–¥—Ö–∏–ª–µ–Ω–Ω—è

---

## –ü—Ä–∏–ø—É—â–µ–Ω–Ω—è –ª—ñ–Ω—ñ–π–Ω–æ—ó —Ä–µ–≥—Ä–µ—Å—ñ—ó

–î–ª—è –∫–æ—Ä–µ–∫—Ç–Ω–æ—ó —Ä–æ–±–æ—Ç–∏ –ú–ù–ö –ø–æ—Ç—Ä—ñ–±–Ω–æ, —â–æ–± –≤–∏–∫–æ–Ω—É–≤–∞–ª–∏—Å—å **–ø—Ä–∏–ø—É—â–µ–Ω–Ω—è –ì–∞—É—Å—Å–∞-–ú–∞—Ä–∫–æ–≤–∞**:

### 1. –õ—ñ–Ω—ñ–π–Ω—ñ—Å—Ç—å

–ó–∞–ª–µ–∂–Ω—ñ—Å—Ç—å –º—ñ–∂ X —Ç–∞ y **–ª—ñ–Ω—ñ–π–Ω–∞**.

**–ü–µ—Ä–µ–≤—ñ—Ä–∫–∞:** scatter plot, residual plot

**–ü–æ—Ä—É—à–µ–Ω–Ω—è ‚Üí –†—ñ—à–µ–Ω–Ω—è:** polynomial features, –Ω–µ–ª—ñ–Ω—ñ–π–Ω—ñ –º–æ–¥–µ–ª—ñ

### 2. –ù–µ–∑–∞–ª–µ–∂–Ω—ñ—Å—Ç—å –ø–æ–º–∏–ª–æ–∫

–ü–æ–º–∏–ª–∫–∏ $\varepsilon_i$ **–Ω–µ–∑–∞–ª–µ–∂–Ω—ñ** –æ–¥–Ω–∞ –≤—ñ–¥ –æ–¥–Ω–æ—ó.

**–ü–µ—Ä–µ–≤—ñ—Ä–∫–∞:** Durbin-Watson test (–¥–ª—è —á–∞—Å–æ–≤–∏—Ö —Ä—è–¥—ñ–≤)

**–ü–æ—Ä—É—à–µ–Ω–Ω—è ‚Üí –†—ñ—à–µ–Ω–Ω—è:** time series models, autoregressive models

### 3. –ì–æ–º–æ—Å–∫–µ–¥–∞—Å—Ç–∏—á–Ω—ñ—Å—Ç—å

**Variance –ø–æ–º–∏–ª–æ–∫ –æ–¥–Ω–∞–∫–æ–≤–∞** –¥–ª—è –≤—Å—ñ—Ö –∑–Ω–∞—á–µ–Ω—å X.

```
–ì–æ–º–æ—Å–∫–µ–¥–∞—Å—Ç–∏—á–Ω—ñ—Å—Ç—å (–¥–æ–±—Ä–µ):
residuals
    |  . . . .
    |. . . . .
    | . . . .
    |________ y_pred

–ì–µ—Ç–µ—Ä–æ—Å–∫–µ–¥–∞—Å—Ç–∏—á–Ω—ñ—Å—Ç—å (–ø–æ–≥–∞–Ω–æ):
residuals
    |      . .
    |    . . .
    |  . .
    | .
    |________ y_pred
```

**–ü–µ—Ä–µ–≤—ñ—Ä–∫–∞:** residual plot, Breusch-Pagan test

**–ü–æ—Ä—É—à–µ–Ω–Ω—è ‚Üí –†—ñ—à–µ–Ω–Ω—è:** weighted least squares, log transformation

### 4. –ù–æ—Ä–º–∞–ª—å–Ω—ñ—Å—Ç—å –ø–æ–º–∏–ª–æ–∫

–ü–æ–º–∏–ª–∫–∏ —Ä–æ–∑–ø–æ–¥—ñ–ª–µ–Ω—ñ **–Ω–æ—Ä–º–∞–ª—å–Ω–æ** $\varepsilon \sim N(0, \sigma^2)$.

**–ü–µ—Ä–µ–≤—ñ—Ä–∫–∞:** Q-Q plot, Shapiro-Wilk test

**–ü–æ—Ä—É—à–µ–Ω–Ω—è ‚Üí –†—ñ—à–µ–Ω–Ω—è:** —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ—è y (log, sqrt), robust regression

### 5. –í—ñ–¥—Å—É—Ç–Ω—ñ—Å—Ç—å –º—É–ª—å—Ç–∏–∫–æ–ª—ñ–Ω–µ–∞—Ä–Ω–æ—Å—Ç—ñ

–û–∑–Ω–∞–∫–∏ **–Ω–µ —Å–∏–ª—å–Ω–æ –∫–æ—Ä–µ–ª—é—é—Ç—å** –º—ñ–∂ —Å–æ–±–æ—é.

**–ü–µ—Ä–µ–≤—ñ—Ä–∫–∞:** correlation matrix, VIF (Variance Inflation Factor)

**–ü–æ—Ä—É—à–µ–Ω–Ω—è ‚Üí –†—ñ—à–µ–Ω–Ω—è:** –≤–∏–¥–∞–ª–∏—Ç–∏ –∫–æ—Ä–µ–ª—å–æ–≤–∞–Ω—ñ –æ–∑–Ω–∞–∫–∏, PCA, regularization (Ridge)

---

## –ú–µ—Ç—Ä–∏–∫–∏ –æ—Ü—ñ–Ω–∫–∏

### 1. MSE (Mean Squared Error)

$$\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$$

- **–Ü–Ω—Ç—É—ó—Ü—ñ—è:** —Å–µ—Ä–µ–¥–Ω—ñ–π –∫–≤–∞–¥—Ä–∞—Ç –ø–æ–º–∏–ª–∫–∏
- **–í–ª–∞—Å—Ç–∏–≤–æ—Å—Ç—ñ:** —Å–∏–ª—å–Ω–æ –∫–∞—Ä–∞—î –≤–µ–ª–∏–∫—ñ –ø–æ–º–∏–ª–∫–∏
- **–í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è:** —Ñ—É–Ω–∫—Ü—ñ—è –≤—Ç—Ä–∞—Ç –ø—Ä–∏ –Ω–∞–≤—á–∞–Ω–Ω—ñ

### 2. RMSE (Root Mean Squared Error)

$$\text{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}$$

- **–Ü–Ω—Ç—É—ó—Ü—ñ—è:** –≤ —Ç–∏—Ö –∂–µ –æ–¥–∏–Ω–∏—Ü—è—Ö, —â–æ –π y
- **–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü—ñ—è:** "—Å–µ—Ä–µ–¥–Ω—è –ø–æ–º–∏–ª–∫–∞ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è"
- **–í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è:** –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –º–æ–¥–µ–ª–µ–π

### 3. MAE (Mean Absolute Error)

$$\text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|$$

- **–Ü–Ω—Ç—É—ó—Ü—ñ—è:** —Å–µ—Ä–µ–¥–Ω—è –∞–±—Å–æ–ª—é—Ç–Ω–∞ –ø–æ–º–∏–ª–∫–∞
- **–í–ª–∞—Å—Ç–∏–≤–æ—Å—Ç—ñ:** –º–µ–Ω—à —á—É—Ç–ª–∏–≤–∞ –¥–æ –≤–∏–∫–∏–¥—ñ–≤, –Ω—ñ–∂ MSE
- **–í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è:** –∫–æ–ª–∏ –≤–∏–∫–∏–¥–∏ –Ω–µ –ø–æ–≤–∏–Ω–Ω—ñ —Å–∏–ª—å–Ω–æ –≤–ø–ª–∏–≤–∞—Ç–∏

### 4. R¬≤ (Coefficient of Determination)

$$R^2 = 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2} = 1 - \frac{SS_{res}}{SS_{tot}}$$

- **–î—ñ–∞–ø–∞–∑–æ–Ω:** $(-\infty, 1]$ (–∑–∞–∑–≤–∏—á–∞–π $[0, 1]$)
- **–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü—ñ—è:** —á–∞—Å—Ç–∫–∞ –¥–∏—Å–ø–µ—Ä—Å—ñ—ó y, –ø–æ—è—Å–Ω–µ–Ω–∞ –º–æ–¥–µ–ª–ª—é
- **–ó–Ω–∞—á–µ–Ω–Ω—è:**
  - R¬≤ = 1 ‚Üí —ñ–¥–µ–∞–ª—å–Ω–∞ –º–æ–¥–µ–ª—å
  - R¬≤ = 0 ‚Üí –º–æ–¥–µ–ª—å –Ω–µ –∫—Ä–∞—â–µ —Å–µ—Ä–µ–¥–Ω—å–æ–≥–æ
  - R¬≤ < 0 ‚Üí –º–æ–¥–µ–ª—å –≥—ñ—Ä—à–∞ –∑–∞ –∫–æ–Ω—Å—Ç–∞–Ω—Ç—É $\bar{y}$

### 5. Adjusted R¬≤ (–°–∫–æ—Ä–∏–≥–æ–≤–∞–Ω–∏–π R¬≤)

$$\bar{R}^2 = 1 - (1 - R^2) \frac{n-1}{n-p-1}$$

–¥–µ $p$ ‚Äî –∫—ñ–ª—å–∫—ñ—Å—Ç—å –æ–∑–Ω–∞–∫.

- **–ü—Ä–∏–∑–Ω–∞—á–µ–Ω–Ω—è:** –≤—Ä–∞—Ö–æ–≤—É—î –∫—ñ–ª—å–∫—ñ—Å—Ç—å –æ–∑–Ω–∞–∫
- **–í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è:** –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –º–æ–¥–µ–ª–µ–π –∑ —Ä—ñ–∑–Ω–æ—é –∫—ñ–ª—å–∫—ñ—Å—Ç—é –æ–∑–Ω–∞–∫
- **–í–ª–∞—Å—Ç–∏–≤–æ—Å—Ç—ñ:** –Ω–µ –∑—Ä–æ—Å—Ç–∞—î –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –ø—Ä–∏ –¥–æ–¥–∞–≤–∞–Ω–Ω—ñ –æ–∑–Ω–∞–∫

---

## –ö–æ–¥ (Python + scikit-learn)

### –ü—Ä–æ—Å—Ç–∏–π –ø—Ä–∏–∫–ª–∞–¥

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# –î–∞–Ω—ñ
X = np.array([40, 50, 60, 70, 80]).reshape(-1, 1)  # –ü–ª–æ—â–∞
y = np.array([80, 95, 110, 120, 140])              # –¶—ñ–Ω–∞

# –ú–æ–¥–µ–ª—å
model = LinearRegression()
model.fit(X, y)

# –ö–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç–∏
print(f"Intercept (Œ≤‚ÇÄ): {model.intercept_:.2f}")        # 10.00
print(f"Slope (Œ≤‚ÇÅ): {model.coef_[0]:.2f}")              # 1.65

# –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
X_new = np.array([[60]])
y_pred = model.predict(X_new)
print(f"–ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –¥–ª—è 60 –º¬≤: {y_pred[0]:.2f} —Ç–∏—Å. $")  # 109.00

# –ú–µ—Ç—Ä–∏–∫–∏
y_pred_all = model.predict(X)
print(f"R¬≤: {r2_score(y, y_pred_all):.4f}")
print(f"RMSE: {np.sqrt(mean_squared_error(y, y_pred_all)):.2f}")

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
plt.scatter(X, y, color='blue', label='–î–∞–Ω—ñ')
plt.plot(X, y_pred_all, color='red', linewidth=2, label='–†–µ–≥—Ä–µ—Å—ñ—è')
plt.xlabel('–ü–ª–æ—â–∞ (–º¬≤)')
plt.ylabel('–¶—ñ–Ω–∞ (—Ç–∏—Å. $)')
plt.legend()
plt.title('–õ—ñ–Ω—ñ–π–Ω–∞ —Ä–µ–≥—Ä–µ—Å—ñ—è: –¶—ñ–Ω–∞ vs –ü–ª–æ—â–∞')
plt.grid(True, alpha=0.3)
plt.show()
```

### –ü–æ–≤–Ω–∏–π –ø—Ä–∏–∫–ª–∞–¥ (–±–∞–≥–∞—Ç–æ –æ–∑–Ω–∞–∫)

```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –¥–∞–Ω–∏—Ö
from sklearn.datasets import make_regression
X, y = make_regression(
    n_samples=1000,
    n_features=10,
    n_informative=7,
    noise=10,
    random_state=42
)

# –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –Ω–∞ train/test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è (–æ–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ, –∞–ª–µ —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–æ)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# –ú–æ–¥–µ–ª—å
model = LinearRegression()
model.fit(X_train_scaled, y_train)

# –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
y_train_pred = model.predict(X_train_scaled)
y_test_pred = model.predict(X_test_scaled)

# –û—Ü—ñ–Ω–∫–∞ –Ω–∞ train
print("=== Train Set ===")
print(f"R¬≤: {r2_score(y_train, y_train_pred):.4f}")
print(f"RMSE: {np.sqrt(mean_squared_error(y_train, y_train_pred)):.2f}")
print(f"MAE: {mean_absolute_error(y_train, y_train_pred):.2f}")

# –û—Ü—ñ–Ω–∫–∞ –Ω–∞ test
print("\n=== Test Set ===")
print(f"R¬≤: {r2_score(y_test, y_test_pred):.4f}")
print(f"RMSE: {np.sqrt(mean_squared_error(y_test, y_test_pred)):.2f}")
print(f"MAE: {mean_absolute_error(y_test, y_test_pred):.2f}")

# –ö–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç–∏
print(f"\nIntercept: {model.intercept_:.4f}")
print("Coefficients:")
for i, coef in enumerate(model.coef_):
    print(f"  Feature {i}: {coef:.4f}")

# –í–∞–∂–ª–∏–≤—ñ—Å—Ç—å –æ–∑–Ω–∞–∫ (–∞–±—Å–æ–ª—é—Ç–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç—ñ–≤)
feature_importance = pd.DataFrame({
    'feature': [f'X{i}' for i in range(X.shape[1])],
    'coefficient': model.coef_,
    'abs_coefficient': np.abs(model.coef_)
}).sort_values('abs_coefficient', ascending=False)

print("\n=== Feature Importance ===")
print(feature_importance)
```

### –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è residuals

```python
import seaborn as sns

# Residual plot
residuals = y_test - y_test_pred

fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# 1. Predicted vs Actual
axes[0].scatter(y_test, y_test_pred, alpha=0.5)
axes[0].plot([y_test.min(), y_test.max()], 
             [y_test.min(), y_test.max()], 
             'r--', lw=2)
axes[0].set_xlabel('Actual')
axes[0].set_ylabel('Predicted')
axes[0].set_title('Predicted vs Actual')
axes[0].grid(True, alpha=0.3)

# 2. Residuals vs Predicted
axes[1].scatter(y_test_pred, residuals, alpha=0.5)
axes[1].axhline(y=0, color='r', linestyle='--', lw=2)
axes[1].set_xlabel('Predicted')
axes[1].set_ylabel('Residuals')
axes[1].set_title('Residual Plot')
axes[1].grid(True, alpha=0.3)

# 3. Distribution of Residuals
axes[2].hist(residuals, bins=30, edgecolor='black', alpha=0.7)
axes[2].set_xlabel('Residuals')
axes[2].set_ylabel('Frequency')
axes[2].set_title('Distribution of Residuals')
axes[2].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Q-Q Plot (–ø–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–æ—Ä–º–∞–ª—å–Ω–æ—Å—Ç—ñ)
from scipy import stats

fig, ax = plt.subplots(figsize=(8, 6))
stats.probplot(residuals, dist="norm", plot=ax)
ax.set_title('Q-Q Plot (Normal Distribution)')
ax.grid(True, alpha=0.3)
plt.show()
```

---

## –ì—Ä–∞–¥—ñ—î–Ω—Ç–Ω–∏–π —Å–ø—É—Å–∫ (–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∏–π –º–µ—Ç–æ–¥ –Ω–∞–≤—á–∞–Ω–Ω—è)

–î–ª—è –≤–µ–ª–∏–∫–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç—ñ–≤ Normal Equation –ø–æ–≤—ñ–ª—å–Ω–∞. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ **Gradient Descent**.

### –ê–ª–≥–æ—Ä–∏—Ç–º

1. –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É–≤–∞—Ç–∏ $\beta$ –≤–∏–ø–∞–¥–∫–æ–≤–∏–º–∏ –∑–Ω–∞—á–µ–Ω–Ω—è–º–∏
2. –ü–æ–≤—Ç–æ—Ä—é–≤–∞—Ç–∏ –¥–æ –∑–±—ñ–∂–Ω–æ—Å—Ç—ñ:
   - –û–±—á–∏—Å–ª–∏—Ç–∏ –≥—Ä–∞–¥—ñ—î–Ω—Ç: $\nabla J(\beta) = -\frac{2}{n} X^T (y - X\beta)$
   - –û–Ω–æ–≤–∏—Ç–∏ –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç–∏: $\beta := \beta - \alpha \nabla J(\beta)$

–¥–µ $\alpha$ ‚Äî learning rate (—à–≤–∏–¥–∫—ñ—Å—Ç—å –Ω–∞–≤—á–∞–Ω–Ω—è).

### –ö–æ–¥ (–≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—á–∏ SGDRegressor)

```python
from sklearn.linear_model import SGDRegressor

model = SGDRegressor(
    max_iter=1000,
    tol=1e-3,
    learning_rate='invscaling',  # adaptive learning rate
    eta0=0.01,                    # –ø–æ—á–∞—Ç–∫–æ–≤–∏–π learning rate
    random_state=42
)

model.fit(X_train_scaled, y_train)

print(f"R¬≤ on test: {model.score(X_test_scaled, y_test):.4f}")
```

### –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –º–µ—Ç–æ–¥—ñ–≤

| –ú–µ—Ç–æ–¥ | –ü–µ—Ä–µ–≤–∞–≥–∏ | –ù–µ–¥–æ–ª—ñ–∫–∏ | –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ |
|-------|----------|----------|----------------------|
| **Normal Equation** | –¢–æ—á–Ω–µ —Ä—ñ—à–µ–Ω–Ω—è –∑–∞ 1 –∫—Ä–æ–∫ | –ü–æ–≤—ñ–ª—å–Ω–æ –¥–ª—è –≤–µ–ª–∏–∫–∏—Ö p (O(p¬≥)) | p < 10,000 |
| **Gradient Descent** | –®–≤–∏–¥–∫–æ –¥–ª—è –≤–µ–ª–∏–∫–∏—Ö –¥–∞–Ω–∏—Ö | –ü–æ—Ç—Ä—ñ–±–µ–Ω –ø—ñ–¥–±—ñ—Ä Œ±, –º–æ–∂–µ –∑–∞—Å—Ç—Ä—è–≥—Ç–∏ | n > 100,000 –∞–±–æ p > 10,000 |

---

## –ü–µ—Ä–µ–≤–∞–≥–∏ —Ç–∞ –Ω–µ–¥–æ–ª—ñ–∫–∏

### –ü–µ—Ä–µ–≤–∞–≥–∏ ‚úì

| –ü–µ—Ä–µ–≤–∞–≥–∞ | –ü–æ—è—Å–Ω–µ–Ω–Ω—è |
|----------|-----------|
| **–ü—Ä–æ—Å—Ç–æ—Ç–∞** | –õ–µ–≥–∫–æ –∑—Ä–æ–∑—É–º—ñ—Ç–∏ —Ç–∞ —Ä–µ–∞–ª—ñ–∑—É–≤–∞—Ç–∏ |
| **–®–≤–∏–¥–∫—ñ—Å—Ç—å** | –®–≤–∏–¥–∫–µ –Ω–∞–≤—á–∞–Ω–Ω—è —Ç–∞ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è |
| **–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å** | –ö–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç–∏ –º–∞—é—Ç—å —á—ñ—Ç–∫–∏–π –∑–º—ñ—Å—Ç |
| **–ù–µ –ø–æ—Ç—Ä–µ–±—É—î –±–∞–≥–∞—Ç–æ –¥–∞–Ω–∏—Ö** | –ü—Ä–∞—Ü—é—î –Ω–∞–≤—ñ—Ç—å –Ω–∞ –º–∞–ª–∏—Ö –≤–∏–±—ñ—Ä–∫–∞—Ö |
| **Baseline** | –•–æ—Ä–æ—à–∞ —Å—Ç–∞—Ä—Ç–æ–≤–∞ –º–æ–¥–µ–ª—å –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è |
| **–¢–µ–æ—Ä–µ—Ç–∏—á–Ω–∞ –±–∞–∑–∞** | –ú–∞—Ç–µ–º–∞—Ç–∏—á–Ω–æ –æ–±“ë—Ä—É–Ω—Ç–æ–≤–∞–Ω–∞ |

### –ù–µ–¥–æ–ª—ñ–∫–∏ ‚úó

| –ù–µ–¥–æ–ª—ñ–∫ | –ü–æ—è—Å–Ω–µ–Ω–Ω—è |
|---------|-----------|
| **–õ—ñ–Ω—ñ–π–Ω—ñ—Å—Ç—å** | –ü–µ—Ä–µ–¥–±–∞—á–∞—î –ª—ñ–Ω—ñ–π–Ω—É –∑–∞–ª–µ–∂–Ω—ñ—Å—Ç—å |
| **–ß—É—Ç–ª–∏–≤—ñ—Å—Ç—å –¥–æ –≤–∏–∫–∏–¥—ñ–≤** | MSE —Å–∏–ª—å–Ω–æ –∫–∞—Ä–∞—î –≤–µ–ª–∏–∫—ñ –ø–æ–º–∏–ª–∫–∏ |
| **–ú—É–ª—å—Ç–∏–∫–æ–ª—ñ–Ω–µ–∞—Ä–Ω—ñ—Å—Ç—å** | –ü—Ä–æ–±–ª–µ–º–∏ –ø—Ä–∏ —Å–∏–ª—å–Ω—ñ–π –∫–æ—Ä–µ–ª—è—Ü—ñ—ó –æ–∑–Ω–∞–∫ |
| **–ü—Ä–∏–ø—É—â–µ–Ω–Ω—è** | –ü–æ—Ç—Ä–µ–±—É—î –≤–∏–∫–æ–Ω–∞–Ω–Ω—è –ø—Ä–∏–ø—É—â–µ–Ω—å –ì–∞—É—Å—Å–∞-–ú–∞—Ä–∫–æ–≤–∞ |
| **Feature engineering** | –ü–æ—Ç—Ä—ñ–±–Ω–∞ —Ä—É—á–Ω–∞ –ø—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –æ–∑–Ω–∞–∫ |

---

## –†–æ–∑–≤'—è–∑–∞–Ω–Ω—è –ø—Ä–æ–±–ª–µ–º

### 1. –ù–µ–ª—ñ–Ω—ñ–π–Ω–∞ –∑–∞–ª–µ–∂–Ω—ñ—Å—Ç—å

**–°–∏–º–ø—Ç–æ–º–∏:** –Ω–∏–∑—å–∫–∏–π R¬≤, residual plot –ø–æ–∫–∞–∑—É—î –ø–∞—Ç—Ç–µ—Ä–Ω

**–†—ñ—à–µ–Ω–Ω—è:**
- Polynomial features: $x, x^2, x^3$
- Log transformation: $\log(y), \log(x)$
- Interaction terms: $x_1 \times x_2$
- –ù–µ–ª—ñ–Ω—ñ–π–Ω—ñ –º–æ–¥–µ–ª—ñ: Random Forest, Neural Networks

### 2. –ú—É–ª—å—Ç–∏–∫–æ–ª—ñ–Ω–µ–∞—Ä–Ω—ñ—Å—Ç—å

**–°–∏–º–ø—Ç–æ–º–∏:** –≤–µ–ª–∏–∫—ñ –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç–∏, VIF > 10

**–†—ñ—à–µ–Ω–Ω—è:**
- –í–∏–¥–∞–ª–∏—Ç–∏ –∫–æ—Ä–µ–ª—å–æ–≤–∞–Ω—ñ –æ–∑–Ω–∞–∫–∏
- PCA (Principal Component Analysis)
- Ridge regression (L2 regularization)

### 3. –í–∏–∫–∏–¥–∏ (Outliers)

**–°–∏–º–ø—Ç–æ–º–∏:** residuals –∑ –≤–µ–ª–∏–∫–∏–º–∏ –∑–Ω–∞—á–µ–Ω–Ω—è–º–∏

**–†—ñ—à–µ–Ω–Ω—è:**
- –í–∏–¥–∞–ª–∏—Ç–∏ –≤–∏–∫–∏–¥–∏ (—è–∫—â–æ —Ü–µ –ø–æ–º–∏–ª–∫–∏)
- Robust regression (Huber, RANSAC)
- Transform y (log, sqrt)

### 4. –ì–µ—Ç–µ—Ä–æ—Å–∫–µ–¥–∞—Å—Ç–∏—á–Ω—ñ—Å—Ç—å

**–°–∏–º–ø—Ç–æ–º–∏:** variance residuals –∑—Ä–æ—Å—Ç–∞—î –∑ y_pred

**–†—ñ—à–µ–Ω–Ω—è:**
- Weighted Least Squares
- Transform y
- Generalized Linear Models (GLM)

### 5. Overfitting

**–°–∏–º–ø—Ç–æ–º–∏:** R¬≤_train >> R¬≤_test

**–†—ñ—à–µ–Ω–Ω—è:**
- Feature selection
- Regularization (Ridge, Lasso)
- Cross-validation
- –ë—ñ–ª—å—à–µ –¥–∞–Ω–∏—Ö

---

## –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ Linear Regression

### –Ü–¥–µ–∞–ª—å–Ω–æ –ø—ñ–¥—Ö–æ–¥–∏—Ç—å ‚úì

- –ó–∞–ª–µ–∂–Ω—ñ—Å—Ç—å **–ª—ñ–Ω—ñ–π–Ω–∞** –∞–±–æ –±–ª–∏–∑—å–∫–∞ –¥–æ –ª—ñ–Ω—ñ–π–Ω–æ—ó
- –ü–æ—Ç—Ä—ñ–±–Ω–∞ **—ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å** –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç—ñ–≤
- **–®–≤–∏–¥–∫—ñ—Å—Ç—å** –≤–∞–∂–ª–∏–≤–∞ (–Ω–∞–≤—á–∞–Ω–Ω—è —Ç–∞ inference)
- –ù–µ–≤–µ–ª–∏–∫–∏–π/—Å–µ—Ä–µ–¥–Ω—ñ–π –¥–∞—Ç–∞—Å–µ—Ç
- **Baseline** –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –∑ —ñ–Ω—à–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏
- –ü–æ—Ç—Ä—ñ–±–Ω—ñ **confidence intervals** –¥–ª—è –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω—å

### –ö—Ä–∞—â–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ —ñ–Ω—à–µ ‚úó

- –ó–∞–ª–µ–∂–Ω—ñ—Å—Ç—å **–Ω–µ–ª—ñ–Ω—ñ–π–Ω–∞** ‚Üí Polynomial Regression, Tree-based, Neural Networks
- –ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è ‚Üí Logistic Regression, Tree-based, SVM
- –ë–∞–≥–∞—Ç–æ **outliers** ‚Üí Robust Regression, Tree-based
- **Interaction effects** –≤–∞–∂–ª–∏–≤—ñ ‚Üí Tree-based, Neural Networks
- –î—É–∂–µ **–≤–µ–ª–∏–∫—ñ –¥–∞–Ω—ñ** ‚Üí SGD, online learning

---

## –ü—Ä–∞–∫—Ç–∏—á–Ω—ñ –ø–æ—Ä–∞–¥–∏ üí°

1. **–ó–∞–≤–∂–¥–∏ –≤—ñ–∑—É–∞–ª—ñ–∑—É–π –¥–∞–Ω—ñ** –ø–µ—Ä–µ–¥ –º–æ–¥–µ–ª—é–≤–∞–Ω–Ω—è–º (scatter plots)
2. **–ü–µ—Ä–µ–≤—ñ—Ä –ø—Ä–∏–ø—É—â–µ–Ω–Ω—è** ‚Äî residual plots, Q-Q plots
3. **–ù–æ—Ä–º–∞–ª—ñ–∑—É–π –æ–∑–Ω–∞–∫–∏** ‚Äî –æ—Å–æ–±–ª–∏–≤–æ —è–∫—â–æ –≤–æ–Ω–∏ –≤ —Ä—ñ–∑–Ω–∏—Ö —à–∫–∞–ª–∞—Ö
4. **–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π cross-validation** –¥–ª—è –æ—Ü—ñ–Ω–∫–∏ —è–∫–æ—Å—Ç—ñ
5. **–ü–æ—á–Ω–∏ –∑ –ø—Ä–æ—Å—Ç–æ—ó –º–æ–¥–µ–ª—ñ** ‚Äî –¥–æ–¥–∞–≤–∞–π —Å–∫–ª–∞–¥–Ω—ñ—Å—Ç—å –ø–æ—Å—Ç—É–ø–æ–≤–æ
6. **Feature engineering** ‚Äî —Å—Ç–≤–æ—Ä—é–π –∫–æ—Ä–∏—Å–Ω—ñ –æ–∑–Ω–∞–∫–∏
7. **Regularization** ‚Äî Ridge/Lasso –¥–æ–ø–æ–º–∞–≥–∞—é—Ç—å –ø—Ä–∏ –±–∞–≥–∞—Ç—å–æ—Ö –æ–∑–Ω–∞–∫–∞—Ö
8. **–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç—É–π –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç–∏** ‚Äî –≤–æ–Ω–∏ –º–∞—é—Ç—å –ø—Ä–∞–∫—Ç–∏—á–Ω–∏–π —Å–µ–Ω—Å
9. **–ü–µ—Ä–µ–≤—ñ—Ä –Ω–∞ overfitting** ‚Äî –ø–æ—Ä—ñ–≤–Ω—é–π train —Ç–∞ test –º–µ—Ç—Ä–∏–∫–∏
10. **–î–æ–∫—É–º–µ–Ω—Ç—É–π** ‚Äî –∑–∞–ø–∏—Å—É–π, —è–∫—ñ –æ–∑–Ω–∞–∫–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–≤ —Ç–∞ —á–æ–º—É

---

## –ü–æ–≤'—è–∑–∞–Ω—ñ —Ç–µ–º–∏

- [[02_Logistic_Regression]] ‚Äî –¥–ª—è –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó
- [[03_Regularization]] ‚Äî Ridge, Lasso, Elastic Net
- [[04_Polynomial_Regression]] ‚Äî –¥–ª—è –Ω–µ–ª—ñ–Ω—ñ–π–Ω–∏—Ö –∑–∞–ª–µ–∂–Ω–æ—Å—Ç–µ–π
- [[05_Gradient_Descent]] ‚Äî –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∏–π –º–µ—Ç–æ–¥ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó
- [[01_Feature_Scaling]] ‚Äî –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è –æ–∑–Ω–∞–∫
- [[Cross_Validation]] ‚Äî –æ—Ü—ñ–Ω–∫–∞ —è–∫–æ—Å—Ç—ñ –º–æ–¥–µ–ª—ñ

## –†–µ—Å—É—Ä—Å–∏

- [Scikit-learn: Linear Models](https://scikit-learn.org/stable/modules/linear_model.html)
- [Andrew Ng: Linear Regression](https://www.coursera.org/learn/machine-learning)
- [StatQuest: Linear Regression](https://www.youtube.com/watch?v=nk2CQITm_eo)

---

## –ö–ª—é—á–æ–≤—ñ –≤–∏—Å–Ω–æ–≤–∫–∏

> –õ—ñ–Ω—ñ–π–Ω–∞ —Ä–µ–≥—Ä–µ—Å—ñ—è ‚Äî —Ü–µ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º supervised learning, —è–∫–∏–π –º–æ–¥–µ–ª—é—î –ª—ñ–Ω—ñ–π–Ω—É –∑–∞–ª–µ–∂–Ω—ñ—Å—Ç—å –º—ñ–∂ –æ–∑–Ω–∞–∫–∞–º–∏ —Ç–∞ —Ü—ñ–ª—å–æ–≤–æ—é –∑–º—ñ–Ω–Ω–æ—é.

**–û—Å–Ω–æ–≤–Ω—ñ –ø—Ä–∏–Ω—Ü–∏–ø–∏:**
- –ú—ñ–Ω—ñ–º—ñ–∑—É—î —Å—É–º—É –∫–≤–∞–¥—Ä–∞—Ç—ñ–≤ –ø–æ–º–∏–ª–æ–∫ (MSE)
- –ú–∞—î –∞–Ω–∞–ª—ñ—Ç–∏—á–Ω–µ —Ä—ñ—à–µ–Ω–Ω—è —á–µ—Ä–µ–∑ Normal Equation
- –ö–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç–∏ –ª–µ–≥–∫–æ —ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç—É–≤–∞—Ç–∏
- –ü–æ—Ç—Ä–µ–±—É—î –≤–∏–∫–æ–Ω–∞–Ω–Ω—è –ø—Ä–∏–ø—É—â–µ–Ω—å –ì–∞—É—Å—Å–∞-–ú–∞—Ä–∫–æ–≤–∞

**–§–æ—Ä–º—É–ª–∞:**
$$y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_p x_p$$

**–ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏:**
- –õ—ñ–Ω—ñ–π–Ω–∞ –∑–∞–ª–µ–∂–Ω—ñ—Å—Ç—å + —ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å + —à–≤–∏–¥–∫—ñ—Å—Ç—å = Linear Regression ‚úì

---

#ml #supervised-learning #regression #linear-regression #baseline
