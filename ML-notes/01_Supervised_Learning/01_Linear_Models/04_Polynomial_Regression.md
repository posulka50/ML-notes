# Polynomial Regression (–ü–æ–ª—ñ–Ω–æ–º—ñ–∞–ª—å–Ω–∞ —Ä–µ–≥—Ä–µ—Å—ñ—è)

## –©–æ —Ü–µ?

**–ü–æ–ª—ñ–Ω–æ–º—ñ–∞–ª—å–Ω–∞ —Ä–µ–≥—Ä–µ—Å—ñ—è** ‚Äî —Ü–µ —Ä–æ–∑—à–∏—Ä–µ–Ω–Ω—è –ª—ñ–Ω—ñ–π–Ω–æ—ó —Ä–µ–≥—Ä–µ—Å—ñ—ó, —è–∫–µ –¥–æ–∑–≤–æ–ª—è—î –º–æ–¥–µ–ª—é–≤–∞—Ç–∏ **–Ω–µ–ª—ñ–Ω—ñ–π–Ω—ñ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ** —à–ª—è—Ö–æ–º –¥–æ–¥–∞–≤–∞–Ω–Ω—è –ø–æ–ª—ñ–Ω–æ–º—ñ–∞–ª—å–Ω–∏—Ö —á–ª–µ–Ω—ñ–≤ (—Å—Ç–µ–ø–µ–Ω—ñ–≤) –≤—Ö—ñ–¥–Ω–∏—Ö –æ–∑–Ω–∞–∫.

**–ì–æ–ª–æ–≤–Ω–∞ —ñ–¥–µ—è:** –ø–µ—Ä–µ—Ç–≤–æ—Ä–∏—Ç–∏ –Ω–µ–ª—ñ–Ω—ñ–π–Ω—É –∑–∞–¥–∞—á—É –≤ –ª—ñ–Ω—ñ–π–Ω—É —á–µ—Ä–µ–∑ feature engineering ‚Äî —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –Ω–æ–≤–∏—Ö –æ–∑–Ω–∞–∫ —è–∫ —Å—Ç–µ–ø–µ–Ω—ñ–≤ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏—Ö.

## –ù–∞–≤—ñ—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–∞?

- üìà **–ù–µ–ª—ñ–Ω—ñ–π–Ω—ñ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ** ‚Äî –º–æ–¥–µ–ª—é–≤–∞–Ω–Ω—è –∫—Ä–∏–≤–∏—Ö, –ø–∞—Ä–∞–±–æ–ª, S-–ø–æ–¥—ñ–±–Ω–∏—Ö –∑–∞–ª–µ–∂–Ω–æ—Å—Ç–µ–π
- üîÑ **–í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –ª—ñ–Ω—ñ–π–Ω–æ—ó —Ä–µ–≥—Ä–µ—Å—ñ—ó** ‚Äî —Ç—ñ –∂ –∞–ª–≥–æ—Ä–∏—Ç–º–∏, —ñ–Ω—à–∞ –ø—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö
- üéØ **–ö—Ä–∞—â—ñ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è** ‚Äî –∫–æ–ª–∏ –ª—ñ–Ω—ñ–π–Ω–∞ –º–æ–¥–µ–ª—å –∑–∞–Ω–∞–¥—Ç–æ –ø—Ä–æ—Å—Ç–∞
- üìä **–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å** ‚Äî –∑—Ä–æ–∑—É–º—ñ–ª—ñ—à–∞ –∑–∞ –Ω–µ–π—Ä–æ–Ω–Ω—ñ –º–µ—Ä–µ–∂—ñ
- üöÄ **–®–≤–∏–¥–∫—ñ—Å—Ç—å** ‚Äî —à–≤–∏–¥—à–µ –∑–∞ —Å–∫–ª–∞–¥–Ω—ñ –Ω–µ–ª—ñ–Ω—ñ–π–Ω—ñ –º–æ–¥–µ–ª—ñ

## –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏?

**–ü–æ—Ç—Ä—ñ–±–Ω–æ:**
- –ó–∞–ª–µ–∂–Ω—ñ—Å—Ç—å **—è–≤–Ω–æ –Ω–µ–ª—ñ–Ω—ñ–π–Ω–∞** (–∫—Ä–∏–≤–∞, –ø–∞—Ä–∞–±–æ–ª–∞)
- **Scatter plot** –ø–æ–∫–∞–∑—É—î –Ω–µ–ª—ñ–Ω—ñ–π–Ω–∏–π –ø–∞—Ç–µ—Ä–Ω
- –õ—ñ–Ω—ñ–π–Ω–∞ —Ä–µ–≥—Ä–µ—Å—ñ—è –¥–∞—î **–Ω–∏–∑—å–∫–∏–π R¬≤**
- **Residual plot** –ø–æ–∫–∞–∑—É—î —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–Ω–∏–π –ø–∞—Ç–µ—Ä–Ω
- –ü–æ—Ç—Ä—ñ–±–Ω–∞ **—ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å** (–Ω–∞ –≤—ñ–¥–º—ñ–Ω—É –≤—ñ–¥ –Ω–µ–π—Ä–æ–Ω–Ω–∏—Ö –º–µ—Ä–µ–∂)

**–ù–µ –ø–æ—Ç—Ä—ñ–±–Ω–æ:**
- –ó–∞–ª–µ–∂–Ω—ñ—Å—Ç—å **–ª—ñ–Ω—ñ–π–Ω–∞** ‚Üí Linear Regression
- –î—É–∂–µ —Å–∫–ª–∞–¥–Ω—ñ –Ω–µ–ª—ñ–Ω—ñ–π–Ω—ñ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ ‚Üí Random Forest, XGBoost, Neural Networks
- –ë–∞–≥–∞—Ç–æ –æ–∑–Ω–∞–∫ + –≤–∏—Å–æ–∫—ñ —Å—Ç–µ–ø–µ–Ω—ñ ‚Üí curse of dimensionality
- –ö–∞—Ç–µ–≥–æ—Ä—ñ–∞–ª—å–Ω–∞ –∑–º—ñ–Ω–Ω–∞ ‚Üí Logistic Regression

---

## –ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞

### –ü—Ä–æ—Å—Ç–∞ –ø–æ–ª—ñ–Ω–æ–º—ñ–∞–ª—å–Ω–∞ —Ä–µ–≥—Ä–µ—Å—ñ—è (1 –æ–∑–Ω–∞–∫–∞)

**–°—Ç–µ–ø—ñ–Ω—å 2 (–∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–∞):**
$$y = \beta_0 + \beta_1 x + \beta_2 x^2 + \varepsilon$$

**–°—Ç–µ–ø—ñ–Ω—å 3 (–∫—É–±—ñ—á–Ω–∞):**
$$y = \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3 + \varepsilon$$

**–°—Ç–µ–ø—ñ–Ω—å d (–∑–∞–≥–∞–ª—å–Ω–∏–π –≤–∏–ø–∞–¥–æ–∫):**
$$y = \beta_0 + \beta_1 x + \beta_2 x^2 + ... + \beta_d x^d + \varepsilon$$

### –ú–Ω–æ–∂–∏–Ω–Ω–∞ –ø–æ–ª—ñ–Ω–æ–º—ñ–∞–ª—å–Ω–∞ —Ä–µ–≥—Ä–µ—Å—ñ—è (–±–∞–≥–∞—Ç–æ –æ–∑–Ω–∞–∫)

**–î–ª—è –¥–≤–æ—Ö –æ–∑–Ω–∞–∫, —Å—Ç–µ–ø—ñ–Ω—å 2:**
$$y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_1^2 + \beta_4 x_2^2 + \beta_5 x_1 x_2 + \varepsilon$$

**–ó–∞–≥–∞–ª—å–Ω–∏–π –≤–∏–ø–∞–¥–æ–∫:**
- –í–∫–ª—é—á–∞—î –≤—Å—ñ –º–æ–∂–ª–∏–≤—ñ –∫–æ–º–±—ñ–Ω–∞—Ü—ñ—ó —Å—Ç–µ–ø–µ–Ω—ñ–≤ –¥–æ d
- **Interaction terms:** $x_1 x_2, x_1 x_2^2$, —Ç–æ—â–æ

### –ö—ñ–ª—å–∫—ñ—Å—Ç—å –æ–∑–Ω–∞–∫ –ø—ñ—Å–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ—ó

–î–ª—è $p$ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏—Ö –æ–∑–Ω–∞–∫ —Ç–∞ —Å—Ç–µ–ø–µ–Ω—è $d$:

$$\text{–ö—ñ–ª—å–∫—ñ—Å—Ç—å –Ω–æ–≤–∏—Ö –æ–∑–Ω–∞–∫} = \binom{p + d}{d} - 1$$

**–ü—Ä–∏–∫–ª–∞–¥:**
- $p=2$ –æ–∑–Ω–∞–∫–∏, $d=2$ —Å—Ç–µ–ø—ñ–Ω—å: $\binom{4}{2} - 1 = 5$ –Ω–æ–≤–∏—Ö –æ–∑–Ω–∞–∫
  - $x_1, x_2, x_1^2, x_2^2, x_1 x_2$

- $p=3$ –æ–∑–Ω–∞–∫–∏, $d=3$ —Å—Ç–µ–ø—ñ–Ω—å: $\binom{6}{3} - 1 = 19$ –Ω–æ–≤–∏—Ö –æ–∑–Ω–∞–∫

‚ö†Ô∏è **Curse of dimensionality:** –∫—ñ–ª—å–∫—ñ—Å—Ç—å –æ–∑–Ω–∞–∫ –∑—Ä–æ—Å—Ç–∞—î –µ–∫—Å–ø–æ–Ω–µ–Ω—Ü—ñ–∞–ª—å–Ω–æ!

---

## –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è —Ä—ñ–∑–Ω–∏—Ö —Å—Ç–µ–ø–µ–Ω—ñ–≤

```
Degree 1 (–õ—ñ–Ω—ñ–π–Ω–∞):          Degree 2 (–ö–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–∞):
    y                            y
    |      ‚ï±                     |        ‚ï±‚ï≤
    |    ‚ï±                       |      ‚ï±    ‚ï≤
    |  ‚ï±                         |    ‚ï±        ‚ï≤
    |‚ï±                           |  ‚ï±            ‚ï≤
    |________ x                  |‚ï±________________ x

Degree 3 (–ö—É–±—ñ—á–Ω–∞):          Degree 5 (–í–∏—Å–æ–∫–∏–π):
    y                            y
    |    ‚ï±‚ï≤                      |   ‚ï±‚ï≤  ‚ï±‚ï≤
    |  ‚ï±    ‚ï≤‚ï≤                   | ‚ï±    ‚ï≤‚ï±  ‚ï≤
    |‚ï±        ‚ï≤                  |‚ï±          ‚ï≤‚ï≤
    |          ‚ï≤‚ï≤                |            ‚ï≤‚ï≤
    |____________ x              |______________ x
```

**–°–ø–æ—Å—Ç–µ—Ä–µ–∂–µ–Ω–Ω—è:**
- **–°—Ç–µ–ø—ñ–Ω—å 1:** –ø—Ä—è–º–∞ –ª—ñ–Ω—ñ—è
- **–°—Ç–µ–ø—ñ–Ω—å 2:** –ø–∞—Ä–∞–±–æ–ª–∞ (–æ–¥–Ω–∞ –∫—Ä–∏–≤–∞)
- **–°—Ç–µ–ø—ñ–Ω—å 3:** S-–ø–æ–¥—ñ–±–Ω–∞ –∫—Ä–∏–≤–∞ (–¥–≤—ñ –∫—Ä–∏–≤–∏–Ω–∏)
- **–í–∏—Å–æ–∫—ñ —Å—Ç–µ–ø–µ–Ω—ñ:** –±–∞–≥–∞—Ç–æ —Ö–≤–∏–ª—å (OVERFITTING! ‚ö†Ô∏è)

---

## –ü—Ä–æ—Å—Ç–∏–π –ø—Ä–∏–∫–ª–∞–¥: –ó–∞—Ä–ø–ª–∞—Ç–∞ vs –î–æ—Å–≤—ñ–¥

### –î–∞–Ω—ñ

| –î–æ—Å–≤—ñ–¥ (—Ä–æ–∫—ñ–≤) | –ó–∞—Ä–ø–ª–∞—Ç–∞ (—Ç–∏—Å. $) |
|----------------|-------------------|
| 1 | 40 |
| 2 | 45 |
| 3 | 55 |
| 4 | 70 |
| 5 | 90 |
| 6 | 115 |
| 7 | 145 |
| 8 | 180 |

### –°–ø—Ä–æ–±—É—î–º–æ –ª—ñ–Ω—ñ–π–Ω—É —Ä–µ–≥—Ä–µ—Å—ñ—é

```python
# –õ—ñ–Ω—ñ–π–Ω–∞ –º–æ–¥–µ–ª—å
Œ≤‚ÇÄ = 10, Œ≤‚ÇÅ = 20
y_pred = 10 + 20 * x

# –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –¥–ª—è –¥–æ—Å–≤—ñ–¥—É 8 —Ä–æ–∫—ñ–≤:
y_pred = 10 + 20 * 8 = 170 —Ç–∏—Å. $
# –†–µ–∞–ª—å–Ω–µ: 180 —Ç–∏—Å. $
# –ü–æ–º–∏–ª–∫–∞: 10 —Ç–∏—Å. $ (5.6%)
```

**–ü—Ä–æ–±–ª–µ–º–∞:** –õ—ñ–Ω—ñ–π–Ω–∞ –º–æ–¥–µ–ª—å **–Ω–µ–¥–æ–æ—Ü—ñ–Ω—é—î** –∑–∞—Ä–ø–ª–∞—Ç—É –Ω–∞ –≤–∏—â–∏—Ö —Ä—ñ–≤–Ω—è—Ö –¥–æ—Å–≤—ñ–¥—É.

**Scatter plot –ø–æ–∫–∞–∑—É—î:** –∑—Ä–æ—Å—Ç–∞–Ω–Ω—è **–ø—Ä–∏—Å–∫–æ—Ä—é—î—Ç—å—Å—è** ‚Üí –Ω–µ–ª—ñ–Ω—ñ–π–Ω–∞ –∑–∞–ª–µ–∂–Ω—ñ—Å—Ç—å!

### –ü–æ–ª—ñ–Ω–æ–º—ñ–∞–ª—å–Ω–∞ —Ä–µ–≥—Ä–µ—Å—ñ—è (—Å—Ç–µ–ø—ñ–Ω—å 2)

$$y = \beta_0 + \beta_1 x + \beta_2 x^2$$

–ü—ñ—Å–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è:
$$y = 35 + 5x + 2.5x^2$$

**–ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –¥–ª—è –¥–æ—Å–≤—ñ–¥—É 8 —Ä–æ–∫—ñ–≤:**
$$y = 35 + 5(8) + 2.5(64) = 35 + 40 + 160 = 235 \text{ —Ç–∏—Å. \$}$$

**–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü—ñ—è:**
- –ó–∞—Ä–ø–ª–∞—Ç–∞ –∑—Ä–æ—Å—Ç–∞—î –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–æ –∑ –¥–æ—Å–≤—ñ–¥–æ–º
- –ë—ñ–ª—å—à–∏–π –¥–æ—Å–≤—ñ–¥ ‚Üí —â–µ –±—ñ–ª—å—à–µ –∑—Ä–æ—Å—Ç–∞–Ω–Ω—è –∑–∞—Ä–ø–ª–∞—Ç–∏

---

## –°–∫–ª–∞–¥–Ω–∏–π –ø—Ä–∏–∫–ª–∞–¥: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ vs –ü—Ä–æ–¥–∞–∂—ñ –º–æ—Ä–æ–∑–∏–≤–∞

### –î–∞–Ω—ñ

200 –¥–Ω—ñ–≤ –∑ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–æ—é —Ç–∞ –ø—Ä–æ–¥–∞–∂–∞–º–∏:

| –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ (¬∞C) | –ü—Ä–æ–¥–∞–∂—ñ (—Ç–∏—Å. —à—Ç) |
|------------------|-------------------|
| 0 | 5 |
| 10 | 20 |
| 15 | 35 |
| 20 | 55 |
| 25 | 80 |
| 30 | 100 |
| 35 | 95 |
| 40 | 70 |

**–°–ø–æ—Å—Ç–µ—Ä–µ–∂–µ–Ω–Ω—è:**
- –ü—Ä–∏ –Ω–∏–∑—å–∫–∏—Ö —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞—Ö: –º–∞–ª–æ –ø—Ä–æ–¥–∞–∂—ñ–≤
- –ü—Ä–∏ —Å–µ—Ä–µ–¥–Ω—ñ—Ö (25-30¬∞C): –ø—ñ–∫ –ø—Ä–æ–¥–∞–∂—ñ–≤
- –ü—Ä–∏ –¥—É–∂–µ –≤–∏—Å–æ–∫–∏—Ö (>35¬∞C): –ø—Ä–æ–¥–∞–∂—ñ –∑–Ω–∏–∂—É—é—Ç—å—Å—è (–∑–∞–Ω–∞–¥—Ç–æ —Å–ø–µ–∫–æ—Ç–Ω–æ!)

**–§–æ—Ä–º–∞ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ:** –ø–µ—Ä–µ–≤–µ—Ä–Ω—É—Ç–∞ –ø–∞—Ä–∞–±–æ–ª–∞ (‚à©)

### –ú–æ–¥–µ–ª—å (—Å—Ç–µ–ø—ñ–Ω—å 2)

$$\text{–ü—Ä–æ–¥–∞–∂—ñ} = \beta_0 + \beta_1 \times \text{Temp} + \beta_2 \times \text{Temp}^2$$

–ü—ñ—Å–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è:
$$\text{–ü—Ä–æ–¥–∞–∂—ñ} = -50 + 8 \times \text{Temp} - 0.12 \times \text{Temp}^2$$

**–ê–Ω–∞–ª—ñ–∑ –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç—ñ–≤:**
- $\beta_0 = -50$: –±–∞–∑–æ–≤–∞ –∫–æ–Ω—Å—Ç–∞–Ω—Ç–∞ (–Ω–µ–≥–∞—Ç–∏–≤–Ω–∞ —á–µ—Ä–µ–∑ –ø–∞—Ä–∞–±–æ–ª—É)
- $\beta_1 = 8$: –ø–æ–∑–∏—Ç–∏–≤–Ω–∏–π –µ—Ñ–µ–∫—Ç —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∏
- $\beta_2 = -0.12$: **–Ω–µ–≥–∞—Ç–∏–≤–Ω–∏–π** ‚Üí –ø–∞—Ä–∞–±–æ–ª–∞ –≤–Ω–∏–∑ (‚à©)

### –ó–Ω–∞—Ö–æ–¥–∂–µ–Ω–Ω—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ—ó —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∏

–ú–∞–∫—Å–∏–º—É–º –ø–∞—Ä–∞–±–æ–ª–∏ –ø—Ä–∏:
$$x_{max} = -\frac{\beta_1}{2\beta_2} = -\frac{8}{2 \times (-0.12)} = \frac{8}{0.24} = 33.3¬∞C$$

**–í–∏—Å–Ω–æ–≤–æ–∫:** –ü—ñ–∫ –ø—Ä–æ–¥–∞–∂—ñ–≤ –ø—Ä–∏ **33.3¬∞C** ‚úì

### –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è

**–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ 28¬∞C:**
$$\text{–ü—Ä–æ–¥–∞–∂—ñ} = -50 + 8(28) - 0.12(784) = -50 + 224 - 94 = 80 \text{ —Ç–∏—Å. —à—Ç}$$

**–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ 38¬∞C (–¥—É–∂–µ —Å–ø–µ–∫–æ—Ç–Ω–æ):**
$$\text{–ü—Ä–æ–¥–∞–∂—ñ} = -50 + 8(38) - 0.12(1444) = -50 + 304 - 173 = 81 \text{ —Ç–∏—Å. —à—Ç}$$

–•–æ—á–∞ 38¬∞C > 28¬∞C, –ø—Ä–æ–¥–∞–∂—ñ –º–∞–π–∂–µ –æ–¥–Ω–∞–∫–æ–≤—ñ —á–µ—Ä–µ–∑ –ø–∞—Ä–∞–±–æ–ª—É!

---

## –ö–æ–¥ (Python + scikit-learn)

### –ü—Ä–æ—Å—Ç–∏–π –ø—Ä–∏–∫–ª–∞–¥ (1 –æ–∑–Ω–∞–∫–∞)

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error

# –î–∞–Ω—ñ
X = np.array([1, 2, 3, 4, 5, 6, 7, 8]).reshape(-1, 1)
y = np.array([40, 45, 55, 70, 90, 115, 145, 180])

# 1. –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –ø–æ–ª—ñ–Ω–æ–º—ñ–∞–ª—å–Ω–∏—Ö –æ–∑–Ω–∞–∫
poly_features = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly_features.fit_transform(X)

print("Original features:")
print(X[:3])
# [[1]
#  [2]
#  [3]]

print("\nPolynomial features (degree=2):")
print(X_poly[:3])
# [[ 1.  1.]    ‚Üê [x, x¬≤]
#  [ 2.  4.]
#  [ 3.  9.]]

# 2. –ù–∞–≤—á–∞–Ω–Ω—è –ª—ñ–Ω—ñ–π–Ω–æ—ó —Ä–µ–≥—Ä–µ—Å—ñ—ó –Ω–∞ –ø–æ–ª—ñ–Ω–æ–º—ñ–∞–ª—å–Ω–∏—Ö –æ–∑–Ω–∞–∫–∞—Ö
model = LinearRegression()
model.fit(X_poly, y)

# 3. –ö–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç–∏
print(f"\nIntercept (Œ≤‚ÇÄ): {model.intercept_:.2f}")
print(f"Coefficients: {model.coef_}")
# [Œ≤‚ÇÅ, Œ≤‚ÇÇ] –¥–ª—è [x, x¬≤]

# 4. –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
y_pred = model.predict(X_poly)

# 5. –ú–µ—Ç—Ä–∏–∫–∏
print(f"\nR¬≤: {r2_score(y, y_pred):.4f}")
print(f"RMSE: {np.sqrt(mean_squared_error(y, y_pred)):.2f}")

# 6. –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
X_plot = np.linspace(0, 9, 100).reshape(-1, 1)
X_plot_poly = poly_features.transform(X_plot)
y_plot = model.predict(X_plot_poly)

plt.figure(figsize=(10, 6))
plt.scatter(X, y, color='blue', s=100, label='–î–∞–Ω—ñ')
plt.plot(X_plot, y_plot, color='red', linewidth=2, 
         label='Polynomial Regression (degree=2)')
plt.xlabel('–î–æ—Å–≤—ñ–¥ (—Ä–æ–∫–∏)', fontsize=12)
plt.ylabel('–ó–∞—Ä–ø–ª–∞—Ç–∞ (—Ç–∏—Å. $)', fontsize=12)
plt.title('–ü–æ–ª—ñ–Ω–æ–º—ñ–∞–ª—å–Ω–∞ —Ä–µ–≥—Ä–µ—Å—ñ—è: –ó–∞—Ä–ø–ª–∞—Ç–∞ vs –î–æ—Å–≤—ñ–¥', 
          fontsize=14, fontweight='bold')
plt.legend(fontsize=11)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

### –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è —Ä—ñ–∑–Ω–∏—Ö —Å—Ç–µ–ø–µ–Ω—ñ–≤

```python
from sklearn.model_selection import train_test_split

# –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –±—ñ–ª—å—à–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç—É
np.random.seed(42)
X = np.linspace(0, 10, 100).reshape(-1, 1)
y = 2 + 3*X.ravel() + 0.5*X.ravel()**2 + np.random.randn(100) * 5

# –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# –¢–µ—Å—Ç—É—î–º–æ —Ä—ñ–∑–Ω—ñ —Å—Ç–µ–ø–µ–Ω—ñ
degrees = [1, 2, 3, 5, 10, 15]
results = []

fig, axes = plt.subplots(2, 3, figsize=(18, 12))
axes = axes.ravel()

X_plot = np.linspace(0, 10, 300).reshape(-1, 1)

for idx, degree in enumerate(degrees):
    # –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ—è
    poly = PolynomialFeatures(degree=degree, include_bias=False)
    X_train_poly = poly.fit_transform(X_train)
    X_test_poly = poly.transform(X_test)
    X_plot_poly = poly.transform(X_plot)
    
    # –ú–æ–¥–µ–ª—å
    model = LinearRegression()
    model.fit(X_train_poly, y_train)
    
    # –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
    y_train_pred = model.predict(X_train_poly)
    y_test_pred = model.predict(X_test_poly)
    y_plot = model.predict(X_plot_poly)
    
    # –ú–µ—Ç—Ä–∏–∫–∏
    train_r2 = r2_score(y_train, y_train_pred)
    test_r2 = r2_score(y_test, y_test_pred)
    
    results.append({
        'Degree': degree,
        'Train R¬≤': train_r2,
        'Test R¬≤': test_r2,
        'Overfitting': train_r2 - test_r2,
        'Features': X_train_poly.shape[1]
    })
    
    # –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
    axes[idx].scatter(X_train, y_train, color='blue', 
                     alpha=0.5, label='Train')
    axes[idx].scatter(X_test, y_test, color='green', 
                     alpha=0.5, label='Test')
    axes[idx].plot(X_plot, y_plot, color='red', linewidth=2, 
                  label=f'Degree {degree}')
    axes[idx].set_xlabel('X', fontsize=10)
    axes[idx].set_ylabel('y', fontsize=10)
    axes[idx].set_title(
        f'Degree {degree}\nTrain R¬≤={train_r2:.3f}, Test R¬≤={test_r2:.3f}',
        fontsize=11, fontweight='bold'
    )
    axes[idx].legend(fontsize=9)
    axes[idx].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# –†–µ–∑—É–ª—å—Ç–∞—Ç–∏
import pandas as pd
results_df = pd.DataFrame(results)
print("\n" + "="*60)
print(results_df.to_string(index=False))
print("="*60)
```

**–¢–∏–ø–æ–≤–∏–π –≤–∏—Ö—ñ–¥:**

```
Degree  Train R¬≤  Test R¬≤  Overfitting  Features
     1    0.925    0.920        0.005         1
     2    0.985    0.980        0.005         2
     3    0.990    0.975        0.015         3
     5    0.995    0.960        0.035         5
    10    0.999    0.850        0.149        10  ‚Üê OVERFITTING!
    15    1.000    0.600        0.400        15  ‚Üê –°–ò–õ–¨–ù–ò–ô OVERFITTING!
```

**–í–∏—Å–Ω–æ–≤–æ–∫:**
- **Degree 2-3:** –æ–ø—Ç–∏–º–∞–ª—å–Ω–∏–π –±–∞–ª–∞–Ω—Å
- **Degree 10+:** —Å–∏–ª—å–Ω–∏–π overfitting (train R¬≤=1, test R¬≤ –ø–∞–¥–∞—î)

---

## –ë–∞–≥–∞—Ç–æ –æ–∑–Ω–∞–∫ –∑ interaction terms

```python
from sklearn.datasets import make_regression

# –î–∞–Ω—ñ –∑ 2 –æ–∑–Ω–∞–∫–∞–º–∏
X, y = make_regression(
    n_samples=200,
    n_features=2,
    noise=10,
    random_state=42
)

# –î–æ–¥–∞—î–º–æ –Ω–µ–ª—ñ–Ω—ñ–π–Ω—ñ—Å—Ç—å –≤—Ä—É—á–Ω—É
y = y + 0.5 * X[:, 0]**2 + 0.3 * X[:, 1]**2 + 0.2 * X[:, 0] * X[:, 1]

# –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# –ü–æ–ª—ñ–Ω–æ–º—ñ–∞–ª—å–Ω—ñ –æ–∑–Ω–∞–∫–∏ –∑ interaction terms
poly = PolynomialFeatures(
    degree=2,
    include_bias=False,
    interaction_only=False  # –í–∫–ª—é—á–∞—î x‚ÇÅ¬≤, x‚ÇÇ¬≤, x‚ÇÅx‚ÇÇ
)

X_train_poly = poly.fit_transform(X_train)
X_test_poly = poly.transform(X_test)

print("Original features: 2")
print(f"Polynomial features: {X_train_poly.shape[1]}")
print("Feature names:")
print(poly.get_feature_names_out(['x1', 'x2']))
# ['x1', 'x2', 'x1^2', 'x1 x2', 'x2^2']

# –ú–æ–¥–µ–ª—å
model = LinearRegression()
model.fit(X_train_poly, y_train)

# –û—Ü—ñ–Ω–∫–∞
print(f"\nTrain R¬≤: {model.score(X_train_poly, y_train):.4f}")
print(f"Test R¬≤: {model.score(X_test_poly, y_test):.4f}")

# –ö–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç–∏
print("\nCoefficients:")
for name, coef in zip(poly.get_feature_names_out(['x1', 'x2']), 
                      model.coef_):
    print(f"  {name}: {coef:.4f}")
```

---

## –í–∏–±—ñ—Ä –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —Å—Ç–µ–ø–µ–Ω—è

### 1. Cross-Validation

```python
from sklearn.model_selection import cross_val_score

degrees = range(1, 11)
train_scores = []
cv_scores = []

for degree in degrees:
    poly = PolynomialFeatures(degree=degree, include_bias=False)
    X_poly = poly.fit_transform(X_train)
    
    model = LinearRegression()
    
    # Train score
    model.fit(X_poly, y_train)
    train_score = model.score(X_poly, y_train)
    train_scores.append(train_score)
    
    # Cross-validation score
    cv_score = cross_val_score(
        model, X_poly, y_train, cv=5, 
        scoring='r2'
    ).mean()
    cv_scores.append(cv_score)

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
plt.figure(figsize=(10, 6))
plt.plot(degrees, train_scores, 'o-', linewidth=2, 
         label='Train Score')
plt.plot(degrees, cv_scores, 's-', linewidth=2, 
         label='CV Score')
plt.xlabel('Polynomial Degree', fontsize=12)
plt.ylabel('R¬≤ Score', fontsize=12)
plt.title('Train vs CV Score by Polynomial Degree', 
          fontsize=14, fontweight='bold')
plt.legend(fontsize=11)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# –û–ø—Ç–∏–º–∞–ª—å–Ω–∏–π —Å—Ç–µ–ø—ñ–Ω—å
optimal_degree = degrees[np.argmax(cv_scores)]
print(f"Optimal degree: {optimal_degree}")
```

### 2. Validation Curve

```python
from sklearn.model_selection import validation_curve

# –ü–æ—Ç—Ä—ñ–±–Ω–æ —Å—Ç–≤–æ—Ä–∏—Ç–∏ custom estimator
from sklearn.pipeline import Pipeline

pipe = Pipeline([
    ('poly', PolynomialFeatures(include_bias=False)),
    ('linear', LinearRegression())
])

degrees = np.arange(1, 16)

train_scores, val_scores = validation_curve(
    pipe, X_train, y_train,
    param_name='poly__degree',
    param_range=degrees,
    cv=5,
    scoring='r2'
)

train_mean = train_scores.mean(axis=1)
train_std = train_scores.std(axis=1)
val_mean = val_scores.mean(axis=1)
val_std = val_scores.std(axis=1)

plt.figure(figsize=(10, 6))
plt.plot(degrees, train_mean, 'o-', linewidth=2, label='Train')
plt.fill_between(degrees, train_mean - train_std, 
                 train_mean + train_std, alpha=0.2)
plt.plot(degrees, val_mean, 's-', linewidth=2, label='Validation')
plt.fill_between(degrees, val_mean - val_std, 
                 val_mean + val_std, alpha=0.2)
plt.xlabel('Polynomial Degree', fontsize=12)
plt.ylabel('R¬≤ Score', fontsize=12)
plt.title('Validation Curve', fontsize=14, fontweight='bold')
plt.legend(fontsize=11)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

---

## Pipeline –¥–ª—è –ø–æ–ª—ñ–Ω–æ–º—ñ–∞–ª—å–Ω–æ—ó —Ä–µ–≥—Ä–µ—Å—ñ—ó

```python
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

# Pipeline: Polynomial ‚Üí Scaling ‚Üí Linear Regression
pipe = Pipeline([
    ('poly', PolynomialFeatures(degree=2, include_bias=False)),
    ('scaler', StandardScaler()),
    ('linear', LinearRegression())
])

# –ù–∞–≤—á–∞–Ω–Ω—è
pipe.fit(X_train, y_train)

# –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
y_pred = pipe.predict(X_test)

# –û—Ü—ñ–Ω–∫–∞
print(f"R¬≤ Score: {pipe.score(X_test, y_test):.4f}")

# –î–æ—Å—Ç—É–ø –¥–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ñ–≤
poly_features = pipe.named_steps['poly']
scaler = pipe.named_steps['scaler']
model = pipe.named_steps['linear']

print(f"\nFeature names: {poly_features.get_feature_names_out()}")
print(f"Coefficients: {model.coef_}")
```

---

## Regularization –¥–ª—è –ø–æ–ª—ñ–Ω–æ–º—ñ–∞–ª—å–Ω–æ—ó —Ä–µ–≥—Ä–µ—Å—ñ—ó

### –ü—Ä–æ–±–ª–µ–º–∞: Overfitting –ø—Ä–∏ –≤–∏—Å–æ–∫–∏—Ö —Å—Ç–µ–ø–µ–Ω—è—Ö

**–†—ñ—à–µ–Ω–Ω—è:** Ridge –∞–±–æ Lasso —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—è

```python
from sklearn.linear_model import Ridge, Lasso

# –ü–æ–ª—ñ–Ω–æ–º—ñ–∞–ª—å–Ω—ñ –æ–∑–Ω–∞–∫–∏ –≤–∏—Å–æ–∫–æ–≥–æ —Å—Ç–µ–ø–µ–Ω—è
poly = PolynomialFeatures(degree=10, include_bias=False)
X_train_poly = poly.fit_transform(X_train)
X_test_poly = poly.transform(X_test)

# –ë–µ–∑ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—ó (OVERFITTING)
lr = LinearRegression()
lr.fit(X_train_poly, y_train)
print("Linear Regression (no regularization):")
print(f"  Train R¬≤: {lr.score(X_train_poly, y_train):.4f}")
print(f"  Test R¬≤: {lr.score(X_test_poly, y_test):.4f}")

# –ó Ridge —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—î—é
ridge = Ridge(alpha=1.0)
ridge.fit(X_train_poly, y_train)
print("\nRidge Regression (alpha=1.0):")
print(f"  Train R¬≤: {ridge.score(X_train_poly, y_train):.4f}")
print(f"  Test R¬≤: {ridge.score(X_test_poly, y_test):.4f}")

# –ó Lasso —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—î—é
lasso = Lasso(alpha=0.1)
lasso.fit(X_train_poly, y_train)
print("\nLasso Regression (alpha=0.1):")
print(f"  Train R¬≤: {lasso.score(X_train_poly, y_train):.4f}")
print(f"  Test R¬≤: {lasso.score(X_test_poly, y_test):.4f}")
print(f"  Non-zero coefs: {np.sum(lasso.coef_ != 0)} / {len(lasso.coef_)}")
```

**–¢–∏–ø–æ–≤–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:**

```
Linear Regression:
  Train R¬≤: 0.9999  ‚Üê –ú–∞–π–∂–µ —ñ–¥–µ–∞–ª—å–Ω–æ –Ω–∞ train
  Test R¬≤: 0.6500   ‚Üê –ö–∞—Ç–∞—Å—Ç—Ä–æ—Ñ–∞ –Ω–∞ test! OVERFITTING

Ridge Regression:
  Train R¬≤: 0.9800
  Test R¬≤: 0.9600   ‚Üê –ù–∞–±–∞–≥–∞—Ç–æ –∫—Ä–∞—â–µ! ‚úì

Lasso Regression:
  Train R¬≤: 0.9750
  Test R¬≤: 0.9580
  Non-zero coefs: 15 / 55  ‚Üê Feature selection
```

---

## –ü–µ—Ä–µ–≤–∞–≥–∏ —Ç–∞ –Ω–µ–¥–æ–ª—ñ–∫–∏

### –ü–µ—Ä–µ–≤–∞–≥–∏ ‚úì

| –ü–µ—Ä–µ–≤–∞–≥–∞ | –ü–æ—è—Å–Ω–µ–Ω–Ω—è |
|----------|-----------|
| **–ù–µ–ª—ñ–Ω—ñ–π–Ω—ñ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ** | –ú–æ–∂–µ –º–æ–¥–µ–ª—é–≤–∞—Ç–∏ –∫—Ä–∏–≤—ñ, –ø–∞—Ä–∞–±–æ–ª–∏ |
| **–í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –ª—ñ–Ω—ñ–π–Ω–æ—ó —Ä–µ–≥—Ä–µ—Å—ñ—ó** | –¢—ñ –∂ –∞–ª–≥–æ—Ä–∏—Ç–º–∏ —Ç–∞ —ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü—ñ—è |
| **–ü—Ä–æ—Å—Ç–æ—Ç–∞** | –õ–µ–≥–∫–æ —Ä–µ–∞–ª—ñ–∑—É–≤–∞—Ç–∏ —á–µ—Ä–µ–∑ feature engineering |
| **–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å** | –ö–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç–∏ –º–∞—é—Ç—å –º–∞—Ç–µ–º–∞—Ç–∏—á–Ω–∏–π –∑–º—ñ—Å—Ç |
| **–®–≤–∏–¥–∫—ñ—Å—Ç—å** | –®–≤–∏–¥—à–µ –∑–∞ —Å–∫–ª–∞–¥–Ω—ñ –Ω–µ–ª—ñ–Ω—ñ–π–Ω—ñ –º–æ–¥–µ–ª—ñ |
| **Flexibility** | –õ–µ–≥–∫–æ –∫–æ–Ω—Ç—Ä–æ–ª—é–≤–∞—Ç–∏ —Å–∫–ª–∞–¥–Ω—ñ—Å—Ç—å (degree) |

### –ù–µ–¥–æ–ª—ñ–∫–∏ ‚úó

| –ù–µ–¥–æ–ª—ñ–∫ | –ü–æ—è—Å–Ω–µ–Ω–Ω—è |
|---------|-----------|
| **Curse of dimensionality** | –ö—ñ–ª—å–∫—ñ—Å—Ç—å –æ–∑–Ω–∞–∫ –∑—Ä–æ—Å—Ç–∞—î –µ–∫—Å–ø–æ–Ω–µ–Ω—Ü—ñ–π–Ω–æ |
| **Overfitting** | –í–∏—Å–æ–∫—ñ —Å—Ç–µ–ø–µ–Ω—ñ –ª–µ–≥–∫–æ –ø–µ—Ä–µ–Ω–∞–≤—á–∞—é—Ç—å—Å—è |
| **–ï–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü—ñ—è** | –ü–æ–≥–∞–Ω–æ –ø–µ—Ä–µ–¥–±–∞—á–∞—î –∑–∞ –º–µ–∂–∞–º–∏ —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö |
| **–í–∏–±—ñ—Ä —Å—Ç–µ–ø–µ–Ω—è** | –ü–æ—Ç—Ä—ñ–±–µ–Ω –ø—ñ–¥–±—ñ—Ä –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ degree |
| **–ú—É–ª—å—Ç–∏–∫–æ–ª—ñ–Ω–µ–∞—Ä–Ω—ñ—Å—Ç—å** | –ü–æ–ª—ñ–Ω–æ–º—ñ–∞–ª—å–Ω—ñ –æ–∑–Ω–∞–∫–∏ —Å–∏–ª—å–Ω–æ –∫–æ—Ä–µ–ª—é—é—Ç—å |
| **–û–±–º–µ–∂–µ–Ω–∞ –≥–Ω—É—á–∫—ñ—Å—Ç—å** | –¢—ñ–ª—å–∫–∏ –ø–æ–ª—ñ–Ω–æ–º—ñ–∞–ª—å–Ω—ñ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ |

---

## –ï–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü—ñ—è: –Ω–µ–±–µ–∑–ø–µ–∫–∞ –ø–æ–ª—ñ–Ω–æ–º—ñ–∞–ª—å–Ω–æ—ó —Ä–µ–≥—Ä–µ—Å—ñ—ó

### –ü—Ä–æ–±–ª–µ–º–∞

–ü–æ–ª—ñ–Ω–æ–º—ñ–∞–ª—å–Ω—ñ –º–æ–¥–µ–ª—ñ **–¥—É–∂–µ –ø–æ–≥–∞–Ω–æ –µ–∫—Å—Ç—Ä–∞–ø–æ–ª—é—é—Ç—å** –∑–∞ –º–µ–∂—ñ —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö.

```python
# –¢—Ä–µ–Ω—É–≤–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ: X –≤—ñ–¥ 0 –¥–æ 10
X_train = np.linspace(0, 10, 50).reshape(-1, 1)
y_train = 2 + 3*X_train.ravel() + 0.5*X_train.ravel()**2

# –ü–æ–ª—ñ–Ω–æ–º—ñ–∞–ª—å–Ω–∞ —Ä–µ–≥—Ä–µ—Å—ñ—è (—Å—Ç–µ–ø—ñ–Ω—å 3)
poly = PolynomialFeatures(degree=3)
X_train_poly = poly.fit_transform(X_train)
model = LinearRegression()
model.fit(X_train_poly, y_train)

# –ï–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü—ñ—è: X –≤—ñ–¥ 0 –¥–æ 20 (–∑–∞ –º–µ–∂—ñ!)
X_extrapolate = np.linspace(0, 20, 100).reshape(-1, 1)
X_extrapolate_poly = poly.transform(X_extrapolate)
y_extrapolate = model.predict(X_extrapolate_poly)

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
plt.figure(figsize=(12, 6))
plt.scatter(X_train, y_train, color='blue', s=50, label='Train data')
plt.plot(X_extrapolate, y_extrapolate, color='red', linewidth=2, 
         label='Polynomial prediction')
plt.axvline(x=10, color='green', linestyle='--', linewidth=2, 
            label='End of training range')
plt.xlabel('X', fontsize=12)
plt.ylabel('y', fontsize=12)
plt.title('Polynomial Regression: Extrapolation Problem', 
          fontsize=14, fontweight='bold')
plt.legend(fontsize=11)
plt.grid(True, alpha=0.3)
plt.xlim(0, 20)
plt.tight_layout()
plt.show()
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç:**
- –í –º–µ–∂–∞—Ö [0, 10]: —Ç–æ—á–Ω—ñ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è ‚úì
- –ó–∞ –º–µ–∂–∞–º–∏ [10, 20]: –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è **–ª–µ—Ç—è—Ç—å —É –Ω–µ–±–æ –∞–±–æ –ø–∞–¥–∞—é—Ç—å** ‚úó

**–ß–æ–º—É?**
- –ü–æ–ª—ñ–Ω–æ–º–∏ –≤–∏—Å–æ–∫–∏—Ö —Å—Ç–µ–ø–µ–Ω—ñ–≤ –º–∞—é—Ç—å **–µ–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—É –ø–æ–≤–µ–¥—ñ–Ω–∫—É** –Ω–∞ –∫—Ä–∞—è—Ö
- $x^3$ –ø—Ä–∏ –≤–µ–ª–∏–∫–∏—Ö x ‚Üí –¥—É–∂–µ –≤–µ–ª–∏–∫—ñ –∑–Ω–∞—á–µ–Ω–Ω—è

---

## –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ Polynomial Regression

### –Ü–¥–µ–∞–ª—å–Ω–æ –ø—ñ–¥—Ö–æ–¥–∏—Ç—å ‚úì

- Scatter plot –ø–æ–∫–∞–∑—É—î **—è–≤–Ω—É –∫—Ä–∏–≤—É** (–ø–∞—Ä–∞–±–æ–ª–∞, S-–∫—Ä–∏–≤–∞)
- **Residual plot** –ª—ñ–Ω—ñ–π–Ω–æ—ó —Ä–µ–≥—Ä–µ—Å—ñ—ó –º–∞—î —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–Ω–∏–π –ø–∞—Ç–µ—Ä–Ω
- –ü–æ—Ç—Ä—ñ–±–Ω–∞ **—ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å** (–º–µ–¥–∏—Ü–∏–Ω–∞, –ø—Ä–∏—Ä–æ–¥–Ω–∏—á—ñ –Ω–∞—É–∫–∏)
- –ù–µ–≤–µ–ª–∏–∫–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –æ–∑–Ω–∞–∫ ($p < 5$)
- –ù–∏–∑—å–∫—ñ/—Å–µ—Ä–µ–¥–Ω—ñ —Å—Ç–µ–ø–µ–Ω—ñ ($d \leq 3-4$)
- –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è **–≤ –º–µ–∂–∞—Ö** —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö

### –ö—Ä–∞—â–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ —ñ–Ω—à–µ ‚úó

- –î—É–∂–µ —Å–∫–ª–∞–¥–Ω—ñ –Ω–µ–ª—ñ–Ω—ñ–π–Ω—ñ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ ‚Üí **Random Forest, XGBoost**
- –ë–∞–≥–∞—Ç–æ –æ–∑–Ω–∞–∫ + –≤–∏—Å–æ–∫—ñ —Å—Ç–µ–ø–µ–Ω—ñ ‚Üí **curse of dimensionality**
- –ü–æ—Ç—Ä—ñ–±–Ω–∞ **–µ–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü—ñ—è** ‚Üí –æ–±–µ—Ä–µ–∂–Ω–æ –∞–±–æ —ñ–Ω—à—ñ –º–µ—Ç–æ–¥–∏
- –í–∑–∞—î–º–æ–¥—ñ—ó –º—ñ–∂ –æ–∑–Ω–∞–∫–∞–º–∏ –Ω–µ–≤—ñ–¥–æ–º—ñ ‚Üí **Tree-based models**
- –ó–æ–±—Ä–∞–∂–µ–Ω–Ω—è, —Ç–µ–∫—Å—Ç ‚Üí **Deep Learning**

---

## –ü—Ä–∞–∫—Ç–∏—á–Ω—ñ –ø–æ—Ä–∞–¥–∏ üí°

1. **–ü–æ—á–Ω–∏ –∑ scatter plot** ‚Äî –ø–æ–¥–∏–≤–∏—Å—å —Ñ–æ—Ä–º—É –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ
2. **–°–ø–æ—á–∞—Ç–∫—É linear regression** ‚Äî –º–æ–∂–ª–∏–≤–æ, –¥–æ—Å—Ç–∞—Ç–Ω—å–æ
3. **Residual plot** ‚Äî –ø–µ—Ä–µ–≤—ñ—Ä —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–Ω—ñ –ø–∞—Ç–µ—Ä–Ω–∏
4. **–ù–µ –ø–µ—Ä–µ–±–æ—Ä—â—É–π –∑—ñ —Å—Ç–µ–ø–µ–Ω–µ–º** ‚Äî –∑–∞–∑–≤–∏—á–∞–π 2-3 –¥–æ—Å—Ç–∞—Ç–Ω—å–æ
5. **Cross-validation** –¥–ª—è –≤–∏–±–æ—Ä—É –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ degree
6. **Regularization** –ø—Ä–∏ –≤–∏—Å–æ–∫–∏—Ö —Å—Ç–µ–ø–µ–Ω—è—Ö (Ridge/Lasso)
7. **–ù–æ—Ä–º–∞–ª—ñ–∑—É–π –¥–∞–Ω—ñ** ‚Äî –æ—Å–æ–±–ª–∏–≤–æ –ø—Ä–∏ –≤–∏—Å–æ–∫–∏—Ö —Å—Ç–µ–ø–µ–Ω—è—Ö
8. **Interaction terms** ‚Äî `interaction_only=True` —è–∫—â–æ —Ç—ñ–ª—å–∫–∏ –≤–∑–∞—î–º–æ–¥—ñ—ó
9. **–ù–µ –µ–∫—Å—Ç—Ä–∞–ø–æ–ª—é–π** ‚Äî –ø–µ—Ä–µ–¥–±–∞—á–∞–π —Ç—ñ–ª—å–∫–∏ –≤ –º–µ–∂–∞—Ö train –¥—ñ–∞–ø–∞–∑–æ–Ω—É
10. **Pipeline** ‚Äî –∑—Ä—É—á–Ω–æ –∫–æ–º–±—ñ–Ω—É–≤–∞—Ç–∏ poly ‚Üí scaling ‚Üí model

---

## –ü–æ—à–∏—Ä–µ–Ω—ñ –ø–æ–º–∏–ª–∫–∏ ‚ùå

### 1. –ó–∞–Ω–∞–¥—Ç–æ –≤–∏—Å–æ–∫–∏–π —Å—Ç–µ–ø—ñ–Ω—å

```python
# ‚ùå –ù–ï–ü–†–ê–í–ò–õ–¨–ù–û
poly = PolynomialFeatures(degree=20)  # Overfitting –≥–∞—Ä–∞–Ω—Ç–æ–≤–∞–Ω–∏–π!

# ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û
# –ü—ñ–¥–±–µ—Ä–∏ —á–µ—Ä–µ–∑ CV –∞–±–æ –ø–æ—á–Ω–∏ –∑ 2-3
poly = PolynomialFeatures(degree=2)
```

### 2. –ó–∞–±—É—Ç–∏ –Ω–æ—Ä–º–∞–ª—ñ–∑—É–≤–∞—Ç–∏ –ø—Ä–∏ –≤–∏—Å–æ–∫–∏—Ö —Å—Ç–µ–ø–µ–Ω—è—Ö

```python
# ‚ùå –ù–ï–ü–†–ê–í–ò–õ–¨–ù–û (x^10 –º–æ–∂–µ –±—É—Ç–∏ –¥—É–∂–µ –≤–µ–ª–∏–∫–∏–º)
X_poly = poly.fit_transform(X)
model.fit(X_poly, y)

# ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û
X_poly = poly.fit_transform(X)
scaler = StandardScaler()
X_poly_scaled = scaler.fit_transform(X_poly)
model.fit(X_poly_scaled, y)
```

### 3. –ï–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü—ñ—è –±–µ–∑ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏

```python
# ‚ùå –ù–ï–ë–ï–ó–ü–ï–ß–ù–û
X_new = [[100]]  # –î–∞–ª–µ–∫–æ –∑–∞ –º–µ–∂–∞–º–∏ train –¥–∞–Ω–∏—Ö
y_pred = model.predict(poly.transform(X_new))

# ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û
if X_new[0][0] > X_train.max() or X_new[0][0] < X_train.min():
    print("WARNING: Extrapolation! Predictions may be unreliable.")
```

### 4. –ù–µ –ø–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ overfitting

```python
# ‚ùå –ù–ï–ü–†–ê–í–ò–õ–¨–ù–û
# –î–∏–≤–∏—Ç–∏—Å—å —Ç—ñ–ª—å–∫–∏ –Ω–∞ train R¬≤

# ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û
print(f"Train R¬≤: {model.score(X_train_poly, y_train):.4f}")
print(f"Test R¬≤: {model.score(X_test_poly, y_test):.4f}")
# –Ø–∫—â–æ Train >> Test ‚Üí overfitting!
```

---

## –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∏ –ø–æ–ª—ñ–Ω–æ–º—ñ–∞–ª—å–Ω—ñ–π —Ä–µ–≥—Ä–µ—Å—ñ—ó

### 1. Spline Regression

**–ö—É—Å–∫–æ–≤–æ-–ø–æ–ª—ñ–Ω–æ–º—ñ–∞–ª—å–Ω—ñ —Ñ—É–Ω–∫—Ü—ñ—ó** ‚Äî —Ä—ñ–∑–Ω—ñ –ø–æ–ª—ñ–Ω–æ–º–∏ –Ω–∞ —Ä—ñ–∑–Ω–∏—Ö –¥—ñ–ª—è–Ω–∫–∞—Ö.

```python
from sklearn.preprocessing import SplineTransformer

spline = SplineTransformer(n_knots=4, degree=3)
X_spline = spline.fit_transform(X)
model = LinearRegression()
model.fit(X_spline, y)
```

**–ü–µ—Ä–µ–≤–∞–≥–∏:**
- –ë—ñ–ª—å—à –≥–Ω—É—á–∫—ñ –∑–∞ –ø—Ä–æ—Å—Ç—ñ –ø–æ–ª—ñ–Ω–æ–º–∏
- –ö—Ä–∞—â–∞ –ª–æ–∫–∞–ª—å–Ω–∞ –∞–ø—Ä–æ–∫—Å–∏–º–∞—Ü—ñ—è

### 2. Kernel Ridge Regression

```python
from sklearn.kernel_ridge import KernelRidge

model = KernelRidge(alpha=1.0, kernel='rbf', gamma=0.1)
model.fit(X_train, y_train)
```

### 3. Tree-based Models

```python
from sklearn.ensemble import RandomForestRegressor

rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)
```

**–ü–µ—Ä–µ–≤–∞–≥–∏:**
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –≤–∏—è–≤–ª—è—é—Ç—å –Ω–µ–ª—ñ–Ω—ñ–π–Ω–æ—Å—Ç—ñ
- –ù–µ –ø–æ—Ç—Ä—ñ–±–µ–Ω –≤–∏–±—ñ—Ä —Å—Ç–µ–ø–µ–Ω—è
- –†–æ–±–∞—Å—Ç–Ω—ñ –¥–æ overfitting

---

## –ü–æ–≤'—è–∑–∞–Ω—ñ —Ç–µ–º–∏

- [[01_Linear_Regression]] ‚Äî –±–∞–∑–æ–≤–∞ –º–æ–¥–µ–ª—å
- [[03_Regularization]] ‚Äî Ridge/Lasso –¥–ª—è –∑–∞–ø–æ–±—ñ–≥–∞–Ω–Ω—è overfitting
- [[Feature_Engineering]] ‚Äî —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –Ω–æ–≤–∏—Ö –æ–∑–Ω–∞–∫
- [[Cross_Validation]] ‚Äî –≤–∏–±—ñ—Ä –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —Å—Ç–µ–ø–µ–Ω—è
- [[Bias_Variance_Tradeoff]] ‚Äî –±–∞–ª–∞–Ω—Å —Å–∫–ª–∞–¥–Ω–æ—Å—Ç—ñ –º–æ–¥–µ–ª—ñ

## –†–µ—Å—É—Ä—Å–∏

- [Scikit-learn: Polynomial Features](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html)
- [Scikit-learn: Pipeline](https://scikit-learn.org/stable/modules/compose.html#pipeline)
- [StatQuest: Polynomial Regression](https://www.youtube.com/watch?v=Ja5jH9FOpqQ)

---

## –ö–ª—é—á–æ–≤—ñ –≤–∏—Å–Ω–æ–≤–∫–∏

> –ü–æ–ª—ñ–Ω–æ–º—ñ–∞–ª—å–Ω–∞ —Ä–µ–≥—Ä–µ—Å—ñ—è —Ä–æ–∑—à–∏—Ä—é—î –ª—ñ–Ω—ñ–π–Ω—É —Ä–µ–≥—Ä–µ—Å—ñ—é –¥–ª—è –Ω–µ–ª—ñ–Ω—ñ–π–Ω–∏—Ö –∑–∞–ª–µ–∂–Ω–æ—Å—Ç–µ–π —á–µ—Ä–µ–∑ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –ø–æ–ª—ñ–Ω–æ–º—ñ–∞–ª—å–Ω–∏—Ö –æ–∑–Ω–∞–∫.

**–û—Å–Ω–æ–≤–Ω—ñ –ø—Ä–∏–Ω—Ü–∏–ø–∏:**
- –°—Ç–≤–æ—Ä—é—î –Ω–æ–≤—ñ –æ–∑–Ω–∞–∫–∏: $x, x^2, x^3, ..., x^d$
- –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î –∑–≤–∏—á–∞–π–Ω—É –ª—ñ–Ω—ñ–π–Ω—É —Ä–µ–≥—Ä–µ—Å—ñ—é –Ω–∞ –Ω–æ–≤–∏—Ö –æ–∑–Ω–∞–∫–∞—Ö
- –°—Ç–µ–ø—ñ–Ω—å $d$ –∫–æ–Ω—Ç—Ä–æ–ª—é—î —Å–∫–ª–∞–¥–Ω—ñ—Å—Ç—å –º–æ–¥–µ–ª—ñ
- –ü–æ—Ç—Ä–µ–±—É—î regularization –ø—Ä–∏ –≤–∏—Å–æ–∫–∏—Ö —Å—Ç–µ–ø–µ–Ω—è—Ö

**–§–æ—Ä–º—É–ª–∞ (—Å—Ç–µ–ø—ñ–Ω—å 2):**
$$y = \beta_0 + \beta_1 x + \beta_2 x^2$$

**–ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏:**
- –ù–µ–ª—ñ–Ω—ñ–π–Ω–∞ –∑–∞–ª–µ–∂–Ω—ñ—Å—Ç—å + —ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å + –º–∞–ª–∏–π $p$ = Polynomial Regression ‚úì

**–í–∞–∂–ª–∏–≤–æ:**
- –ù–µ –µ–∫—Å—Ç—Ä–∞–ø–æ–ª—é–π!
- –ü—ñ–¥–±–∏—Ä–∞–π degree —á–µ—Ä–µ–∑ CV
- –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π regularization –ø—Ä–∏ $d > 3$

---

#ml #supervised-learning #regression #polynomial-regression #nonlinear #feature-engineering
