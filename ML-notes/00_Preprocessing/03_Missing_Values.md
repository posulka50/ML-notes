# Missing Values (–ü—Ä–æ–ø—É—â–µ–Ω—ñ –¥–∞–Ω—ñ)

## –©–æ —Ü–µ?

–ü—Ä–æ–ø—É—â–µ–Ω—ñ –¥–∞–Ω—ñ (Missing Values) ‚Äî —Ü–µ –≤—ñ–¥—Å—É—Ç–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è –≤ –Ω–∞–±–æ—Ä—ñ –¥–∞–Ω–∏—Ö, –ø–æ–∑–Ω–∞—á–µ–Ω—ñ —è–∫ NaN (Not a Number), None, null –∞–±–æ —ñ–Ω—à—ñ —Å–ø–µ—Ü—ñ–∞–ª—å–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è.

## –ß–æ–º—É –≤–∏–Ω–∏–∫–∞—é—Ç—å? ü§î

- üìù –ü–æ–º–∏–ª–∫–∏ –ø—Ä–∏ –≤–≤–µ–¥–µ–Ω–Ω—ñ –¥–∞–Ω–∏—Ö
- üîß –¢–µ—Ö–Ω—ñ—á–Ω—ñ –ø—Ä–æ–±–ª–µ–º–∏ –ø—Ä–∏ –∑–±–∏—Ä–∞–Ω–Ω—ñ
- üôÖ –†–µ—Å–ø–æ–Ω–¥–µ–Ω—Ç–∏ –Ω–µ –≤—ñ–¥–ø–æ–≤—ñ–ª–∏ –Ω–∞ –ø–∏—Ç–∞–Ω–Ω—è
- üîÑ –û–±'—î–¥–Ω–∞–Ω–Ω—è —Ä—ñ–∑–Ω–∏—Ö –¥–∂–µ—Ä–µ–ª –¥–∞–Ω–∏—Ö
- ‚è≥ –î–∞–Ω—ñ —â–µ –Ω–µ –¥–æ—Å—Ç—É–ø–Ω—ñ

---

## –¢–∏–ø–∏ –ø—Ä–æ–ø—É—â–µ–Ω–∏—Ö –¥–∞–Ω–∏—Ö

### 1. MCAR (Missing Completely At Random)

**–í–∏–ø–∞–¥–∫–æ–≤—ñ –ø—Ä–æ–ø—É—Å–∫–∏ –±–µ–∑ –∑–≤'—è–∑–∫—É –∑ –¥–∞–Ω–∏–º–∏**
- –ü—Ä–∏–∫–ª–∞–¥: —Ç–µ—Ö–Ω—ñ—á–Ω–∞ –ø–æ–º–∏–ª–∫–∞ –¥–∞—Ç—á–∏–∫–∞
- –ù–∞–π–ø—Ä–æ—Å—Ç—ñ—à–∏–π –≤–∏–ø–∞–¥–æ–∫
- –ë–µ–∑–ø–µ—á–Ω–æ –≤–∏–¥–∞–ª—è—Ç–∏

### 2. MAR (Missing At Random)

**–ü—Ä–æ–ø—É—Å–∫–∏ –∑–∞–ª–µ–∂–∞—Ç—å –≤—ñ–¥ —ñ–Ω—à–∏—Ö –∑–º—ñ–Ω–Ω–∏—Ö**
- –ü—Ä–∏–∫–ª–∞–¥: —á–æ–ª–æ–≤—ñ–∫–∏ —Ä—ñ–¥—à–µ –≤–∫–∞–∑—É—é—Ç—å –≤–∞–≥—É
- –ú–æ–∂–Ω–∞ –º–æ–¥–µ–ª—é–≤–∞—Ç–∏ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ
- –ü–æ—Ç—Ä—ñ–±–Ω–∞ –æ–±–µ—Ä–µ–∂–Ω—ñ—Å—Ç—å –ø—Ä–∏ –≤–∏–¥–∞–ª–µ–Ω–Ω—ñ

### 3. MNAR (Missing Not At Random)

**–ü—Ä–æ–ø—É—Å–∫–∏ –∑–∞–ª–µ–∂–∞—Ç—å –≤—ñ–¥ —Å–∞–º–∏—Ö –≤—ñ–¥—Å—É—Ç–Ω—ñ—Ö –∑–Ω–∞—á–µ–Ω—å**
- –ü—Ä–∏–∫–ª–∞–¥: –ª—é–¥–∏ –∑ –Ω–∏–∑—å–∫–∏–º –¥–æ—Ö–æ–¥–æ–º –Ω–µ –≤–∫–∞–∑—É—é—Ç—å –∑–∞—Ä–ø–ª–∞—Ç—É
- –ù–∞–π—Å–∫–ª–∞–¥–Ω—ñ—à–∏–π –≤–∏–ø–∞–¥–æ–∫
- –í–∏–¥–∞–ª–µ–Ω–Ω—è –º–æ–∂–µ –ø—Ä–∏–∑–≤–µ—Å—Ç–∏ –¥–æ bias

---

## –í–∏—è–≤–ª–µ–Ω–Ω—è –ø—Ä–æ–ø—É—Å–∫—ñ–≤

### –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞—è–≤–Ω–æ—Å—Ç—ñ

```python
import pandas as pd
import numpy as np

# –ü—ñ–¥—Ä–∞—Ö—É–Ω–æ–∫ –ø—Ä–æ–ø—É—Å–∫—ñ–≤
print(df.isnull().sum())

# –í—ñ–¥—Å–æ—Ç–æ–∫ –ø—Ä–æ–ø—É—Å–∫—ñ–≤
print(df.isnull().sum() / len(df) * 100)

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
import missingno as msno
msno.matrix(df)
```

### –î–µ—Ç–∞–ª—å–Ω–∏–π –∞–Ω–∞–ª—ñ–∑

```python
# –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –∑–≤—ñ—Ç—É
missing_report = pd.DataFrame({
    '–ö—ñ–ª—å–∫—ñ—Å—Ç—å': df.isnull().sum(),
    '–í—ñ–¥—Å–æ—Ç–æ–∫': df.isnull().sum() / len(df) * 100
})
missing_report = missing_report[missing_report['–ö—ñ–ª—å–∫—ñ—Å—Ç—å'] > 0]
print(missing_report.sort_values('–í—ñ–¥—Å–æ—Ç–æ–∫', ascending=False))
```

### –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –ø–∞—Ç–µ—Ä–Ω—ñ–≤

```python
import missingno as msno
import matplotlib.pyplot as plt

# –ú–∞—Ç—Ä–∏—Ü—è –ø—Ä–æ–ø—É—Å–∫—ñ–≤
msno.matrix(df)

# Heatmap –∫–æ—Ä–µ–ª—è—Ü—ñ–π –ø—Ä–æ–ø—É—Å–∫—ñ–≤
msno.heatmap(df)

# Dendrogram –¥–ª—è –≥—Ä—É–ø—É–≤–∞–Ω–Ω—è
msno.dendrogram(df)
```

---

## –ú–µ—Ç–æ–¥–∏ –æ–±—Ä–æ–±–∫–∏

## 1. –í–∏–¥–∞–ª–µ–Ω–Ω—è (Deletion)

### A) –í–∏–¥–∞–ª–µ–Ω–Ω—è —Ä—è–¥–∫—ñ–≤

```python
# –í–∏–¥–∞–ª–∏—Ç–∏ –≤—Å—ñ —Ä—è–¥–∫–∏ –∑ –±—É–¥—å-—è–∫–∏–º NaN
df_clean = df.dropna()

# –í–∏–¥–∞–ª–∏—Ç–∏ —Ä—è–¥–∫–∏, –¥–µ –≤—Å—ñ –∑–Ω–∞—á–µ–Ω–Ω—è NaN
df_clean = df.dropna(how='all')

# –í–∏–¥–∞–ª–∏—Ç–∏ —Ä—è–¥–∫–∏ –∑ NaN —É –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∏—Ö —Å—Ç–æ–≤–ø—Ü—è—Ö
df_clean = df.dropna(subset=['–≤—ñ–∫', '–∑–∞—Ä–ø–ª–∞—Ç–∞'])

# –ü–æ—Ä—ñ–≥: –∑–∞–ª–∏—à–∏—Ç–∏ —Ä—è–¥–∫–∏ –∑ –º—ñ–Ω—ñ–º—É–º N –Ω–µ-NaN –∑–Ω–∞—á–µ–Ω—å
df_clean = df.dropna(thresh=5)
```

### B) –í–∏–¥–∞–ª–µ–Ω–Ω—è —Å—Ç–æ–≤–ø—Ü—ñ–≤

```python
# –í–∏–¥–∞–ª–∏—Ç–∏ —Å—Ç–æ–≤–ø—Ü—ñ –∑ –±—É–¥—å-—è–∫–∏–º NaN
df_clean = df.dropna(axis=1)

# –í–∏–¥–∞–ª–∏—Ç–∏ —Å—Ç–æ–≤–ø—Ü—ñ –∑ >50% –ø—Ä–æ–ø—É—Å–∫—ñ–≤
threshold = 0.5
df_clean = df.loc[:, df.isnull().mean() < threshold]
```

### ‚ö†Ô∏è –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏?

‚úÖ –ú–∞–ª–∏–π –≤—ñ–¥—Å–æ—Ç–æ–∫ –ø—Ä–æ–ø—É—Å–∫—ñ–≤ (< 5%)
‚úÖ MCAR —Ç–∏–ø –ø—Ä–æ–ø—É—Å–∫—ñ–≤
‚úÖ –ë–∞–≥–∞—Ç–æ –¥–∞–Ω–∏—Ö (–º–æ–∂–Ω–∞ –¥–æ–∑–≤–æ–ª–∏—Ç–∏ –≤—Ç—Ä–∞—Ç—É)

‚ùå –í–µ–ª–∏–∫–∏–π –≤—ñ–¥—Å–æ—Ç–æ–∫ –ø—Ä–æ–ø—É—Å–∫—ñ–≤
‚ùå MAR/MNAR —Ç–∏–ø–∏ (—Ä–∏–∑–∏–∫ bias)

---

## 2. Imputation (–ó–∞–ø–æ–≤–Ω–µ–Ω–Ω—è)

### A) –ö–æ–Ω—Å—Ç–∞–Ω—Ç–æ—é

```python
# –ó–∞–ø–æ–≤–Ω–∏—Ç–∏ –Ω—É–ª—è–º–∏
df['–≤—ñ–∫'].fillna(0, inplace=True)

# –ó–∞–ø–æ–≤–Ω–∏—Ç–∏ –≤–ª–∞—Å–Ω–∏–º –∑–Ω–∞—á–µ–Ω–Ω—è–º
df['–∫–∞—Ç–µ–≥–æ—Ä—ñ—è'].fillna('–ù–µ–≤—ñ–¥–æ–º–æ', inplace=True)

# –ó–∞–ø–æ–≤–Ω–∏—Ç–∏ —Ä—ñ–∑–Ω–∏–º–∏ –∑–Ω–∞—á–µ–Ω–Ω—è–º–∏ –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö —Å—Ç–æ–≤–ø—Ü—ñ–≤
df.fillna({
    '–≤—ñ–∫': 0,
    '–∑–∞—Ä–ø–ª–∞—Ç–∞': df['–∑–∞—Ä–ø–ª–∞—Ç–∞'].median(),
    '–º—ñ—Å—Ç–æ': '–Ü–Ω—à–µ'
}, inplace=True)
```

### B) –°—Ç–∞—Ç–∏—Å—Ç–∏—á–Ω–∏–º–∏ –ø–æ–∫–∞–∑–Ω–∏–∫–∞–º–∏

#### –°–µ—Ä–µ–¥–Ω—î (Mean)

```python
# Pandas
df['–≤—ñ–∫'].fillna(df['–≤—ñ–∫'].mean(), inplace=True)

# Scikit-learn
from sklearn.impute import SimpleImputer

imputer = SimpleImputer(strategy='mean')
df[['–≤—ñ–∫', '–∑–∞—Ä–ø–ª–∞—Ç–∞']] = imputer.fit_transform(df[['–≤—ñ–∫', '–∑–∞—Ä–ø–ª–∞—Ç–∞']])
```

#### –ú–µ–¥—ñ–∞–Ω–∞ (Median)

```python
# Pandas
df['—Ü—ñ–Ω–∞'].fillna(df['—Ü—ñ–Ω–∞'].median(), inplace=True)

# Scikit-learn
imputer = SimpleImputer(strategy='median')
df[['—Ü—ñ–Ω–∞']] = imputer.fit_transform(df[['—Ü—ñ–Ω–∞']])
```

#### –ú–æ–¥–∞ (Most Frequent)

```python
# Pandas
df['–∫–∞—Ç–µ–≥–æ—Ä—ñ—è'].fillna(df['–∫–∞—Ç–µ–≥–æ—Ä—ñ—è'].mode()[0], inplace=True)

# Scikit-learn
imputer = SimpleImputer(strategy='most_frequent')
df[['–∫–∞—Ç–µ–≥–æ—Ä—ñ—è']] = imputer.fit_transform(df[['–∫–∞—Ç–µ–≥–æ—Ä—ñ—è']])
```

### –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è —Å—Ç—Ä–∞—Ç–µ–≥—ñ–π

| –°—Ç—Ä–∞—Ç–µ–≥—ñ—è | –¢–∏–ø –¥–∞–Ω–∏—Ö | –ü–µ—Ä–µ–≤–∞–≥–∏ | –ù–µ–¥–æ–ª—ñ–∫–∏ |
|-----------|-----------|----------|----------|
| Mean | –ß–∏—Å–ª–æ–≤—ñ | –ü—Ä–æ—Å—Ç–∞ | –ß—É—Ç–ª–∏–≤–∞ –¥–æ –≤–∏–∫–∏–¥—ñ–≤ |
| Median | –ß–∏—Å–ª–æ–≤—ñ | –†–æ–±–∞—Å—Ç–Ω–∞ –¥–æ –≤–∏–∫–∏–¥—ñ–≤ | –ú–æ–∂–µ –Ω–µ –≤—ñ–¥–æ–±—Ä–∞–∂–∞—Ç–∏ —Ä–æ–∑–ø–æ–¥—ñ–ª |
| Mode | –ö–∞—Ç–µ–≥–æ—Ä—ñ–∞–ª—å–Ω—ñ | –ù–∞–π–±—ñ–ª—å—à –≤—ñ—Ä–æ–≥—ñ–¥–Ω–µ –∑–Ω–∞—á–µ–Ω–Ω—è | –ú–æ–∂–µ –±—É—Ç–∏ –±–∞–≥–∞—Ç–æ –º–æ–¥ |
| –ö–æ–Ω—Å—Ç–∞–Ω—Ç–∞ | –ë—É–¥—å-—è–∫—ñ | –ü–æ–≤–Ω–∏–π –∫–æ–Ω—Ç—Ä–æ–ª—å | –î–æ–≤—ñ–ª—å–Ω–∏–π –≤–∏–±—ñ—Ä |

---

### C) Forward Fill / Backward Fill

**–î–ª—è —á–∞—Å–æ–≤–∏—Ö —Ä—è–¥—ñ–≤**

```python
# Forward fill (–ø–æ–ø–µ—Ä–µ–¥–Ω—î –∑–Ω–∞—á–µ–Ω–Ω—è)
df['—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞'].fillna(method='ffill', inplace=True)

# Backward fill (–Ω–∞—Å—Ç—É–ø–Ω–µ –∑–Ω–∞—á–µ–Ω–Ω—è)
df['—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞'].fillna(method='bfill', inplace=True)

# –ó –æ–±–º–µ–∂–µ–Ω–Ω—è–º –∫—ñ–ª—å–∫–æ—Å—Ç—ñ –∑–∞–ø–æ–≤–Ω–µ–Ω—å
df['—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞'].fillna(method='ffill', limit=2, inplace=True)
```

### D) –Ü–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü—ñ—è

```python
# –õ—ñ–Ω—ñ–π–Ω–∞ —ñ–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü—ñ—è
df['–∑–Ω–∞—á–µ–Ω–Ω—è'].interpolate(method='linear', inplace=True)

# –ü–æ–ª—ñ–Ω–æ–º—ñ–∞–ª—å–Ω–∞
df['–∑–Ω–∞—á–µ–Ω–Ω—è'].interpolate(method='polynomial', order=2, inplace=True)

# –î–ª—è —á–∞—Å–æ–≤–∏—Ö —Ä—è–¥—ñ–≤
df['–¥–∞—Ç–∞'] = pd.to_datetime(df['–¥–∞—Ç–∞'])
df.set_index('–¥–∞—Ç–∞', inplace=True)
df['–∑–Ω–∞—á–µ–Ω–Ω—è'].interpolate(method='time', inplace=True)
```

---

### E) KNN Imputation

**–ó–∞–ø–æ–≤–Ω–µ–Ω–Ω—è –Ω–∞ –æ—Å–Ω–æ–≤—ñ k –Ω–∞–π–±–ª–∏–∂—á–∏—Ö —Å—É—Å—ñ–¥—ñ–≤**

```python
from sklearn.impute import KNNImputer

imputer = KNNImputer(n_neighbors=5)
df_imputed = pd.DataFrame(
    imputer.fit_transform(df),
    columns=df.columns
)
```

### –Ø–∫ –ø—Ä–∞—Ü—é—î?

1. –î–ª—è –∫–æ–∂–Ω–æ–≥–æ –ø—Ä–æ–ø—É—Å–∫—É –∑–Ω–∞—Ö–æ–¥–∏—Ç—å k –Ω–∞–π–±–ª–∏–∂—á–∏—Ö —Ä—è–¥–∫—ñ–≤ (–±–µ–∑ –ø—Ä–æ–ø—É—Å–∫—ñ–≤)
2. –û–±—á–∏—Å–ª—é—î —Å–µ—Ä–µ–¥–Ω—î –∑–Ω–∞—á–µ–Ω–Ω—è –∑ —Ü–∏—Ö k —Å—É—Å—ñ–¥—ñ–≤
3. –ó–∞–ø–æ–≤–Ω—é—î –ø—Ä–æ–ø—É—Å–∫ —Ü–∏–º —Å–µ—Ä–µ–¥–Ω—ñ–º

### –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏?

‚úÖ –î–∞–Ω—ñ –º–∞—é—Ç—å —Å–∏–ª—å–Ω—ñ –∫–æ—Ä–µ–ª—è—Ü—ñ—ó –º—ñ–∂ –æ–∑–Ω–∞–∫–∞–º–∏
‚úÖ –ù–µ–≤–µ–ª–∏–∫—ñ/—Å–µ—Ä–µ–¥–Ω—ñ –¥–∞—Ç–∞—Å–µ—Ç–∏ (–æ–±—á–∏—Å–ª—é–≤–∞–ª—å–Ω–æ –¥–æ—Ä–æ–≥–æ)
‚úÖ –°–∫–ª–∞–¥–Ω—ñ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ –º—ñ–∂ –∑–º—ñ–Ω–Ω–∏–º–∏

‚ùå –î—É–∂–µ –≤–µ–ª–∏–∫—ñ –¥–∞—Ç–∞—Å–µ—Ç–∏ (–ø–æ–≤—ñ–ª—å–Ω–æ)
‚ùå –ö–∞—Ç–µ–≥–æ—Ä—ñ–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ (–ø–æ—Ç—Ä–µ–±—É—é—Ç—å –∫–æ–¥—É–≤–∞–Ω–Ω—è)

---

### F) Iterative Imputation (MICE)

**Multiple Imputation by Chained Equations**

```python
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer

imputer = IterativeImputer(
    max_iter=10,
    random_state=42
)
df_imputed = pd.DataFrame(
    imputer.fit_transform(df),
    columns=df.columns
)
```

### –Ø–∫ –ø—Ä–∞—Ü—é—î?

1. –ü–æ—á–∞—Ç–∫–æ–≤–µ –∑–∞–ø–æ–≤–Ω–µ–Ω–Ω—è (median/mean)
2. –î–ª—è –∫–æ–∂–Ω–æ—ó –∑–º—ñ–Ω–Ω–æ—ó:
   - –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î —ñ–Ω—à—ñ –∑–º—ñ–Ω–Ω—ñ –¥–ª—è –ø—Ä–µ–¥–∏–∫—Ü—ñ—ó
   - –ù–∞–≤—á–∞—î –º–æ–¥–µ–ª—å —Ä–µ–≥—Ä–µ—Å—ñ—ó
   - –û–Ω–æ–≤–ª—é—î –ø—Ä–æ–ø—É—â–µ–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è
3. –ü–æ–≤—Ç–æ—Ä—é—î —Ü–∏–∫–ª –∫—ñ–ª—å–∫–∞ —Ä–∞–∑—ñ–≤

### –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏?

‚úÖ –°–∫–ª–∞–¥–Ω—ñ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ –º—ñ–∂ –∑–º—ñ–Ω–Ω–∏–º–∏
‚úÖ MAR —Ç–∏–ø –ø—Ä–æ–ø—É—Å–∫—ñ–≤
‚úÖ –ü–æ—Ç—Ä—ñ–±–Ω–∞ –≤–∏—Å–æ–∫–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å

‚ùå –û–±—á–∏—Å–ª—é–≤–∞–ª—å–Ω–æ –¥–æ—Ä–æ–≥–æ
‚ùå –ú–æ–∂–ª–∏–≤–∏–π overfitting

---

## 3. –°—Ç–≤–æ—Ä–µ–Ω–Ω—è —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞ –ø—Ä–æ–ø—É—Å–∫—ñ–≤

### –ß–æ–º—É –≤–∞–∂–ª–∏–≤–æ?

–°–∞–º —Ñ–∞–∫—Ç –ø—Ä–æ–ø—É—Å–∫—É –º–æ–∂–µ –±—É—Ç–∏ —ñ–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–∏–º!

```python
# –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –±—ñ–Ω–∞—Ä–Ω–æ–≥–æ —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞
df['–≤—ñ–∫_–≤—ñ–¥—Å—É—Ç–Ω—ñ–π'] = df['–≤—ñ–∫'].isnull().astype(int)

# –ü–æ—Ç—ñ–º –∑–∞–ø–æ–≤–Ω–µ–Ω–Ω—è
df['–≤—ñ–∫'].fillna(df['–≤—ñ–∫'].median(), inplace=True)
```

### –ü—Ä–∏–∫–ª–∞–¥ –∑ MissingIndicator

```python
from sklearn.impute import MissingIndicator

indicator = MissingIndicator()
indicator_array = indicator.fit_transform(df)

# –î–æ–¥–∞–≤–∞–Ω–Ω—è –¥–æ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º—É
df_with_indicators = pd.concat([
    df,
    pd.DataFrame(indicator_array, columns=[f'{col}_missing' 
                                           for col in df.columns])
], axis=1)
```

---

## –ü–æ–≤–Ω–∏–π Pipeline

```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier

# 1. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö
df = pd.read_csv('data.csv')

# 2. –ê–Ω–∞–ª—ñ–∑ –ø—Ä–æ–ø—É—Å–∫—ñ–≤
print("–ü—Ä–æ–ø—É—Å–∫–∏ –¥–æ –æ–±—Ä–æ–±–∫–∏:")
print(df.isnull().sum())

# 3. –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –Ω–∞ —á–∏—Å–ª–æ–≤—ñ —Ç–∞ –∫–∞—Ç–µ–≥–æ—Ä—ñ–∞–ª—å–Ω—ñ
numeric_features = df.select_dtypes(include=[np.number]).columns
categorical_features = df.select_dtypes(include=['object']).columns

# 4. –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –Ω–∞ train/test
X = df.drop('target', axis=1)
y = df['target']
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# 5. Imputation –¥–ª—è —á–∏—Å–ª–æ–≤–∏—Ö –∑–º—ñ–Ω–Ω–∏—Ö
numeric_imputer = SimpleImputer(strategy='median')
X_train[numeric_features] = numeric_imputer.fit_transform(
    X_train[numeric_features]
)
X_test[numeric_features] = numeric_imputer.transform(
    X_test[numeric_features]
)

# 6. Imputation –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä—ñ–∞–ª—å–Ω–∏—Ö –∑–º—ñ–Ω–Ω–∏—Ö
categorical_imputer = SimpleImputer(strategy='most_frequent')
X_train[categorical_features] = categorical_imputer.fit_transform(
    X_train[categorical_features]
)
X_test[categorical_features] = categorical_imputer.transform(
    X_test[categorical_features]
)

# 7. –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞
print("\n–ü—Ä–æ–ø—É—Å–∫–∏ –ø—ñ—Å–ª—è –æ–±—Ä–æ–±–∫–∏:")
print(X_train.isnull().sum())
print(X_test.isnull().sum())
```

---

## Pipeline –∑ sklearn

```python
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder

# –í–∏–∑–Ω–∞—á–µ–Ω–Ω—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ñ–≤ –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö —Ç–∏–ø—ñ–≤ –¥–∞–Ω–∏—Ö
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# –û–±'—î–¥–Ω–∞–Ω–Ω—è
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)
    ]
)

# –ü–æ–≤–Ω–∏–π pipeline –∑ –º–æ–¥–µ–ª–ª—é
full_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', RandomForestClassifier())
])

# –ù–∞–≤—á–∞–Ω–Ω—è
full_pipeline.fit(X_train, y_train)

# –ü—Ä–µ–¥–∏–∫—Ü—ñ—è
y_pred = full_pipeline.predict(X_test)
```

---

## –í–∏–±—ñ—Ä –º–µ—Ç–æ–¥—É: Decision Tree üå≥

```
                       –ü—Ä–æ–ø—É—Å–∫–∏ < 5%?
                      /              \
                   –¢–∞–∫                –ù—ñ
                    |                  |
              –í–∏–¥–∞–ª–∏—Ç–∏          –¢–∏–ø –¥–∞–Ω–∏—Ö?
                                /         \
                         –ß–∏—Å–ª–æ–≤—ñ      –ö–∞—Ç–µ–≥–æ—Ä—ñ–∞–ª—å–Ω—ñ
                             |               |
                      –í–∏–∫–∏–¥–∏?         Most Frequent
                        /    \
                     –¢–∞–∫      –ù—ñ
                      |        |
                  Median    Mean/KNN
```

---

## –ü–æ—Ä–∞–¥–∏ —Ç–∞ Best Practices üí°

### 1. –ó–∞–≤–∂–¥–∏ –∞–Ω–∞–ª—ñ–∑—É–π—Ç–µ –ø–∞—Ç–µ—Ä–Ω–∏

```python
# –ö–æ—Ä–µ–ª—è—Ü—ñ—è –º—ñ–∂ –ø—Ä–æ–ø—É—Å–∫–∞–º–∏
import seaborn as sns

missing_corr = df.isnull().corr()
sns.heatmap(missing_corr, annot=True)
```

### 2. Fit –Ω–∞ train, transform –Ω–∞ test

```python
# ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û
imputer.fit(X_train)
X_train_imputed = imputer.transform(X_train)
X_test_imputed = imputer.transform(X_test)

# ‚ùå –ù–ï–ü–†–ê–í–ò–õ–¨–ù–û
imputer.fit(X_test)  # –í–∏—Ç—ñ–∫ –¥–∞–Ω–∏—Ö!
```

### 3. –î–æ–∫—É–º–µ–Ω—Ç—É–π—Ç–µ —Ä—ñ—à–µ–Ω–Ω—è

```python
# –°—Ç–≤–æ—Ä—é—î–º–æ —Å–ª–æ–≤–Ω–∏–∫ —Å—Ç—Ä–∞—Ç–µ–≥—ñ–π
imputation_strategy = {
    '–≤—ñ–∫': 'median',
    '–∑–∞—Ä–ø–ª–∞—Ç–∞': 'mean',
    '–º—ñ—Å—Ç–æ': 'most_frequent',
    '–∫–∞—Ç–µ–≥–æ—Ä—ñ—è': 'constant:–ù–µ–≤—ñ–¥–æ–º–æ'
}

# –ó–±–µ—Ä—ñ–≥–∞—î–º–æ metadata
import json
with open('imputation_config.json', 'w') as f:
    json.dump(imputation_strategy, f)
```

### 4. –ü–µ—Ä–µ–≤—ñ—Ä—è–π—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏

```python
# –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è —Ä–æ–∑–ø–æ–¥—ñ–ª—ñ–≤ –¥–æ/–ø—ñ—Å–ª—è
import matplotlib.pyplot as plt

fig, axes = plt.subplots(1, 2, figsize=(12, 4))

# –î–æ imputation
df['–≤—ñ–∫'].dropna().hist(bins=30, ax=axes[0])
axes[0].set_title('–î–æ imputation')

# –ü—ñ—Å–ª—è imputation
df_imputed['–≤—ñ–∫'].hist(bins=30, ax=axes[1])
axes[1].set_title('–ü—ñ—Å–ª—è imputation')

plt.show()
```

### 5. –ï–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—É–π—Ç–µ

```python
# –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è —Ä—ñ–∑–Ω–∏—Ö —Å—Ç—Ä–∞—Ç–µ–≥—ñ–π
from sklearn.model_selection import cross_val_score

strategies = ['mean', 'median', 'most_frequent']
results = {}

for strategy in strategies:
    imputer = SimpleImputer(strategy=strategy)
    X_imputed = imputer.fit_transform(X_train)
    
    score = cross_val_score(model, X_imputed, y_train, cv=5).mean()
    results[strategy] = score

print(results)
```

---

## –ü–æ—à–∏—Ä–µ–Ω—ñ –ø–æ–º–∏–ª–∫–∏ ‚ùå

### 1. –í–∏—Ç—ñ–∫ –¥–∞–Ω–∏—Ö

```python
# ‚ùå –ù–ï–ü–†–ê–í–ò–õ–¨–ù–û
imputer.fit(pd.concat([X_train, X_test]))

# ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û
imputer.fit(X_train)
```

### 2. –Ü–≥–Ω–æ—Ä—É–≤–∞–Ω–Ω—è —Ç–∏–ø—É –ø—Ä–æ–ø—É—Å–∫—ñ–≤

```python
# –Ø–∫—â–æ MNAR, –ø—Ä–æ—Å—Ç–∏–π imputation –º–æ–∂–µ –¥–∞—Ç–∏ bias
# –ö—Ä–∞—â–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä–∏ –∞–±–æ –º–æ–¥–µ–ª—é–≤–∞–Ω–Ω—è
```

### 3. –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è mean –ø—Ä–∏ –≤–∏–∫–∏–¥–∞—Ö

```python
# ‚ùå Mean –ø—Ä–∏ –≤–∏–∫–∏–¥–∞—Ö
df['—Ü—ñ–Ω–∞'].fillna(df['—Ü—ñ–Ω–∞'].mean())

# ‚úÖ Median –ø—Ä–∏ –≤–∏–∫–∏–¥–∞—Ö
df['—Ü—ñ–Ω–∞'].fillna(df['—Ü—ñ–Ω–∞'].median())
```

---

## –ü–æ–≤'—è–∑–∞–Ω—ñ —Ç–µ–º–∏

- [[01_Feature_Scaling]] ‚Äî –º–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è –ø—ñ—Å–ª—è imputation
- [[02_Categorical_Encoding]] ‚Äî –∫–æ–¥—É–≤–∞–Ω–Ω—è –∫–∞—Ç–µ–≥–æ—Ä—ñ–π
- [[04_Outliers_Handling]] ‚Äî —Ä–æ–±–æ—Ç–∞ –∑ –≤–∏–∫–∏–¥–∞–º–∏

## –†–µ—Å—É—Ä—Å–∏

- [Scikit-learn Imputation](https://scikit-learn.org/stable/modules/impute.html)
- [Missing Data: A Gentle Introduction](https://stefvanbuuren.name/fimd/)
- [Missingno Library](https://github.com/ResidentMario/missingno)

---

#ml #preprocessing #missing-values #imputation #datascience
