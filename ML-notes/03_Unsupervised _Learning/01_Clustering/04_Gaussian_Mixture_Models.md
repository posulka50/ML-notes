# Gaussian Mixture Models (GMM)

## –©–æ —Ü–µ?

**Gaussian Mixture Model (GMM)** ‚Äî —Ü–µ **probabilistic** –∞–ª–≥–æ—Ä–∏—Ç–º unsupervised learning, —è–∫–∏–π –º–æ–¥–µ–ª—é—î –¥–∞–Ω—ñ —è–∫ —Å—É–º—ñ—à (mixture) –∫—ñ–ª—å–∫–æ—Ö **–±–∞–≥–∞—Ç–æ–≤–∏–º—ñ—Ä–Ω–∏—Ö –Ω–æ—Ä–º–∞–ª—å–Ω–∏—Ö (Gaussian) —Ä–æ–∑–ø–æ–¥—ñ–ª—ñ–≤**.

**–ì–æ–ª–æ–≤–Ω–∞ —ñ–¥–µ—è:** –∫–æ–∂–µ–Ω –∫–ª–∞—Å—Ç–µ—Ä –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π Gaussian —Ä–æ–∑–ø–æ–¥—ñ–ª–æ–º –∑ –≤–ª–∞—Å–Ω–∏–º —Å–µ—Ä–µ–¥–Ω—ñ–º (mean) —Ç–∞ –∫–æ–≤–∞—Ä—ñ–∞—Ü—ñ—î—é (covariance). –ö–æ–∂–Ω–∞ —Ç–æ—á–∫–∞ –º–∞—î **–π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å –Ω–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ** –¥–æ –∫–æ–∂–Ω–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞, –∞ –Ω–µ –∂–æ—Ä—Å—Ç–∫—É –º—ñ—Ç–∫—É.

## –ù–∞–≤—ñ—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–∞?

- üé≤ **Soft clustering** ‚Äî –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ –∑–∞–º—ñ—Å—Ç—å –∂–æ—Ä—Å—Ç–∫–∏—Ö –º—ñ—Ç–æ–∫
- üîî **–ì–Ω—É—á–∫–∞ —Ñ–æ—Ä–º–∞** ‚Äî –µ–ª—ñ–ø—Ç–∏—á–Ω—ñ –∫–ª–∞—Å—Ç–µ—Ä–∏ —Ä—ñ–∑–Ω–∏—Ö —Ñ–æ—Ä–º
- üìä **Density estimation** ‚Äî –º–æ–¥–µ–ª—é–≤–∞–Ω–Ω—è —Ä–æ–∑–ø–æ–¥—ñ–ª—É –¥–∞–Ω–∏—Ö
- üéØ **Uncertainty** ‚Äî –∫–≤–∞–Ω—Ç–∏—Ñ—ñ–∫–∞—Ü—ñ—è –≤–ø–µ–≤–Ω–µ–Ω–æ—Å—Ç—ñ –≤ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—ó
- üß¨ **Generative model** ‚Äî –º–æ–∂–Ω–∞ –≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ –Ω–æ–≤—ñ —Ç–æ—á–∫–∏
- üìà **–ü–µ—Ä–µ–∫—Ä–∏—Ç—Ç—è –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤** ‚Äî —Ç–æ—á–∫–∏ –º–æ–∂—É—Ç—å —á–∞—Å—Ç–∫–æ–≤–æ –Ω–∞–ª–µ–∂–∞—Ç–∏ –¥–µ–∫—ñ–ª—å–∫–æ–º –∫–ª–∞—Å—Ç–µ—Ä–∞–º

## –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏?

**–ü–æ—Ç—Ä—ñ–±–Ω–æ:**
- **Soft clustering** ‚Äî –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ –Ω–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ –≤–∞–∂–ª–∏–≤—ñ
- **–ï–ª—ñ–ø—Ç–∏—á–Ω—ñ –∫–ª–∞—Å—Ç–µ—Ä–∏** —Ä—ñ–∑–Ω–∏—Ö —Ñ–æ—Ä–º/–æ—Ä—ñ—î–Ω—Ç–∞—Ü—ñ–π
- **–ü–µ—Ä–µ–∫—Ä–∏—Ç—Ç—è –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤** ‚Äî –Ω–µ—á—ñ—Ç–∫—ñ –≥—Ä–∞–Ω–∏—Ü—ñ
- **Density estimation** ‚Äî –º–æ–¥–µ–ª—é–≤–∞–Ω–Ω—è —Ä–æ–∑–ø–æ–¥—ñ–ª—É
- **–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –¥–∞–Ω–∏—Ö** ‚Äî –ø–æ—Ç—Ä—ñ–±–µ–Ω generative model
- **Uncertainty quantification** ‚Äî –Ω–∞—Å–∫—ñ–ª—å–∫–∏ –≤–ø–µ–≤–Ω–µ–Ω—ñ –≤ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—ó
- **Statistically motivated** ‚Äî –ø–æ—Ç—Ä—ñ–±–Ω–µ —Ç–µ–æ—Ä–µ—Ç–∏—á–Ω–µ –æ–±“ë—Ä—É–Ω—Ç—É–≤–∞–Ω–Ω—è

**–ù–µ –ø–æ—Ç—Ä—ñ–±–Ω–æ:**
- –ö–ª–∞—Å—Ç–µ—Ä–∏ **–¥—É–∂–µ —Å–∫–ª–∞–¥–Ω–æ—ó —Ñ–æ—Ä–º–∏** ‚Üí DBSCAN
- –ü–æ—Ç—Ä—ñ–±–Ω–∞ **—à–≤–∏–¥–∫—ñ—Å—Ç—å** ‚Üí K-Means
- **Categorical features** ‚Üí K-Modes
- –î—É–∂–µ **–≤–µ–ª–∏–∫—ñ –¥–∞–Ω—ñ** ‚Üí K-Means (—à–≤–∏–¥—à–µ)

---

## –û—Å–Ω–æ–≤–Ω—ñ –∫–æ–Ω—Ü–µ–ø—Ü—ñ—ó

### 1. Gaussian (Normal) Distribution

**–û–¥–Ω–æ–≤–∏–º—ñ—Ä–Ω–∏–π Gaussian:**

$$p(x | \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)$$

–¥–µ:
- $\mu$ ‚Äî —Å–µ—Ä–µ–¥–Ω—î (mean)
- $\sigma^2$ ‚Äî –¥–∏—Å–ø–µ—Ä—Å—ñ—è (variance)

**–ë–∞–≥–∞—Ç–æ–≤–∏–º—ñ—Ä–Ω–∏–π Gaussian:**

$$p(\mathbf{x} | \boldsymbol{\mu}, \boldsymbol{\Sigma}) = \frac{1}{(2\pi)^{d/2}|\boldsymbol{\Sigma}|^{1/2}} \exp\left(-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^T\boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})\right)$$

–¥–µ:
- $\boldsymbol{\mu}$ ‚Äî –≤–µ–∫—Ç–æ—Ä —Å–µ—Ä–µ–¥–Ω—ñ—Ö (mean vector)
- $\boldsymbol{\Sigma}$ ‚Äî –º–∞—Ç—Ä–∏—Ü—è –∫–æ–≤–∞—Ä—ñ–∞—Ü—ñ—ó (covariance matrix)
- $d$ ‚Äî —Ä–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å
- $|\boldsymbol{\Sigma}|$ ‚Äî –¥–µ—Ç–µ—Ä–º—ñ–Ω–∞–Ω—Ç –∫–æ–≤–∞—Ä—ñ–∞—Ü—ñ—ó

### 2. Mixture Model

**GMM = –∑–≤–∞–∂–µ–Ω–∞ —Å—É–º–∞ K Gaussian —Ä–æ–∑–ø–æ–¥—ñ–ª—ñ–≤:**

$$p(\mathbf{x}) = \sum_{k=1}^{K} \pi_k \mathcal{N}(\mathbf{x} | \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)$$

–¥–µ:
- $K$ ‚Äî –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ñ–≤ (–∫–ª–∞—Å—Ç–µ—Ä—ñ–≤)
- $\pi_k$ ‚Äî **mixing coefficient** (–≤–∞–≥–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞), $\sum_{k=1}^{K} \pi_k = 1$
- $\mathcal{N}(\mathbf{x} | \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)$ ‚Äî k-–π Gaussian –∫–æ–º–ø–æ–Ω–µ–Ω—Ç

### 3. Soft Clustering (Responsibilities)

**Posterior probability (responsibility):** –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å —â–æ —Ç–æ—á–∫–∞ $\mathbf{x}_i$ –Ω–∞–ª–µ–∂–∏—Ç—å –∫–ª–∞—Å—Ç–µ—Ä—É $k$:

$$\gamma_{ik} = p(z_i = k | \mathbf{x}_i) = \frac{\pi_k \mathcal{N}(\mathbf{x}_i | \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)}{\sum_{j=1}^{K} \pi_j \mathcal{N}(\mathbf{x}_i | \boldsymbol{\mu}_j, \boldsymbol{\Sigma}_j)}$$

–¥–µ $z_i$ ‚Äî –ø—Ä–∏—Ö–æ–≤–∞–Ω–∞ –∑–º—ñ–Ω–Ω–∞ (latent variable), —â–æ –≤–∫–∞–∑—É—î –Ω–∞ –∫–ª–∞—Å—Ç–µ—Ä.

**–ü—Ä–∏–∫–ª–∞–¥:**
```
–¢–æ—á–∫–∞ x –º–∞—î:
- 70% –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å –Ω–∞–ª–µ–∂–∞—Ç–∏ –∫–ª–∞—Å—Ç–µ—Ä—É 1
- 25% –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å –Ω–∞–ª–µ–∂–∞—Ç–∏ –∫–ª–∞—Å—Ç–µ—Ä—É 2  
- 5% –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å –Ω–∞–ª–µ–∂–∞—Ç–∏ –∫–ª–∞—Å—Ç–µ—Ä—É 3
```

### 4. Covariance Types

**–§–æ—Ä–º–∞ –º–∞—Ç—Ä–∏—Ü—ñ –∫–æ–≤–∞—Ä—ñ–∞—Ü—ñ—ó –≤–∏–∑–Ω–∞—á–∞—î —Ñ–æ—Ä–º—É –∫–ª–∞—Å—Ç–µ—Ä–∞:**

#### Full Covariance

$$\boldsymbol{\Sigma}_k = \begin{bmatrix} \sigma_{11} & \sigma_{12} \\ \sigma_{21} & \sigma_{22} \end{bmatrix}$$

- ‚úÖ –ö–æ–∂–µ–Ω –∫–ª–∞—Å—Ç–µ—Ä –º–∞—î **–≤–ª–∞—Å–Ω—É —Ñ–æ—Ä–º—É** —Ç–∞ **–æ—Ä—ñ—î–Ω—Ç–∞—Ü—ñ—é**
- ‚úÖ –ù–∞–π–≥–Ω—É—á–∫—ñ—à–∏–π
- ‚ùå –ë–∞–≥–∞—Ç–æ –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤: $K \times d \times (d+1)/2$

```
–ü—Ä–∏–∫–ª–∞–¥: –µ–ª—ñ–ø—Å–∏ —Ä—ñ–∑–Ω–∏—Ö —Ñ–æ—Ä–º
    
    ‚óè‚óè‚óè‚óè           ‚óè
   ‚óè    ‚óè        ‚óè ‚óè ‚óè
  ‚óè      ‚óè      ‚óè  ‚óè  ‚óè
   ‚óè    ‚óè        ‚óè ‚óè ‚óè
    ‚óè‚óè‚óè‚óè           ‚óè
–®–∏—Ä–æ–∫–∏–π        –í—É–∑—å–∫–∏–π
–≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω–∏–π –≤–µ—Ä—Ç–∏–∫–∞–ª—å–Ω–∏–π
```

#### Tied Covariance

$$\boldsymbol{\Sigma}_k = \boldsymbol{\Sigma} \text{ –¥–ª—è –≤—Å—ñ—Ö } k$$

- –í—Å—ñ –∫–ª–∞—Å—Ç–µ—Ä–∏ –º–∞—é—Ç—å **–æ–¥–Ω–∞–∫–æ–≤—É —Ñ–æ—Ä–º—É** —Ç–∞ –æ—Ä—ñ—î–Ω—Ç–∞—Ü—ñ—é
- –¢—ñ–ª—å–∫–∏ –ø–æ–∑–∏—Ü—ñ—è –≤—ñ–¥—Ä—ñ–∑–Ω—è—î—Ç—å—Å—è
- –ú–µ–Ω—à–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤

#### Diagonal Covariance

$$\boldsymbol{\Sigma}_k = \begin{bmatrix} \sigma_{11} & 0 \\ 0 & \sigma_{22} \end{bmatrix}$$

- –ï–ª—ñ–ø—Å–∏ **–≤–∏—Ä—ñ–≤–Ω—è–Ω—ñ –ø–æ –æ—Å—è—Ö** (–±–µ–∑ –æ–±–µ—Ä—Ç–∞–Ω–Ω—è)
- –ö–æ–∂–Ω–∞ –≤—ñ—Å—å –Ω–µ–∑–∞–ª–µ–∂–Ω–∞
- –°–µ—Ä–µ–¥–Ω—è –∫—ñ–ª—å–∫—ñ—Å—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤

```
–ü—Ä–∏–∫–ª–∞–¥:
    ‚óè‚óè‚óè
   ‚óè   ‚óè
  ‚óè     ‚óè
   ‚óè   ‚óè
    ‚óè‚óè‚óè
–ï–ª—ñ–ø—Å –≤–∏—Ä—ñ–≤–Ω—è–Ω–∏–π –ø–æ –æ—Å—è—Ö
```

#### Spherical Covariance

$$\boldsymbol{\Sigma}_k = \sigma_k^2 \mathbf{I}$$

- **–ö—Ä—É–≥–æ–≤—ñ/—Å—Ñ–µ—Ä–∏—á–Ω—ñ** –∫–ª–∞—Å—Ç–µ—Ä–∏
- –ù–∞–π–º–µ–Ω—à–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤
- –ê–Ω–∞–ª–æ–≥—ñ—á–Ω–æ K-Means

```
–ü—Ä–∏–∫–ª–∞–¥:
   ‚óè‚óè‚óè
  ‚óè   ‚óè
  ‚óè   ‚óè
   ‚óè‚óè‚óè
–ö–æ–ª–æ (—Å—Ñ–µ—Ä–∞)
```

---

## EM Algorithm (Expectation-Maximization)

**GMM –Ω–∞–≤—á–∞—î—Ç—å—Å—è —á–µ—Ä–µ–∑ EM algorithm** ‚Äî —ñ—Ç–µ—Ä–∞—Ü—ñ–π–Ω–∏–π –º–µ—Ç–æ–¥ –¥–ª—è –∑–Ω–∞—Ö–æ–¥–∂–µ–Ω–Ω—è –º–∞–∫—Å–∏–º—É–º—É likelihood.

### –ê–ª–≥–æ—Ä–∏—Ç–º

**–í—Ö—ñ–¥:** –¥–∞–Ω—ñ $X$, –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ñ–≤ $K$

**1. –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è:**
   - –í–∏–ø–∞–¥–∫–æ–≤—ñ $\boldsymbol{\mu}_k$, $\boldsymbol{\Sigma}_k$, $\pi_k$
   - –ê–±–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ K-Means –¥–ª—è –ø–æ—á–∞—Ç–∫–æ–≤–∏—Ö —Ü–µ–Ω—Ç—Ä—ñ–≤

**2. –ü–æ–≤—Ç–æ—Ä—é–≤–∞—Ç–∏ –¥–æ –∑–±—ñ–∂–Ω–æ—Å—Ç—ñ:**

   **E-step (Expectation):**
   - –û–±—á–∏—Å–ª–∏—Ç–∏ responsibilities (posterior probabilities):
   
   $$\gamma_{ik} = \frac{\pi_k \mathcal{N}(\mathbf{x}_i | \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)}{\sum_{j=1}^{K} \pi_j \mathcal{N}(\mathbf{x}_i | \boldsymbol{\mu}_j, \boldsymbol{\Sigma}_j)}$$

   **M-step (Maximization):**
   - –û–Ω–æ–≤–∏—Ç–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –¥–ª—è –º–∞–∫—Å–∏–º—ñ–∑–∞—Ü—ñ—ó expected log-likelihood:
   
   **Mixing coefficients:**
   $$\pi_k^{new} = \frac{1}{N} \sum_{i=1}^{N} \gamma_{ik}$$
   
   **Means:**
   $$\boldsymbol{\mu}_k^{new} = \frac{\sum_{i=1}^{N} \gamma_{ik} \mathbf{x}_i}{\sum_{i=1}^{N} \gamma_{ik}}$$
   
   **Covariances:**
   $$\boldsymbol{\Sigma}_k^{new} = \frac{\sum_{i=1}^{N} \gamma_{ik} (\mathbf{x}_i - \boldsymbol{\mu}_k^{new})(\mathbf{x}_i - \boldsymbol{\mu}_k^{new})^T}{\sum_{i=1}^{N} \gamma_{ik}}$$

**3. –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –∑–±—ñ–∂–Ω–æ—Å—Ç—ñ:**
   - –Ø–∫—â–æ –∑–º—ñ–Ω–∞ log-likelihood < threshold ‚Üí –∑—É–ø–∏–Ω–∏—Ç–∏

**4. –í–∏—Ö—ñ–¥:** –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ $\{\pi_k, \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k\}_{k=1}^{K}$

### –ü—Å–µ–≤–¥–æ–∫–æ–¥

```
EM_GMM(X, K, max_iter=100, tol=1e-3):
    # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è
    initialize(Œº, Œ£, œÄ)  # –í–∏–ø–∞–¥–∫–æ–≤–æ –∞–±–æ —á–µ—Ä–µ–∑ K-Means
    
    log_likelihood_old = -‚àû
    
    for iter in range(max_iter):
        # E-step: –æ–±—á–∏—Å–ª–∏—Ç–∏ responsibilities
        for i in 1..N:
            for k in 1..K:
                Œ≥[i,k] = œÄ[k] * N(x[i] | Œº[k], Œ£[k])
            Œ≥[i] = Œ≥[i] / sum(Œ≥[i])  # –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è
        
        # M-step: –æ–Ω–æ–≤–∏—Ç–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏
        for k in 1..K:
            N_k = sum(Œ≥[:, k])
            
            œÄ[k] = N_k / N
            Œº[k] = sum(Œ≥[:, k] * X) / N_k
            Œ£[k] = sum(Œ≥[:, k] * (X - Œº[k])@(X - Œº[k]).T) / N_k
        
        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –∑–±—ñ–∂–Ω–æ—Å—Ç—ñ
        log_likelihood = compute_log_likelihood(X, œÄ, Œº, Œ£)
        
        if abs(log_likelihood - log_likelihood_old) < tol:
            break
        
        log_likelihood_old = log_likelihood
    
    return œÄ, Œº, Œ£, Œ≥
```

### –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è EM –ø—Ä–æ—Ü–µ—Å—É

```
–Ü—Ç–µ—Ä–∞—Ü—ñ—è 0 (–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è):
    ‚óè  ‚óè  ‚óè     ‚óè  ‚óè
      ‚óè  ‚óè   ‚óè  ‚óè
    ‚óè     ‚óè  ‚óè
    
–í–∏–ø–∞–¥–∫–æ–≤—ñ —Ü–µ–Ω—Ç—Ä–∏ —Ç–∞ –∫–æ–≤–∞—Ä—ñ–∞—Ü—ñ—ó

–Ü—Ç–µ—Ä–∞—Ü—ñ—è 1 (E-step ‚Üí M-step):
    üî¥  ‚óè  üîµ     ‚óè  üü¢
      ‚óè  üî¥   üîµ  ‚óè
    üî¥     ‚óè  üü¢
    
Responsibilities –æ–±—á–∏—Å–ª–µ–Ω—ñ, –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –æ–Ω–æ–≤–ª–µ–Ω—ñ

–Ü—Ç–µ—Ä–∞—Ü—ñ—è 5:
   üî¥üî¥üî¥     üîµüîµ     üü¢üü¢
    üî¥üî¥      üîµüîµüîµ    üü¢üü¢üü¢
   üî¥üî¥üî¥      üîµüîµ     üü¢üü¢
   
–ö–ª–∞—Å—Ç–µ—Ä–∏ —Å—Ç–∞–±—ñ–ª—ñ–∑—É–≤–∞–ª–∏—Å—å
```

---

## –ü—Ä–æ—Å—Ç–∏–π –ø—Ä–∏–∫–ª–∞–¥: –í–∏—Å–æ—Ç–∞ –ª—é–¥–µ–π

### –î–∞–Ω—ñ

–í–∏—Å–æ—Ç–∞ 100 –ª—é–¥–µ–π (–≤ —Å–º):

```
–ñ—ñ–Ω–∫–∏: Œº=165, œÉ=6 (40 –ª—é–¥–µ–π)
–ß–æ–ª–æ–≤—ñ–∫–∏: Œº=178, œÉ=7 (60 –ª—é–¥–µ–π)
```

–ú–∏ **–Ω–µ –∑–Ω–∞—î–º–æ** —Å—Ç–∞—Ç—å, —Ç—ñ–ª—å–∫–∏ –≤–∏—Å–æ—Ç—É. GMM –º–∞—î –∑–Ω–∞–π—Ç–∏ 2 —Ä–æ–∑–ø–æ–¥—ñ–ª–∏.

### GMM –∑ K=2

**–ü—ñ—Å–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è:**

```
–ö–æ–º–ø–æ–Ω–µ–Ω—Ç 1: Œº‚ÇÅ=165.2, œÉ‚ÇÅ=6.1, œÄ‚ÇÅ=0.39
–ö–æ–º–ø–æ–Ω–µ–Ω—Ç 2: Œº‚ÇÇ=177.8, œÉ‚ÇÇ=7.2, œÄ‚ÇÇ=0.61
```

**–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü—ñ—è:**
- –ö–æ–º–ø–æ–Ω–µ–Ω—Ç 1 ‚âà –ñ—ñ–Ω–∫–∏ (39% –ø–æ–ø—É–ª—è—Ü—ñ—ó)
- –ö–æ–º–ø–æ–Ω–µ–Ω—Ç 2 ‚âà –ß–æ–ª–æ–≤—ñ–∫–∏ (61% –ø–æ–ø—É–ª—è—Ü—ñ—ó)

### Soft Clustering

**–õ—é–¥–∏–Ω–∞ –≤–∏—Å–æ—Ç–æ—é 170 —Å–º:**

$$\gamma_1 = P(\text{–ö–æ–º–ø–æ–Ω–µ–Ω—Ç 1} | x=170) = 0.42$$
$$\gamma_2 = P(\text{–ö–æ–º–ø–æ–Ω–µ–Ω—Ç 2} | x=170) = 0.58$$

**–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü—ñ—è:**
- 42% –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å –∂—ñ–Ω–∫–∞
- 58% –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å —á–æ–ª–æ–≤—ñ–∫
- –ù–µ–≤–ø–µ–≤–Ω–µ–Ω—ñ—Å—Ç—å —á–µ—Ä–µ–∑ –ø–µ—Ä–µ–∫—Ä–∏—Ç—Ç—è —Ä–æ–∑–ø–æ–¥—ñ–ª—ñ–≤

**–õ—é–¥–∏–Ω–∞ –≤–∏—Å–æ—Ç–æ—é 160 —Å–º:**

$$\gamma_1 = 0.89, \quad \gamma_2 = 0.11$$

**–õ—é–¥–∏–Ω–∞ –≤–∏—Å–æ—Ç–æ—é 185 —Å–º:**

$$\gamma_1 = 0.05, \quad \gamma_2 = 0.95$$

---

## –°–∫–ª–∞–¥–Ω–∏–π –ø—Ä–∏–∫–ª–∞–¥: Iris Dataset

### –ó–∞–¥–∞—á–∞

Iris dataset: 150 –∫–≤—ñ—Ç—ñ–≤, 4 –æ–∑–Ω–∞–∫–∏ (–¥–æ–≤–∂–∏–Ω–∞/—à–∏—Ä–∏–Ω–∞ sepal/petal), 3 –≤–∏–¥–∏.

**–ú–µ—Ç–∞:** –ö–ª–∞—Å—Ç–µ—Ä–∏–∑—É–≤–∞—Ç–∏ –±–µ–∑ –∑–Ω–∞–Ω–Ω—è –≤–∏–¥—É (unsupervised).

### –†–µ–∑—É–ª—å—Ç–∞—Ç GMM

**K=3 –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∏:**

| –ö–æ–º–ø–æ–Ω–µ–Ω—Ç | Mean Petal Length | Mean Petal Width | œÄ_k | –Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü—ñ—è |
|-----------|-------------------|------------------|-----|---------------|
| 0 | 1.46 | 0.25 | 0.33 | Setosa |
| 1 | 4.26 | 1.33 | 0.33 | Versicolor |
| 2 | 5.60 | 2.03 | 0.34 | Virginica |

### Soft Clustering

**–ö–≤—ñ—Ç–∫–∞ –∑ petal_length=3.5, petal_width=1.0:**

```
Responsibilities:
- Setosa: 5%
- Versicolor: 85%
- Virginica: 10%

‚Üí –°–∫–æ—Ä—ñ—à –∑–∞ –≤—Å–µ Versicolor, –∞–ª–µ —î –Ω–µ–≤–ø–µ–≤–Ω–µ–Ω—ñ—Å—Ç—å
```

**–ü–µ—Ä–µ–≤–∞–≥–∏ GMM –Ω–∞–¥ K-Means:**
- –ü–æ–∫–∞–∑—É—î **–Ω–µ–≤–ø–µ–≤–Ω–µ–Ω—ñ—Å—Ç—å** (–ø–µ—Ä–µ—Ö—ñ–¥–Ω—ñ –∫–≤—ñ—Ç–∏)
- **–ï–ª—ñ–ø—Ç–∏—á–Ω—ñ –∫–ª–∞—Å—Ç–µ—Ä–∏** (–∫—Ä–∞—â–µ –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—é—Ç—å –¥–∞–Ω–∏–º)
- **–°—Ç–∞—Ç–∏—Å—Ç–∏—á–Ω–∞ —ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü—ñ—è** (—Ä–æ–∑–ø–æ–¥—ñ–ª–∏)

---

## –ö–æ–¥ (Python + scikit-learn)

### –ë–∞–∑–æ–≤–∏–π –ø—Ä–∏–∫–ª–∞–¥

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.mixture import GaussianMixture
from sklearn.datasets import make_blobs

# 1. –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –¥–∞–Ω–∏—Ö
X, y_true = make_blobs(n_samples=300, centers=3, 
                       cluster_std=[1.0, 1.5, 0.5],
                       random_state=42)

# 2. GMM
gmm = GaussianMixture(
    n_components=3,           # –ö—ñ–ª—å–∫—ñ—Å—Ç—å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ñ–≤ (–∫–ª–∞—Å—Ç–µ—Ä—ñ–≤)
    covariance_type='full',   # –¢–∏–ø –∫–æ–≤–∞—Ä—ñ–∞—Ü—ñ—ó
    max_iter=100,             # –ú–∞–∫—Å–∏–º—É–º —ñ—Ç–µ—Ä–∞—Ü—ñ–π EM
    n_init=10,                # –ö—ñ–ª—å–∫—ñ—Å—Ç—å random —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ–π
    random_state=42
)

# 3. –ù–∞–≤—á–∞–Ω–Ω—è
gmm.fit(X)

# 4. Predict (hard clustering)
labels = gmm.predict(X)

# 5. Predict probabilities (soft clustering)
probs = gmm.predict_proba(X)

print("=== GMM Results ===")
print(f"Converged: {gmm.converged_}")
print(f"Iterations: {gmm.n_iter_}")
print(f"\nMeans:\n{gmm.means_}")
print(f"\nCovariances shape: {gmm.covariances_.shape}")
print(f"\nWeights (œÄ): {gmm.weights_}")
print(f"\nLog-likelihood: {gmm.score(X) * len(X):.2f}")

# 6. –ü—Ä–∏–∫–ª–∞–¥ soft clustering
print("\n=== Example: Soft Clustering ===")
sample_idx = 0
print(f"Point: {X[sample_idx]}")
print(f"Probabilities: {probs[sample_idx]}")
print(f"Hard label: {labels[sample_idx]}")

# 7. –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# Hard clustering
axes[0].scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=50, alpha=0.6)
axes[0].scatter(gmm.means_[:, 0], gmm.means_[:, 1], 
               c='red', marker='X', s=200, edgecolors='black', linewidths=2)
axes[0].set_title('Hard Clustering', fontsize=14, fontweight='bold')
axes[0].set_xlabel('Feature 1')
axes[0].set_ylabel('Feature 2')
axes[0].grid(True, alpha=0.3)

# Soft clustering (—Ä–æ–∑–º—ñ—Ä–∏ —Ç–æ—á–æ–∫ = –≤–ø–µ–≤–Ω–µ–Ω—ñ—Å—Ç—å)
max_prob = probs.max(axis=1)
axes[1].scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', 
               s=max_prob*200, alpha=0.6)
axes[1].scatter(gmm.means_[:, 0], gmm.means_[:, 1],
               c='red', marker='X', s=200, edgecolors='black', linewidths=2)
axes[1].set_title('Soft Clustering (size = confidence)', 
                 fontsize=14, fontweight='bold')
axes[1].set_xlabel('Feature 1')
axes[1].set_ylabel('Feature 2')
axes[1].grid(True, alpha=0.3)

# Density (contours)
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),
                     np.linspace(y_min, y_max, 100))
Z = -gmm.score_samples(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

axes[2].scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=30, alpha=0.6)
contours = axes[2].contour(xx, yy, Z, levels=10, linewidths=2, alpha=0.7)
axes[2].clabel(contours, inline=True, fontsize=8)
axes[2].scatter(gmm.means_[:, 0], gmm.means_[:, 1],
               c='red', marker='X', s=200, edgecolors='black', linewidths=2)
axes[2].set_title('Density Contours', fontsize=14, fontweight='bold')
axes[2].set_xlabel('Feature 1')
axes[2].set_ylabel('Feature 2')
axes[2].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è covariance types

```python
from sklearn.mixture import GaussianMixture
import matplotlib.pyplot as plt

# –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –¥–∞–Ω–∏—Ö –∑ –µ–ª—ñ–ø—Ç–∏—á–Ω–∏–º–∏ –∫–ª–∞—Å—Ç–µ—Ä–∞–º–∏
np.random.seed(42)

# –ö–ª–∞—Å—Ç–µ—Ä 1: –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω–∏–π –µ–ª—ñ–ø—Å
cov1 = [[2.0, 0.5], [0.5, 0.5]]
X1 = np.random.multivariate_normal([0, 0], cov1, 100)

# –ö–ª–∞—Å—Ç–µ—Ä 2: –≤–µ—Ä—Ç–∏–∫–∞–ª—å–Ω–∏–π –µ–ª—ñ–ø—Å
cov2 = [[0.5, 0.3], [0.3, 2.0]]
X2 = np.random.multivariate_normal([5, 5], cov2, 100)

# –ö–ª–∞—Å—Ç–µ—Ä 3: –¥—ñ–∞–≥–æ–Ω–∞–ª—å–Ω–∏–π
cov3 = [[1.0, 0.8], [0.8, 1.0]]
X3 = np.random.multivariate_normal([5, 0], cov3, 100)

X = np.vstack([X1, X2, X3])

# –ü–æ—Ä—ñ–≤–Ω—è—Ç–∏ —Ä—ñ–∑–Ω—ñ covariance types
covariance_types = ['full', 'tied', 'diag', 'spherical']

fig, axes = plt.subplots(2, 2, figsize=(14, 12))
axes = axes.ravel()

for idx, cov_type in enumerate(covariance_types):
    gmm = GaussianMixture(
        n_components=3,
        covariance_type=cov_type,
        random_state=42
    )
    
    labels = gmm.fit_predict(X)
    
    # –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
    axes[idx].scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', 
                     s=30, alpha=0.6)
    axes[idx].scatter(gmm.means_[:, 0], gmm.means_[:, 1],
                     c='red', marker='X', s=200, 
                     edgecolors='black', linewidths=2)
    
    # Density contours
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),
                        np.linspace(y_min, y_max, 100))
    Z = -gmm.score_samples(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    
    axes[idx].contour(xx, yy, Z, levels=10, alpha=0.5)
    
    axes[idx].set_title(f'{cov_type.capitalize()} Covariance', 
                       fontsize=13, fontweight='bold')
    axes[idx].set_xlabel('Feature 1', fontsize=11)
    axes[idx].set_ylabel('Feature 2', fontsize=11)
    axes[idx].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("\nCovariance Types Comparison:")
print("Full: Most flexible, captures ellipses at any angle")
print("Tied: All clusters same shape, different positions")
print("Diag: Ellipses aligned with axes")
print("Spherical: Circular clusters (like K-Means)")
```

### –ü–æ–≤–Ω–∏–π –ø—Ä–∏–∫–ª–∞–¥: Customer Segmentation –∑ soft clustering

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.mixture import GaussianMixture
from sklearn.preprocessing import StandardScaler

# –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –¥–∞–Ω–∏—Ö –ø—Ä–æ –∫–ª—ñ—î–Ω—Ç—ñ–≤
np.random.seed(42)

# 4 —Å–µ–≥–º–µ–Ω—Ç–∏ –∑ —Ä—ñ–∑–Ω–∏–º–∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º–∏
segments = {
    'VIP': {'n': 50, 'recency': (5, 2), 'frequency': (25, 5), 'monetary': (1500, 300)},
    'Active': {'n': 100, 'recency': (15, 5), 'frequency': (12, 3), 'monetary': (500, 100)},
    'Regular': {'n': 80, 'recency': (30, 8), 'frequency': (6, 2), 'monetary': (300, 80)},
    'At-Risk': {'n': 70, 'recency': (90, 20), 'frequency': (2, 1), 'monetary': (150, 50)}
}

data_list = []
for seg_name, params in segments.items():
    n = params['n']
    recency = np.random.normal(params['recency'][0], params['recency'][1], n)
    frequency = np.random.normal(params['frequency'][0], params['frequency'][1], n)
    monetary = np.random.normal(params['monetary'][0], params['monetary'][1], n)
    
    for i in range(n):
        data_list.append({
            'Recency': max(1, recency[i]),
            'Frequency': max(1, frequency[i]),
            'Monetary': max(50, monetary[i]),
            'True_Segment': seg_name
        })

df = pd.DataFrame(data_list)

print("=== Dataset Info ===")
print(f"Total customers: {len(df)}")
print(f"\n{df.groupby('True_Segment').size()}")
print(f"\n{df.describe()}")

# –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è
scaler = StandardScaler()
X_scaled = scaler.fit_transform(df[['Recency', 'Frequency', 'Monetary']])

# GMM –∑ —Ä—ñ–∑–Ω–∏–º–∏ K
n_components_range = range(2, 8)
bic_scores = []
aic_scores = []

for n_components in n_components_range:
    gmm = GaussianMixture(n_components=n_components, 
                         covariance_type='full',
                         random_state=42)
    gmm.fit(X_scaled)
    bic_scores.append(gmm.bic(X_scaled))
    aic_scores.append(gmm.aic(X_scaled))

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è BIC/AIC
fig, ax = plt.subplots(figsize=(10, 6))
ax.plot(n_components_range, bic_scores, 'o-', label='BIC', linewidth=2)
ax.plot(n_components_range, aic_scores, 's-', label='AIC', linewidth=2)
ax.set_xlabel('Number of Components', fontsize=12)
ax.set_ylabel('Information Criterion', fontsize=12)
ax.set_title('Model Selection: BIC and AIC', fontsize=14, fontweight='bold')
ax.legend(fontsize=11)
ax.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

optimal_k = n_components_range[np.argmin(bic_scores)]
print(f"\nOptimal K (BIC): {optimal_k}")

# –ù–∞–≤—á–∏—Ç–∏ —Ñ—ñ–Ω–∞–ª—å–Ω—É –º–æ–¥–µ–ª—å
gmm = GaussianMixture(
    n_components=4,
    covariance_type='full',
    n_init=20,
    random_state=42
)

gmm.fit(X_scaled)

# Hard clustering
df['Cluster'] = gmm.predict(X_scaled)

# Soft clustering (probabilities)
probs = gmm.predict_proba(X_scaled)
df['Prob_0'] = probs[:, 0]
df['Prob_1'] = probs[:, 1]
df['Prob_2'] = probs[:, 2]
df['Prob_3'] = probs[:, 3]

# –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å (–≤–ø–µ–≤–Ω–µ–Ω—ñ—Å—Ç—å)
df['Confidence'] = probs.max(axis=1)

print("\n" + "="*70)
print("=== GMM Results ===")
print("="*70)
print(f"Converged: {gmm.converged_}")
print(f"Iterations: {gmm.n_iter_}")
print(f"\nWeights (œÄ): {gmm.weights_}")
print(f"\nMeans (in scaled space):\n{gmm.means_}")

# Inverse transform means –¥–ª—è —ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü—ñ—ó
means_original = scaler.inverse_transform(gmm.means_)
means_df = pd.DataFrame(means_original, 
                       columns=['Recency', 'Frequency', 'Monetary'])
print(f"\nMeans (original scale):\n{means_df}")

# –ê–Ω–∞–ª—ñ–∑ –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤
print("\n" + "="*70)
print("=== Cluster Analysis ===")
print("="*70)

for cluster in range(4):
    cluster_data = df[df['Cluster'] == cluster]
    print(f"\nCluster {cluster} (n={len(cluster_data)}):")
    print(cluster_data[['Recency', 'Frequency', 'Monetary', 'Confidence']].describe())
    
    # –ù–∞–π–±—ñ–ª—å—à –≤–ø–µ–≤–Ω–µ–Ω—ñ —Ç–æ—á–∫–∏
    most_confident = cluster_data.nlargest(5, 'Confidence')
    print(f"\nMost confident assignments (top 5):")
    print(most_confident[['Recency', 'Frequency', 'Monetary', 'Confidence']])

# –ê–Ω–∞–ª—ñ–∑ –Ω–µ–≤–ø–µ–≤–Ω–µ–Ω–æ—Å—Ç—ñ
print("\n" + "="*70)
print("=== Uncertainty Analysis ===")
print("="*70)

# –¢–æ—á–∫–∏ –∑ –Ω–∏–∑—å–∫–æ—é –≤–ø–µ–≤–Ω–µ–Ω—ñ—Å—Ç—é (–ø–µ—Ä–µ—Ö—ñ–¥–Ω—ñ)
uncertain_threshold = 0.6
uncertain_points = df[df['Confidence'] < uncertain_threshold]

print(f"Points with confidence < {uncertain_threshold}: {len(uncertain_points)} "
      f"({len(uncertain_points)/len(df)*100:.1f}%)")

if len(uncertain_points) > 0:
    print("\nExample uncertain points:")
    print(uncertain_points.nlargest(5, 'Confidence', keep='last')[
        ['Recency', 'Frequency', 'Monetary', 'Cluster', 
         'Prob_0', 'Prob_1', 'Prob_2', 'Prob_3', 'Confidence']
    ])

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
fig = plt.figure(figsize=(18, 12))

# 3D scatter –∑ confidence
ax1 = fig.add_subplot(2, 3, 1, projection='3d')
scatter = ax1.scatter(df['Recency'], df['Frequency'], df['Monetary'],
                     c=df['Cluster'], cmap='viridis', 
                     s=df['Confidence']*100, alpha=0.6)
ax1.set_xlabel('Recency', fontsize=10)
ax1.set_ylabel('Frequency', fontsize=10)
ax1.set_zlabel('Monetary', fontsize=10)
ax1.set_title('3D Clusters (size = confidence)', fontsize=12, fontweight='bold')
plt.colorbar(scatter, ax=ax1, label='Cluster')

# Recency vs Frequency –∑ confidence
ax2 = fig.add_subplot(2, 3, 2)
scatter2 = ax2.scatter(df['Recency'], df['Frequency'], 
                      c=df['Cluster'], cmap='viridis',
                      s=df['Confidence']*100, alpha=0.6,
                      edgecolors='black', linewidths=0.5)
ax2.set_xlabel('Recency', fontsize=11)
ax2.set_ylabel('Frequency', fontsize=11)
ax2.set_title('Recency vs Frequency', fontsize=12, fontweight='bold')
ax2.grid(True, alpha=0.3)
plt.colorbar(scatter2, ax=ax2, label='Cluster')

# Frequency vs Monetary
ax3 = fig.add_subplot(2, 3, 3)
scatter3 = ax3.scatter(df['Frequency'], df['Monetary'],
                      c=df['Cluster'], cmap='viridis',
                      s=df['Confidence']*100, alpha=0.6,
                      edgecolors='black', linewidths=0.5)
ax3.set_xlabel('Frequency', fontsize=11)
ax3.set_ylabel('Monetary', fontsize=11)
ax3.set_title('Frequency vs Monetary', fontsize=12, fontweight='bold')
ax3.grid(True, alpha=0.3)
plt.colorbar(scatter3, ax=ax3, label='Cluster')

# Cluster profiles
ax4 = fig.add_subplot(2, 3, 4)
cluster_profiles = df.groupby('Cluster')[['Recency', 'Frequency', 'Monetary']].mean()
cluster_profiles_norm = (cluster_profiles - cluster_profiles.mean()) / cluster_profiles.std()
sns.heatmap(cluster_profiles_norm.T, annot=True, fmt='.2f',
           cmap='RdYlGn_r', center=0, ax=ax4, cbar_kws={'label': 'Std. Value'})
ax4.set_title('Cluster Profiles (Normalized)', fontsize=12, fontweight='bold')
ax4.set_xlabel('Cluster', fontsize=11)

# Confidence distribution
ax5 = fig.add_subplot(2, 3, 5)
ax5.hist(df['Confidence'], bins=30, edgecolor='black', alpha=0.7)
ax5.axvline(uncertain_threshold, color='red', linestyle='--', 
           linewidth=2, label=f'Threshold={uncertain_threshold}')
ax5.set_xlabel('Confidence', fontsize=11)
ax5.set_ylabel('Count', fontsize=11)
ax5.set_title('Distribution of Confidence Scores', fontsize=12, fontweight='bold')
ax5.legend()
ax5.grid(True, alpha=0.3)

# Cluster sizes
ax6 = fig.add_subplot(2, 3, 6)
cluster_sizes = df['Cluster'].value_counts().sort_index()
bars = ax6.bar(cluster_sizes.index, cluster_sizes.values, 
              edgecolor='black', alpha=0.7)
for bar, weight in zip(bars, gmm.weights_):
    height = bar.get_height()
    ax6.text(bar.get_x() + bar.get_width()/2., height,
            f'œÄ={weight:.2f}',
            ha='center', va='bottom', fontsize=10)
ax6.set_xlabel('Cluster', fontsize=11)
ax6.set_ylabel('Count', fontsize=11)
ax6.set_title('Cluster Sizes (with weights)', fontsize=12, fontweight='bold')
ax6.grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.show()

# –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –Ω–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö
print("\n" + "="*70)
print("=== Generative Model: Sample New Customers ===")
print("="*70)

# –ó–≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ 5 –Ω–æ–≤–∏—Ö –∫–ª—ñ—î–Ω—Ç—ñ–≤
new_samples_scaled, labels = gmm.sample(5)
new_samples = scaler.inverse_transform(new_samples_scaled)

new_df = pd.DataFrame(new_samples, columns=['Recency', 'Frequency', 'Monetary'])
new_df['Generated_Cluster'] = labels

print("\nGenerated customers:")
print(new_df)
```

### Density Estimation

```python
# GMM –¥–ª—è density estimation
gmm = GaussianMixture(n_components=3, covariance_type='full')
gmm.fit(X)

# –û–±—á–∏—Å–ª–∏—Ç–∏ log-likelihood –¥–ª—è –∫–æ–∂–Ω–æ—ó —Ç–æ—á–∫–∏
log_likelihood = gmm.score_samples(X)

# –ê–±–æ –¥–ª—è –Ω–æ–≤–∏—Ö —Ç–æ—á–æ–∫
X_new = np.array([[0, 0], [10, 10]])
log_prob_new = gmm.score_samples(X_new)

print(f"Log probability for new points: {log_prob_new}")
print(f"Probability: {np.exp(log_prob_new)}")

# –í–∏–∑–Ω–∞—á–∏—Ç–∏ outliers (–Ω–∏–∑—å–∫–∞ –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å)
threshold = np.percentile(log_likelihood, 5)  # 5% –Ω–∞–π–Ω–∏–∂—á–∏—Ö
outliers = X[log_likelihood < threshold]

print(f"Detected {len(outliers)} outliers")

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
plt.figure(figsize=(10, 6))
plt.scatter(X[:, 0], X[:, 1], c=log_likelihood, cmap='viridis', s=50)
plt.scatter(outliers[:, 0], outliers[:, 1], c='red', marker='x', 
           s=100, linewidths=2, label='Outliers')
plt.colorbar(label='Log Likelihood')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.title('Density Estimation with GMM')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
```

---

## –í–∏–±—ñ—Ä –∫—ñ–ª—å–∫–æ—Å—Ç—ñ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ñ–≤ K

### –ü—Ä–æ–±–ª–µ–º–∞

**GMM –ø–æ—Ç—Ä–µ–±—É—î –∑–∞–∑–¥–∞–ª–µ–≥—ñ–¥—å –∑–∞–¥–∞—Ç–∏ K** (—è–∫ K-Means).

### 1. BIC (Bayesian Information Criterion) ‚≠ê

**–ù–∞–π–ø–æ–ø—É–ª—è—Ä–Ω—ñ—à–∏–π –º–µ—Ç–æ–¥ –¥–ª—è GMM.**

$$\text{BIC} = -2 \ln(\mathcal{L}) + k \ln(n)$$

–¥–µ:
- $\mathcal{L}$ ‚Äî likelihood
- $k$ ‚Äî –∫—ñ–ª—å–∫—ñ—Å—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤
- $n$ ‚Äî –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ç–æ—á–æ–∫

**–ú–µ–Ω—à–µ BIC = –∫—Ä–∞—â–µ** (–∫–æ–º–ø—Ä–æ–º—ñ—Å –º—ñ–∂ fit —Ç–∞ —Å–∫–ª–∞–¥–Ω—ñ—Å—Ç—é)

```python
from sklearn.mixture import GaussianMixture

# –ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ —Ä—ñ–∑–Ω—ñ K
n_components_range = range(1, 11)
bic_scores = []

for n_components in n_components_range:
    gmm = GaussianMixture(n_components=n_components, 
                         covariance_type='full',
                         random_state=42)
    gmm.fit(X)
    bic_scores.append(gmm.bic(X))

# –û–ø—Ç–∏–º–∞–ª—å–Ω–∏–π K
optimal_k = n_components_range[np.argmin(bic_scores)]

print(f"Optimal K (BIC): {optimal_k}")

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
plt.figure(figsize=(10, 6))
plt.plot(n_components_range, bic_scores, 'o-', linewidth=2, markersize=8)
plt.xlabel('Number of Components (K)', fontsize=12)
plt.ylabel('BIC', fontsize=12)
plt.title('BIC vs Number of Components', fontsize=14, fontweight='bold')
plt.axvline(optimal_k, color='red', linestyle='--', label=f'Optimal K={optimal_k}')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
```

### 2. AIC (Akaike Information Criterion)

$$\text{AIC} = -2 \ln(\mathcal{L}) + 2k$$

**–ú–µ–Ω—à–µ AIC = –∫—Ä–∞—â–µ**

AIC –º–µ–Ω—à –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–∏–π –Ω—ñ–∂ BIC (—Å—Ö–∏–ª—å–Ω–∏–π –¥–æ –±—ñ–ª—å—à–∏—Ö K).

```python
aic_scores = []

for n_components in n_components_range:
    gmm = GaussianMixture(n_components=n_components, covariance_type='full')
    gmm.fit(X)
    aic_scores.append(gmm.aic(X))

optimal_k_aic = n_components_range[np.argmin(aic_scores)]
print(f"Optimal K (AIC): {optimal_k_aic}")
```

### 3. Cross-Validation

```python
from sklearn.model_selection import cross_val_score

scores = []

for n_components in range(2, 11):
    gmm = GaussianMixture(n_components=n_components, covariance_type='full')
    # Negative log-likelihood (lower is better)
    score = cross_val_score(gmm, X, cv=5, scoring='neg_log_loss').mean()
    scores.append(-score)

optimal_k = range(2, 11)[np.argmax(scores)]
```

### 4. Silhouette Score

```python
from sklearn.metrics import silhouette_score

silhouette_scores = []

for n_components in range(2, 11):
    gmm = GaussianMixture(n_components=n_components)
    labels = gmm.fit_predict(X)
    score = silhouette_score(X, labels)
    silhouette_scores.append(score)

optimal_k = range(2, 11)[np.argmax(silhouette_scores)]
```

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—è:** –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π **BIC** —è–∫ –æ—Å–Ω–æ–≤–Ω–∏–π –º–µ—Ç–æ–¥ ‚úì

---

## –ü–µ—Ä–µ–≤–∞–≥–∏ —Ç–∞ –Ω–µ–¥–æ–ª—ñ–∫–∏

### –ü–µ—Ä–µ–≤–∞–≥–∏ ‚úì

| –ü–µ—Ä–µ–≤–∞–≥–∞ | –ü–æ—è—Å–Ω–µ–Ω–Ω—è |
|----------|-----------|
| **Soft clustering** | –ô–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ –Ω–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ –∑–∞–º—ñ—Å—Ç—å –∂–æ—Ä—Å—Ç–∫–∏—Ö –º—ñ—Ç–æ–∫ |
| **Uncertainty quantification** | –ü–æ–∫–∞–∑—É—î –Ω–µ–≤–ø–µ–≤–Ω–µ–Ω—ñ—Å—Ç—å —É –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—ó |
| **–ì–Ω—É—á–∫–∞ —Ñ–æ—Ä–º–∞ –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤** | –ï–ª—ñ–ø—Ç–∏—á–Ω—ñ –∫–ª–∞—Å—Ç–µ—Ä–∏ —Ä—ñ–∑–Ω–∏—Ö —Ñ–æ—Ä–º/–æ—Ä—ñ—î–Ω—Ç–∞—Ü—ñ–π |
| **–°—Ç–∞—Ç–∏—Å—Ç–∏—á–Ω–µ –æ–±“ë—Ä—É–Ω—Ç—É–≤–∞–Ω–Ω—è** | Probabilistic model –∑ —Ç–µ–æ—Ä–µ—Ç–∏—á–Ω–æ—é –±–∞–∑–æ—é |
| **Density estimation** | –ú–æ–∂–µ –º–æ–¥–µ–ª—é–≤–∞—Ç–∏ —Ä–æ–∑–ø–æ–¥—ñ–ª –¥–∞–Ω–∏—Ö |
| **Generative model** | –ú–æ–∂–Ω–∞ –≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ –Ω–æ–≤—ñ —Ç–æ—á–∫–∏ |
| **–ü–µ—Ä–µ–∫—Ä–∏—Ç—Ç—è –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤** | –î–æ–±—Ä–µ –ø—Ä–∞—Ü—é—î –∑ –Ω–µ—á—ñ—Ç–∫–∏–º–∏ –≥—Ä–∞–Ω–∏—Ü—è–º–∏ |

### –ù–µ–¥–æ–ª—ñ–∫–∏ ‚úó

| –ù–µ–¥–æ–ª—ñ–∫ | –ü–æ—è—Å–Ω–µ–Ω–Ω—è |
|---------|-----------|
| **–ü–æ—Ç—Ä—ñ–±–Ω–æ –∑–Ω–∞—Ç–∏ K** | –ö—ñ–ª—å–∫—ñ—Å—Ç—å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ñ–≤ –∑–∞–¥–∞—î—Ç—å—Å—è –∑–∞–∑–¥–∞–ª–µ–≥—ñ–¥—å |
| **–°–∫–ª–∞–¥–Ω—ñ—Å—Ç—å O(nKd¬≥)** | –ü–æ–≤—ñ–ª—å–Ω—ñ—à–µ –∑–∞ K-Means |
| **–ü—Ä–∏–ø—É—â–µ–Ω–Ω—è Gaussian** | –ü—Ä–∞—Ü—é—î –ø–æ–≥–∞–Ω–æ —è–∫—â–æ –¥–∞–Ω—ñ –Ω–µ Gaussian |
| **–õ–æ–∫–∞–ª—å–Ω—ñ –º—ñ–Ω—ñ–º—É–º–∏** | EM –º–æ–∂–µ –∑–∞—Å—Ç—Ä—è–≥—Ç–∏ (–ø–æ—Ç—Ä—ñ–±–Ω–æ n_init) |
| **–ß—É—Ç–ª–∏–≤—ñ—Å—Ç—å –¥–æ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—ó** | –†–µ–∑—É–ª—å—Ç–∞—Ç –∑–∞–ª–µ–∂–∏—Ç—å –≤—ñ–¥ –ø–æ—á–∞—Ç–∫–æ–≤–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ |
| **–ë–∞–≥–∞—Ç–æ –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤** | Full covariance: K √ó d √ó (d+1)/2 –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ |
| **–ù–µ –¥–ª—è —Å–∫–ª–∞–¥–Ω–∏—Ö —Ñ–æ—Ä–º** | –ù–µ –∑–Ω–∞–π–¥–µ S-–∫—Ä–∏–≤—ñ, –∫—ñ–ª—å—Ü—è (‚Üí DBSCAN) |

---

## –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –∑ —ñ–Ω—à–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏

| –ú–µ—Ç–æ–¥ | Clustering Type | –ü–æ—Ç—Ä—ñ–±–Ω–æ K? | –§–æ—Ä–º–∞ –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤ | Outliers | –°–∫–ª–∞–¥–Ω—ñ—Å—Ç—å |
|-------|-----------------|-------------|-----------------|----------|------------|
| **GMM** | Soft (probabilistic) | ‚úÖ –¢–∞–∫ | –ï–ª—ñ–ø—Ç–∏—á–Ω—ñ | ‚ö†Ô∏è –°–µ—Ä–µ–¥–Ω—å–æ | O(nKd¬≥) |
| **K-Means** | Hard | ‚úÖ –¢–∞–∫ | –°—Ñ–µ—Ä–∏—á–Ω—ñ | ‚ùå –ß—É—Ç–ª–∏–≤–∏–π | O(nKdi) |
| **DBSCAN** | Hard | ‚ùå –ù—ñ | –ë—É–¥—å-—è–∫—ñ | ‚úÖ –í–∏—è–≤–ª—è—î | O(n log n) |
| **Hierarchical** | Hard | ‚ùå –ù—ñ | –ë—É–¥—å-—è–∫—ñ | ‚ö†Ô∏è –°–µ—Ä–µ–¥–Ω—å–æ | O(n¬≤-n¬≥) |

### GMM vs K-Means

**GMM:**
- ‚úÖ Soft clustering (–π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ)
- ‚úÖ –ï–ª—ñ–ø—Ç–∏—á–Ω—ñ –∫–ª–∞—Å—Ç–µ—Ä–∏
- ‚úÖ –°—Ç–∞—Ç–∏—Å—Ç–∏—á–Ω–∞ —ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü—ñ—è
- ‚ùå –ü–æ–≤—ñ–ª—å–Ω—ñ—à–µ
- ‚ùå –ë—ñ–ª—å—à–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤

**K-Means:**
- ‚úÖ –®–≤–∏–¥—à–µ
- ‚úÖ –ü—Ä–æ—Å—Ç—ñ—à–µ
- ‚ùå –¢—ñ–ª—å–∫–∏ —Å—Ñ–µ—Ä–∏—á–Ω—ñ –∫–ª–∞—Å—Ç–µ—Ä–∏
- ‚ùå Hard clustering

```python
# –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –Ω–∞ –æ–¥–Ω–∏—Ö –¥–∞–Ω–∏—Ö
from sklearn.cluster import KMeans

kmeans = KMeans(n_clusters=3, random_state=42)
labels_kmeans = kmeans.fit_predict(X)

gmm = GaussianMixture(n_components=3, random_state=42)
labels_gmm = gmm.fit_predict(X)

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

axes[0].scatter(X[:, 0], X[:, 1], c=labels_kmeans, cmap='viridis')
axes[0].set_title('K-Means (Hard, Spherical)', fontsize=13, fontweight='bold')

axes[1].scatter(X[:, 0], X[:, 1], c=labels_gmm, cmap='viridis')
axes[1].set_title('GMM (Soft, Elliptical)', fontsize=13, fontweight='bold')

plt.show()
```

---

## –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ GMM

### –Ü–¥–µ–∞–ª—å–Ω–æ –ø—ñ–¥—Ö–æ–¥–∏—Ç—å ‚úì

- **Soft clustering** –ø–æ—Ç—Ä—ñ–±–µ–Ω ‚Äî –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ –≤–∞–∂–ª–∏–≤—ñ
- **Uncertainty quantification** ‚Äî –Ω–∞—Å–∫—ñ–ª—å–∫–∏ –≤–ø–µ–≤–Ω–µ–Ω—ñ
- **–ï–ª—ñ–ø—Ç–∏—á–Ω—ñ –∫–ª–∞—Å—Ç–µ—Ä–∏** —Ä—ñ–∑–Ω–∏—Ö —Ñ–æ—Ä–º
- **Density estimation** ‚Äî –º–æ–¥–µ–ª—é–≤–∞–Ω–Ω—è —Ä–æ–∑–ø–æ–¥—ñ–ª—É
- **Generative tasks** ‚Äî –≥–µ–Ω–µ—Ä–∞—Ü—ñ—è –Ω–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö
- **–ü–µ—Ä–µ–∫—Ä–∏—Ç—Ç—è –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤** ‚Äî –Ω–µ—á—ñ—Ç–∫—ñ –≥—Ä–∞–Ω–∏—Ü—ñ
- **–°—Ç–∞—Ç–∏—Å—Ç–∏—á–Ω–∞ —ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü—ñ—è** –≤–∞–∂–ª–∏–≤–∞
- –î–∞–Ω—ñ **–ø—Ä–∏–±–ª–∏–∑–Ω–æ Gaussian**

### –ö—Ä–∞—â–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ —ñ–Ω—à–µ ‚úó

- **–î—É–∂–µ —Å–∫–ª–∞–¥–Ω–∞ —Ñ–æ—Ä–º–∞** (S-–∫—Ä–∏–≤—ñ, –∫—ñ–ª—å—Ü—è) ‚Üí DBSCAN
- –ü–æ—Ç—Ä—ñ–±–Ω–∞ **—à–≤–∏–¥–∫—ñ—Å—Ç—å** ‚Üí K-Means
- **–î—É–∂–µ –≤–µ–ª–∏–∫—ñ –¥–∞–Ω—ñ** ‚Üí K-Means, Mini-Batch K-Means
- –î–∞–Ω—ñ **–Ω–µ Gaussian** ‚Üí DBSCAN, Mean Shift
- **–ù–µ –∑–Ω–∞—î–º–æ K** —ñ –≤–∞–∂–ª–∏–≤–æ –∑–Ω–∞–π—Ç–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ ‚Üí DBSCAN, HDBSCAN

---

## –ü—Ä–∞–∫—Ç–∏—á–Ω—ñ –ø–æ—Ä–∞–¥–∏ üí°

### 1. –ó–∞–≤–∂–¥–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π BIC –¥–ª—è –≤–∏–±–æ—Ä—É K

```python
# –ü–µ—Ä–µ–≤—ñ—Ä —Ä—ñ–∑–Ω—ñ K
bic_scores = []
for k in range(1, 11):
    gmm = GaussianMixture(n_components=k, covariance_type='full')
    gmm.fit(X)
    bic_scores.append(gmm.bic(X))

optimal_k = np.argmin(bic_scores) + 1
```

### 2. –ü–æ—á–Ω–∏ –∑ full covariance

```python
# Full covariance –Ω–∞–π–≥–Ω—É—á–∫—ñ—à–∏–π
gmm = GaussianMixture(n_components=3, covariance_type='full')

# –Ø–∫—â–æ overfitting ‚Üí —Å–ø—Ä–æ–±—É–π tied –∞–±–æ diag
```

### 3. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π n_init –¥–ª—è —Å—Ç–∞–±—ñ–ª—å–Ω–æ—Å—Ç—ñ

```python
# EM –º–æ–∂–µ –∑–∞—Å—Ç—Ä—è–≥—Ç–∏ –≤ –ª–æ–∫–∞–ª—å–Ω–∏—Ö –º—ñ–Ω—ñ–º—É–º–∞—Ö
# –°–ø—Ä–æ–±—É–π –∫—ñ–ª—å–∫–∞ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ–π
gmm = GaussianMixture(
    n_components=3,
    n_init=20,  # 20 —Ä—ñ–∑–Ω–∏—Ö —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ–π
    random_state=42
)
```

### 4. Scaling –ö–†–ò–¢–ò–ß–ù–ò–ô

```python
# –ó–ê–í–ñ–î–ò –Ω–æ—Ä–º–∞–ª—ñ–∑—É–π
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

gmm.fit(X_scaled)
```

### 5. –ê–Ω–∞–ª—ñ–∑—É–π uncertainty

```python
# –ó–Ω–∞–π–¥–∏ —Ç–æ—á–∫–∏ –∑ –Ω–∏–∑—å–∫–æ—é –≤–ø–µ–≤–Ω–µ–Ω—ñ—Å—Ç—é
probs = gmm.predict_proba(X)
confidence = probs.max(axis=1)

uncertain = X[confidence < 0.6]
print(f"Uncertain points: {len(uncertain)}")
```

### 6. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π –¥–ª—è outlier detection

```python
# –ù–∏–∑—å–∫–∞ –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å = outlier
log_prob = gmm.score_samples(X)
threshold = np.percentile(log_prob, 5)
outliers = X[log_prob < threshold]
```

### 7. –í—ñ–∑—É–∞–ª—ñ–∑—É–π density contours

```python
# –ü–æ–∫–∞–∑—É–π density –¥–ª—è —ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü—ñ—ó
x_grid, y_grid = np.meshgrid(
    np.linspace(X[:, 0].min(), X[:, 0].max(), 100),
    np.linspace(X[:, 1].min(), X[:, 1].max(), 100)
)

Z = -gmm.score_samples(np.c_[x_grid.ravel(), y_grid.ravel()])
Z = Z.reshape(x_grid.shape)

plt.contour(x_grid, y_grid, Z, levels=10)
```

### 8. –ü–æ—Ä—ñ–≤–Ω—è–π –∑ K-Means

```python
# –Ø–∫—â–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –¥—É–∂–µ —Å—Ö–æ–∂—ñ ‚Üí –º–æ–∂–Ω–∞ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ K-Means (—à–≤–∏–¥—à–µ)
# –Ø–∫—â–æ –¥—É–∂–µ –≤—ñ–¥—Ä—ñ–∑–Ω—è—é—Ç—å—Å—è ‚Üí –µ–ª—ñ–ø—Ç–∏—á–Ω—ñ –∫–ª–∞—Å—Ç–µ—Ä–∏, GMM –∫—Ä–∞—â–µ
```

### 9. –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—è –¥–ª—è –º–∞–ª–∏—Ö –¥–∞–Ω–∏—Ö

```python
# –î–æ–¥–∞–π —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—é –¥–æ –∫–æ–≤–∞—Ä—ñ–∞—Ü—ñ—ó
gmm = GaussianMixture(
    n_components=3,
    covariance_type='full',
    reg_covar=1e-6  # –î–æ–¥–∞—Ç–∏ –¥–æ –¥—ñ–∞–≥–æ–Ω–∞–ª—ñ
)
```

### 10. –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è —á–µ—Ä–µ–∑ K-Means

```python
# –ó–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º GMM –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î K-Means –¥–ª—è —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—ó
gmm = GaussianMixture(
    n_components=3,
    init_params='kmeans'  # –∞–±–æ 'random'
)
```

---

## –†–µ–∞–ª—å–Ω—ñ –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è

### 1. Customer Segmentation –∑ uncertainty

**–ó–∞–¥–∞—á–∞:** –°–µ–≥–º–µ–Ω—Ç—É–≤–∞—Ç–∏ –∫–ª—ñ—î–Ω—Ç—ñ–≤, –≤–∏—è–≤–∏—Ç–∏ "–≥—Ä–∞–Ω–∏—á–Ω—ñ" –≤–∏–ø–∞–¥–∫–∏.

**–ü—ñ–¥—Ö—ñ–¥:**
- RFM features
- GMM –¥–ª—è soft clustering
- –¢–æ—á–∫–∏ –∑ low confidence ‚Üí –ø–µ—Ä—Å–æ–Ω–∞–ª—ñ–∑–æ–≤–∞–Ω—ñ –∫–∞–º–ø–∞–Ω—ñ—ó

**–ü–µ—Ä–µ–≤–∞–≥–∏:**
- –ó–Ω–∞—î–º–æ –Ω–∞—Å–∫—ñ–ª—å–∫–∏ –≤–ø–µ–≤–Ω–µ–Ω—ñ –≤ —Å–µ–≥–º–µ–Ω—Ç—ñ
- –ú–æ–∂–µ–º–æ –≤–∏—è–≤–∏—Ç–∏ –∫–ª—ñ—î–Ω—Ç—ñ–≤ —É –ø–µ—Ä–µ—Ö—ñ–¥–Ω–∏—Ö —Å—Ç–∞–Ω–∞—Ö

### 2. Image Segmentation

**–ó–∞–¥–∞—á–∞:** –†–æ–∑–¥—ñ–ª–∏—Ç–∏ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –Ω–∞ —Ä–µ–≥—ñ–æ–Ω–∏.

**–ü—ñ–¥—Ö—ñ–¥:**
- –ö–æ–∂–µ–Ω –ø—ñ–∫—Å–µ–ª—å = —Ç–æ—á–∫–∞ –≤ –∫–æ–ª—å–æ—Ä–æ–≤–æ–º—É –ø—Ä–æ—Å—Ç–æ—Ä—ñ (RGB)
- GMM –º–æ–¥–µ–ª—é—î —Ä–æ–∑–ø–æ–¥—ñ–ª –∫–æ–ª—å–æ—Ä—ñ–≤
- –ö–æ–∂–µ–Ω –∫–æ–º–ø–æ–Ω–µ–Ω—Ç = –æ–¥–∏–Ω –∫–æ–ª—ñ—Ä/–æ–±'—î–∫—Ç

### 3. Anomaly Detection

**–ó–∞–¥–∞—á–∞:** –í–∏—è–≤–∏—Ç–∏ –∞–Ω–æ–º–∞–ª—ñ—ó –≤ –¥–∞–Ω–∏—Ö.

**–ü—ñ–¥—Ö—ñ–¥:**
- GMM –º–æ–¥–µ–ª—é—î –Ω–æ—Ä–º–∞–ª—å–Ω–∏–π —Ä–æ–∑–ø–æ–¥—ñ–ª
- Low probability points = anomalies
- Density estimation –ø—Ä–∏—Ä–æ–¥–Ω–æ –¥–ª—è —Ü—å–æ–≥–æ

### 4. Speech Recognition

**–ó–∞–¥–∞—á–∞:** –†–æ–∑–ø—ñ–∑–Ω–∞–≤–∞–Ω–Ω—è –º–æ–≤–ª–µ–Ω–Ω—è.

**–ü—ñ–¥—Ö—ñ–¥:**
- GMM –º–æ–¥–µ–ª—é—î —Ñ–æ–Ω–µ–º–∏ (–∑–≤—É–∫–∏)
- –ö–æ–∂–Ω–∞ —Ñ–æ–Ω–µ–º–∞ = Gaussian mixture
- Hidden Markov Models + GMM

### 5. Background Subtraction (Computer Vision)

**–ó–∞–¥–∞—á–∞:** –í–∏–¥—ñ–ª–∏—Ç–∏ —Ä—É—Ö–æ–º—ñ –æ–±'—î–∫—Ç–∏ –Ω–∞ –≤—ñ–¥–µ–æ.

**–ü—ñ–¥—Ö—ñ–¥:**
- GMM –º–æ–¥–µ–ª—é—î background –∫–æ–∂–Ω–æ–≥–æ –ø—ñ–∫—Å–µ–ª—è
- –í—ñ–¥—Ö–∏–ª–µ–Ω–Ω—è –≤—ñ–¥ –º–æ–¥–µ–ª—ñ = foreground (–æ–±'—î–∫—Ç)

---

## –ü–æ—à–∏—Ä–µ–Ω—ñ –ø–æ–º–∏–ª–∫–∏ ‚ùå

### 1. –ù–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ scaling

```python
# ‚ùå –í—ñ–∫ (0-100) + –î–æ—Ö—ñ–¥ (0-150K)
gmm = GaussianMixture(n_components=3)
gmm.fit(X)  # –î–æ—Ö—ñ–¥ –¥–æ–º—ñ–Ω—É—î!

# ‚úÖ Scaling
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
gmm.fit(X_scaled)
```

### 2. –ù–µ –ø–µ—Ä–µ–≤—ñ—Ä—è—Ç–∏ convergence

```python
# ‚ùå –ü—Ä–æ—Å—Ç–æ fit
gmm.fit(X)

# ‚úÖ –ü–µ—Ä–µ–≤—ñ—Ä—è–π —á–∏ –∑—ñ–π—à–ª–æ—Å—å
gmm.fit(X)
if not gmm.converged_:
    print("WARNING: EM did not converge!")
    print(f"Iterations: {gmm.n_iter_}")
```

### 3. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ —Ç—ñ–ª—å–∫–∏ 1 —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—é

```python
# ‚ùå –ú–æ–∂–µ –∑–∞—Å—Ç—Ä—è–≥—Ç–∏ –≤ –ª–æ–∫–∞–ª—å–Ω–æ–º—É –º—ñ–Ω—ñ–º—É–º—ñ
gmm = GaussianMixture(n_components=3, n_init=1)

# ‚úÖ –ö—ñ–ª—å–∫–∞ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ–π
gmm = GaussianMixture(n_components=3, n_init=10)
```

### 4. –Ü–≥–Ω–æ—Ä—É–≤–∞—Ç–∏ covariance type

```python
# ‚ùå –ó–∞–≤–∂–¥–∏ full (–º–æ–∂–µ –±—É—Ç–∏ overkill)
gmm = GaussianMixture(n_components=10, covariance_type='full')
# –î—É–∂–µ –±–∞–≥–∞—Ç–æ –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤!

# ‚úÖ –°–ø—Ä–æ–±—É–π tied –∞–±–æ diag —è–∫—â–æ full –ø–µ—Ä–µ–Ω–∞–≤—á–∞—î—Ç—å—Å—è
gmm = GaussianMixture(n_components=10, covariance_type='tied')
```

### 5. –ù–µ –∞–Ω–∞–ª—ñ–∑—É–≤–∞—Ç–∏ uncertainty

```python
# ‚ùå –¢—ñ–ª—å–∫–∏ hard labels
labels = gmm.predict(X)

# ‚úÖ –ê–Ω–∞–ª—ñ–∑—É–π –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ
probs = gmm.predict_proba(X)
confidence = probs.max(axis=1)

# –ó–Ω–∞–π–¥–∏ –Ω–µ–≤–ø–µ–≤–Ω–µ–Ω—ñ —Ç–æ—á–∫–∏
uncertain = X[confidence < 0.7]
```

### 6. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –Ω–∞ non-Gaussian –¥–∞–Ω–∏—Ö

```python
# ‚ùå –î–∞–Ω—ñ –∑ heavy tails, multimodal –∞–ª–µ –Ω–µ Gaussian
# GMM –¥–∞—Å—Ç—å –ø–æ–≥–∞–Ω—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏

# ‚úÖ –°–ø–æ—á–∞—Ç–∫—É –ø–µ—Ä–µ–≤—ñ—Ä —Ä–æ–∑–ø–æ–¥—ñ–ª
import scipy.stats as stats

for feature in range(X.shape[1]):
    stat, p_value = stats.normaltest(X[:, feature])
    print(f"Feature {feature}: p={p_value:.4f}")
    # –Ø–∫—â–æ p < 0.05 ‚Üí –ù–ï Gaussian
```

---

## –ü–æ–≤'—è–∑–∞–Ω—ñ —Ç–µ–º–∏

- [[01_KMeans]] ‚Äî hard clustering, —à–≤–∏–¥—à–∞ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞
- [[03_DBSCAN]] ‚Äî density-based, —Å–∫–ª–∞–¥–Ω—ñ —Ñ–æ—Ä–º–∏
- [[02_Hierarchical_Clustering]] ‚Äî —ñ—î—Ä–∞—Ä—Ö—ñ—á–Ω–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—è
- [[05_Clustering_Evaluation]] ‚Äî –º–µ—Ç—Ä–∏–∫–∏ –æ—Ü—ñ–Ω–∫–∏
- [[EM_Algorithm]] ‚Äî –∞–ª–≥–æ—Ä–∏—Ç–º –Ω–∞–≤—á–∞–Ω–Ω—è
- [[Bayesian_GMM]] ‚Äî Bayesian –ø—ñ–¥—Ö—ñ–¥ –¥–æ GMM

## –†–µ—Å—É—Ä—Å–∏

- [Scikit-learn: Gaussian Mixture Models](https://scikit-learn.org/stable/modules/mixture.html)
- [Original Paper: Dempster et al. (1977) - EM Algorithm](https://www.jstor.org/stable/2984875)
- [Bishop: Pattern Recognition (Chapter 9)](https://www.microsoft.com/en-us/research/people/cmbishop/)
- [StatQuest: Gaussian Mixture Models](https://www.youtube.com/watch?v=qMTuMa86NzU)

---

## –ö–ª—é—á–æ–≤—ñ –≤–∏—Å–Ω–æ–≤–∫–∏

> Gaussian Mixture Models (GMM) –º–æ–¥–µ–ª—é—é—Ç—å –¥–∞–Ω—ñ —è–∫ —Å—É–º—ñ—à Gaussian —Ä–æ–∑–ø–æ–¥—ñ–ª—ñ–≤, –∑–∞–±–µ–∑–ø–µ—á—É—é—á–∏ soft clustering –∑ –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—è–º–∏ –Ω–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ –∑–∞–º—ñ—Å—Ç—å –∂–æ—Ä—Å—Ç–∫–∏—Ö –º—ñ—Ç–æ–∫.

**–û—Å–Ω–æ–≤–Ω—ñ –ø—Ä–∏–Ω—Ü–∏–ø–∏:**
- **Probabilistic model:** –∫–æ–∂–Ω–∞ —Ç–æ—á–∫–∞ –º–∞—î –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å –Ω–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ –¥–æ –∫–æ–∂–Ω–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞
- **EM algorithm:** —ñ—Ç–µ—Ä–∞—Ç–∏–≤–Ω–µ –Ω–∞–≤—á–∞–Ω–Ω—è (E-step ‚Üí M-step)
- **Soft clustering:** responsibilities (Œ≥·µ¢‚Çñ) –ø–æ–∫–∞–∑—É—é—Ç—å –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ
- **Covariance types:** full, tied, diag, spherical (—Ñ–æ—Ä–º–∞ –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤)

**–ö–ª—é—á–æ–≤—ñ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∏:**
- **Mixing coefficients (œÄ):** –≤–∞–≥–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ñ–≤
- **Means (Œº):** —Ü–µ–Ω—Ç—Ä–∏ —Ä–æ–∑–ø–æ–¥—ñ–ª—ñ–≤
- **Covariances (Œ£):** —Ñ–æ—Ä–º–∞ —Ç–∞ –æ—Ä—ñ—î–Ω—Ç–∞—Ü—ñ—è –µ–ª—ñ–ø—Å—ñ–≤

**–í–∏–±—ñ—Ä K:**
- **BIC** (Bayesian Information Criterion) ‚Äî –Ω–∞–π–∫—Ä–∞—â–∏–π –º–µ—Ç–æ–¥ ‚úì
- AIC (–º–µ–Ω—à –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–∏–π)
- Cross-validation, Silhouette

**–ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏:**
- Soft clustering + uncertainty + –µ–ª—ñ–ø—Ç–∏—á–Ω—ñ –∫–ª–∞—Å—Ç–µ—Ä–∏ = GMM ‚úì
- –®–≤–∏–¥–∫—ñ—Å—Ç—å + —Å—Ñ–µ—Ä–∏—á–Ω—ñ ‚Üí K-Means ‚úì
- –°–∫–ª–∞–¥–Ω–∞ —Ñ–æ—Ä–º–∞ ‚Üí DBSCAN ‚úì

**–ù–∞–π–≤–∞–∂–ª–∏–≤—ñ—à–µ:**
- Scaling –∫—Ä–∏—Ç–∏—á–Ω–∏–π
- –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π BIC –¥–ª—è –≤–∏–±–æ—Ä—É K
- n_init ‚â• 10 –¥–ª—è —Å—Ç–∞–±—ñ–ª—å–Ω–æ—Å—Ç—ñ
- Full covariance –Ω–∞–π–≥–Ω—É—á–∫—ñ—à–∏–π
- –ê–Ω–∞–ª—ñ–∑—É–π uncertainty (confidence scores)
- –ü—Ä–∞—Ü—é—î –Ω–∞–π–∫—Ä–∞—â–µ –Ω–∞ Gaussian –¥–∞–Ω–∏—Ö

---

#ml #unsupervised-learning #clustering #gmm #gaussian-mixture-models #probabilistic #soft-clustering #em-algorithm #density-estimation
