# DBSCAN (Density-Based Spatial Clustering)

## –©–æ —Ü–µ?

**DBSCAN (Density-Based Spatial Clustering of Applications with Noise)** ‚Äî —Ü–µ –∞–ª–≥–æ—Ä–∏—Ç–º unsupervised learning –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—ó –Ω–∞ –æ—Å–Ω–æ–≤—ñ **—â—ñ–ª—å–Ω–æ—Å—Ç—ñ**, —è–∫–∏–π –º–æ–∂–µ –∑–Ω–∞—Ö–æ–¥–∏—Ç–∏ –∫–ª–∞—Å—Ç–µ—Ä–∏ **–¥–æ–≤—ñ–ª—å–Ω–æ—ó —Ñ–æ—Ä–º–∏** —Ç–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –≤–∏–∑–Ω–∞—á–∞—Ç–∏ **outliers**.

**–ì–æ–ª–æ–≤–Ω–∞ —ñ–¥–µ—è:** –∫–ª–∞—Å—Ç–µ—Ä ‚Äî —Ü–µ –æ–±–ª–∞—Å—Ç—å –∑ –≤–∏—Å–æ–∫–æ—é —â—ñ–ª—å–Ω—ñ—Å—Ç—é —Ç–æ—á–æ–∫, –æ—Ç–æ—á–µ–Ω–∞ –æ–±–ª–∞—Å—Ç—è–º–∏ –∑ –Ω–∏–∑—å–∫–æ—é —â—ñ–ª—å–Ω—ñ—Å—Ç—é. –¢–æ—á–∫–∏, —è–∫—ñ –Ω–µ –Ω–∞–ª–µ–∂–∞—Ç—å –∂–æ–¥–Ω–æ–º—É —â—ñ–ª—å–Ω–æ–º—É —Ä–µ–≥–∏–æ–Ω—É, –≤–≤–∞–∂–∞—é—Ç—å—Å—è —à—É–º–æ–º (outliers).

## –ù–∞–≤—ñ—â–æ –ø–æ—Ç—Ä—ñ–±–µ–Ω?

- üéØ **–°–∫–ª–∞–¥–Ω–∞ —Ñ–æ—Ä–º–∞ –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤** ‚Äî –Ω–µ —Ç—ñ–ª—å–∫–∏ –∫—Ä—É–≥–ª—ñ, –∞ –±—É–¥—å-—è–∫—ñ
- üîç **–ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–µ –≤–∏—è–≤–ª–µ–Ω–Ω—è outliers** ‚Äî —à—É–º –≤–∏—è–≤–ª—è—î—Ç—å—Å—è –ø—Ä–∏—Ä–æ–¥–Ω–æ
- üìä **–ù–µ –ø–æ—Ç—Ä—ñ–±–Ω–æ –∑–Ω–∞—Ç–∏ K** ‚Äî –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤ –≤–∏–∑–Ω–∞—á–∞—î—Ç—å—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ
- üó∫Ô∏è **–ì–µ–æ–≥—Ä–∞—Ñ—ñ—á–Ω—ñ –¥–∞–Ω—ñ** ‚Äî –≥—Ä—É–ø—É–≤–∞–Ω–Ω—è –ª–æ–∫–∞—Ü—ñ–π –∑ –Ω–µ–æ–¥–Ω–æ—Ä—ñ–¥–Ω–æ—é —â—ñ–ª—å–Ω—ñ—Å—Ç—é
- üõ°Ô∏è **–†–æ–±–∞—Å—Ç–Ω—ñ—Å—Ç—å –¥–æ —à—É–º—É** ‚Äî –¥–æ–±—Ä–µ –ø—Ä–∞—Ü—é—î –∑ –∑–∞—à—É–º–ª–µ–Ω–∏–º–∏ –¥–∞–Ω–∏–º–∏
- üåå **–ö–ª–∞—Å—Ç–µ—Ä–∏ —Ä—ñ–∑–Ω–æ–≥–æ —Ä–æ–∑–º—ñ—Ä—É** ‚Äî –Ω–µ –≤–∏–º–∞–≥–∞—î –æ–¥–Ω–∞–∫–æ–≤–æ—ó –≤–µ–ª–∏—á–∏–Ω–∏

## –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏?

**–ü–æ—Ç—Ä—ñ–±–Ω–æ:**
- **–ù–µ –∑–Ω–∞—î–º–æ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤** ‚Äî DBSCAN –∑–Ω–∞—Ö–æ–¥–∏—Ç—å —Å–∞–º
- –ö–ª–∞—Å—Ç–µ—Ä–∏ **—Å–∫–ª–∞–¥–Ω–æ—ó —Ñ–æ—Ä–º–∏** ‚Äî S-–ø–æ–¥—ñ–±–Ω—ñ, –∫—ñ–ª—å—Ü—è, –¥–æ–≤–≥—ñ
- –ö–ª–∞—Å—Ç–µ—Ä–∏ **—Ä—ñ–∑–Ω–æ–≥–æ —Ä–æ–∑–º—ñ—Ä—É/—â—ñ–ª—å–Ω–æ—Å—Ç—ñ**
- –ë–∞–≥–∞—Ç–æ **outliers** —É –¥–∞–Ω–∏—Ö
- **–ì–µ–æ–≥—Ä–∞—Ñ—ñ—á–Ω—ñ/–ø—Ä–æ—Å—Ç–æ—Ä–æ–≤—ñ –¥–∞–Ω—ñ**
- –ü–æ—Ç—Ä—ñ–±–Ω–∞ **—Ä–æ–±–∞—Å—Ç–Ω—ñ—Å—Ç—å –¥–æ —à—É–º—É**

**–ù–µ –ø–æ—Ç—Ä—ñ–±–Ω–æ:**
- –ö–ª–∞—Å—Ç–µ—Ä–∏ **–¥—É–∂–µ —Ä—ñ–∑–Ω–æ—ó —â—ñ–ª—å–Ω–æ—Å—Ç—ñ** ‚Üí OPTICS, HDBSCAN
- **–í–∏—Å–æ–∫–æ—Ä–æ–∑–º—ñ—Ä–Ω—ñ –¥–∞–Ω—ñ** (>10-20 features) ‚Üí curse of dimensionality
- –ü–æ—Ç—Ä—ñ–±–Ω–∞ **—à–≤–∏–¥–∫—ñ—Å—Ç—å** –Ω–∞ –≤–µ–ª–∏–∫–∏—Ö –¥–∞–Ω–∏—Ö ‚Üí K-Means
- **–°—Ñ–µ—Ä–∏—á–Ω—ñ –∫–ª–∞—Å—Ç–µ—Ä–∏ –æ–¥–Ω–∞–∫–æ–≤–æ–≥–æ —Ä–æ–∑–º—ñ—Ä—É** ‚Üí K-Means –ø—Ä–æ—Å—Ç—ñ—à–∏–π

---

## –û—Å–Ω–æ–≤–Ω—ñ –∫–æ–Ω—Ü–µ–ø—Ü—ñ—ó

### 1. –ü–∞—Ä–∞–º–µ—Ç—Ä–∏

**DBSCAN –º–∞—î 2 –∫–ª—é—á–æ–≤—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏:**

**Œµ (epsilon)** ‚Äî **—Ä–∞–¥—ñ—É—Å –æ–∫–æ–ª–∏—Ü—ñ**
- –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –≤—ñ–¥—Å—Ç–∞–Ω—å –º—ñ–∂ –¥–≤–æ–º–∞ —Ç–æ—á–∫–∞–º–∏, —â–æ–± –≤–æ–Ω–∏ –≤–≤–∞–∂–∞–ª–∏—Å—è —Å—É—Å—ñ–¥–∞–º–∏
- –í–∏–∑–Ω–∞—á–∞—î "–Ω–∞—Å–∫—ñ–ª—å–∫–∏ –¥–∞–ª–µ–∫–æ –¥–∏–≤–∏—Ç–∏—Å—è"

**MinPts (min_samples)** ‚Äî **–º—ñ–Ω—ñ–º–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ç–æ—á–æ–∫**
- –ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ç–æ—á–æ–∫ –≤ –æ–∫–æ–ª–∏—Ü—ñ Œµ –¥–ª—è —Ñ–æ—Ä–º—É–≤–∞–Ω–Ω—è –∫–ª–∞—Å—Ç–µ—Ä–∞
- –ó–∞–∑–≤–∏—á–∞–π MinPts ‚â• dimensions + 1

### 2. –¢–∏–ø–∏ —Ç–æ—á–æ–∫

**Core Point (—è–¥—Ä–æ–≤–∞ —Ç–æ—á–∫–∞):**

- –ú–∞—î ‚â• MinPts —Å—É—Å—ñ–¥—ñ–≤ –≤ —Ä–∞–¥—ñ—É—Å—ñ Œµ (–≤–∫–ª—é—á–∞—é—á–∏ —Å–µ–±–µ)
- –§–æ—Ä–º—É—î "—è–¥—Ä–æ" –∫–ª–∞—Å—Ç–µ—Ä–∞

**Border Point (–ø—Ä–∏–∫–æ—Ä–¥–æ–Ω–Ω–∞ —Ç–æ—á–∫–∞):**

- –ú–∞—î < MinPts —Å—É—Å—ñ–¥—ñ–≤ –≤ —Ä–∞–¥—ñ—É—Å—ñ Œµ
- –ê–ª–µ –∑–Ω–∞—Ö–æ–¥–∏—Ç—å—Å—è –≤ –æ–∫–æ–ª–∏—Ü—ñ —è–∫–æ—ó—Å—å core point
- –ù–∞–ª–µ–∂–∏—Ç—å –∫–ª–∞—Å—Ç–µ—Ä—É, –∞–ª–µ –Ω–µ –º–æ–∂–µ —Ä–æ–∑—à–∏—Ä—é–≤–∞—Ç–∏ –π–æ–≥–æ

**Noise Point (—à—É–º/outlier):**

- –ú–∞—î < MinPts —Å—É—Å—ñ–¥—ñ–≤ –≤ —Ä–∞–¥—ñ—É—Å—ñ Œµ
- –ù–ï –∑–Ω–∞—Ö–æ–¥–∏—Ç—å—Å—è –≤ –æ–∫–æ–ª–∏—Ü—ñ –∂–æ–¥–Ω–æ—ó core point
- –ù–µ –Ω–∞–ª–µ–∂–∏—Ç—å –∂–æ–¥–Ω–æ–º—É –∫–ª–∞—Å—Ç–µ—Ä—É

### 3. Density Reachability

**–¢–æ—á–∫–∞ q –¥–æ—Å—è–∂–Ω–∞ –∑ —Ç–æ—á–∫–∏ p** (density-reachable), —è–∫—â–æ —ñ—Å–Ω—É—î –ª–∞–Ω—Ü—é–∂–æ–∫ core points –≤—ñ–¥ p –¥–æ q.

```
p ‚Üí p‚ÇÅ ‚Üí p‚ÇÇ ‚Üí ... ‚Üí q
(core) (core) (core)  (–º–æ–∂–µ –±—É—Ç–∏ border)
```

**–î–≤–∞ —Ç–∏–ø–∏ –¥–æ—Å—è–∂–Ω–æ—Å—Ç—ñ:**

**Directly density-reachable:**
- q –≤ Œµ-–æ–∫–æ–ª–∏—Ü—ñ core point p

**Density-reachable:**
- –Ü—Å–Ω—É—î –ª–∞–Ω—Ü—é–∂–æ–∫ directly density-reachable core points –≤—ñ–¥ p –¥–æ q

---

## –Ø–∫ –ø—Ä–∞—Ü—é—î DBSCAN?

### –ê–ª–≥–æ—Ä–∏—Ç–º

**–í—Ö—ñ–¥:** –¥–∞–Ω—ñ $X$, –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ $\varepsilon$ (epsilon), MinPts

**1. –ü–æ—á–∞—Ç–æ–∫:**
   - –í—Å—ñ —Ç–æ—á–∫–∏ –ø–æ–∑–Ω–∞—á–∏—Ç–∏ —è–∫ –Ω–µ–≤—ñ–¥–≤—ñ–¥–∞–Ω—ñ
   - –õ—ñ—á–∏–ª—å–Ω–∏–∫ –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤ = 0

**2. –î–ª—è –∫–æ–∂–Ω–æ—ó –Ω–µ–≤—ñ–¥–≤—ñ–¥–∞–Ω–æ—ó —Ç–æ—á–∫–∏ $p$:**

   **a) –ó–Ω–∞–π—Ç–∏ –≤—Å—ñ—Ö —Å—É—Å—ñ–¥—ñ–≤** –≤ —Ä–∞–¥—ñ—É—Å—ñ $\varepsilon$:
   $$N_\varepsilon(p) = \{q \in X : \text{dist}(p, q) \leq \varepsilon\}$$

   **b) –Ø–∫—â–æ $|N_\varepsilon(p)| < \text{MinPts}$:**
   - –ü–æ–∫–∏ —â–æ –ø–æ–∑–Ω–∞—á–∏—Ç–∏ —è–∫ **noise**
   - (–ú–æ–∂–µ –ø—ñ–∑–Ω—ñ—à–µ —Å—Ç–∞—Ç–∏ border point)

   **c) –Ø–∫—â–æ $|N_\varepsilon(p)| \geq \text{MinPts}$:**
   - $p$ ‚Äî **core point**
   - –°—Ç–≤–æ—Ä–∏—Ç–∏ –Ω–æ–≤–∏–π –∫–ª–∞—Å—Ç–µ—Ä $C$
   - –î–æ–¥–∞—Ç–∏ $p$ –¥–æ $C$
   - **–†–æ–∑—à–∏—Ä–∏—Ç–∏ –∫–ª–∞—Å—Ç–µ—Ä:**
     - –î–ª—è –∫–æ–∂–Ω–æ–≥–æ —Å—É—Å—ñ–¥–∞ $q$ –∑ $N_\varepsilon(p)$:
       - –Ø–∫—â–æ $q$ noise ‚Üí –∑–º—ñ–Ω–∏—Ç–∏ –Ω–∞ border point –∫–ª–∞—Å—Ç–µ—Ä–∞ $C$
       - –Ø–∫—â–æ $q$ –Ω–µ–≤—ñ–¥–≤—ñ–¥–∞–Ω–∏–π:
         - –î–æ–¥–∞—Ç–∏ –¥–æ $C$
         - –ó–Ω–∞–π—Ç–∏ $N_\varepsilon(q)$
         - –Ø–∫—â–æ $|N_\varepsilon(q)| \geq \text{MinPts}$ ‚Üí –¥–æ–¥–∞—Ç–∏ —Å—É—Å—ñ–¥—ñ–≤ –¥–æ —á–µ—Ä–≥–∏

**3. –ü–æ–≤—Ç–æ—Ä—é–≤–∞—Ç–∏ –¥–æ –≤—ñ–¥–≤—ñ–¥—É–≤–∞–Ω–Ω—è –≤—Å—ñ—Ö —Ç–æ—á–æ–∫**

### –ü—Å–µ–≤–¥–æ–∫–æ–¥

```
DBSCAN(X, Œµ, MinPts):
    C = 0  # –õ—ñ—á–∏–ª—å–Ω–∏–∫ –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤
    for each point p in X:
        if p.visited:
            continue
        p.visited = True
        
        NeighborPts = regionQuery(p, Œµ)
        
        if |NeighborPts| < MinPts:
            p.label = NOISE
        else:
            C = C + 1
            expandCluster(p, NeighborPts, C, Œµ, MinPts)
    
    return labels

expandCluster(p, NeighborPts, C, Œµ, MinPts):
    p.label = C
    
    for each point q in NeighborPts:
        if not q.visited:
            q.visited = True
            NeighborPts' = regionQuery(q, Œµ)
            
            if |NeighborPts'| >= MinPts:
                NeighborPts = NeighborPts ‚à™ NeighborPts'
        
        if q.label == UNDEFINED or q.label == NOISE:
            q.label = C

regionQuery(p, Œµ):
    return {q ‚àà X : dist(p, q) ‚â§ Œµ}
```

---

## –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è —Ä–æ–±–æ—Ç–∏ –∞–ª–≥–æ—Ä–∏—Ç–º—É

### –ö—Ä–æ–∫ 1: –ü–æ—á–∞—Ç–∫–æ–≤—ñ –¥–∞–Ω—ñ

```
    y
    |  ‚Ä¢   ‚Ä¢ ‚Ä¢
    |    ‚Ä¢   ‚Ä¢  ‚Ä¢
    | ‚Ä¢  ‚Ä¢ ‚Ä¢  ‚Ä¢
    |‚Ä¢  ‚Ä¢  ‚Ä¢
    |     ‚óä      ‚Ä¢
    |_________ x
    
–í—Å—ñ —Ç–æ—á–∫–∏ –Ω–µ–≤—ñ–¥–≤—ñ–¥–∞–Ω—ñ
```

### –ö—Ä–æ–∫ 2: –í–∏–±–∏—Ä–∞—î–º–æ –ø–µ—Ä—à—É —Ç–æ—á–∫—É

```
Œµ = 0.5, MinPts = 3

    y
    |  ‚Ä¢   ‚Ä¢ ‚Ä¢
    |    ‚Ä¢   ‚Ä¢  ‚Ä¢
    | ‚Ä¢  ‚äï ‚Ä¢  ‚Ä¢   ‚Üê –ø–æ—Ç–æ—á–Ω–∞ —Ç–æ—á–∫–∞ p
    |‚Ä¢  ‚Ä¢  ‚Ä¢
    |     ‚óä      ‚Ä¢
    |_________ x
    
–û–∫–æ–ª–∏—Ü—è p (–≤ —Ä–∞–¥—ñ—É—Å—ñ Œµ):
‚Ä¢ ‚Ä¢ ‚äï ‚Ä¢ ‚Ä¢ ‚Üí 5 —Ç–æ—á–æ–∫ ‚â• MinPts=3
‚áí p ‚Äî CORE POINT ‚úì
‚áí –°—Ç–≤–æ—Ä–∏—Ç–∏ –∫–ª–∞—Å—Ç–µ—Ä 1
```

### –ö—Ä–æ–∫ 3: –†–æ–∑—à–∏—Ä—é—î–º–æ –∫–ª–∞—Å—Ç–µ—Ä

```
    y
    |  ‚Ä¢   ‚Ä¢ ‚Ä¢
    |    ‚Ä¢   ‚Ä¢  ‚Ä¢
    | üî¥  üî¥ üî¥  üî¥   ‚Üê –∫–ª–∞—Å—Ç–µ—Ä 1
    |üî¥  üî¥  üî¥
    |     ‚óä      ‚Ä¢
    |_________ x
    
–†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –¥–æ–¥–∞—î–º–æ –≤—Å—ñ density-reachable —Ç–æ—á–∫–∏
```

### –ö—Ä–æ–∫ 4: –ù–∞—Å—Ç—É–ø–Ω–∞ –Ω–µ–≤—ñ–¥–≤—ñ–¥–∞–Ω–∞ —Ç–æ—á–∫–∞

```
    y
    |  ‚äï   ‚Ä¢ ‚Ä¢      ‚Üê –Ω–æ–≤–∞ –ø–æ—Ç–æ—á–Ω–∞ —Ç–æ—á–∫–∞
    |    ‚Ä¢   ‚Ä¢  ‚Ä¢
    | üî¥  üî¥ üî¥  üî¥
    |üî¥  üî¥  üî¥
    |     ‚óä      ‚Ä¢
    |_________ x
    
–û–∫–æ–ª–∏—Ü—è: 3 —Ç–æ—á–∫–∏ ‚â• MinPts=3
‚áí CORE POINT
‚áí –°—Ç–≤–æ—Ä–∏—Ç–∏ –∫–ª–∞—Å—Ç–µ—Ä 2
```

### –ö—Ä–æ–∫ 5: –§—ñ–Ω–∞–ª—å–Ω–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç

```
    y
    |  üîµ   üîµ üîµ    ‚Üê –∫–ª–∞—Å—Ç–µ—Ä 2
    |    üîµ   üîµ  üîµ
    | üî¥  üî¥ üî¥  üî¥   ‚Üê –∫–ª–∞—Å—Ç–µ—Ä 1
    |üî¥  üî¥  üî¥
    |     ‚ö´      üü¢   ‚Üê noise    ‚Üê –∫–ª–∞—Å—Ç–µ—Ä 3
    |_________ x
    
üî¥ –ö–ª–∞—Å—Ç–µ—Ä 1
üîµ –ö–ª–∞—Å—Ç–µ—Ä 2  
üü¢ –ö–ª–∞—Å—Ç–µ—Ä 3
‚ö´ Noise (outlier)
```

---

## –ü—Ä–æ—Å—Ç–∏–π –ø—Ä–∏–∫–ª–∞–¥: –ì–µ–æ–≥—Ä–∞—Ñ—ñ—á–Ω–µ –≥—Ä—É–ø—É–≤–∞–Ω–Ω—è

### –î–∞–Ω—ñ

–ö–æ–æ—Ä–¥–∏–Ω–∞—Ç–∏ —Ä–µ—Å—Ç–æ—Ä–∞–Ω—ñ–≤ —É –º—ñ—Å—Ç—ñ:

| ID | Latitude | Longitude | –†–∞–π–æ–Ω |
|----|----------|-----------|-------|
| 1 | 50.45 | 30.52 | –¶–µ–Ω—Ç—Ä |
| 2 | 50.46 | 30.53 | –¶–µ–Ω—Ç—Ä |
| 3 | 50.44 | 30.51 | –¶–µ–Ω—Ç—Ä |
| 4 | 50.40 | 30.60 | –°—Ö—ñ–¥ |
| 5 | 50.41 | 30.61 | –°—Ö—ñ–¥ |
| 6 | 50.50 | 30.45 | –ó–∞—Ö—ñ–¥ |
| 7 | 50.30 | 30.70 | Outlier |

### –ü–∞—Ä–∞–º–µ—Ç—Ä–∏

- **Œµ = 0.05** (‚âà5 –∫–º –≤ lat/lon)
- **MinPts = 2**

### –†–µ–∑—É–ª—å—Ç–∞—Ç DBSCAN

**–ö–ª–∞—Å—Ç–µ—Ä 1 (–¶–µ–Ω—Ç—Ä):** {1, 2, 3}
- –©—ñ–ª—å–Ω–∞ –≥—Ä—É–ø–∞ —Ä–µ—Å—Ç–æ—Ä–∞–Ω—ñ–≤ —É —Ü–µ–Ω—Ç—Ä—ñ

**–ö–ª–∞—Å—Ç–µ—Ä 2 (–°—Ö—ñ–¥):** {4, 5}
- –ì—Ä—É–ø–∞ –Ω–∞ —Å—Ö–æ–¥—ñ –º—ñ—Å—Ç–∞

**Noise:** {6, 7}
- –û–∫—Ä–µ–º—ñ —Ä–µ—Å—Ç–æ—Ä–∞–Ω–∏, –Ω–µ —Ñ–æ—Ä–º—É—é—Ç—å –∫–ª–∞—Å—Ç–µ—Ä

### –Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü—ñ—è

- **–¶–µ–Ω—Ç—Ä:** –≥—É—Å—Ç–æ–Ω–∞—Å–µ–ª–µ–Ω–∞ –∑–æ–Ω–∞, –±–∞–≥–∞—Ç–æ —Ä–µ—Å—Ç–æ—Ä–∞–Ω—ñ–≤ –ø–æ—Ä—è–¥
- **–°—Ö—ñ–¥:** –º–µ–Ω—à–∞ –≥—Ä—É–ø–∞
- **–ó–∞—Ö—ñ–¥/Outlier:** –ø–æ–æ–¥–∏–Ω–æ–∫—ñ –ª–æ–∫–∞—Ü—ñ—ó

---

## –ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞

### –í—ñ–¥—Å—Ç–∞–Ω—å (–∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º ‚Äî Euclidean)

$$d(p, q) = \sqrt{\sum_{i=1}^{n} (p_i - q_i)^2}$$

**–î–ª—è 2D:**
$$d = \sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}$$

### Œµ-–æ–∫–æ–ª–∏—Ü—è —Ç–æ—á–∫–∏ p

$$N_\varepsilon(p) = \{q \in X : d(p, q) \leq \varepsilon\}$$

**–ü—Ä–∏–∫–ª–∞–¥:**

–¢–æ—á–∫–∞ $p = [2, 3]$, $\varepsilon = 1.5$

–¢–æ—á–∫–∏:
- $q_1 = [2.5, 3.5]$ ‚Üí $d = \sqrt{0.25 + 0.25} = 0.71 \leq 1.5$ ‚úì
- $q_2 = [4, 5]$ ‚Üí $d = \sqrt{4 + 4} = 2.83 > 1.5$ ‚úó

–°—É—Å—ñ–¥–∏: $N_\varepsilon(p) = \{q_1\}$

### –£–º–æ–≤–∞ Core Point

$$|N_\varepsilon(p)| \geq \text{MinPts}$$

–Ø–∫—â–æ –≤ –æ–∫–æ–ª–∏—Ü—ñ ‚â• MinPts —Ç–æ—á–æ–∫ ‚Üí core point.

---

## –°–∫–ª–∞–¥–Ω–∏–π –ø—Ä–∏–∫–ª–∞–¥: –ê–Ω–æ–º–∞–ª—ñ—ó –≤ —Ç—Ä–∞–Ω–∑–∞–∫—Ü—ñ—è—Ö

### –ó–∞–¥–∞—á–∞

–ë–∞–Ω–∫ –º–∞—î —Ç—Ä–∞–Ω–∑–∞–∫—Ü—ñ—ó –∫–ª—ñ—î–Ω—Ç—ñ–≤:
- **Amount** ‚Äî —Å—É–º–∞ —Ç—Ä–∞–Ω–∑–∞–∫—Ü—ñ—ó
- **Time** ‚Äî —á–∞—Å –¥–æ–±–∏ (–≥–æ–¥–∏–Ω–∏)

**–ú–µ—Ç–∞:** –∑–Ω–∞–π—Ç–∏ –≥—Ä—É–ø–∏ –Ω–æ—Ä–º–∞–ª—å–Ω–∏—Ö —Ç—Ä–∞–Ω–∑–∞–∫—Ü—ñ–π —Ç–∞ –≤–∏—è–≤–∏—Ç–∏ –∞–Ω–æ–º–∞–ª—ñ—ó.

### –î–∞–Ω—ñ (—Å–ø—Ä–æ—â–µ–Ω–æ)

```python
–¢—Ä–∞–Ω–∑–∞–∫—Ü—ñ—ó:
- –†–∞–Ω–æ–∫ (7-9): $20-50 (–∫–∞–≤–∞, —Å–Ω—ñ–¥–∞–Ω–æ–∫)
- –û–±—ñ–¥ (12-14): $50-100 (–æ–±—ñ–¥)
- –í–µ—á—ñ—Ä (18-20): $100-200 (–≤–µ—á–µ—Ä—è, –ø–æ–∫—É–ø–∫–∏)
- –ê–ù–û–ú–ê–õ–Ü–á: $5000 –æ 3:00 (–ø—ñ–¥–æ–∑—Ä—ñ–ª–∞ —Ç—Ä–∞–Ω–∑–∞–∫—Ü—ñ—è)
```

### DBSCAN

**–ü–∞—Ä–∞–º–µ—Ç—Ä–∏:**
- Œµ = 2.0 (–Ω–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω—ñ –æ–¥–∏–Ω–∏—Ü—ñ)
- MinPts = 5

**–†–µ–∑—É–ª—å—Ç–∞—Ç:**

**–ö–ª–∞—Å—Ç–µ—Ä 1:** –†–∞–Ω–∫–æ–≤—ñ —Ç—Ä–∞–Ω–∑–∞–∫—Ü—ñ—ó ($20-50, 7-9 –≥–æ–¥)
**–ö–ª–∞—Å—Ç–µ—Ä 2:** –û–±—ñ–¥–Ω—ñ —Ç—Ä–∞–Ω–∑–∞–∫—Ü—ñ—ó ($50-100, 12-14 –≥–æ–¥)
**–ö–ª–∞—Å—Ç–µ—Ä 3:** –í–µ—á—ñ—Ä–Ω—ñ —Ç—Ä–∞–Ω–∑–∞–∫—Ü—ñ—ó ($100-200, 18-20 –≥–æ–¥)
**Noise:** –¢—Ä–∞–Ω–∑–∞–∫—Ü—ñ—è $5000 –æ 3:00 ‚Üí **FRAUD ALERT!** üö®

### –ü–µ—Ä–µ–≤–∞–≥–∏ –ø—ñ–¥—Ö–æ–¥—É

- ‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –≤–∏—è–≤–ª—è—î –∞–Ω–æ–º–∞–ª—ñ—ó
- ‚úÖ –ù–µ –ø–æ—Ç—Ä–µ–±—É—î –∑–∞–∑–¥–∞–ª–µ–≥—ñ–¥—å –∑–Ω–∞—Ç–∏ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ç–∏–ø—ñ–≤ —Ç—Ä–∞–Ω–∑–∞–∫—Ü—ñ–π
- ‚úÖ –†–æ–±–∞—Å—Ç–Ω–∏–π –¥–æ –Ω–æ–≤–∏—Ö —Ç–∏–ø—ñ–≤ —à–∞—Ö—Ä–∞–π—Å—Ç–≤–∞

---

## –ö–æ–¥ (Python + scikit-learn)

### –ë–∞–∑–æ–≤–∏–π –ø—Ä–∏–∫–ª–∞–¥

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import DBSCAN
from sklearn.datasets import make_moons

# 1. –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –¥–∞–Ω–∏—Ö (2 "–ø—ñ–≤–º—ñ—Å—è—Ü—ñ")
X, _ = make_moons(n_samples=300, noise=0.05, random_state=42)

# 2. DBSCAN
dbscan = DBSCAN(
    eps=0.3,           # –†–∞–¥—ñ—É—Å Œµ
    min_samples=5,     # –ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ç–æ—á–æ–∫
    metric='euclidean' # –ú–µ—Ç—Ä–∏–∫–∞ –≤—ñ–¥—Å—Ç–∞–Ω—ñ
)

# 3. –ù–∞–≤—á–∞–Ω–Ω—è (fit_predict)
labels = dbscan.fit_predict(X)

# 4. –†–µ–∑—É–ª—å—Ç–∞—Ç–∏
n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
n_noise = list(labels).count(-1)

print(f"–ö—ñ–ª—å–∫—ñ—Å—Ç—å –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤: {n_clusters}")
print(f"–ö—ñ–ª—å–∫—ñ—Å—Ç—å noise —Ç–æ—á–æ–∫: {n_noise}")
print(f"–£–Ω—ñ–∫–∞–ª—å–Ω—ñ –º—ñ—Ç–∫–∏: {set(labels)}")

# -1 –æ–∑–Ω–∞—á–∞—î NOISE!

# 5. –î–æ–¥–∞—Ç–∫–æ–≤–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è
core_samples_mask = np.zeros_like(labels, dtype=bool)
core_samples_mask[dbscan.core_sample_indices_] = True

print(f"\nCore points: {np.sum(core_samples_mask)}")
print(f"Border points: {len(labels) - n_noise - np.sum(core_samples_mask)}")

# 6. –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# –î–æ DBSCAN
axes[0].scatter(X[:, 0], X[:, 1], s=50, alpha=0.6)
axes[0].set_title('Before DBSCAN', fontsize=14, fontweight='bold')
axes[0].set_xlabel('Feature 1')
axes[0].set_ylabel('Feature 2')
axes[0].grid(True, alpha=0.3)

# –ü—ñ—Å–ª—è DBSCAN
unique_labels = set(labels)
colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]

for k, col in zip(unique_labels, colors):
    if k == -1:
        # Noise: —á–æ—Ä–Ω–∏–π –∫–æ–ª—ñ—Ä –∑ x-–º–∞—Ä–∫–µ—Ä–æ–º
        col = [0, 0, 0, 1]
        marker = 'x'
        label = 'Noise'
    else:
        marker = 'o'
        label = f'Cluster {k}'
    
    class_member_mask = (labels == k)
    
    xy = X[class_member_mask & core_samples_mask]
    axes[1].scatter(xy[:, 0], xy[:, 1], s=100, marker=marker, 
                   c=[col], edgecolors='black', linewidths=1.5,
                   label=f'{label} (core)')
    
    xy = X[class_member_mask & ~core_samples_mask]
    axes[1].scatter(xy[:, 0], xy[:, 1], s=50, marker=marker,
                   c=[col], alpha=0.3, edgecolors='black', linewidths=0.5,
                   label=f'{label} (border)' if k != -1 else label)

axes[1].set_title(f'After DBSCAN (eps={dbscan.eps}, min_samples={dbscan.min_samples})',
                 fontsize=14, fontweight='bold')
axes[1].set_xlabel('Feature 1')
axes[1].set_ylabel('Feature 2')
axes[1].legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### –ü–æ–≤–Ω–∏–π –ø—Ä–∏–∫–ª–∞–¥: –ì–µ–æ–≥—Ä–∞—Ñ—ñ—á–Ω–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—è

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler

# –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –¥–∞–Ω–∏—Ö –ø—Ä–æ —Ä–µ—Å—Ç–æ—Ä–∞–Ω–∏
np.random.seed(42)

# –ö–ª–∞—Å—Ç–µ—Ä 1: –¶–µ–Ω—Ç—Ä –º—ñ—Å—Ç–∞
center1 = np.random.normal([50.45, 30.52], [0.01, 0.01], (50, 2))

# –ö–ª–∞—Å—Ç–µ—Ä 2: –†–∞–π–æ–Ω –Ω–∞ –ø—ñ–≤–Ω–æ—á—ñ
center2 = np.random.normal([50.50, 30.45], [0.008, 0.008], (30, 2))

# –ö–ª–∞—Å—Ç–µ—Ä 3: –†–∞–π–æ–Ω –Ω–∞ —Å—Ö–æ–¥—ñ
center3 = np.random.normal([50.40, 30.60], [0.012, 0.012], (40, 2))

# Outliers
outliers = np.array([
    [50.35, 30.70],
    [50.55, 30.40],
    [50.38, 30.55],
    [50.48, 30.58]
])

# –û–±'—î–¥–Ω–∞—Ç–∏ –≤—Å—ñ –¥–∞–Ω—ñ
X = np.vstack([center1, center2, center3, outliers])

df = pd.DataFrame(X, columns=['Latitude', 'Longitude'])
df['ID'] = range(len(df))

print("=== Dataset Info ===")
print(f"Total points: {len(df)}")
print(df.head())

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –¥–æ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—ó
plt.figure(figsize=(12, 10))
plt.subplot(2, 1, 1)
plt.scatter(df['Longitude'], df['Latitude'], s=50, alpha=0.6)
plt.xlabel('Longitude', fontsize=12)
plt.ylabel('Latitude', fontsize=12)
plt.title('Restaurant Locations (Before DBSCAN)', fontsize=14, fontweight='bold')
plt.grid(True, alpha=0.3)

# DBSCAN
# –ù–µ –ø–æ—Ç—Ä—ñ–±–µ–Ω scaling –¥–ª—è –≥–µ–æ–≥—Ä–∞—Ñ—ñ—á–Ω–∏—Ö –¥–∞–Ω–∏—Ö –≤ –æ–¥–Ω–∞–∫–æ–≤–∏—Ö –æ–¥–∏–Ω–∏—Ü—è—Ö,
# –∞–ª–µ –º–æ–∂–Ω–∞ –¥–ª—è —É–∑–∞–≥–∞–ª—å–Ω–µ–Ω–æ—Å—Ç—ñ
dbscan = DBSCAN(
    eps=0.02,          # ‚âà 2–∫–º –≤ lat/lon –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞—Ö
    min_samples=5,
    metric='euclidean'
)

labels = dbscan.fit_predict(df[['Latitude', 'Longitude']])
df['Cluster'] = labels

# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
n_noise = list(labels).count(-1)

print("\n" + "="*60)
print("=== DBSCAN Results ===")
print("="*60)
print(f"Number of clusters: {n_clusters}")
print(f"Number of noise points: {n_noise}")
print(f"Labels: {set(labels)}")

# –ê–Ω–∞–ª—ñ–∑ –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤
for cluster in sorted(set(labels)):
    cluster_data = df[df['Cluster'] == cluster]
    if cluster == -1:
        print(f"\nNoise points: {len(cluster_data)}")
        print(cluster_data[['Latitude', 'Longitude']])
    else:
        print(f"\nCluster {cluster}: {len(cluster_data)} points")
        print(f"  Center: Lat={cluster_data['Latitude'].mean():.4f}, "
              f"Lon={cluster_data['Longitude'].mean():.4f}")
        print(f"  Spread: Lat¬±{cluster_data['Latitude'].std():.4f}, "
              f"Lon¬±{cluster_data['Longitude'].std():.4f}")

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –ø—ñ—Å–ª—è DBSCAN
plt.subplot(2, 1, 2)

unique_labels = set(labels)
colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]

core_samples_mask = np.zeros_like(labels, dtype=bool)
core_samples_mask[dbscan.core_sample_indices_] = True

for k, col in zip(unique_labels, colors):
    if k == -1:
        col = [0, 0, 0, 1]  # –ß–æ—Ä–Ω–∏–π –¥–ª—è noise
        marker = 'x'
        label = 'Noise'
    else:
        marker = 'o'
        label = f'Cluster {k}'
    
    class_member_mask = (labels == k)
    
    # Core points
    xy = df.loc[class_member_mask & core_samples_mask, ['Longitude', 'Latitude']].values
    if len(xy) > 0:
        plt.scatter(xy[:, 0], xy[:, 1], s=100, marker=marker,
                   c=[col], edgecolors='black', linewidths=1.5,
                   label=f'{label} (core)')
    
    # Border points
    xy = df.loc[class_member_mask & ~core_samples_mask, ['Longitude', 'Latitude']].values
    if len(xy) > 0:
        plt.scatter(xy[:, 0], xy[:, 1], s=50, marker=marker,
                   c=[col], alpha=0.5, edgecolors='black', linewidths=0.5,
                   label=f'{label} (border)' if k != -1 else '')

plt.xlabel('Longitude', fontsize=12)
plt.ylabel('Latitude', fontsize=12)
plt.title(f'After DBSCAN (eps={dbscan.eps}, min_samples={dbscan.min_samples})',
         fontsize=14, fontweight='bold')
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# –ï–∫—Å–ø–æ—Ä—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
df_clusters = df.groupby('Cluster').agg({
    'Latitude': ['mean', 'std', 'count'],
    'Longitude': ['mean', 'std']
}).round(4)

print("\n" + "="*60)
print("=== Cluster Summary ===")
print("="*60)
print(df_clusters)
```

### –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –∑ K-Means

```python
from sklearn.cluster import KMeans

# –î–∞–Ω—ñ –∑ —Å–∫–ª–∞–¥–Ω–æ—é —Ñ–æ—Ä–º–æ—é (2 –ø—ñ–≤–º—ñ—Å—è—Ü—ñ)
X, _ = make_moons(n_samples=300, noise=0.05, random_state=42)

# K-Means
kmeans = KMeans(n_clusters=2, random_state=42)
kmeans_labels = kmeans.fit_predict(X)

# DBSCAN
dbscan = DBSCAN(eps=0.3, min_samples=5)
dbscan_labels = dbscan.fit_predict(X)

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# K-Means
axes[0].scatter(X[:, 0], X[:, 1], c=kmeans_labels, cmap='viridis', s=50)
axes[0].scatter(kmeans.cluster_centers_[:, 0], 
               kmeans.cluster_centers_[:, 1],
               c='red', marker='X', s=200, edgecolors='black', linewidths=2)
axes[0].set_title('K-Means (K=2)', fontsize=14, fontweight='bold')
axes[0].set_xlabel('Feature 1')
axes[0].set_ylabel('Feature 2')
axes[0].grid(True, alpha=0.3)

# DBSCAN
unique_labels = set(dbscan_labels)
colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]

for k, col in zip(unique_labels, colors):
    if k == -1:
        col = [0, 0, 0, 1]
    
    class_member_mask = (dbscan_labels == k)
    xy = X[class_member_mask]
    axes[1].scatter(xy[:, 0], xy[:, 1], c=[col], s=50,
                   marker='x' if k == -1 else 'o')

axes[1].set_title('DBSCAN (eps=0.3, min_samples=5)', 
                 fontsize=14, fontweight='bold')
axes[1].set_xlabel('Feature 1')
axes[1].set_ylabel('Feature 2')
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("\nK-Means: –ü–æ–≥–∞–Ω–æ —Å–ø—Ä–∞–≤–ª—è—î—Ç—å—Å—è –∑ –Ω–µ–ø—Ä—è–º–æ–∫—É—Ç–Ω–∏–º–∏ –∫–ª–∞—Å—Ç–µ—Ä–∞–º–∏")
print("DBSCAN: –Ü–¥–µ–∞–ª—å–Ω–æ –∑–Ω–∞—Ö–æ–¥–∏—Ç—å –ø—ñ–≤–º—ñ—Å—è—Ü—ñ! ‚úì")
```

---

## –í–∏–±—ñ—Ä –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ Œµ —Ç–∞ MinPts

### –ü—Ä–æ–±–ª–µ–º–∞

**DBSCAN –¥—É–∂–µ —á—É—Ç–ª–∏–≤–∏–π –¥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤!**

- Œµ –∑–∞–Ω–∞–¥—Ç–æ –º–∞–ª–µ ‚Üí –±–∞–≥–∞—Ç–æ noise, –º–∞–ª–æ –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤
- Œµ –∑–∞–Ω–∞–¥—Ç–æ –≤–µ–ª–∏–∫–µ ‚Üí –≤—Å–µ –æ–¥–∏–Ω –∫–ª–∞—Å—Ç–µ—Ä
- MinPts –∑–∞–Ω–∞–¥—Ç–æ –º–∞–ª–µ ‚Üí –±–∞–≥–∞—Ç–æ –º–∞–ª–µ–Ω—å–∫–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤
- MinPts –∑–∞–Ω–∞–¥—Ç–æ –≤–µ–ª–∏–∫–µ ‚Üí –±–∞–≥–∞—Ç–æ noise

### 1. K-Distance Graph (—Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–æ ‚úì)

**–ú–µ—Ç–æ–¥ –¥–ª—è –≤–∏–±–æ—Ä—É Œµ:**

1. –î–ª—è –∫–æ–∂–Ω–æ—ó —Ç–æ—á–∫–∏ –∑–Ω–∞–π—Ç–∏ –≤—ñ–¥—Å—Ç–∞–Ω—å –¥–æ k-–≥–æ –Ω–∞–π–±–ª–∏–∂—á–æ–≥–æ —Å—É—Å—ñ–¥–∞
2. –í—ñ–¥—Å–æ—Ä—Ç—É–≤–∞—Ç–∏ —Ü—ñ –≤—ñ–¥—Å—Ç–∞–Ω—ñ
3. –ü–æ–±—É–¥—É–≤–∞—Ç–∏ –≥—Ä–∞—Ñ—ñ–∫
4. –ó–Ω–∞–π—Ç–∏ "–ª—ñ–∫–æ—Ç—å" (—Ä—ñ–∑–∫–µ –∑—Ä–æ—Å—Ç–∞–Ω–Ω—è) ‚Üí —Ü–µ Œµ

```python
from sklearn.neighbors import NearestNeighbors

# MinPts = 5 (–ø—Ä–∏–∫–ª–∞–¥)
min_samples = 5

# –ó–Ω–∞–π—Ç–∏ –≤—ñ–¥—Å—Ç–∞–Ω—ñ –¥–æ k-–≥–æ —Å—É—Å—ñ–¥–∞
neighbors = NearestNeighbors(n_neighbors=min_samples)
neighbors.fit(X)
distances, indices = neighbors.kneighbors(X)

# –í–∑—è—Ç–∏ –≤—ñ–¥—Å—Ç–∞–Ω—å –¥–æ –æ—Å—Ç–∞–Ω–Ω—å–æ–≥–æ (k-–≥–æ) —Å—É—Å—ñ–¥–∞
k_distances = distances[:, -1]
k_distances = np.sort(k_distances)

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
plt.figure(figsize=(10, 6))
plt.plot(k_distances)
plt.xlabel('Points sorted by distance', fontsize=12)
plt.ylabel(f'{min_samples}-th Nearest Neighbor Distance', fontsize=12)
plt.title('K-Distance Graph', fontsize=14, fontweight='bold')
plt.axhline(y=0.3, color='red', linestyle='--', 
            label='Suggested Œµ=0.3 (elbow)')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# –ó–Ω–∞–π—Ç–∏ "–ª—ñ–∫–æ—Ç—å" –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ (–Ω–∞–±–ª–∏–∂–µ–Ω–æ)
# –¢–æ—á–∫–∞ –Ω–∞–π–±—ñ–ª—å—à–æ—ó –∫—Ä–∏–≤–∏–∑–Ω–∏
diff = np.diff(k_distances)
diff2 = np.diff(diff)
elbow_idx = np.argmax(diff2) + 1
suggested_eps = k_distances[elbow_idx]

print(f"Suggested Œµ: {suggested_eps:.4f}")
```

**–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü—ñ—è –≥—Ä–∞—Ñ—ñ–∫–∞:**

```
Distance
    |            ‚ï±‚ï±‚ï±‚ï±  ‚Üê —Ä—ñ–∑–∫–µ –∑—Ä–æ—Å—Ç–∞–Ω–Ω—è (outliers)
    |          ‚ï±
    |        ‚ï±  ‚Üê "–ª—ñ–∫–æ—Ç—å" = –æ–ø—Ç–∏–º–∞–ª—å–Ω–∏–π Œµ
    |      ‚ï±
    |    ‚ï±
    |  ‚ï±
    |‚ï±_____________ Point index
```

### 2. MinPts (–µ–º–ø—ñ—Ä–∏—á–Ω–µ –ø—Ä–∞–≤–∏–ª–æ)

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó:**

$$\text{MinPts} \geq \text{dimensions} + 1$$

**–ü—Ä–∞–∫—Ç–∏—á–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è:**

| –†–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å | MinPts |
|-------------|--------|
| 2D | 4-5 |
| 3D | 5-6 |
| –í–∏—Å–æ–∫–æ—Ä–æ–∑–º—ñ—Ä–Ω—ñ | 2 √ó dimensions |

**–ó–∞–≥–∞–ª—å–Ω–µ –ø—Ä–∞–≤–∏–ª–æ:**
- **–ë—ñ–ª—å—à–µ —à—É–º—É** ‚Üí –∑–±—ñ–ª—å—à–∏—Ç–∏ MinPts
- **–ú–∞–ª–µ–Ω—å–∫—ñ –∫–ª–∞—Å—Ç–µ—Ä–∏** ‚Üí –∑–º–µ–Ω—à–∏—Ç–∏ MinPts

### 3. Grid Search (—è–∫—â–æ —î ground truth)

```python
from sklearn.metrics import silhouette_score

# –°—ñ—Ç–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤
eps_values = np.arange(0.1, 1.0, 0.1)
min_samples_values = [3, 5, 7, 10]

best_score = -1
best_params = {}

results = []

for eps in eps_values:
    for min_samples in min_samples_values:
        dbscan = DBSCAN(eps=eps, min_samples=min_samples)
        labels = dbscan.fit_predict(X)
        
        # –ü—Ä–æ–ø—É—Å—Ç–∏—Ç–∏, —è–∫—â–æ –≤—Å—ñ —Ç–æ—á–∫–∏ noise –∞–±–æ –æ–¥–∏–Ω –∫–ª–∞—Å—Ç–µ—Ä
        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
        if n_clusters < 2:
            continue
        
        # Silhouette score (—Ç—ñ–ª—å–∫–∏ –¥–ª—è non-noise —Ç–æ—á–æ–∫)
        mask = labels != -1
        if np.sum(mask) > 0:
            score = silhouette_score(X[mask], labels[mask])
            results.append({
                'eps': eps,
                'min_samples': min_samples,
                'n_clusters': n_clusters,
                'n_noise': np.sum(~mask),
                'silhouette': score
            })
            
            if score > best_score:
                best_score = score
                best_params = {'eps': eps, 'min_samples': min_samples}

# –†–µ–∑—É–ª—å—Ç–∞—Ç–∏
results_df = pd.DataFrame(results)
print("=== Top 5 Parameter Combinations ===")
print(results_df.nlargest(5, 'silhouette'))

print(f"\nBest parameters: {best_params}")
print(f"Best silhouette score: {best_score:.4f}")
```

### 4. Domain Knowledge

**–ì–µ–æ–≥—Ä–∞—Ñ—ñ—á–Ω—ñ –¥–∞–Ω—ñ:**
- Œµ = –≤—ñ–¥—Å—Ç–∞–Ω—å, –Ω–∞ —è–∫—ñ–π —Ç–æ—á–∫–∏ –≤–≤–∞–∂–∞—é—Ç—å—Å—è "–±–ª–∏–∑—å–∫–∏–º–∏"
- –ù–∞–ø—Ä–∏–∫–ª–∞–¥: 500–º –¥–ª—è –ø—ñ—à–æ—Ö–æ–¥—ñ–≤, 5–∫–º –¥–ª—è –∞–≤—Ç–æ

**–ß–∞—Å–æ–≤—ñ –¥–∞–Ω—ñ:**
- Œµ = —á–∞—Å–æ–≤–∏–π —ñ–Ω—Ç–µ—Ä–≤–∞–ª (–≥–æ–¥–∏–Ω–∏, –¥–Ω—ñ)

**–ë—ñ–∑–Ω–µ—Å-–ø—Ä–∞–≤–∏–ª–∞:**
- MinPts = –º—ñ–Ω—ñ–º–∞–ª—å–Ω–∏–π —Ä–æ–∑–º—ñ—Ä –∑–Ω–∞—á—É—â–æ—ó –≥—Ä—É–ø–∏
- –ù–∞–ø—Ä–∏–∫–ª–∞–¥: –º—ñ–Ω—ñ–º—É–º 10 –∫–ª—ñ—î–Ω—Ç—ñ–≤ –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞

---

## –ú–µ—Ç—Ä–∏–∫–∏ –≤—ñ–¥—Å—Ç–∞–Ω—ñ

### –ó–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º: Euclidean

$$d(p, q) = \sqrt{\sum_{i=1}^{n} (p_i - q_i)^2}$$

### –Ü–Ω—à—ñ –º–µ—Ç—Ä–∏–∫–∏

```python
from sklearn.cluster import DBSCAN

# Manhattan distance
dbscan = DBSCAN(eps=0.5, min_samples=5, metric='manhattan')

# Cosine distance
dbscan = DBSCAN(eps=0.5, min_samples=5, metric='cosine')

# –í–ª–∞—Å–Ω–∞ –º–µ—Ç—Ä–∏–∫–∞
def custom_distance(x, y):
    return np.sum(np.abs(x - y))

dbscan = DBSCAN(eps=0.5, min_samples=5, metric=custom_distance)
```

**–ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ —Ä—ñ–∑–Ω—ñ –º–µ—Ç—Ä–∏–∫–∏:**

| –ú–µ—Ç—Ä–∏–∫–∞ | –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è |
|---------|--------------|
| **Euclidean** | –ó–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º, –≥–µ–æ–≥—Ä–∞—Ñ—ñ—á–Ω—ñ –¥–∞–Ω—ñ |
| **Manhattan** | Grid-based –¥–∞–Ω—ñ (–º—ñ—Å—å–∫—ñ –∫–≤–∞—Ä—Ç–∞–ª–∏) |
| **Cosine** | –¢–µ–∫—Å—Ç–æ–≤—ñ –¥–∞–Ω—ñ (TF-IDF vectors) |
| **Haversine** | –ì–µ–æ–≥—Ä–∞—Ñ—ñ—á–Ω—ñ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∏ (lat/lon –Ω–∞ —Å—Ñ–µ—Ä—ñ) |

### Haversine –¥–ª—è –≥–µ–æ–≥—Ä–∞—Ñ—ñ—á–Ω–∏—Ö –¥–∞–Ω–∏—Ö

```python
from sklearn.metrics.pairwise import haversine_distances
import math

# –ö–æ–Ω–≤–µ—Ä—Ç—É–≤–∞—Ç–∏ –≤ —Ä–∞–¥—ñ–∞–Ω–∏
X_radians = np.radians(X)

# DBSCAN –∑ Haversine
dbscan = DBSCAN(
    eps=0.01,  # –≤ —Ä–∞–¥—ñ–∞–Ω–∞—Ö (‚âà111km)
    min_samples=5,
    metric='haversine'
)

labels = dbscan.fit_predict(X_radians)

# –ê–±–æ —á–µ—Ä–µ–∑ precomputed distance matrix
distance_matrix = haversine_distances(X_radians) * 6371  # —Ä–∞–¥—ñ—É—Å –ó–µ–º–ª—ñ –≤ –∫–º
dbscan = DBSCAN(eps=5, min_samples=5, metric='precomputed')
labels = dbscan.fit_predict(distance_matrix)
```

---

## Preprocessing –¥–ª—è DBSCAN

### 1. Scaling (–í–ê–ñ–õ–ò–í–û!)

**–Ø–∫—â–æ –æ–∑–Ω–∞–∫–∏ –≤ —Ä—ñ–∑–Ω–∏—Ö –º–∞—Å—à—Ç–∞–±–∞—Ö:**

```python
from sklearn.preprocessing import StandardScaler

# –ü—Ä–∏–∫–ª–∞–¥: –≤—ñ–∫ (0-100) —Ç–∞ –¥–æ—Ö—ñ–¥ (0-150000)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

dbscan = DBSCAN(eps=0.5, min_samples=5)
labels = dbscan.fit_predict(X_scaled)
```

**–ö–æ–ª–∏ –ù–ï –ø–æ—Ç—Ä—ñ–±–µ–Ω scaling:**
- –í—Å—ñ –æ–∑–Ω–∞–∫–∏ –≤ –æ–¥–Ω–∞–∫–æ–≤–∏—Ö –æ–¥–∏–Ω–∏—Ü—è—Ö (lat/lon, –≤—Å—ñ –≤ –º–µ—Ç—Ä–∞—Ö)
- –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—à cosine distance

### 2. Dimensionality Reduction

**PCA –ø–µ—Ä–µ–¥ DBSCAN –¥–ª—è –≤–∏—Å–æ–∫–æ—Ä–æ–∑–º—ñ—Ä–Ω–∏—Ö –¥–∞–Ω–∏—Ö:**

```python
from sklearn.decomposition import PCA

# –ó–º–µ–Ω—à–∏—Ç–∏ —Ä–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å
pca = PCA(n_components=0.95)  # –ó–∞–ª–∏—à–∏—Ç–∏ 95% variance
X_pca = pca.fit_transform(X_scaled)

dbscan = DBSCAN(eps=0.5, min_samples=5)
labels = dbscan.fit_predict(X_pca)
```

**–ß–æ–º—É:** Curse of dimensionality ‚Äî —É –≤–∏—Å–æ–∫–∏—Ö —Ä–æ–∑–º—ñ—Ä–Ω–æ—Å—Ç—è—Ö –≤—Å—ñ —Ç–æ—á–∫–∏ –æ–¥–Ω–∞–∫–æ–≤–æ –¥–∞–ª–µ–∫—ñ.

### 3. Outlier Removal (–æ–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ)

**–Ø–∫—â–æ –¥—É–∂–µ –±–∞–≥–∞—Ç–æ outliers:**

```python
# –ü–æ–ø–µ—Ä–µ–¥–Ω—è –æ—á–∏—Å—Ç–∫–∞ –µ–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–∏—Ö outliers
from scipy import stats

z_scores = np.abs(stats.zscore(X))
mask = (z_scores < 3).all(axis=1)
X_clean = X[mask]

# –ü–æ—Ç—ñ–º DBSCAN
dbscan = DBSCAN(eps=0.5, min_samples=5)
labels = dbscan.fit_predict(X_clean)
```

---

## –û—Ü—ñ–Ω–∫–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—ó DBSCAN

### 1. Silhouette Score

**–¢—ñ–ª—å–∫–∏ –¥–ª—è non-noise —Ç–æ—á–æ–∫!**

```python
from sklearn.metrics import silhouette_score

# –í–∏–¥–∞–ª–∏—Ç–∏ noise —Ç–æ—á–∫–∏
mask = labels != -1
if np.sum(mask) > 0 and len(set(labels[mask])) > 1:
    score = silhouette_score(X[mask], labels[mask])
    print(f"Silhouette Score: {score:.4f}")
```

### 2. Davies-Bouldin Index

```python
from sklearn.metrics import davies_bouldin_score

mask = labels != -1
if np.sum(mask) > 0:
    score = davies_bouldin_score(X[mask], labels[mask])
    print(f"Davies-Bouldin Index: {score:.4f}")  # –ú–µ–Ω—à–µ = –∫—Ä–∞—â–µ
```

### 3. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤

```python
unique_labels = set(labels)
n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)
n_noise = list(labels).count(-1)

print(f"–ö—ñ–ª—å–∫—ñ—Å—Ç—å –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤: {n_clusters}")
print(f"–ö—ñ–ª—å–∫—ñ—Å—Ç—å noise: {n_noise} ({n_noise/len(labels)*100:.1f}%)")

# –†–æ–∑–º—ñ—Ä–∏ –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤
for label in sorted(unique_labels):
    if label == -1:
        continue
    cluster_size = np.sum(labels == label)
    print(f"–ö–ª–∞—Å—Ç–µ—Ä {label}: {cluster_size} —Ç–æ—á–æ–∫")

# Core vs Border points
core_mask = np.zeros_like(labels, dtype=bool)
core_mask[dbscan.core_sample_indices_] = True

n_core = np.sum(core_mask)
n_border = np.sum((labels != -1) & ~core_mask)

print(f"\nCore points: {n_core}")
print(f"Border points: {n_border}")
print(f"Noise points: {n_noise}")
```

---

## –ü–µ—Ä–µ–≤–∞–≥–∏ —Ç–∞ –Ω–µ–¥–æ–ª—ñ–∫–∏

### –ü–µ—Ä–µ–≤–∞–≥–∏ ‚úì

| –ü–µ—Ä–µ–≤–∞–≥–∞ | –ü–æ—è—Å–Ω–µ–Ω–Ω—è |
|----------|-----------|
| **–ù–µ –ø–æ—Ç—Ä—ñ–±–Ω–æ –∑–Ω–∞—Ç–∏ K** | –ö—ñ–ª—å–∫—ñ—Å—Ç—å –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤ –≤–∏–∑–Ω–∞—á–∞—î—Ç—å—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ |
| **–°–∫–ª–∞–¥–Ω–∞ —Ñ–æ—Ä–º–∞ –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤** | –ó–Ω–∞—Ö–æ–¥–∏—Ç—å –∫–ª–∞—Å—Ç–µ—Ä–∏ –±—É–¥—å-—è–∫–æ—ó —Ñ–æ—Ä–º–∏ (–Ω–µ —Ç—ñ–ª—å–∫–∏ –∫—Ä—É–≥–ª—ñ) |
| **–í–∏—è–≤–ª–µ–Ω–Ω—è outliers** | –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ —ñ–¥–µ–Ω—Ç–∏—Ñ—ñ–∫—É—î noise |
| **–†–æ–±–∞—Å—Ç–Ω—ñ—Å—Ç—å –¥–æ —à—É–º—É** | –ù–µ —á—É—Ç–ª–∏–≤–∏–π –¥–æ outliers —è–∫ K-Means |
| **–†—ñ–∑–Ω—ñ —Ä–æ–∑–º—ñ—Ä–∏ –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤** | –ù–µ –≤–∏–º–∞–≥–∞—î –æ–¥–Ω–∞–∫–æ–≤–∏—Ö —Ä–æ–∑–º—ñ—Ä—ñ–≤ |
| **–û–¥–∏–Ω –ø—Ä–æ—Ö—ñ–¥ –¥–∞–Ω–∏—Ö** | –ï—Ñ–µ–∫—Ç–∏–≤–Ω–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º |
| **–î–µ—Ç–µ—Ä–º—ñ–Ω—ñ—Å—Ç–∏—á–Ω–∏–π** | –û–¥–Ω–∞–∫–æ–≤—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ (–ø—Ä–∏ –æ–¥–Ω–∞–∫–æ–≤–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–∞—Ö) |

### –ù–µ–¥–æ–ª—ñ–∫–∏ ‚úó

| –ù–µ–¥–æ–ª—ñ–∫ | –ü–æ—è—Å–Ω–µ–Ω–Ω—è |
|---------|-----------|
| **–ß—É—Ç–ª–∏–≤—ñ—Å—Ç—å –¥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤** | Œµ —Ç–∞ MinPts –ø–æ—Ç—Ä—ñ–±–Ω–æ –ø—ñ–¥–±–∏—Ä–∞—Ç–∏ |
| **–†—ñ–∑–Ω–∞ —â—ñ–ª—å–Ω—ñ—Å—Ç—å** | –ü–æ–≥–∞–Ω–æ –∑ –∫–ª–∞—Å—Ç–µ—Ä–∞–º–∏ –¥—É–∂–µ —Ä—ñ–∑–Ω–æ—ó —â—ñ–ª—å–Ω–æ—Å—Ç—ñ |
| **–í–∏—Å–æ–∫–æ—Ä–æ–∑–º—ñ—Ä–Ω—ñ –¥–∞–Ω—ñ** | Curse of dimensionality (distance —Å—Ç–∞–Ω–æ–≤–∏—Ç—å—Å—è –±–µ–∑–≥–ª—É–∑–¥–æ—é) |
| **–°–∫–ª–∞–¥–Ω—ñ—Å—Ç—å** | O(n¬≤) –±–µ–∑ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ–π, O(n log n) –∑ —ñ–Ω–¥–µ–∫—Å–∞–º–∏ |
| **–ù–µ –ø—Ä–∞—Ü—é—î –∑ –∫–∞—Ç–µ–≥–æ—Ä—ñ–∞–ª—å–Ω–∏–º–∏** | –¢—ñ–ª—å–∫–∏ —á–∏—Å–ª–æ–≤—ñ –¥–∞–Ω—ñ (–ø–æ—Ç—Ä—ñ–±–Ω–∞ –º–µ—Ç—Ä–∏–∫–∞ –≤—ñ–¥—Å—Ç–∞–Ω—ñ) |
| **–°–∫–ª–∞–¥–Ω–æ —ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç—É–≤–∞—Ç–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏** | –ù–µ —ñ–Ω—Ç—É—ó—Ç–∏–≤–Ω–æ –∑—Ä–æ–∑—É–º—ñ–ª–æ, —â–æ —Ç–∞–∫–µ Œµ |

---

## –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –∑ —ñ–Ω—à–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏

| –ú–µ—Ç–æ–¥ | –ü–æ—Ç—Ä—ñ–±–Ω–æ K? | –§–æ—Ä–º–∞ | Outliers | –†—ñ–∑–Ω–∞ —â—ñ–ª—å–Ω—ñ—Å—Ç—å | –°–∫–ª–∞–¥–Ω—ñ—Å—Ç—å |
|-------|-------------|-------|----------|------------------|------------|
| **DBSCAN** | ‚ùå –ù—ñ | –ë—É–¥—å-—è–∫–∞ | ‚úÖ –í–∏—è–≤–ª—è—î | ‚ö†Ô∏è –ü–æ–≥–∞–Ω–æ | O(n log n) |
| **K-Means** | ‚úÖ –¢–∞–∫ | –°—Ñ–µ—Ä–∏—á–Ω—ñ | ‚ùå –ß—É—Ç–ª–∏–≤–∏–π | ‚ùå –ü–æ–≥–∞–Ω–æ | O(nKdi) |
| **Hierarchical** | ‚ùå –ù—ñ | –ë—É–¥—å-—è–∫–∞ | ‚ö†Ô∏è –°–µ—Ä–µ–¥–Ω—å–æ | ‚úÖ –î–æ–±—Ä–µ | O(n¬≤) |
| **OPTICS** | ‚ùå –ù—ñ | –ë—É–¥—å-—è–∫–∞ | ‚úÖ –í–∏—è–≤–ª—è—î | ‚úÖ –î–æ–±—Ä–µ | O(n log n) |
| **HDBSCAN** | ‚ùå –ù—ñ | –ë—É–¥—å-—è–∫–∞ | ‚úÖ –í–∏—è–≤–ª—è—î | ‚úÖ –î–æ–±—Ä–µ | O(n log n) |

---

## DBSCAN vs –í–∞—Ä—ñ–∞—Ü—ñ—ó

### OPTICS

**Ordering Points To Identify the Clustering Structure**

**–í—ñ–¥–º—ñ–Ω–Ω—ñ—Å—Ç—å:** –°—Ç–≤–æ—Ä—é—î reachability plot –∑–∞–º—ñ—Å—Ç—å –ø—Ä—è–º–æ—ó –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—ó.

**–ü–µ—Ä–µ–≤–∞–≥–∏:**
- ‚úÖ –ü—Ä–∞—Ü—é—î –∑ —Ä—ñ–∑–Ω–æ—é —â—ñ–ª—å–Ω—ñ—Å—Ç—é
- ‚úÖ –ù–µ –ø–æ—Ç—Ä—ñ–±–Ω–æ –∑–∞–¥–∞–≤–∞—Ç–∏ Œµ (—Ç—ñ–ª—å–∫–∏ MinPts)

```python
from sklearn.cluster import OPTICS

optics = OPTICS(min_samples=5, max_eps=2.0)
labels = optics.fit_predict(X)
```

### HDBSCAN

**Hierarchical DBSCAN**

**–í—ñ–¥–º—ñ–Ω–Ω—ñ—Å—Ç—å:** –Ü—î—Ä–∞—Ä—Ö—ñ—á–Ω–∏–π –ø—ñ–¥—Ö—ñ–¥ + –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∏–π –≤–∏–±—ñ—Ä Œµ.

**–ü–µ—Ä–µ–≤–∞–≥–∏:**
- ‚úÖ –ü—Ä–∞—Ü—é—î –∑ —Ä—ñ–∑–Ω–æ—é —â—ñ–ª—å–Ω—ñ—Å—Ç—é
- ‚úÖ –ë—ñ–ª—å—à —Ä–æ–±–∞—Å—Ç–Ω–∏–π –¥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤
- ‚úÖ –ö—Ä–∞—â–µ –∑ —Ä—ñ–∑–Ω–∏–º–∏ —Ä–æ–∑–º—ñ—Ä–∞–º–∏ –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤

```python
import hdbscan

clusterer = hdbscan.HDBSCAN(min_cluster_size=5, min_samples=5)
labels = clusterer.fit_predict(X)
```

---

## –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ DBSCAN

### –Ü–¥–µ–∞–ª—å–Ω–æ –ø—ñ–¥—Ö–æ–¥–∏—Ç—å ‚úì

- **–ù–µ –∑–Ω–∞—î–º–æ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤** ‚Äî –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –≤–∏–∑–Ω–∞—á–∞—î
- **–°–∫–ª–∞–¥–Ω–∞ —Ñ–æ—Ä–º–∞** ‚Äî S-–∫—Ä–∏–≤—ñ, –∫—ñ–ª—å—Ü—è, –¥–æ–≤–≥—ñ –∫–ª–∞—Å—Ç–µ—Ä–∏
- **Outliers –≤–∞–∂–ª–∏–≤—ñ** ‚Äî –ø–æ—Ç—Ä—ñ–±–Ω–æ —ó—Ö –∑–Ω–∞–π—Ç–∏ —Ç–∞ –æ–±—Ä–æ–±–∏—Ç–∏ –æ–∫—Ä–µ–º–æ
- **–ì–µ–æ–≥—Ä–∞—Ñ—ñ—á–Ω—ñ –¥–∞–Ω—ñ** ‚Äî –≥—Ä—É–ø—É–≤–∞–Ω–Ω—è –ª–æ–∫–∞—Ü—ñ–π
- **–†—ñ–∑–Ω—ñ —Ä–æ–∑–º—ñ—Ä–∏ –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤**
- –†–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å **2-10** features
- **–ê–Ω–æ–º–∞–ª—ñ—ó/fraud detection**

### –ö—Ä–∞—â–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ —ñ–Ω—à–µ ‚úó

- **–î—É–∂–µ —Ä—ñ–∑–Ω–∞ —â—ñ–ª—å–Ω—ñ—Å—Ç—å** ‚Üí OPTICS, HDBSCAN
- **–í–∏—Å–æ–∫–æ—Ä–æ–∑–º—ñ—Ä–Ω—ñ –¥–∞–Ω—ñ** (>20D) ‚Üí dimensionality reduction ‚Üí DBSCAN
- **–ü–æ—Ç—Ä—ñ–±–Ω–∞ —à–≤–∏–¥–∫—ñ—Å—Ç—å** –Ω–∞ –≤–µ–ª–∏–∫–∏—Ö –¥–∞–Ω–∏—Ö ‚Üí K-Means, Mini-Batch K-Means
- **–°—Ñ–µ—Ä–∏—á–Ω—ñ –∫–ª–∞—Å—Ç–µ—Ä–∏ –æ–¥–Ω–∞–∫–æ–≤–æ–≥–æ —Ä–æ–∑–º—ñ—Ä—É** ‚Üí K-Means (–ø—Ä–æ—Å—Ç—ñ—à–∏–π)
- **–Ü—î—Ä–∞—Ä—Ö—ñ—è** –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤ –ø–æ—Ç—Ä—ñ–±–Ω–∞ ‚Üí Hierarchical Clustering

---

## –ü—Ä–∞–∫—Ç–∏—á–Ω—ñ –ø–æ—Ä–∞–¥–∏ üí°

### 1. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π K-Distance Graph –¥–ª—è Œµ

```python
# –ó–ê–í–ñ–î–ò –±—É–¥—É–π K-distance graph –ø–µ—Ä–µ–¥ DBSCAN
from sklearn.neighbors import NearestNeighbors

neighbors = NearestNeighbors(n_neighbors=5)
neighbors.fit(X)
distances, _ = neighbors.kneighbors(X)
distances = np.sort(distances[:, -1])

plt.plot(distances)
plt.ylabel('5-th Nearest Neighbor Distance')
plt.show()

# –í—ñ–∑—É–∞–ª—å–Ω–æ –∑–Ω–∞–π–¥–∏ "–ª—ñ–∫–æ—Ç—å"
```

### 2. MinPts = 2 √ó dimensions (–º—ñ–Ω—ñ–º—É–º)

```python
# –ï–º–ø—ñ—Ä–∏—á–Ω–µ –ø—Ä–∞–≤–∏–ª–æ
min_samples = max(4, 2 * X.shape[1])

dbscan = DBSCAN(eps=0.5, min_samples=min_samples)
```

### 3. Scaling —è–∫—â–æ —Ä—ñ–∑–Ω—ñ –æ–¥–∏–Ω–∏—Ü—ñ

```python
# –Ø–∫—â–æ –≤—ñ–∫ (0-100) —Ç–∞ –¥–æ—Ö—ñ–¥ (0-150K) ‚Üí SCALING!
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

dbscan = DBSCAN(eps=0.5, min_samples=5)
labels = dbscan.fit_predict(X_scaled)
```

### 4. –ï–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—É–π –∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏

```python
# –°–ø—Ä–æ–±—É–π –∫—ñ–ª—å–∫–∞ –∫–æ–º–±—ñ–Ω–∞—Ü—ñ–π
for eps in [0.3, 0.5, 0.7]:
    for min_samples in [3, 5, 7]:
        dbscan = DBSCAN(eps=eps, min_samples=min_samples)
        labels = dbscan.fit_predict(X)
        
        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
        n_noise = list(labels).count(-1)
        
        print(f"eps={eps}, min_samples={min_samples}: "
              f"{n_clusters} clusters, {n_noise} noise")
```

### 5. –í—ñ–∑—É–∞–ª—ñ–∑—É–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏

```python
# –ó–ê–í–ñ–î–ò –≤—ñ–∑—É–∞–ª—ñ–∑—É–π (–Ω–∞–≤—ñ—Ç—å —è–∫—â–æ >2D, –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–π PCA)
from sklearn.decomposition import PCA

if X.shape[1] > 2:
    pca = PCA(n_components=2)
    X_plot = pca.fit_transform(X)
else:
    X_plot = X

plt.scatter(X_plot[:, 0], X_plot[:, 1], c=labels, cmap='viridis')
plt.scatter(X_plot[labels == -1, 0], X_plot[labels == -1, 1],
           c='black', marker='x', s=100, label='Noise')
plt.legend()
plt.show()
```

### 6. –ê–Ω–∞–ª—ñ–∑—É–π noise points –æ–∫—Ä–µ–º–æ

```python
# Noise –º–æ–∂–µ –º—ñ—Å—Ç–∏—Ç–∏ —Ü—ñ–Ω–Ω—É —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é!
noise_points = X[labels == -1]

print(f"Noise points: {len(noise_points)}")
if len(noise_points) > 0:
    print("–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ noise:")
    print(pd.DataFrame(noise_points).describe())
    
    # –ú–æ–∂–ª–∏–≤–æ, —Ü–µ –æ–∫—Ä–µ–º–∏–π –∫–ª–∞—Å—Ç–µ—Ä –∑ —ñ–Ω—à–æ—é —â—ñ–ª—å–Ω—ñ—Å—Ç—é?
```

### 7. Grid Search —è–∫—â–æ –Ω–µ–≤–ø–µ–≤–Ω–µ–Ω–∏–π

```python
# –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π Silhouette –¥–ª—è –æ—Ü—ñ–Ω–∫–∏
best_score = -1
best_params = {}

for eps in np.arange(0.1, 2.0, 0.1):
    for min_samples in [3, 5, 7, 10]:
        dbscan = DBSCAN(eps=eps, min_samples=min_samples)
        labels = dbscan.fit_predict(X)
        
        mask = labels != -1
        if np.sum(mask) > 0 and len(set(labels[mask])) > 1:
            score = silhouette_score(X[mask], labels[mask])
            if score > best_score:
                best_score = score
                best_params = {'eps': eps, 'min_samples': min_samples}

print(f"Best: {best_params}, Score: {best_score:.4f}")
```

### 8. Domain knowledge > –∞–≤—Ç–æ–º–∞—Ç–∏–∫–∞

```python
# –ì–µ–æ–≥—Ä–∞—Ñ—ñ—á–Ω—ñ –¥–∞–Ω—ñ: 500–º = 0.005 –≥—Ä–∞–¥—É—Å—ñ–≤ (–ø—Ä–∏–±–ª–∏–∑–Ω–æ)
# MinPts = 5 –¥–ª—è "—Ä–∞–π–æ–Ω—É" (–º—ñ–Ω—ñ–º—É–º 5 —Ç–æ—á–æ–∫)

dbscan = DBSCAN(eps=0.005, min_samples=5, metric='haversine')
```

### 9. –†–æ–∑–≥–ª—è–Ω—å HDBSCAN –¥–ª—è —Å–∫–ª–∞–¥–Ω–∏—Ö –≤–∏–ø–∞–¥–∫—ñ–≤

```python
# –Ø–∫—â–æ —Ä—ñ–∑–Ω–∞ —â—ñ–ª—å–Ω—ñ—Å—Ç—å –∞–±–æ –Ω–µ–≤–ø–µ–≤–Ω–µ–Ω–∏–π —É –ø–∞—Ä–∞–º–µ—Ç—Ä–∞—Ö
import hdbscan

clusterer = hdbscan.HDBSCAN(min_cluster_size=10)
labels = clusterer.fit_predict(X)
```

### 10. –ó–±–µ—Ä—ñ–≥–∞–π –ø–∞—Ä–∞–º–µ—Ç—Ä–∏

```python
import joblib

# –ó–±–µ—Ä–µ–≥—Ç–∏ –º–æ–¥–µ–ª—å —Ç–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏
params = {
    'eps': dbscan.eps,
    'min_samples': dbscan.min_samples,
    'metric': dbscan.metric
}

joblib.dump(params, 'dbscan_params.pkl')
joblib.dump(dbscan, 'dbscan_model.pkl')

# –î–ª—è –Ω–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö
dbscan_loaded = joblib.load('dbscan_model.pkl')
new_labels = dbscan_loaded.fit_predict(X_new)
```

---

## –†–µ–∞–ª—å–Ω—ñ –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è

### 1. Fraud Detection (–í–∏—è–≤–ª–µ–Ω–Ω—è —à–∞—Ö—Ä–∞–π—Å—Ç–≤–∞)

**–ó–∞–¥–∞—á–∞:** –ó–Ω–∞–π—Ç–∏ –∞–Ω–æ–º–∞–ª—å–Ω—ñ —Ç—Ä–∞–Ω–∑–∞–∫—Ü—ñ—ó.

**–ü—ñ–¥—Ö—ñ–¥:**
- –û–∑–Ω–∞–∫–∏: —Å—É–º–∞, —á–∞—Å, –ª–æ–∫–∞—Ü—ñ—è, —Ç–∏–ø
- DBSCAN –≥—Ä—É–ø—É—î –Ω–æ—Ä–º–∞–ª—å–Ω—ñ —Ç—Ä–∞–Ω–∑–∞–∫—Ü—ñ—ó
- Noise = –ø–æ—Ç–µ–Ω—Ü—ñ–π–Ω–µ —à–∞—Ö—Ä–∞–π—Å—Ç–≤–æ

**–ü–µ—Ä–µ–≤–∞–≥–∏:**
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –≤–∏—è–≤–ª—è—î –Ω–æ–≤—ñ —Ç–∏–ø–∏ fraud
- –ù–µ –ø–æ—Ç—Ä—ñ–±–Ω–æ –∑–Ω–∞—Ç–∏ —Ç–∏–ø–∏ —à–∞—Ö—Ä–∞–π—Å—Ç–≤–∞ –Ω–∞–ø–µ—Ä–µ–¥

### 2. –ì–µ–æ–≥—Ä–∞—Ñ—ñ—á–Ω–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—è

**–ó–∞–¥–∞—á–∞:** –ó–Ω–∞–π—Ç–∏ —Ä–∞–π–æ–Ω–∏ –º—ñ—Å—Ç–∞ –∑ –≤–∏—Å–æ–∫–æ—é –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü—ñ—î—é —Ä–µ—Å—Ç–æ—Ä–∞–Ω—ñ–≤.

**–ü—ñ–¥—Ö—ñ–¥:**
- –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç–∏ —Ä–µ—Å—Ç–æ—Ä–∞–Ω—ñ–≤
- DBSCAN –∑ Haversine metric
- –ö–ª–∞—Å—Ç–µ—Ä–∏ = —Ä–∞–π–æ–Ω–∏
- Noise = –ø–æ–æ–¥–∏–Ω–æ–∫—ñ —Ä–µ—Å—Ç–æ—Ä–∞–Ω–∏

### 3. Network Traffic Analysis

**–ó–∞–¥–∞—á–∞:** –í–∏—è–≤–∏—Ç–∏ DDoS –∞—Ç–∞–∫–∏.

**–ü—ñ–¥—Ö—ñ–¥:**
- –û–∑–Ω–∞–∫–∏: IP, —á–∞—Å, —Ä–æ–∑–º—ñ—Ä –ø–∞–∫–µ—Ç–∞
- DBSCAN –≥—Ä—É–ø—É—î –Ω–æ—Ä–º–∞–ª—å–Ω–∏–π —Ç—Ä–∞—Ñ—ñ–∫
- Noise = –∞–Ω–æ–º–∞–ª—å–Ω–∏–π —Ç—Ä–∞—Ñ—ñ–∫ (–ø–æ—Ç–µ–Ω—Ü—ñ–π–Ω–∞ –∞—Ç–∞–∫–∞)

### 4. Customer Segmentation (–∑ outliers)

**–ó–∞–¥–∞—á–∞:** –°–µ–≥–º–µ–Ω—Ç—É–≤–∞—Ç–∏ –∫–ª—ñ—î–Ω—Ç—ñ–≤, –æ–∫—Ä–µ–º–æ –≤–∏–¥—ñ–ª–∏—Ç–∏ VIP/–∞–Ω–æ–º–∞–ª—ñ—ó.

**–ü—ñ–¥—Ö—ñ–¥:**
- RFM features
- DBSCAN –∑–Ω–∞—Ö–æ–¥–∏—Ç—å –æ—Å–Ω–æ–≤–Ω—ñ —Å–µ–≥–º–µ–Ω—Ç–∏
- Noise –º–æ–∂–µ –±—É—Ç–∏ VIP –∞–±–æ –ø—Ä–æ–±–ª–µ–º–Ω—ñ –∫–ª—ñ—î–Ω—Ç–∏

### 5. Image Segmentation

**–ó–∞–¥–∞—á–∞:** –†–æ–∑–¥—ñ–ª–∏—Ç–∏ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –Ω–∞ —Ä–µ–≥—ñ–æ–Ω–∏.

**–ü—ñ–¥—Ö—ñ–¥:**
- –ö–æ–∂–µ–Ω –ø—ñ–∫—Å–µ–ª—å = —Ç–æ—á–∫–∞ –≤ –∫–æ–ª—å–æ—Ä–æ–≤–æ–º—É –ø—Ä–æ—Å—Ç–æ—Ä—ñ + –ø–æ–∑–∏—Ü—ñ—è
- DBSCAN –≥—Ä—É–ø—É—î —Å—Ö–æ–∂—ñ —Ä–µ–≥—ñ–æ–Ω–∏
- –ü—Ä–∞—Ü—é—î –∑ –æ–±'—î–∫—Ç–∞–º–∏ —Å–∫–ª–∞–¥–Ω–æ—ó —Ñ–æ—Ä–º–∏

---

## –ü–æ—à–∏—Ä–µ–Ω—ñ –ø–æ–º–∏–ª–∫–∏ ‚ùå

### 1. –ù–µ –ø—ñ–¥–±–∏—Ä–∞—Ç–∏ Œµ

```python
# ‚ùå –ü—Ä–æ—Å—Ç–æ –≤–≥–∞–¥–∞—Ç–∏
dbscan = DBSCAN(eps=0.5, min_samples=5)

# ‚úÖ –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π K-distance graph
from sklearn.neighbors import NearestNeighbors
neighbors = NearestNeighbors(n_neighbors=5)
neighbors.fit(X)
distances, _ = neighbors.kneighbors(X)
plt.plot(np.sort(distances[:, -1]))
plt.show()
# –ó–Ω–∞–π–¥–∏ –ª—ñ–∫–æ—Ç—å –≤—ñ–∑—É–∞–ª—å–Ω–æ
```

### 2. –Ü–≥–Ω–æ—Ä—É–≤–∞—Ç–∏ scaling

```python
# ‚ùå –Ø–∫—â–æ –≤—ñ–∫ (0-100) —Ç–∞ –¥–æ—Ö—ñ–¥ (0-150K)
dbscan = DBSCAN(eps=0.5, min_samples=5)
dbscan.fit(X)  # –î–æ—Ö—ñ–¥ –¥–æ–º—ñ–Ω—É—î!

# ‚úÖ Scaling
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
dbscan.fit(X_scaled)
```

### 3. MinPts = 2 (–∑–∞–Ω–∞–¥—Ç–æ –º–∞–ª–æ)

```python
# ‚ùå –ë–∞–≥–∞—Ç–æ –º–∞–ª–µ–Ω—å–∫–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤
dbscan = DBSCAN(eps=0.5, min_samples=2)

# ‚úÖ MinPts ‚â• dimensions + 1
min_samples = max(4, X.shape[1] + 1)
dbscan = DBSCAN(eps=0.5, min_samples=min_samples)
```

### 4. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –Ω–∞ –≤–∏—Å–æ–∫–æ—Ä–æ–∑–º—ñ—Ä–Ω–∏—Ö –¥–∞–Ω–∏—Ö

```python
# ‚ùå 50 –æ–∑–Ω–∞–∫ ‚Üí –≤—Å—ñ –≤—ñ–¥—Å—Ç–∞–Ω—ñ –æ–¥–Ω–∞–∫–æ–≤—ñ (curse of dimensionality)
dbscan = DBSCAN(eps=0.5, min_samples=5)
dbscan.fit(X_50d)

# ‚úÖ –°–ø–æ—á–∞—Ç–∫—É PCA
pca = PCA(n_components=10)
X_reduced = pca.fit_transform(X_50d)
dbscan.fit(X_reduced)
```

### 5. –ù–µ –∞–Ω–∞–ª—ñ–∑—É–≤–∞—Ç–∏ noise

```python
# ‚ùå –Ü–≥–Ω–æ—Ä—É–≤–∞—Ç–∏ —Ç–æ—á–∫–∏ –∑ –º—ñ—Ç–∫–æ—é -1
labels = dbscan.fit_predict(X)

# ‚úÖ –ê–Ω–∞–ª—ñ–∑—É–≤–∞—Ç–∏ noise –æ–∫—Ä–µ–º–æ
noise_points = X[labels == -1]
print(f"Noise: {len(noise_points)} points")
# –ú–æ–∂–ª–∏–≤–æ, —Ü–µ –≤–∞–∂–ª–∏–≤—ñ –∞–Ω–æ–º–∞–ª—ñ—ó –∞–±–æ –æ–∫—Ä–µ–º–∏–π –∫–ª–∞—Å—Ç–µ—Ä!
```

### 6. –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞ –º–µ—Ç—Ä–∏–∫–∞ –≤—ñ–¥—Å—Ç–∞–Ω—ñ

```python
# ‚ùå Euclidean –¥–ª—è –≥–µ–æ–≥—Ä–∞—Ñ—ñ—á–Ω–∏—Ö –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç
dbscan = DBSCAN(eps=0.5, min_samples=5, metric='euclidean')
dbscan.fit(lat_lon_data)

# ‚úÖ Haversine –¥–ª—è lat/lon
dbscan = DBSCAN(eps=0.01, min_samples=5, metric='haversine')
X_radians = np.radians(lat_lon_data)
dbscan.fit(X_radians)
```

---

## –ü–æ–≤'—è–∑–∞–Ω—ñ —Ç–µ–º–∏

- [[01_KMeans]] ‚Äî –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ –¥–ª—è —Å—Ñ–µ—Ä–∏—á–Ω–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤
- [[02_Hierarchical_Clustering]] ‚Äî —ñ—î—Ä–∞—Ä—Ö—ñ—á–Ω–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—è
- [[04_Gaussian_Mixture_Models]] ‚Äî probabilistic clustering
- [[05_Clustering_Evaluation]] ‚Äî –º–µ—Ç—Ä–∏–∫–∏ –æ—Ü—ñ–Ω–∫–∏
- [[OPTICS]] ‚Äî –ø–æ–∫—Ä–∞—â–µ–Ω–∞ –≤–µ—Ä—Å—ñ—è DBSCAN
- [[HDBSCAN]] ‚Äî hierarchical DBSCAN

## –†–µ—Å—É—Ä—Å–∏

- [Scikit-learn: DBSCAN](https://scikit-learn.org/stable/modules/clustering.html#dbscan)
- [Original Paper: Ester et al. (1996)](https://www.aaai.org/Papers/KDD/1996/KDD96-037.pdf)
- [DBSCAN Revisited: Why and How You Should (Still) Use DBSCAN](https://dl.acm.org/doi/10.1145/3068335)
- [StatQuest: DBSCAN](https://www.youtube.com/watch?v=RDZUdRSDOok)

---

## –ö–ª—é—á–æ–≤—ñ –≤–∏—Å–Ω–æ–≤–∫–∏

> DBSCAN ‚Äî —Ü–µ density-based –∞–ª–≥–æ—Ä–∏—Ç–º –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—ó, —è–∫–∏–π –∑–Ω–∞—Ö–æ–¥–∏—Ç—å –∫–ª–∞—Å—Ç–µ—Ä–∏ –¥–æ–≤—ñ–ª—å–Ω–æ—ó —Ñ–æ—Ä–º–∏ —Ç–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –≤–∏—è–≤–ª—è—î outliers.

**–û—Å–Ω–æ–≤–Ω—ñ –ø—Ä–∏–Ω—Ü–∏–ø–∏:**
- **Density-based:** –∫–ª–∞—Å—Ç–µ—Ä = —â—ñ–ª—å–Ω–∞ –æ–±–ª–∞—Å—Ç—å —Ç–æ—á–æ–∫
- **–î–≤–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏:** Œµ (—Ä–∞–¥—ñ—É—Å) —Ç–∞ MinPts (–º—ñ–Ω. —Ç–æ—á–æ–∫)
- **–¢—Ä–∏ —Ç–∏–ø–∏ —Ç–æ—á–æ–∫:** Core, Border, Noise
- **–ù–µ –ø–æ—Ç—Ä—ñ–±–Ω–æ –∑–Ω–∞—Ç–∏ K** ‚Äî –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤ –≤–∏–∑–Ω–∞—á–∞—î—Ç—å—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ

**–ö–ª—é—á–æ–≤—ñ –ø–æ–Ω—è—Ç—Ç—è:**
- **Core point:** ‚â• MinPts —Å—É—Å—ñ–¥—ñ–≤ –≤ —Ä–∞–¥—ñ—É—Å—ñ Œµ
- **Density-reachable:** —ñ—Å–Ω—É—î –ª–∞–Ω—Ü—é–∂–æ–∫ core points
- **Noise (outliers):** —Ç–æ—á–∫–∏, —è–∫—ñ –Ω–µ –Ω–∞–ª–µ–∂–∞—Ç—å –∂–æ–¥–Ω–æ–º—É –∫–ª–∞—Å—Ç–µ—Ä—É

**–ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏:**
- –°–∫–ª–∞–¥–Ω–∞ —Ñ–æ—Ä–º–∞ + –Ω–µ –∑–Ω–∞—î–º–æ K + outliers = DBSCAN ‚úì
- –†—ñ–∑–Ω–∞ —â—ñ–ª—å–Ω—ñ—Å—Ç—å ‚Üí OPTICS –∞–±–æ HDBSCAN ‚úì
- –°—Ñ–µ—Ä–∏—á–Ω—ñ –∫–ª–∞—Å—Ç–µ—Ä–∏ + –∑–Ω–∞—î–º–æ K ‚Üí K-Means ‚úì

**–ù–∞–π–≤–∞–∂–ª–∏–≤—ñ—à–µ:**
- –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π K-distance graph –¥–ª—è –≤–∏–±–æ—Ä—É Œµ
- MinPts ‚â• dimensions + 1
- Scaling —è–∫—â–æ —Ä—ñ–∑–Ω—ñ –æ–¥–∏–Ω–∏—Ü—ñ –≤–∏–º—ñ—Ä—é–≤–∞–Ω–Ω—è
- PCA –¥–ª—è –≤–∏—Å–æ–∫–æ—Ä–æ–∑–º—ñ—Ä–Ω–∏—Ö –¥–∞–Ω–∏—Ö
- –ê–Ω–∞–ª—ñ–∑—É–π noise points –æ–∫—Ä–µ–º–æ!

---

#ml #unsupervised-learning #clustering #dbscan #density-based #outlier-detection #anomaly-detection
