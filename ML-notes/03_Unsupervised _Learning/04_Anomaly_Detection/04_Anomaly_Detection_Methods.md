# Anomaly Detection Methods: –ü–æ–≤–Ω–∏–π –û–≥–ª—è–¥

## –©–æ —Ü–µ?

**Anomaly Detection (Outlier Detection)** ‚Äî —Ü–µ –∑–∞–¥–∞—á–∞ –∑–Ω–∞—Ö–æ–¥–∂–µ–Ω–Ω—è –Ω–µ–∑–≤–∏—á–∞–π–Ω–∏—Ö, —Ä—ñ–¥–∫—ñ—Å–Ω–∏—Ö –∑—Ä–∞–∑–∫—ñ–≤, —â–æ –∑–Ω–∞—á–Ω–æ –≤—ñ–¥—Ä—ñ–∑–Ω—è—é—Ç—å—Å—è –≤—ñ–¥ –±—ñ–ª—å—à–æ—Å—Ç—ñ –¥–∞–Ω–∏—Ö. –¶–µ **unsupervised learning** –∑–∞–¥–∞—á–∞ (–∑–∞–∑–≤–∏—á–∞–π –Ω–µ–º–∞—î labeled anomalies).

## –¢–∏–ø–∏ –∞–Ω–æ–º–∞–ª—ñ–π

### 1. Point Anomalies (Global)

**–©–æ:** –û–∫—Ä–µ–º–∞ —Ç–æ—á–∫–∞ –¥–∞–ª–µ–∫–æ –≤—ñ–¥ –≤—Å—ñ—Ö —ñ–Ω—à–∏—Ö.

```
        ‚óè‚óè‚óè‚óè‚óè
        ‚óè‚óè‚óè‚óè‚óè
        ‚óè‚óè‚óè‚óè‚óè
                    ‚óã ‚Üê Point anomaly

–ì–ª–æ–±–∞–ª—å–Ω–æ –≤—ñ–¥—Ä—ñ–∑–Ω—è—î—Ç—å—Å—è –≤—ñ–¥ –≤—Å—ñ—Ö
```

**–ü—Ä–∏–∫–ª–∞–¥–∏:**
- –¢—Ä–∞–Ω–∑–∞–∫—Ü—ñ—è –Ω–∞ $10,000 –∫–æ–ª–∏ –∑–∞–∑–≤–∏—á–∞–π $50
- –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ —Å–µ–Ω—Å–æ—Ä–∞ 500¬∞C –∫–æ–ª–∏ –Ω–æ—Ä–º–∞ 20-30¬∞C

### 2. Contextual Anomalies (Conditional)

**–©–æ:** –ù–æ—Ä–º–∞–ª—å–Ω–∞ –≤ –æ–¥–Ω–æ–º—É –∫–æ–Ω—Ç–µ–∫—Å—Ç—ñ, –∞–Ω–æ–º–∞–ª—å–Ω–∞ –≤ —ñ–Ω—à–æ–º—É.

```
Temperature over time:

Summer:  30¬∞C ‚Üê Normal
Winter:  30¬∞C ‚Üê ANOMALY! (–∑–∏–º–æ—é —Ö–æ–ª–æ–¥–Ω–æ)

Context matters!
```

**–ü—Ä–∏–∫–ª–∞–¥–∏:**
- –ü–æ–∫—É–ø–∫–∞ –∑–∏–º–æ–≤–æ—ó –∫—É—Ä—Ç–∫–∏ –≤–ª—ñ—Ç–∫—É
- High traffic –æ 3 AM (–∑–∞–∑–≤–∏—á–∞–π –Ω–∏–∑—å–∫–∏–π)

### 3. Collective Anomalies

**–©–æ:** –ì—Ä—É–ø–∞ —Ç–æ—á–æ–∫ —Ä–∞–∑–æ–º –∞–Ω–æ–º–∞–ª—å–Ω–∞.

```
Heartbeat:
Normal:   ‚óè-‚óè-‚óè-‚óè-‚óè-‚óè-‚óè
Anomaly:  ‚óè-‚óè-‚óè‚óè‚óè‚óè‚óè-‚óè-‚óè  ‚Üê Rapid sequence

–ö–æ–∂–Ω–∞ —Ç–æ—á–∫–∞ OK, –∞–ª–µ sequence abnormal
```

**–ü—Ä–∏–∫–ª–∞–¥–∏:**
- DDoS attack (–±–∞–≥–∞—Ç–æ requests —Ä–∞–∑–æ–º)
- Credit card: –±–∞–≥–∞—Ç–æ —Ç—Ä–∞–Ω–∑–∞–∫—Ü—ñ–π –∑–∞ 5 —Ö–≤–∏–ª–∏–Ω

---

## –û—Å–Ω–æ–≤–Ω—ñ –º–µ—Ç–æ–¥–∏

### –®–≤–∏–¥–∫–µ –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è

| –ú–µ—Ç–æ–¥ | –¢–∏–ø | –®–≤–∏–¥–∫—ñ—Å—Ç—å | –ú–∞—Å—à—Ç–∞–±–æ–≤–∞–Ω—ñ—Å—Ç—å | –õ–æ–∫–∞–ª—å–Ω—ñ outliers | Interpretability |
|-------|-----|-----------|-----------------|-------------------|------------------|
| **Isolation Forest** | Tree-based | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **One-Class SVM** | Kernel | ‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê |
| **LOF** | Density | ‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |
| **Autoencoder** | Neural Net | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê |
| **DBSCAN** | Clustering | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **Statistical** | Statistics | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |

---

## 1. Isolation Forest

### –ü—Ä–∏–Ω—Ü–∏–ø

**–Ü–∑–æ–ª—é–≤–∞—Ç–∏ –∞–Ω–æ–º–∞–ª—ñ—ó —á–µ—Ä–µ–∑ random splits.**

```
–ê–Ω–æ–º–∞–ª—ñ—è:         Normal point:
‚óã                 ‚óè‚óè‚óè‚óè‚óè
|                 ‚óè‚óè‚óè‚óè‚óè
1 split!          ‚óè‚óè‚óè‚óè‚óè
                  |||||||
                  Many splits
```

### –ü–µ—Ä–µ–≤–∞–≥–∏ ‚úì
- ‚ö° –î—É–∂–µ —à–≤–∏–¥–∫–∏–π (O(n log n))
- üìà –ú–∞—Å—à—Ç–∞–±—É—î—Ç—å—Å—è –Ω–∞ –º—ñ–ª—å–π–æ–Ω–∏
- üî¢ –î–æ–±—Ä–µ –Ω–∞ high-dimensional
- üíæ Memory efficient

### –ù–µ–¥–æ–ª—ñ–∫–∏ ‚úó
- ‚ùå –ü–æ—Ç—Ä–µ–±—É—î contamination parameter
- ‚ùå –ú–æ–∂–µ –ø—Ä–æ–ø—É—Å–∫–∞—Ç–∏ –ª–æ–∫–∞–ª—å–Ω—ñ outliers
- ‚ùå –ü–æ–≥–∞–Ω–æ –Ω–∞ categorical features

### –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏
- ‚úÖ –í–µ–ª–∏–∫—ñ –¥–∞–Ω—ñ (> 10K points)
- ‚úÖ High-dimensional (> 50 features)
- ‚úÖ –®–≤–∏–¥–∫—ñ—Å—Ç—å –∫—Ä–∏—Ç–∏—á–Ω–∞
- ‚úÖ Global anomalies

### –ö–æ–¥
```python
from sklearn.ensemble import IsolationForest

clf = IsolationForest(
    n_estimators=100,
    contamination=0.1,
    random_state=42
)

clf.fit(X_train)
y_pred = clf.predict(X_test)  # 1=normal, -1=anomaly
scores = clf.score_samples(X_test)  # Lower = more anomalous
```

---

## 2. One-Class SVM

### –ü—Ä–∏–Ω—Ü–∏–ø

**–ü–æ–±—É–¥—É–≤–∞—Ç–∏ boundary –Ω–∞–≤–∫–æ–ª–æ normal data.**

```
Feature space:
    ‚óè‚óè‚óè‚óè‚óè‚óè‚óè
    ‚óè     ‚óè
    ‚óè  ‚óã  ‚óè  ‚Üê Decision boundary
    ‚óè     ‚óè
    ‚óè‚óè‚óè‚óè‚óè‚óè‚óè

Outside boundary = anomaly
```

### –ü–µ—Ä–µ–≤–∞–≥–∏ ‚úì
- üéØ Kernel trick (–Ω–µ–ª—ñ–Ω—ñ–π–Ω—ñ boundaries)
- üìê Solid theory (SVM math)
- üé® Smooth boundaries
- ‚úÖ Novelty detection

### –ù–µ–¥–æ–ª—ñ–∫–∏ ‚úó
- ‚ùå –ü–æ–≤—ñ–ª—å–Ω–∏–π (O(n¬≤-n¬≥))
- ‚ùå –ù–µ –º–∞—Å—à—Ç–∞–±—É—î—Ç—å—Å—è (< 10K)
- ‚ùå –ü–æ—Ç—Ä–µ–±—É—î parameter tuning (nu, gamma)
- ‚ùå –û–ë–û–í'–Ø–ó–ö–û–í–ò–ô scaling

### –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏
- ‚úÖ –ú–∞–ª—ñ-—Å–µ—Ä–µ–¥–Ω—ñ –¥–∞–Ω—ñ (< 10K)
- ‚úÖ –ù–µ–ª—ñ–Ω—ñ–π–Ω—ñ patterns
- ‚úÖ Smooth decision boundary –≤–∞–∂–ª–∏–≤–∞
- ‚úÖ Novelty detection

### –ö–æ–¥
```python
from sklearn.svm import OneClassSVM
from sklearn.preprocessing import StandardScaler

# –ó–ê–í–ñ–î–ò scale!
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_train)

clf = OneClassSVM(
    kernel='rbf',
    gamma='scale',
    nu=0.1
)

clf.fit(X_scaled)
y_pred = clf.predict(scaler.transform(X_test))
```

---

## 3. Local Outlier Factor (LOF)

### –ü—Ä–∏–Ω—Ü–∏–ø

**–ü–æ—Ä—ñ–≤–Ω—è—Ç–∏ –ª–æ–∫–∞–ª—å–Ω—É —â—ñ–ª—å–Ω—ñ—Å—Ç—å –∑ —Å—É—Å—ñ–¥–∞–º–∏.**

```
Dense cluster:        Sparse area:
  ‚óè‚óè‚óè‚óè‚óè                 ‚óã    ‚óã
  ‚óè‚óè‚óè‚óè‚óè               ‚óã  ‚òÖ    ‚óã
  ‚óè‚óè‚óè‚óè‚óè                 ‚óã    ‚óã

Point in dense:       Point ‚òÖ:
LOF ‚âà 1 (normal)      LOF >> 1 (outlier!)
```

### –ü–µ—Ä–µ–≤–∞–≥–∏ ‚úì
- üéØ –ó–Ω–∞—Ö–æ–¥–∏—Ç—å –ª–æ–∫–∞–ª—å–Ω—ñ outliers
- üìä Variable density (—Ä—ñ–∑–Ω–∞ —â—ñ–ª—å–Ω—ñ—Å—Ç—å)
- üíØ Interpretable score (LOF value)
- üîç Cluster outliers

### –ù–µ–¥–æ–ª—ñ–∫–∏ ‚úó
- ‚ùå –ü–æ–≤—ñ–ª—å–Ω–∏–π (O(n¬≤))
- ‚ùå –ù–µ –º–∞—Å—à—Ç–∞–±—É—î—Ç—å—Å—è (< 50K)
- ‚ùå –ß—É—Ç–ª–∏–≤–∏–π –¥–æ n_neighbors
- ‚ùå High-dimensional –ø—Ä–æ–±–ª–µ–º–∏

### –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏
- ‚úÖ –õ–æ–∫–∞–ª—å–Ω—ñ outliers –≤–∞–∂–ª–∏–≤—ñ
- ‚úÖ Variable density –≤ –¥–∞–Ω–∏—Ö
- ‚úÖ –°–µ—Ä–µ–¥–Ω—ñ –¥–∞–Ω—ñ (< 50K)
- ‚úÖ Outliers –º—ñ–∂ –∫–ª–∞—Å—Ç–µ—Ä–∞–º–∏

### –ö–æ–¥
```python
from sklearn.neighbors import LocalOutlierFactor

# For fit_predict
clf = LocalOutlierFactor(
    n_neighbors=20,
    contamination=0.1,
    novelty=False
)

y_pred = clf.fit_predict(X_train)
lof_scores = -clf.negative_outlier_factor_

# For novelty detection
clf_novelty = LocalOutlierFactor(novelty=True)
clf_novelty.fit(X_train)
y_pred = clf_novelty.predict(X_test)
```

---

## 4. Autoencoders

### –ü—Ä–∏–Ω—Ü–∏–ø

**–ù–∞–≤—á–∏—Ç–∏—Å—å —Å—Ç–∏—Å–∫–∞—Ç–∏ —Ç–∞ –≤—ñ–¥–Ω–æ–≤–ª—é–≤–∞—Ç–∏. –ê–Ω–æ–º–∞–ª—ñ—ó = –≤–∏—Å–æ–∫–∏–π reconstruction error.**

```
Input ‚Üí Encoder ‚Üí Bottleneck ‚Üí Decoder ‚Üí Output
 ‚óè‚óè‚óè     ‚Üì          ‚óè‚óè           ‚Üì        ‚óè‚óè‚óè

Normal:  Input ‚âà Output (low error)
Anomaly: Input ‚â† Output (high error)
```

### –ü–µ—Ä–µ–≤–∞–≥–∏ ‚úì
- üß† –°–∫–ª–∞–¥–Ω—ñ –Ω–µ–ª—ñ–Ω—ñ–π–Ω—ñ patterns
- üìà –ú–∞—Å—à—Ç–∞–±—É—î—Ç—å—Å—è –¥–æ–±—Ä–µ
- üé® Unsupervised feature learning
- üîß –ì–Ω—É—á–∫—ñ—Å—Ç—å (—Ä—ñ–∑–Ω—ñ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏)

### –ù–µ–¥–æ–ª—ñ–∫–∏ ‚úó
- ‚ùå –ü–æ—Ç—Ä–µ–±—É—î –±–∞–≥–∞—Ç–æ –¥–∞–Ω–∏—Ö –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è
- ‚ùå –°–∫–ª–∞–¥–Ω–æ –Ω–∞–ª–∞—à—Ç—É–≤–∞—Ç–∏
- ‚ùå –ü–æ–≤—ñ–ª—å–Ω–µ –Ω–∞–≤—á–∞–Ω–Ω—è
- ‚ùå –ù–µ —ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–Ω—ñ

### –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏
- ‚úÖ –í–µ–ª–∏–∫—ñ –¥–∞–Ω—ñ (> 10K)
- ‚úÖ –°–∫–ª–∞–¥–Ω—ñ –Ω–µ–ª—ñ–Ω—ñ–π–Ω—ñ patterns
- ‚úÖ Image/sequence data
- ‚úÖ Deep learning infrastructure —î

### –ö–æ–¥
```python
import torch
import torch.nn as nn

class Autoencoder(nn.Module):
    def __init__(self, input_dim=64, latent_dim=16):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 32),
            nn.ReLU(),
            nn.Linear(32, latent_dim)
        )
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, 32),
            nn.ReLU(),
            nn.Linear(32, input_dim)
        )
    
    def forward(self, x):
        z = self.encoder(x)
        x_recon = self.decoder(z)
        return x_recon

# Train
model = Autoencoder()
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# ... training loop ...

# Detect
with torch.no_grad():
    recon = model(X_test)
    errors = torch.mean((X_test - recon)**2, dim=1)
    
threshold = np.percentile(errors, 95)
anomalies = errors > threshold
```

---

## 5. Statistical Methods

### Z-Score

**–í—ñ–¥—Å—Ç–∞–Ω—å –≤—ñ–¥ mean –≤ standard deviations.**

$$z = \frac{x - \mu}{\sigma}$$

```python
from scipy.stats import zscore

z_scores = np.abs(zscore(X, axis=0))
outliers = (z_scores > 3).any(axis=1)  # 3-sigma rule
```

**–ü–µ—Ä–µ–≤–∞–≥–∏:** –ü—Ä–æ—Å—Ç–∏–π, —à–≤–∏–¥–∫–∏–π, —ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω–∏–π
**–ù–µ–¥–æ–ª—ñ–∫–∏:** –¢—ñ–ª—å–∫–∏ Gaussian, univariate

### IQR (Interquartile Range)

**–í–∏–∫–∏–¥–∏ –∑–∞ –º–µ–∂–∞–º–∏ Q1-1.5√óIQR —Ç–∞ Q3+1.5√óIQR.**

```python
Q1 = np.percentile(X, 25)
Q3 = np.percentile(X, 75)
IQR = Q3 - Q1

lower = Q1 - 1.5 * IQR
upper = Q3 + 1.5 * IQR

outliers = (X < lower) | (X > upper)
```

**–ü–µ—Ä–µ–≤–∞–≥–∏:** Robust –¥–æ outliers, –Ω–µ –ø—Ä–∏–ø—É—Å–∫–∞—î —Ä–æ–∑–ø–æ–¥—ñ–ª—É
**–ù–µ–¥–æ–ª—ñ–∫–∏:** Univariate, –º–æ–∂–µ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç–∏ multivariate outliers

### Mahalanobis Distance

**Multivariate –≤—ñ–¥—Å—Ç–∞–Ω—å –∑ —É—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º covariance.**

$$D = \sqrt{(x - \mu)^T \Sigma^{-1} (x - \mu)}$$

```python
from scipy.spatial.distance import mahalanobis

mean = np.mean(X, axis=0)
cov = np.cov(X.T)
inv_cov = np.linalg.inv(cov)

distances = [mahalanobis(x, mean, inv_cov) for x in X]
threshold = np.percentile(distances, 95)
outliers = np.array(distances) > threshold
```

**–ü–µ—Ä–µ–≤–∞–≥–∏:** Multivariate, –≤—Ä–∞—Ö–æ–≤—É—î –∫–æ—Ä—Ä–µ–ª—è—Ü—ñ—ó
**–ù–µ–¥–æ–ª—ñ–∫–∏:** –ü—Ä–∏–ø—É—Å–∫–∞—î Gaussian, –ø–æ—Ç—Ä–µ–±—É—î invertible covariance

---

## 6. DBSCAN (—è–∫ outlier detector)

### –ü—Ä–∏–Ω—Ü–∏–ø

**Points –Ω–µ –≤ –∂–æ–¥–Ω–æ–º—É –∫–ª–∞—Å—Ç–µ—Ä—ñ = outliers.**

```python
from sklearn.cluster import DBSCAN

dbscan = DBSCAN(eps=0.5, min_samples=5)
labels = dbscan.fit_predict(X)

outliers = labels == -1  # -1 = noise/outlier
```

**–ü–µ—Ä–µ–≤–∞–≥–∏:** –ó–Ω–∞—Ö–æ–¥–∏—Ç—å outliers –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ, –ø—Ä–∞—Ü—é—î –∑ variable density
**–ù–µ–¥–æ–ª—ñ–∫–∏:** –ß—É—Ç–ª–∏–≤–∏–π –¥–æ eps —Ç–∞ min_samples

---

## Decision Tree: –Ø–∫–∏–π –º–µ—Ç–æ–¥ –≤–∏–±—Ä–∞—Ç–∏?

```
–°–∫—ñ–ª—å–∫–∏ –¥–∞–Ω–∏—Ö?
‚îú‚îÄ < 1,000
‚îÇ  ‚îî‚îÄ Statistical methods (Z-score, IQR)
‚îÇ
‚îú‚îÄ 1,000 - 10,000
‚îÇ  ‚îÇ
‚îÇ  –ß–∏ –≤–∞–∂–ª–∏–≤—ñ –ª–æ–∫–∞–ª—å–Ω—ñ outliers?
‚îÇ  ‚îú‚îÄ –¢–∞–∫ ‚Üí LOF
‚îÇ  ‚îî‚îÄ –ù—ñ
‚îÇ     ‚îÇ
‚îÇ     –ß–∏ –Ω–µ–ª—ñ–Ω—ñ–π–Ω—ñ patterns?
‚îÇ     ‚îú‚îÄ –¢–∞–∫ ‚Üí One-Class SVM
‚îÇ     ‚îî‚îÄ –ù—ñ ‚Üí Isolation Forest
‚îÇ
‚îî‚îÄ > 10,000
   ‚îÇ
   –ß–∏ —î GPU —Ç–∞ –±–∞–≥–∞—Ç–æ –¥–∞–Ω–∏—Ö (>100K)?
   ‚îú‚îÄ –¢–∞–∫ ‚Üí Autoencoder
   ‚îî‚îÄ –ù—ñ ‚Üí Isolation Forest
```

### –ó–∞ —Ç–∏–ø–æ–º –∑–∞–¥–∞—á—ñ

**Global outliers:**
1. Isolation Forest (best)
2. One-Class SVM
3. Statistical methods

**Local outliers:**
1. LOF (best)
2. DBSCAN
3. Autoencoder

**High-dimensional (>50 features):**
1. Isolation Forest (best)
2. Autoencoder
3. PCA + any method

**Streaming/Real-time:**
1. Statistical methods (fastest)
2. Isolation Forest
3. Incremental refit others

**Interpretability:**
1. Statistical methods (best)
2. Isolation Forest
3. LOF

---

## –ü–æ—Ä—ñ–≤–Ω—è–ª—å–Ω–∏–π benchmark

### Synthetic data experiment

```python
import numpy as np
import time
from sklearn.metrics import roc_auc_score

# Generate data
np.random.seed(42)
n_normal = 1000
n_anomalies = 50

X_normal = np.random.randn(n_normal, 10)
X_anomalies = np.random.uniform(-4, 4, (n_anomalies, 10))

X = np.vstack([X_normal, X_anomalies])
y_true = np.array([0]*n_normal + [1]*n_anomalies)

# Methods to compare
from sklearn.ensemble import IsolationForest
from sklearn.svm import OneClassSVM
from sklearn.neighbors import LocalOutlierFactor

methods = {
    'Isolation Forest': IsolationForest(contamination=0.05, random_state=42),
    'One-Class SVM': OneClassSVM(nu=0.05),
    'LOF': LocalOutlierFactor(n_neighbors=20, contamination=0.05, novelty=False)
}

results = []

for name, clf in methods.items():
    print(f"\nTesting {name}...")
    
    # Time
    start = time.time()
    
    if name == 'LOF':
        y_pred = clf.fit_predict(X)
        scores = -clf.negative_outlier_factor_
    else:
        clf.fit(X)
        y_pred = clf.predict(X)
        scores = -clf.score_samples(X)
    
    elapsed = time.time() - start
    
    # Metrics
    y_pred_binary = (y_pred == -1).astype(int)
    
    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
    
    acc = accuracy_score(y_true, y_pred_binary)
    prec = precision_score(y_true, y_pred_binary)
    rec = recall_score(y_true, y_pred_binary)
    f1 = f1_score(y_true, y_pred_binary)
    auc = roc_auc_score(y_true, scores)
    
    results.append({
        'Method': name,
        'Accuracy': acc,
        'Precision': prec,
        'Recall': rec,
        'F1-Score': f1,
        'ROC-AUC': auc,
        'Time (s)': elapsed
    })

# Display results
import pandas as pd
df_results = pd.DataFrame(results)
print("\n=== Benchmark Results ===")
print(df_results.to_string(index=False))

# Visualize
import matplotlib.pyplot as plt

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Metrics
df_results.set_index('Method')[['Accuracy', 'Precision', 'Recall', 'F1-Score']].plot(
    kind='bar', ax=axes[0], rot=45
)
axes[0].set_title('Performance Metrics', fontsize=13, fontweight='bold')
axes[0].set_ylabel('Score')
axes[0].legend(loc='lower right')
axes[0].grid(True, alpha=0.3)

# Time
df_results.plot(x='Method', y='Time (s)', kind='bar', ax=axes[1], 
               legend=False, rot=45, color='steelblue')
axes[1].set_title('Execution Time', fontsize=13, fontweight='bold')
axes[1].set_ylabel('Time (seconds)')
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

---

## Ensemble Methods

### Voting

**–ö–æ–º–±—ñ–Ω—É–≤–∞—Ç–∏ predictions —Ä—ñ–∑–Ω–∏—Ö –º–µ—Ç–æ–¥—ñ–≤.**

```python
# Multiple methods
if_pred = IsolationForest().fit_predict(X)
lof_pred = LocalOutlierFactor().fit_predict(X)
svm_pred = OneClassSVM().fit(X).predict(X)

# Voting: —è–∫—â–æ >= 2 –º–µ—Ç–æ–¥–∏ –∫–∞–∂—É—Ç—å outlier
votes = (if_pred == -1).astype(int) + \
        (lof_pred == -1).astype(int) + \
        (svm_pred == -1).astype(int)

ensemble_pred = votes >= 2  # Majority vote
```

### Score averaging

**–ö–æ–º–±—ñ–Ω—É–≤–∞—Ç–∏ anomaly scores.**

```python
from sklearn.preprocessing import MinMaxScaler

# Get scores
if_scores = IsolationForest().fit(X).score_samples(X)
lof_scores = LocalOutlierFactor(novelty=True).fit(X).score_samples(X)

# Normalize
scaler = MinMaxScaler()
if_norm = scaler.fit_transform(if_scores.reshape(-1, 1)).ravel()
lof_norm = scaler.fit_transform(lof_scores.reshape(-1, 1)).ravel()

# Average
ensemble_scores = (if_norm + lof_norm) / 2

# Threshold
threshold = np.percentile(ensemble_scores, 5)
outliers = ensemble_scores < threshold
```

---

## –ü—Ä–∞–∫—Ç–∏—á–Ω—ñ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó üí°

### 1. –ü–æ—á–Ω–∏ –∑ exploratory analysis

```python
# –ü–æ–¥–∏–≤–∏—Å—å –Ω–∞ –¥–∞–Ω—ñ!
import seaborn as sns

# Distributions
for col in X.columns:
    plt.figure()
    sns.histplot(X[col])
    plt.title(f'Distribution: {col}')
    plt.show()

# Correlations
sns.heatmap(X.corr(), annot=True)
plt.show()

# Outliers –≤—ñ–∑—É–∞–ª—å–Ω–æ
for col in X.columns:
    plt.boxplot(X[col])
    plt.title(f'Boxplot: {col}')
    plt.show()
```

### 2. –°–ø—Ä–æ–±—É–π –∫—ñ–ª—å–∫–∞ –º–µ—Ç–æ–¥—ñ–≤

```python
# –ù–µ –æ–±–º–µ–∂—É–π—Å—è –æ–¥–Ω–∏–º!
methods = [
    ('IF', IsolationForest()),
    ('LOF', LocalOutlierFactor(novelty=False)),
    ('OCSVM', OneClassSVM())
]

for name, clf in methods:
    if name == 'LOF':
        y_pred = clf.fit_predict(X)
    else:
        clf.fit(X)
        y_pred = clf.predict(X)
    
    print(f"{name}: {(y_pred == -1).sum()} outliers detected")
```

### 3. Validate —è–∫—â–æ —î labels

```python
# –Ø–∫—â–æ —î –Ω–∞–≤—ñ—Ç—å —Ç—Ä–æ—Ö–∏ labeled data
from sklearn.model_selection import train_test_split

X_train, X_val, y_train, y_val = train_test_split(
    X, y_true, test_size=0.2, random_state=42
)

# Test methods
for name, clf in methods:
    clf.fit(X_train)
    y_pred = clf.predict(X_val)
    
    from sklearn.metrics import f1_score
    f1 = f1_score(y_val, (y_pred == -1).astype(int))
    print(f"{name}: F1={f1:.3f}")
```

### 4. Feature engineering

```python
# –î–æ–¥–∞–π derived features
X['feature_ratio'] = X['feature1'] / (X['feature2'] + 1e-6)
X['feature_diff'] = X['feature1'] - X['feature2']
X['feature_product'] = X['feature1'] * X['feature2']

# Time-based features (—è–∫—â–æ —î timestamp)
X['hour'] = df['timestamp'].dt.hour
X['day_of_week'] = df['timestamp'].dt.dayofweek
```

### 5. Preprocessing

```python
# Scaling (–¥–ª—è SVM —Ç–∞ LOF)
from sklearn.preprocessing import StandardScaler, RobustScaler

# StandardScaler (—á—É—Ç–ª–∏–≤–∏–π –¥–æ outliers)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# RobustScaler (robust –¥–æ outliers)
scaler = RobustScaler()
X_scaled = scaler.fit_transform(X)

# For Isolation Forest: –Ω–µ –ø–æ—Ç—Ä—ñ–±–µ–Ω scaling
```

### 6. Dimensionality reduction

```python
# –î–ª—è high-dimensional
from sklearn.decomposition import PCA

# PCA —Å–ø–æ—á–∞—Ç–∫—É
pca = PCA(n_components=20)
X_reduced = pca.fit_transform(X)

# –ü–æ—Ç—ñ–º anomaly detection
clf = IsolationForest()
y_pred = clf.fit_predict(X_reduced)
```

### 7. Cross-validation –¥–ª—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤

```python
# Grid search —è–∫—â–æ —î validation
param_grid = {
    'contamination': [0.01, 0.05, 0.1, 0.15],
    'n_estimators': [50, 100, 200]
}

best_f1 = 0
best_params = None

for contamination in param_grid['contamination']:
    for n_estimators in param_grid['n_estimators']:
        clf = IsolationForest(
            contamination=contamination,
            n_estimators=n_estimators
        )
        
        clf.fit(X_train)
        y_pred = clf.predict(X_val)
        
        f1 = f1_score(y_val, (y_pred == -1).astype(int))
        
        if f1 > best_f1:
            best_f1 = f1
            best_params = {
                'contamination': contamination,
                'n_estimators': n_estimators
            }

print(f"Best params: {best_params}")
```

### 8. Monitor –≤ production

```python
# Track metrics over time
class AnomalyMonitor:
    def __init__(self):
        self.history = []
    
    def log(self, timestamp, n_anomalies, avg_score):
        self.history.append({
            'timestamp': timestamp,
            'n_anomalies': n_anomalies,
            'avg_score': avg_score
        })
    
    def plot_trends(self):
        df = pd.DataFrame(self.history)
        
        fig, axes = plt.subplots(2, 1, figsize=(12, 8))
        
        axes[0].plot(df['timestamp'], df['n_anomalies'])
        axes[0].set_title('Anomalies Over Time')
        axes[0].set_ylabel('Count')
        
        axes[1].plot(df['timestamp'], df['avg_score'])
        axes[1].set_title('Average Anomaly Score')
        axes[1].set_ylabel('Score')
        
        plt.tight_layout()
        plt.show()
```

### 9. Explainability

```python
# –î–ª—è Isolation Forest - feature importance
def get_anomaly_explanation(clf, X, sample_idx):
    """–Ø–∫—ñ features –Ω–∞–π–±—ñ–ª—å—à –∞–Ω–æ–º–∞–ª—å–Ω—ñ?"""
    
    sample = X[sample_idx]
    
    # Permutation importance
    base_score = clf.score_samples([sample])[0]
    
    importances = []
    for feature_idx in range(X.shape[1]):
        X_perm = sample.copy()
        X_perm[feature_idx] = np.median(X[:, feature_idx])
        
        score_perm = clf.score_samples([X_perm])[0]
        importance = abs(base_score - score_perm)
        importances.append(importance)
    
    return np.array(importances)

# Usage
sample_idx = 0  # –ê–Ω–æ–º–∞–ª—å–Ω–∞ —Ç–æ—á–∫–∞
importances = get_anomaly_explanation(clf, X, sample_idx)

# –¢–æ–ø features
top_features = np.argsort(importances)[-5:][::-1]
print("Most anomalous features:")
for idx in top_features:
    print(f"  Feature {idx}: importance={importances[idx]:.4f}")
```

### 10. A/B testing

```python
# –ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ —á–∏ –ø—Ä–∞—Ü—é—î detection

# Baseline period (before detection system)
baseline_fraud_rate = 0.05  # 5%
baseline_loss = 100000  # $100K

# Test period (with detection)
detected_fraud = 40  # Caught
missed_fraud = 10    # Missed
total_fraud = 50

detection_rate = detected_fraud / total_fraud
test_loss = missed_fraud * avg_fraud_loss

print(f"Detection rate: {detection_rate:.1%}")
print(f"Loss reduction: ${baseline_loss - test_loss:,.0f}")
```

---

## –ü–æ—à–∏—Ä–µ–Ω—ñ –ø–æ–º–∏–ª–∫–∏ ‚ùå

### 1. –ù–µ –ø–µ—Ä–µ–≤—ñ—Ä—è—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏

```python
# ‚ùå –°–ª—ñ–ø–æ –¥–æ–≤—ñ—Ä—è—Ç–∏
y_pred = clf.fit_predict(X)

# ‚úÖ –ó–∞–≤–∂–¥–∏ inspect
outlier_indices = np.where(y_pred == -1)[0]

print(f"Found {len(outlier_indices)} outliers")
print("Sample outliers:")
print(X[outlier_indices[:5]])

# –í—ñ–∑—É–∞–ª—ñ–∑—É–π
if X.shape[1] == 2:
    plt.scatter(X[:, 0], X[:, 1], c=y_pred)
    plt.show()
```

### 2. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∏–π –º–µ—Ç–æ–¥

```python
# ‚ùå LOF –Ω–∞ 100,000 points
# –î—É–∂–µ –ø–æ–≤—ñ–ª—å–Ω–æ!

# ‚úÖ –í–∏–±–µ—Ä–∏ –º–µ—Ç–æ–¥ –ø—ñ–¥ —Ä–æ–∑–º—ñ—Ä –¥–∞–Ω–∏—Ö
if len(X) > 50000:
    clf = IsolationForest()  # –®–≤–∏–¥–∫–æ
else:
    clf = LocalOutlierFactor()  # –Ø–∫—ñ—Å—Ç—å
```

### 3. –ó–∞–±—É—Ç–∏ –ø—Ä–æ scaling

```python
# ‚ùå –î–ª—è One-Class SVM —Ç–∞ LOF
clf = OneClassSVM()
clf.fit(X_raw)  # –ü–æ–≥–∞–Ω–æ!

# ‚úÖ Scale —Å–ø–æ—á–∞—Ç–∫—É
X_scaled = StandardScaler().fit_transform(X_raw)
clf.fit(X_scaled)
```

### 4. –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∏–π contamination

```python
# ‚ùå contamination=0.5 (50%!)
# –ü–æ–ª–æ–≤–∏–Ω–∞ –¥–∞–Ω–∏—Ö –∞–Ω–æ–º–∞–ª—ñ—ó?

# ‚úÖ Realistic estimate
contamination = 0.01  # 1% –¥–ª—è fraud
contamination = 0.05  # 5% –¥–ª—è defects
```

### 5. –ù–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ ensemble

```python
# ‚ùå –û–¥–∏–Ω –º–µ—Ç–æ–¥
# –ú–æ–∂–µ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç–∏ –¥–µ—è–∫—ñ —Ç–∏–ø–∏ –∞–Ω–æ–º–∞–ª—ñ–π

# ‚úÖ Ensemble
if_pred = IsolationForest().fit_predict(X)
lof_pred = LocalOutlierFactor().fit_predict(X)

ensemble = (if_pred == -1) | (lof_pred == -1)
```

---

## –†–µ–∞–ª—å–Ω—ñ –∫–µ–π—Å–∏

### Case 1: Credit Card Fraud

**–í–∏–º–æ–≥–∏:**
- Real-time (< 100ms)
- High precision (few false positives)
- Millions of transactions/day

**–†—ñ—à–µ–Ω–Ω—è:**
```
1. Feature engineering: RFM, velocity, geo
2. Isolation Forest (fast, scalable)
3. Rules-based override (high amount + night)
4. Human review queue for borderline cases
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç:**
- 95% fraud detection rate
- < 50ms latency
- 2% false positive rate

### Case 2: Manufacturing Defects

**–í–∏–º–æ–≥–∏:**
- –õ–æ–∫–∞–ª—å–Ω—ñ –∞–Ω–æ–º–∞–ª—ñ—ó (—Ä—ñ–∑–Ω—ñ product types)
- Sensor time series data
- –ü–æ—è—Å–Ω–µ–Ω–Ω—è —á–æ–º—É defect

**–†—ñ—à–µ–Ω–Ω—è:**
```
1. Segment by product type
2. LOF (handles variable density)
3. Feature importance –¥–ª—è –ø–æ—è—Å–Ω–µ–Ω—å
4. Dashboard –¥–ª—è operators
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç:**
- 85% defect detection (—Ä–∞–Ω—ñ—à–µ 60%)
- –ó–º–µ–Ω—à–µ–Ω–Ω—è false positives –Ω–∞ 40%

### Case 3: Network Intrusion

**–í–∏–º–æ–≥–∏:**
- Novel attack types (zero-day)
- High-dimensional features (100+)
- Streaming data

**–†—ñ—à–µ–Ω–Ω—è:**
```
1. PCA –¥–ª—è dimensionality reduction
2. Autoencoder (learns normal patterns)
3. Incremental retraining
4. Alert prioritization by score
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç:**
- –í–∏—è–≤–ª–µ–Ω–Ω—è 98% –≤—ñ–¥–æ–º–∏—Ö attacks
- 60% novel attacks (–Ω–µ –±–∞—á–µ–Ω–∏—Ö —Ä–∞–Ω—ñ—à–µ)

---

## Metrics –¥–ª—è evaluation

### –Ø–∫—â–æ —î labels

```python
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    roc_auc_score,
    average_precision_score,
    confusion_matrix
)

y_pred_binary = (y_pred == -1).astype(int)

print("=== Metrics ===")
print(f"Accuracy: {accuracy_score(y_true, y_pred_binary):.3f}")
print(f"Precision: {precision_score(y_true, y_pred_binary):.3f}")
print(f"Recall: {recall_score(y_true, y_pred_binary):.3f}")
print(f"F1-Score: {f1_score(y_true, y_pred_binary):.3f}")
print(f"ROC-AUC: {roc_auc_score(y_true, scores):.3f}")
print(f"PR-AUC: {average_precision_score(y_true, scores):.3f}")

# Confusion matrix
cm = confusion_matrix(y_true, y_pred_binary)
print("\nConfusion Matrix:")
print(cm)
```

### –Ø–∫—â–æ –Ω–µ–º–∞—î labels

```python
# Unsupervised metrics

# 1. Silhouette score (—è–∫—â–æ —î –∫–ª–∞—Å—Ç–µ—Ä–∏)
from sklearn.metrics import silhouette_score
sil = silhouette_score(X, y_pred)

# 2. Visual inspection
plt.scatter(X[:, 0], X[:, 1], c=y_pred)
plt.show()

# 3. Domain expert review
outlier_samples = X[y_pred == -1]
print("Review these samples:")
print(outlier_samples[:10])

# 4. Stability (run multiple times)
predictions = []
for seed in range(10):
    clf = IsolationForest(random_state=seed)
    pred = clf.fit_predict(X)
    predictions.append(pred)

# –Ø–∫—â–æ —Å—Ç–∞–±—ñ–ª—å–Ω–æ ‚Üí more confident
stability = np.mean([np.array_equal(predictions[0], p) 
                     for p in predictions[1:]])
print(f"Stability: {stability:.1%}")
```

---

## –ü–æ–≤'—è–∑–∞–Ω—ñ —Ç–µ–º–∏

- [[01_Isolation_Forest]] ‚Äî tree-based –º–µ—Ç–æ–¥
- [[02_One_Class_SVM]] ‚Äî kernel-based
- [[03_Local_Outlier_Factor]] ‚Äî density-based
- [[Clustering_Methods]] ‚Äî DBSCAN –¥–ª—è outliers
- [[Autoencoders]] ‚Äî deep learning approach

## –†–µ—Å—É—Ä—Å–∏

- [Scikit-learn: Outlier Detection](https://scikit-learn.org/stable/modules/outlier_detection.html)
- [Anomaly Detection Survey](https://arxiv.org/abs/1901.03407)
- [PyOD Library](https://github.com/yzhao062/pyod) ‚Äî –±–∞–≥–∞—Ç–æ –º–µ—Ç–æ–¥—ñ–≤

---

## –ö–ª—é—á–æ–≤—ñ –≤–∏—Å–Ω–æ–≤–∫–∏

> Anomaly Detection ‚Äî —Ü–µ –∑–∞–¥–∞—á–∞ –∑–Ω–∞—Ö–æ–¥–∂–µ–Ω–Ω—è –Ω–µ–∑–≤–∏—á–∞–π–Ω–∏—Ö –∑—Ä–∞–∑–∫—ñ–≤. –ù–µ —ñ—Å–Ω—É—î –æ–¥–Ω–æ–≥–æ "–Ω–∞–π–∫—Ä–∞—â–æ–≥–æ" –º–µ—Ç–æ–¥—É ‚Äî –≤–∏–±—ñ—Ä –∑–∞–ª–µ–∂–∏—Ç—å –≤—ñ–¥ —Ä–æ–∑–º—ñ—Ä—É –¥–∞–Ω–∏—Ö, —Ç–∏–ø—É –∞–Ω–æ–º–∞–ª—ñ–π, –≤–∏–º–æ–≥ –¥–æ —à–≤–∏–¥–∫–æ—Å—Ç—ñ —Ç–∞ —ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω–æ—Å—Ç—ñ.

**–û—Å–Ω–æ–≤–Ω—ñ –º–µ—Ç–æ–¥–∏:**

| –ú–µ—Ç–æ–¥ | Best –¥–ª—è | –†–æ–∑–º—ñ—Ä –¥–∞–Ω–∏—Ö |
|-------|---------|--------------|
| **Isolation Forest** | Global outliers, —à–≤–∏–¥–∫—ñ—Å—Ç—å | > 10K |
| **One-Class SVM** | –ù–µ–ª—ñ–Ω—ñ–π–Ω—ñ boundaries | < 10K |
| **LOF** | –õ–æ–∫–∞–ª—å–Ω—ñ outliers | < 50K |
| **Autoencoder** | –°–∫–ª–∞–¥–Ω—ñ patterns | > 100K |
| **Statistical** | –ü—Ä–æ—Å—Ç–æ—Ç–∞, —à–≤–∏–¥–∫—ñ—Å—Ç—å | –ë—É–¥—å-—è–∫–∏–π |

**Quick Decision Guide:**

```
START
  ‚Üì
–°–∫—ñ–ª—å–∫–∏ –¥–∞–Ω–∏—Ö?
  ‚îú‚îÄ < 10K ‚Üí One-Class SVM –∞–±–æ LOF
  ‚îî‚îÄ > 10K ‚Üí Isolation Forest

–õ–æ–∫–∞–ª—å–Ω—ñ outliers –≤–∞–∂–ª–∏–≤—ñ?
  ‚îú‚îÄ –¢–∞–∫ ‚Üí LOF
  ‚îî‚îÄ –ù—ñ ‚Üí Isolation Forest

High-dimensional (>50)?
  ‚îú‚îÄ –¢–∞–∫ ‚Üí Isolation Forest –∞–±–æ PCA+method
  ‚îî‚îÄ –ù—ñ ‚Üí –ë—É–¥—å-—è–∫–∏–π –º–µ—Ç–æ–¥

–®–≤–∏–¥–∫—ñ—Å—Ç—å –∫—Ä–∏—Ç–∏—á–Ω–∞?
  ‚îú‚îÄ –¢–∞–∫ ‚Üí Isolation Forest
  ‚îî‚îÄ –ù—ñ ‚Üí –ó–∞–ª–µ–∂–∏—Ç—å –≤—ñ–¥ –∑–∞–¥–∞—á—ñ
```

**Best Practices:**
1. –°–ø—Ä–æ–±—É–π –∫—ñ–ª—å–∫–∞ –º–µ—Ç–æ–¥—ñ–≤
2. Ensemble –¥–ª—è –∫—Ä–∞—â–∏—Ö results
3. Validate —è–∫—â–æ —î labels
4. Feature engineering –∫—Ä–∏—Ç–∏—á–Ω–∏–π
5. Scale features (SVM, LOF)
6. Monitor –≤ production
7. Explain decisions
8. Iterate based –Ω–∞ feedback

**–ù–∞–π–≤–∞–∂–ª–∏–≤—ñ—à–µ:**
- –†–æ–∑—É–º—ñ–π —Å–≤–æ—ó –¥–∞–Ω—ñ —Ç–∞ domain
- –ù–µ —ñ—Å–Ω—É—î silver bullet
- Validate, validate, validate
- Ensemble —á–∞—Å—Ç–æ –Ω–∞–π–∫—Ä–∞—â–µ
- Interpretability vs accuracy trade-off

---

#ml #unsupervised-learning #anomaly-detection #outlier-detection #comparison #ensemble #isolation-forest #one-class-svm #lof #methods-overview
