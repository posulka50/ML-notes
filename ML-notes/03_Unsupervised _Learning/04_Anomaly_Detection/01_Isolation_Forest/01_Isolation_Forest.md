# Isolation Forest

## –©–æ —Ü–µ?

**Isolation Forest** ‚Äî —Ü–µ **tree-based** –∞–ª–≥–æ—Ä–∏—Ç–º –¥–ª—è –≤–∏—è–≤–ª–µ–Ω–Ω—è –∞–Ω–æ–º–∞–ª—ñ–π (outlier detection), —â–æ –±–∞–∑—É—î—Ç—å—Å—è –Ω–∞ –ø—Ä–æ—Å—Ç—ñ–π —ñ–¥–µ—ó: **–∞–Ω–æ–º–∞–ª—ñ—ó –ª–µ–≥—à–µ —ñ–∑–æ–ª—é–≤–∞—Ç–∏** –Ω—ñ–∂ –Ω–æ—Ä–º–∞–ª—å–Ω—ñ —Ç–æ—á–∫–∏. –ó–∞–º—ñ—Å—Ç—å –º–æ–¥–µ–ª—é–≤–∞—Ç–∏ –Ω–æ—Ä–º–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ, –≤—ñ–Ω —è–≤–Ω–æ —ñ–∑–æ–ª—é—î –∞–Ω–æ–º–∞–ª—ñ—ó.

**–ì–æ–ª–æ–≤–Ω–∞ —ñ–¥–µ—è:** –∞–Ω–æ–º–∞–ª—ñ—ó ‚Äî —Ü–µ "few and different". –á—Ö –º–æ–∂–Ω–∞ —à–≤–∏–¥–∫–æ –≤—ñ–¥–æ–∫—Ä–µ–º–∏—Ç–∏ –≤–∏–ø–∞–¥–∫–æ–≤–∏–º–∏ —Ä–æ–∑–±–∏—Ç—Ç—è–º–∏, —Ç–æ–¥—ñ —è–∫ –Ω–æ—Ä–º–∞–ª—å–Ω—ñ —Ç–æ—á–∫–∏ –ø–æ—Ç—Ä–µ–±—É—é—Ç—å –±–∞–≥–∞—Ç–æ —Ä–æ–∑–±–∏—Ç—Ç—ñ–≤ —â–æ–± –±—É—Ç–∏ —ñ–∑–æ–ª—å–æ–≤–∞–Ω–∏–º–∏.

## –ù–∞–≤—ñ—â–æ –ø–æ—Ç—Ä—ñ–±–µ–Ω?

- üîç **Anomaly Detection** ‚Äî –≤–∏—è–≤–ª–µ–Ω–Ω—è –Ω–µ–∑–≤–∏—á–∞–π–Ω–∏—Ö –ø–∞—Ç—Ç–µ—Ä–Ω—ñ–≤
- üõ°Ô∏è **Fraud Detection** ‚Äî —à–∞—Ö—Ä–∞–π—Å—å–∫—ñ —Ç—Ä–∞–Ω–∑–∞–∫—Ü—ñ—ó
- üè≠ **Industrial Monitoring** ‚Äî –Ω–µ—Å–ø—Ä–∞–≤–Ω–æ—Å—Ç—ñ –æ–±–ª–∞–¥–Ω–∞–Ω–Ω—è
- üåê **Network Security** ‚Äî –∫—ñ–±–µ—Ä–∞—Ç–∞–∫–∏, intrusions
- üí≥ **Credit Card Fraud** ‚Äî –ø—ñ–¥–æ–∑—Ä—ñ–ª—ñ –ø–æ–∫—É–ø–∫–∏
- üìä **Data Quality** ‚Äî –≤–∏—è–≤–ª–µ–Ω–Ω—è –ø–æ–º–∏–ª–æ–∫ –≤ –¥–∞–Ω–∏—Ö
- üè• **Medical Diagnosis** ‚Äî —Ä—ñ–¥–∫—ñ—Å–Ω—ñ –∑–∞—Ö–≤–æ—Ä—é–≤–∞–Ω–Ω—è

## –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏?

**–ü–æ—Ç—Ä—ñ–±–Ω–æ:**

- **Unsupervised detection** ‚Äî –Ω–µ–º–∞—î labeled anomalies
- **–í–µ–ª–∏–∫—ñ –¥–∞–Ω—ñ** ‚Äî —à–≤–∏–¥–∫–∏–π —Ç–∞ –µ—Ñ–µ–∫—Ç–∏–≤–Ω–∏–π
- **High-dimensional data** ‚Äî –ø—Ä–∞—Ü—é—î –¥–æ–±—Ä–µ
- **Contamination –≤—ñ–¥–æ–º–∏–π** ‚Äî –∑–Ω–∞—î–º–æ –ø—Ä–∏–±–ª–∏–∑–Ω–æ % –∞–Ω–æ–º–∞–ª—ñ–π
- **Real-time** ‚Äî —à–≤–∏–¥–∫—ñ predictions
- **–ß–∏—Å–ª–æ–≤—ñ features** ‚Äî –Ω–µ–ø–µ—Ä–µ—Ä–≤–Ω—ñ –¥–∞–Ω—ñ

**–ù–µ –ø–æ—Ç—Ä—ñ–±–Ω–æ:**

- **Labeled anomalies** —î ‚Üí Supervised methods
- **Categorical data** ‚Üí —ñ–Ω—à—ñ –º–µ—Ç–æ–¥–∏ (LOF, HDBSCAN)
- **–î—É–∂–µ –º–∞–ª—ñ –¥–∞–Ω—ñ** (< 100) ‚Üí statistical methods
- **–°–∫–ª–∞–¥–Ω—ñ patterns** ‚Üí Deep Learning (Autoencoders)

---

## –Ü–Ω—Ç—É—ó—Ü—ñ—è

### –ß–æ–º—É –∞–Ω–æ–º–∞–ª—ñ—ó –ª–µ–≥—à–µ —ñ–∑–æ–ª—é–≤–∞—Ç–∏?

**–ü—Ä–∏–∫–ª–∞–¥ 1D:**

```
–ù–æ—Ä–º–∞–ª—å–Ω—ñ —Ç–æ—á–∫–∏ (–≥—É—Å—Ç–æ):
|‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè|           |‚óã|
0              10          20

Anomaly (‚óã) –¥–∞–ª–µ–∫–æ –≤—ñ–¥ –∫–ª–∞—Å—Ç–µ—Ä–∞

–°–∫—ñ–ª—å–∫–∏ random splits –ø–æ—Ç—Ä—ñ–±–Ω–æ —â–æ–± —ñ–∑–æ–ª—é–≤–∞—Ç–∏?
- Anomaly: 1 split (|15)
- Normal point: –±–∞–≥–∞—Ç–æ splits (—Ç—Ä–µ–±–∞ —Ä–æ–∑–¥—ñ–ª–∏—Ç–∏ dense cluster)
```

**–ü—Ä–∏–∫–ª–∞–¥ 2D:**

```
        y
    10  |              ‚óã anomaly
        |
     5  |  ‚óè‚óè‚óè‚óè‚óè
        |  ‚óè‚óè‚óè‚óè‚óè
        |  ‚óè‚óè‚óè‚óè‚óè  normal cluster
     0  |_____________ x
        0    5    10

–Ü–∑–æ–ª—é–≤–∞—Ç–∏ –∞–Ω–æ–º–∞–ª—ñ—é:
Split 1: x > 8  ‚Üí anomaly –≤—ñ–¥–æ–∫—Ä–µ–º–ª–µ–Ω–∞! (1 split)

–Ü–∑–æ–ª—é–≤–∞—Ç–∏ normal point:
Split 1: x > 3
Split 2: y > 2
Split 3: x < 6
Split 4: y < 7
... (–±–∞–≥–∞—Ç–æ splits)
```

**–í–∏—Å–Ω–æ–≤–æ–∫:** –ê–Ω–æ–º–∞–ª—ñ—ó –º–∞—é—Ç—å **shorter average path length** –≤ —ñ–∑–æ–ª—è—Ü—ñ–π–Ω–æ–º—É –¥–µ—Ä–µ–≤—ñ.

---

## –ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞

### Isolation Tree (iTree)

**–ü–æ–±—É–¥–æ–≤–∞:**

1. –í–∏–ø–∞–¥–∫–æ–≤–æ –≤–∏–±—Ä–∞—Ç–∏ feature
2. –í–∏–ø–∞–¥–∫–æ–≤–æ –≤–∏–±—Ä–∞—Ç–∏ split value –º—ñ–∂ min —Ç–∞ max
3. –†–æ–∑–¥—ñ–ª–∏—Ç–∏ –¥–∞–Ω—ñ
4. –ü–æ–≤—Ç–æ—Ä—é–≤–∞—Ç–∏ —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ –¥–æ:
   - –¢–æ—á–∫–∞ —ñ–∑–æ–ª—å–æ–≤–∞–Ω–∞ (–æ–¥–Ω–∞ –≤ node), –∞–±–æ
   - –î–æ—Å—è–≥–Ω—É—Ç–æ max depth

**Path length** $h(x)$ ‚Äî –∫—ñ–ª—å–∫—ñ—Å—Ç—å edges –≤—ñ–¥ root –¥–æ leaf –¥–ª—è —Ç–æ—á–∫–∏ $x$.

### Anomaly Score

**–î–ª—è —Ç–æ—á–∫–∏ $x$:**

$$s(x, n) = 2^{-\frac{E(h(x))}{c(n)}}$$

–¥–µ:
- $E(h(x))$ ‚Äî average path length –ø–æ –≤—Å—ñ—Ö –¥–µ—Ä–µ–≤–∞—Ö
- $c(n)$ ‚Äî average path length –¥–ª—è BST –∑ $n$ —Ç–æ—á–æ–∫ (–Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è)

$$c(n) = 2H(n-1) - \frac{2(n-1)}{n}$$

–¥–µ $H(i)$ ‚Äî harmonic number ‚âà $\ln(i) + 0.5772$

**–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü—ñ—è:**

```
s(x) ‚âà 1   ‚Üí Anomaly (–∫–æ—Ä–æ—Ç–∫–∏–π path)
s(x) ‚âà 0.5 ‚Üí Normal (—Å–µ—Ä–µ–¥–Ω—ñ–π path)
s(x) < 0.5 ‚Üí –¢–æ—á–Ω–æ normal (–¥–æ–≤–≥–∏–π path)
```

### –ß–æ–º—É —Ü–µ –ø—Ä–∞—Ü—é—î?

**Average path length –¥–ª—è:**

- **Anomaly:** $E(h(x)) \ll c(n)$ ‚Üí $s(x) \rightarrow 1$
- **Normal:** $E(h(x)) \approx c(n)$ ‚Üí $s(x) \rightarrow 0.5$

---

### –Ü–Ω—Ç—É—ó—Ü—ñ—è

–ê–ª–≥–æ—Ä–∏—Ç–º –±–∞–≥–∞—Ç–æ—Ä–∞–∑–æ–≤–æ –≤–∏–ø–∞–¥–∫–æ–≤–æ "—Ä–æ–∑—Ä—ñ–∑–∞—î" –ø—Ä–æ—Å—Ç—ñ—Ä.  
–¢–æ—á–∫–∏, —è–∫—ñ —à–≤–∏–¥–∫–æ –≤—ñ–¥–æ–∫—Ä–µ–º–ª—é—é—Ç—å—Å—è –≤—ñ–¥ —ñ–Ω—à–∏—Ö, –≤–≤–∞–∂–∞—é—Ç—å—Å—è –ø—ñ–¥–æ–∑—Ä—ñ–ª–∏–º–∏.

---

## –ö–æ–¥ (Python + scikit-learn)

### –ë–∞–∑–æ–≤–∏–π –ø—Ä–∏–∫–ª–∞–¥

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import IsolationForest
from sklearn.datasets import make_blobs

# 1. –ì–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ –¥–∞–Ω—ñ –∑ –∞–Ω–æ–º–∞–ª—ñ—è–º–∏
np.random.seed(42)

# Normal points (dense cluster)
X_normal, _ = make_blobs(
    n_samples=300,
    centers=[[0, 0]],
    cluster_std=0.5,
    random_state=42
)

# Anomalies (scattered outliers)
X_anomalies = np.random.uniform(low=-4, high=4, size=(20, 2))

# Combine
X = np.vstack([X_normal, X_anomalies])
y_true = np.array([0]*300 + [1]*20)  # 0=normal, 1=anomaly

print(f"Total points: {len(X)}")
print(f"Normal: {(y_true == 0).sum()}")
print(f"Anomalies: {(y_true == 1).sum()}")
print(f"Contamination: {(y_true == 1).sum() / len(X):.2%}")

# 2. Fit Isolation Forest
clf = IsolationForest(
    n_estimators=100,           # –ö—ñ–ª—å–∫—ñ—Å—Ç—å –¥–µ—Ä–µ–≤
    max_samples=256,            # –†–æ–∑–º—ñ—Ä sample –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –¥–µ—Ä–µ–≤–∞
    contamination=0.1,          # –û—á—ñ–∫—É–≤–∞–Ω–∏–π % –∞–Ω–æ–º–∞–ª—ñ–π
    random_state=42
)

clf.fit(X)

# 3. Predict
y_pred = clf.predict(X)  # 1=normal, -1=anomaly
scores = clf.score_samples(X)  # Anomaly scores (—á–∏–º –º–µ–Ω—à–µ, —Ç–∏–º –±—ñ–ª—å—à–∞ –∞–Ω–æ–º–∞–ª—ñ—è)

print(f"\n=== Predictions ===")
print(f"Predicted anomalies: {(y_pred == -1).sum()}")
print(f"Predicted normal: {(y_pred == 1).sum()}")

# 4. Metrics
from sklearn.metrics import classification_report, confusion_matrix

# Convert: -1 ‚Üí 1 (anomaly), 1 ‚Üí 0 (normal)
y_pred_binary = (y_pred == -1).astype(int)

print("\n=== Classification Report ===")
print(classification_report(y_true, y_pred_binary, 
                           target_names=['Normal', 'Anomaly']))

print("\n=== Confusion Matrix ===")
cm = confusion_matrix(y_true, y_pred_binary)
print(cm)
print(f"True Negatives: {cm[0,0]}")
print(f"False Positives: {cm[0,1]}")
print(f"False Negatives: {cm[1,0]}")
print(f"True Positives: {cm[1,1]}")

# 5. –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# Plot 1: True labels
axes[0].scatter(X[y_true == 0, 0], X[y_true == 0, 1],
               c='blue', s=20, alpha=0.6, label='Normal')
axes[0].scatter(X[y_true == 1, 0], X[y_true == 1, 1],
               c='red', s=50, alpha=0.8, marker='x', label='Anomaly')
axes[0].set_title('True Labels', fontsize=13, fontweight='bold')
axes[0].set_xlabel('Feature 1')
axes[0].set_ylabel('Feature 2')
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# Plot 2: Predictions (color by anomaly score)
scatter = axes[1].scatter(X[:, 0], X[:, 1],
                         c=scores, cmap='RdYlBu_r',
                         s=30, alpha=0.7, edgecolors='black', linewidths=0.5)
plt.colorbar(scatter, ax=axes[1], label='Anomaly Score\n(lower = more anomalous)')

# Mark predicted anomalies
anomaly_mask = y_pred == -1
axes[1].scatter(X[anomaly_mask, 0], X[anomaly_mask, 1],
               facecolors='none', edgecolors='red',
               s=100, linewidths=2, label='Predicted Anomalies')

axes[1].set_title('Isolation Forest Predictions', fontsize=13, fontweight='bold')
axes[1].set_xlabel('Feature 1')
axes[1].set_ylabel('Feature 2')
axes[1].legend()
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### Decision boundary visualization

```python
# –í—ñ–∑—É–∞–ª—ñ–∑—É–≤–∞—Ç–∏ decision boundary

# Create mesh
xx, yy = np.meshgrid(
    np.linspace(X[:, 0].min()-1, X[:, 0].max()+1, 200),
    np.linspace(X[:, 1].min()-1, X[:, 1].max()+1, 200)
)

# Predict –Ω–∞ mesh
Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

plt.figure(figsize=(10, 7))

# Contour plot
plt.contourf(xx, yy, Z, levels=20, cmap='RdYlBu_r', alpha=0.6)
plt.colorbar(label='Decision Function')

# Threshold contour
plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors='black')

# Data points
plt.scatter(X[y_true == 0, 0], X[y_true == 0, 1],
           c='blue', s=20, alpha=0.6, edgecolors='black', 
           linewidths=0.5, label='Normal')
plt.scatter(X[y_true == 1, 0], X[y_true == 1, 1],
           c='red', s=50, alpha=0.8, marker='x', 
           linewidths=2, label='Anomaly')

plt.title('Isolation Forest Decision Boundary', fontsize=14, fontweight='bold')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

### Real example: Credit Card Fraud

```python
# –°–∏–º—É–ª—é—î–º–æ credit card transactions

np.random.seed(42)

# Normal transactions
n_normal = 9000
amount_normal = np.random.lognormal(mean=3, sigma=1, size=n_normal)
time_normal = np.random.uniform(0, 24, size=n_normal)

X_normal = np.column_stack([amount_normal, time_normal])

# Fraudulent transactions (different patterns)
n_fraud = 100

# Type 1: Unusually high amounts
amount_fraud1 = np.random.uniform(1000, 5000, size=n_fraud//2)
time_fraud1 = np.random.uniform(0, 24, size=n_fraud//2)

# Type 2: Night transactions with medium amounts
amount_fraud2 = np.random.uniform(200, 500, size=n_fraud//2)
time_fraud2 = np.random.uniform(2, 5, size=n_fraud//2)  # 2-5 AM

X_fraud = np.vstack([
    np.column_stack([amount_fraud1, time_fraud1]),
    np.column_stack([amount_fraud2, time_fraud2])
])

# Combine
X_transactions = np.vstack([X_normal, X_fraud])
y_true = np.array([0]*n_normal + [1]*n_fraud)

print(f"Total transactions: {len(X_transactions)}")
print(f"Fraud rate: {(y_true == 1).sum() / len(y_true):.2%}")

# Fit Isolation Forest
clf = IsolationForest(
    n_estimators=100,
    contamination=0.01,  # Expect 1% fraud
    random_state=42
)

clf.fit(X_transactions)

# Predictions
y_pred = clf.predict(X_transactions)
y_pred_binary = (y_pred == -1).astype(int)

# Metrics
from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score

print("\n=== Fraud Detection Results ===")
print(f"Precision: {precision_score(y_true, y_pred_binary):.3f}")
print(f"Recall: {recall_score(y_true, y_pred_binary):.3f}")
print(f"F1-Score: {f1_score(y_true, y_pred_binary):.3f}")

# ROC-AUC (–≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ scores –∑–∞–º—ñ—Å—Ç—å binary predictions)
scores = -clf.score_samples(X_transactions)  # Invert (higher = more anomalous)
print(f"ROC-AUC: {roc_auc_score(y_true, scores):.3f}")

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# True labels
axes[0].scatter(X_transactions[y_true == 0, 0], 
               X_transactions[y_true == 0, 1],
               c='green', s=5, alpha=0.3, label='Legitimate')
axes[0].scatter(X_transactions[y_true == 1, 0],
               X_transactions[y_true == 1, 1],
               c='red', s=20, alpha=0.8, marker='x', label='Fraud')
axes[0].set_xlabel('Transaction Amount ($)')
axes[0].set_ylabel('Time of Day (hour)')
axes[0].set_title('True Labels', fontsize=13, fontweight='bold')
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# Predictions
axes[1].scatter(X_transactions[y_pred == 1, 0],
               X_transactions[y_pred == 1, 1],
               c='green', s=5, alpha=0.3, label='Predicted Legitimate')
axes[1].scatter(X_transactions[y_pred == -1, 0],
               X_transactions[y_pred == -1, 1],
               c='red', s=20, alpha=0.8, marker='x', label='Predicted Fraud')
axes[1].set_xlabel('Transaction Amount ($)')
axes[1].set_ylabel('Time of Day (hour)')
axes[1].set_title('Isolation Forest Predictions', fontsize=13, fontweight='bold')
axes[1].legend()
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

---

## –ü–∞—Ä–∞–º–µ—Ç—Ä–∏

### –û—Å–Ω–æ–≤–Ω—ñ

```python
IsolationForest(
    n_estimators=100,       # –ö—ñ–ª—å–∫—ñ—Å—Ç—å –¥–µ—Ä–µ–≤
    max_samples='auto',     # –†–æ–∑–º—ñ—Ä sample (default: min(256, n_samples))
    contamination=0.1,      # –û—á—ñ–∫—É–≤–∞–Ω–∏–π % –∞–Ω–æ–º–∞–ª—ñ–π
    max_features=1.0,       # Features –¥–ª—è split
    bootstrap=False,        # Sample –∑/–±–µ–∑ replacement
    random_state=None       # –í—ñ–¥—Ç–≤–æ—Ä—é–≤–∞–Ω—ñ—Å—Ç—å
)
```

| –ü–∞—Ä–∞–º–µ—Ç—Ä | –û–ø–∏—Å | –¢–∏–ø–æ–≤—ñ –∑–Ω–∞—á–µ–Ω–Ω—è | –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó |
|----------|------|-----------------|--------------|
| **n_estimators** | –ö—ñ–ª—å–∫—ñ—Å—Ç—å –¥–µ—Ä–µ–≤ | 50-200 | 100 (default) –¥–æ–±—Ä–µ |
| **max_samples** | –†–æ–∑–º—ñ—Ä sample | 256 (default), 'auto' | 256 –¥–æ—Å—Ç–∞—Ç–Ω—å–æ –¥–ª—è –±—ñ–ª—å—à–æ—Å—Ç—ñ |
| **contamination** | % –∞–Ω–æ–º–∞–ª—ñ–π | 0.01-0.1 | –ë–∞–∑—É—î—Ç—å—Å—è –Ω–∞ domain knowledge |
| **max_features** | Features per split | 1.0 (all) | 1.0 –¥–ª—è –ø–æ–≤–Ω–æ—ó randomness |

### contamination

**–ö—Ä–∏—Ç–∏—á–Ω–∏–π –ø–∞—Ä–∞–º–µ—Ç—Ä!**

```python
# –Ø–∫—â–æ –∑–Ω–∞—î–º–æ –ø—Ä–∏–±–ª–∏–∑–Ω–æ —Å–∫—ñ–ª—å–∫–∏ –∞–Ω–æ–º–∞–ª—ñ–π
contamination = n_anomalies / n_total

# –ü—Ä–∏–∫–ª–∞–¥–∏:
# Fraud detection: 0.01 (1%)
# Network intrusion: 0.05 (5%)
# Manufacturing defects: 0.001 (0.1%)

# –Ø–∫—â–æ –ù–ï –∑–Ω–∞—î–º–æ ‚Üí –ø–æ—á–∞—Ç–æ–∫ –∑ 0.1 —Ç–∞ adjust
```

**–í–ø–ª–∏–≤:**
```python
contamination = 0.01  # –°—Ç—Ä–æ–≥–∏–π (—Ç—ñ–ª—å–∫–∏ top 1%)
contamination = 0.1   # –ú'—è–∫–∏–π (top 10%)
contamination = 0.5   # –î—É–∂–µ –º'—è–∫–∏–π (–ø–æ–ª–æ–≤–∏–Ω–∞!)
```

### n_estimators

**Trade-off: accuracy vs speed**

```python
# –ú–∞–ª–æ –¥–µ—Ä–µ–≤ ‚Üí —à–≤–∏–¥—à–µ, –∞–ª–µ –º–µ–Ω—à —Å—Ç–∞–±—ñ–ª—å–Ω–æ
n_estimators = 50

# –ë–∞–≥–∞—Ç–æ –¥–µ—Ä–µ–≤ ‚Üí –ø–æ–≤—ñ–ª—å–Ω—ñ—à–µ, –∞–ª–µ —Å—Ç–∞–±—ñ–ª—å–Ω—ñ—à–µ
n_estimators = 200

# Default 100 ‚Äî –¥–æ–±—Ä–∏–π –±–∞–ª–∞–Ω—Å
```

---

## –ü–µ—Ä–µ–≤–∞–≥–∏ —Ç–∞ –Ω–µ–¥–æ–ª—ñ–∫–∏

### –ü–µ—Ä–µ–≤–∞–≥–∏ ‚úì

| –ü–µ—Ä–µ–≤–∞–≥–∞ | –ü–æ—è—Å–Ω–µ–Ω–Ω—è |
|----------|-----------|
| **–®–≤–∏–¥–∫—ñ—Å—Ç—å** | O(n log n) ‚Äî –¥—É–∂–µ —à–≤–∏–¥–∫–∏–π |
| **–ú–∞—Å—à—Ç–∞–±–æ–≤–∞–Ω—ñ—Å—Ç—å** | –ü—Ä–∞—Ü—é—î –Ω–∞ –≤–µ–ª–∏–∫–∏—Ö –¥–∞–Ω–∏—Ö |
| **High-dimensional** | –î–æ–±—Ä–µ –Ω–∞ –±–∞–≥–∞—Ç—å–æ—Ö features |
| **Unsupervised** | –ù–µ –ø–æ—Ç—Ä–µ–±—É—î labels |
| **–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å** | Path length –º–∞—î —Å–µ–Ω—Å |
| **Memory efficient** | Subsampling –∑–º–µ–Ω—à—É—î –ø–∞–º'—è—Ç—å |

### –ù–µ–¥–æ–ª—ñ–∫–∏ ‚úó

| –ù–µ–¥–æ–ª—ñ–∫ | –ü–æ—è—Å–Ω–µ–Ω–Ω—è |
|---------|-----------|
| **Contamination** –ø–æ—Ç—Ä—ñ–±–µ–Ω | –¢—Ä–µ–±–∞ –∑–Ω–∞—Ç–∏ –ø—Ä–∏–±–ª–∏–∑–Ω–æ % |
| **Categorical features** | –ü–æ–≥–∞–Ω–æ –ø—Ä–∞—Ü—é—î |
| **–õ–æ–∫–∞–ª—å–Ω—ñ –∞–Ω–æ–º–∞–ª—ñ—ó** | –ú–æ–∂–µ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç–∏ (LOF –∫—Ä–∞—â–µ) |
| **Imbalanced splits** | –ú–æ–∂–µ –¥–∞–≤–∞—Ç–∏ –±—ñ–∞—Å–∏ |
| **–ù–µ –¥–ª—è clusters** | –ù–µ –¥–µ—Ç–µ–∫—Ç—É—î cluster-based outliers |

---

## –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –∑ —ñ–Ω—à–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏

| –ú–µ—Ç–æ–¥ | –®–≤–∏–¥–∫—ñ—Å—Ç—å | High-dim | –õ–æ–∫–∞–ª—å–Ω—ñ –∞–Ω–æ–º–∞–ª—ñ—ó | Supervised |
|-------|-----------|----------|-------------------|------------|
| **Isolation Forest** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚ùå |
| **LOF** | ‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚ùå |
| **One-Class SVM** | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚ö†Ô∏è |
| **Autoencoder** | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚ùå |

**–ö–æ–ª–∏ —â–æ:**
- **–í–µ–ª–∏–∫—ñ –¥–∞–Ω—ñ + —à–≤–∏–¥–∫—ñ—Å—Ç—å** ‚Üí Isolation Forest ‚úì
- **–õ–æ–∫–∞–ª—å–Ω—ñ outliers** ‚Üí LOF ‚úì
- **–°–∫–ª–∞–¥–Ω—ñ patterns** ‚Üí Autoencoder ‚úì
- **Labeled anomalies** ‚Üí Supervised methods ‚úì

---

## –ü—Ä–∞–∫—Ç–∏—á–Ω—ñ –ø–æ—Ä–∞–¥–∏ üí°

### 1. –ù–∞–ª–∞—à—Ç—É–π contamination –ø—ñ–¥ –¥–∞–Ω—ñ

```python
# –ü–æ—á–∞—Ç–æ–∫: estimate –∑ EDA
anomaly_rate_estimate = 0.05  # 5%

clf = IsolationForest(contamination=anomaly_rate_estimate)

# Adjust based –Ω–∞ results
# –Ø–∫—â–æ –∑–∞–Ω–∞–¥—Ç–æ –±–∞–≥–∞—Ç–æ false positives ‚Üí –∑–º–µ–Ω—à contamination
# –Ø–∫—â–æ –ø—Ä–æ–ø—É—Å–∫–∞—î –æ—á–µ–≤–∏–¥–Ω—ñ outliers ‚Üí –∑–±—ñ–ª—å—à
```

### 2. Feature scaling –ù–ï –æ–±–æ–≤'—è–∑–∫–æ–≤–∏–π

```python
# ‚úÖ Isolation Forest –ø—Ä–∞—Ü—é—î –±–µ–∑ scaling
# (random splits –Ω–µ –∑–∞–ª–µ–∂–∞—Ç—å –≤—ñ–¥ scale)

clf = IsolationForest()
clf.fit(X)  # Raw features OK

# –ê–ª–µ —è–∫—â–æ —î features –∑ —Ä—ñ–∑–Ω–∏–º–∏ ranges
# scaling –º–æ–∂–µ —Ç—Ä–æ—Ö–∏ –¥–æ–ø–æ–º–æ–≥—Ç–∏
from sklearn.preprocessing import StandardScaler
X_scaled = StandardScaler().fit_transform(X)
```

### 3. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π score_samples –¥–ª—è ranking

```python
# –ó–∞–º—ñ—Å—Ç—å binary predictions (anomaly/normal)
# –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π scores –¥–ª—è ranking

scores = clf.score_samples(X)
sorted_indices = np.argsort(scores)  # Ascending (most anomalous first)

# Top-N –Ω–∞–π–±—ñ–ª—å—à –∞–Ω–æ–º–∞–ª—å–Ω–∏—Ö
top_n_anomalies = sorted_indices[:10]

print("Most anomalous samples:")
for idx in top_n_anomalies:
    print(f"  Index {idx}: score = {scores[idx]:.4f}")
```

### 4. Cross-validation –¥–ª—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤

```python
# –Ø–∫—â–æ —î –Ω–µ–≤–µ–ª–∏–∫–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å labeled anomalies
# –ú–æ–∂–Ω–∞ optimize contamination

from sklearn.model_selection import GridSearchCV
from sklearn.metrics import make_scorer, f1_score

# Wrapper –¥–ª—è compatibility
class IFWrapper:
    def __init__(self, contamination=0.1):
        self.contamination = contamination
        self.clf = IsolationForest(contamination=contamination)
    
    def fit(self, X, y=None):
        self.clf.fit(X)
        return self
    
    def predict(self, X):
        return (self.clf.predict(X) == -1).astype(int)

# Grid search (—è–∫—â–æ —î labels)
param_grid = {'contamination': [0.01, 0.05, 0.1, 0.15]}
grid = GridSearchCV(IFWrapper(), param_grid, scoring='f1', cv=3)
grid.fit(X, y_true)

print(f"Best contamination: {grid.best_params_['contamination']}")
```

### 5. Ensemble –∑ —ñ–Ω—à–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏

```python
# –ö–æ–º–±—ñ–Ω—É–≤–∞—Ç–∏ Isolation Forest –∑ LOF

from sklearn.neighbors import LocalOutlierFactor

# Isolation Forest
if_scores = IsolationForest().fit(X).score_samples(X)

# LOF
lof_scores = LocalOutlierFactor(novelty=True).fit(X).score_samples(X)

# Combine (average)
combined_scores = (if_scores + lof_scores) / 2

# –ê–±–æ voting
if_pred = (if_scores < np.percentile(if_scores, 10))
lof_pred = (lof_scores < np.percentile(lof_scores, 10))
ensemble_pred = if_pred | lof_pred  # Union
```

### 6. Feature importance

```python
# –•–æ—á–∞ IF –Ω–µ –º–∞—î –ø—Ä—è–º–æ–≥–æ feature_importances_
# –ú–æ–∂–Ω–∞ approximate —á–µ—Ä–µ–∑ permutation

from sklearn.inspection import permutation_importance

# Wrapper –¥–ª—è scoring
def anomaly_score(X):
    return clf.score_samples(X)

# Permutation importance –ø–æ—Ç—Ä–µ–±—É—î sklearn estimator
# –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–æ: —Ä—É—á–Ω–∏–π –ø—ñ–¥—Ä–∞—Ö—É–Ω–æ–∫

def calculate_feature_importance(clf, X, n_permutations=10):
    """Manual feature importance"""
    base_scores = clf.score_samples(X)
    importances = []
    
    for feature_idx in range(X.shape[1]):
        perm_scores = []
        
        for _ in range(n_permutations):
            X_perm = X.copy()
            np.random.shuffle(X_perm[:, feature_idx])
            
            scores_perm = clf.score_samples(X_perm)
            diff = np.mean(np.abs(base_scores - scores_perm))
            perm_scores.append(diff)
        
        importances.append(np.mean(perm_scores))
    
    return np.array(importances)

importances = calculate_feature_importance(clf, X)

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
plt.figure(figsize=(10, 6))
plt.barh(range(len(importances)), importances)
plt.yticks(range(len(importances)), [f'Feature {i}' for i in range(len(importances))])
plt.xlabel('Importance (permutation effect)')
plt.title('Feature Importance for Anomaly Detection')
plt.tight_layout()
plt.show()
```

### 7. Incremental detection (streaming)

```python
# –î–ª—è streaming data: –ø–µ—Ä—ñ–æ–¥–∏—á–Ω–æ refit

class IncrementalIF:
    def __init__(self, window_size=1000, contamination=0.1):
        self.window_size = window_size
        self.contamination = contamination
        self.clf = IsolationForest(contamination=contamination)
        self.buffer = []
    
    def add_sample(self, x):
        """–î–æ–¥–∞—Ç–∏ –Ω–æ–≤–∏–π –∑—Ä–∞–∑–æ–∫"""
        self.buffer.append(x)
        
        # –Ø–∫—â–æ buffer –ø–æ–≤–Ω–∏–π ‚Üí refit
        if len(self.buffer) >= self.window_size:
            X_train = np.array(self.buffer[-self.window_size:])
            self.clf.fit(X_train)
            self.buffer = self.buffer[-self.window_size//2:]  # Keep half
    
    def predict(self, x):
        """Predict –¥–ª—è –Ω–æ–≤–æ–≥–æ –∑—Ä–∞–∑–∫–∞"""
        return self.clf.predict([x])[0]

# Usage
detector = IncrementalIF()

for new_sample in data_stream:
    detector.add_sample(new_sample)
    is_anomaly = detector.predict(new_sample)
    
    if is_anomaly == -1:
        alert("Anomaly detected!")
```

### 8. Threshold tuning

```python
# –ó–∞–º—ñ—Å—Ç—å contamination, –º–æ–∂–Ω–∞ –∑–∞–¥–∞—Ç–∏ custom threshold

scores = clf.score_samples(X)

# –ú–µ—Ç–æ–¥ 1: Percentile
threshold = np.percentile(scores, 5)  # Bottom 5%

# –ú–µ—Ç–æ–¥ 2: Standard deviations
mean_score = np.mean(scores)
std_score = np.std(scores)
threshold = mean_score - 2 * std_score  # 2 std below

# –ú–µ—Ç–æ–¥ 3: Visual inspection (histogram)
plt.hist(scores, bins=50)
plt.axvline(threshold, color='red', linestyle='--')
plt.show()

# Predict –∑ custom threshold
y_pred_custom = (scores < threshold).astype(int)
```

### 9. Multivariate vs Univariate

```python
# Univariate: –æ–∫—Ä–µ–º–æ –¥–ª—è –∫–æ–∂–Ω–æ—ó feature
from scipy.stats import zscore

z_scores = np.abs(zscore(X, axis=0))
univariate_outliers = (z_scores > 3).any(axis=1)

# Multivariate: Isolation Forest
multivariate_outliers = (clf.predict(X) == -1)

# –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è
print(f"Univariate outliers: {univariate_outliers.sum()}")
print(f"Multivariate outliers: {multivariate_outliers.sum()}")
print(f"Both: {(univariate_outliers & multivariate_outliers).sum()}")
```

### 10. Visualize anomaly regions

```python
# –î–ª—è 2D –¥–∞–Ω–∏—Ö ‚Äî –ø–æ–∫–∞–∑–∞—Ç–∏ decision regions

if X.shape[1] == 2:
    # Mesh
    xx, yy = np.meshgrid(
        np.linspace(X[:, 0].min()-1, X[:, 0].max()+1, 100),
        np.linspace(X[:, 1].min()-1, X[:, 1].max()+1, 100)
    )
    
    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    
    plt.figure(figsize=(10, 7))
    plt.contourf(xx, yy, Z, levels=20, cmap='RdYlBu_r', alpha=0.6)
    plt.scatter(X[:, 0], X[:, 1], c='black', s=20)
    plt.colorbar(label='Anomaly Score')
    plt.title('Anomaly Regions')
    plt.show()
```

---

## –ü–æ—à–∏—Ä–µ–Ω—ñ –ø–æ–º–∏–ª–∫–∏ ‚ùå

### 1. –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∏–π contamination

```python
# ‚ùå Contamination = 0.5 (50%!)
# –ü–æ–ª–æ–≤–∏–Ω–∞ –¥–∞–Ω–∏—Ö ‚Äî –∞–Ω–æ–º–∞–ª—ñ—ó? Nonsense!

# ‚úÖ –†–æ–∑—É–º–Ω–∏–π contamination –Ω–∞ –æ—Å–Ω–æ–≤—ñ domain
contamination = 0.01  # 1% –¥–ª—è fraud
contamination = 0.05  # 5% –¥–ª—è sensor errors
```

### 2. –ù–µ –ø–µ—Ä–µ–≤—ñ—Ä—è—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏

```python
# ‚ùå –°–ª—ñ–ø–æ –¥–æ–≤—ñ—Ä—è—Ç–∏ predictions

# ‚úÖ –ó–∞–≤–∂–¥–∏ inspect
scores = clf.score_samples(X)

plt.hist(scores, bins=50)
plt.title('Distribution of Anomaly Scores')
plt.show()

# Check –Ω–∞–π–±—ñ–ª—å—à –∞–Ω–æ–º–∞–ª—å–Ω—ñ manually
```

### 3. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –Ω–∞ categorical data

```python
# ‚ùå Isolation Forest –ø–æ–≥–∞–Ω–æ –Ω–∞ categorical
X_cat = ['Red', 'Blue', 'Green', ...]

# ‚úÖ One-hot encode —Å–ø–æ—á–∞—Ç–∫—É, –ê–ë–û –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–π LOF
from sklearn.preprocessing import OneHotEncoder
encoder = OneHotEncoder()
X_encoded = encoder.fit_transform(X_cat)
```

### 4. –ó–∞–±—É—Ç–∏ –ø—Ä–æ data preprocessing

```python
# ‚ùå Missing values, duplicates
clf.fit(X_with_nans)  # Error!

# ‚úÖ Clean data —Å–ø–æ—á–∞—Ç–∫—É
X_clean = handle_missing_values(X)
X_clean = remove_duplicates(X_clean)
clf.fit(X_clean)
```

### 5. –ù–µ –≤–∞–ª—ñ–¥—É–≤–∞—Ç–∏ (—è–∫—â–æ —î labels)

```python
# –Ø–∫—â–æ —î labels ‚Äî USE THEM –¥–ª—è validation!

from sklearn.metrics import classification_report

y_pred = (clf.predict(X) == -1).astype(int)

print(classification_report(y_true, y_pred))

# Check precision/recall
```

---

## –†–µ–∞–ª—å–Ω—ñ –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è

### 1. Credit Card Fraud Detection

```python
# Features: amount, time, merchant, location
# Anomaly: fraudulent transactions

clf = IsolationForest(contamination=0.001)  # 0.1% fraud rate
clf.fit(transactions[features])

# Real-time detection
new_transaction = [amount, time, merchant_id, ...]
is_fraud = clf.predict([new_transaction])[0] == -1

if is_fraud:
    block_transaction()
    send_alert()
```

### 2. Network Intrusion Detection

```python
# Features: packet size, duration, protocol, ports
# Anomaly: attacks, malware

clf = IsolationForest(contamination=0.05)
clf.fit(network_traffic[features])

# Monitor
for packet in stream:
    score = clf.score_samples([packet])[0]
    
    if score < threshold:
        log_alert(packet)
```

### 3. Manufacturing Quality Control

```python
# Features: temperature, pressure, speed, vibration
# Anomaly: defective products

clf = IsolationForest(contamination=0.01)
clf.fit(sensor_readings[features])

# Detect defects
predictions = clf.predict(new_batch)
defect_indices = np.where(predictions == -1)[0]

reject_items(defect_indices)
```

### 4. Health Monitoring (IoT sensors)

```python
# Features: heart rate, blood pressure, activity
# Anomaly: health issues

clf = IsolationForest(contamination=0.05)
clf.fit(patient_data[features])

# Real-time monitoring
current_readings = get_sensor_data()
is_abnormal = clf.predict([current_readings])[0] == -1

if is_abnormal:
    notify_doctor()
```

---

## –ü–æ–≤'—è–∑–∞–Ω—ñ —Ç–µ–º–∏

- [[02_One_Class_SVM]] ‚Äî —ñ–Ω—à–∏–π unsupervised –º–µ—Ç–æ–¥
- [[03_Local_Outlier_Factor]] ‚Äî density-based detection
- [[04_Anomaly_Detection_Methods]] ‚Äî –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –º–µ—Ç–æ–¥—ñ–≤
- [[Autoencoders]] ‚Äî deep learning approach

## –†–µ—Å—É—Ä—Å–∏

- [Original Paper (Liu et al., 2008)](https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf)
- [Scikit-learn: Isolation Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html)
- [Anomaly Detection with Isolation Forest](https://towardsdatascience.com/anomaly-detection-with-isolation-forest-e41f1f55cc6)

---

## –ö–ª—é—á–æ–≤—ñ –≤–∏—Å–Ω–æ–≤–∫–∏

> Isolation Forest ‚Äî tree-based –∞–ª–≥–æ—Ä–∏—Ç–º –¥–ª—è anomaly detection, —â–æ –±–∞–∑—É—î—Ç—å—Å—è –Ω–∞ –ø—Ä–∏–Ω—Ü–∏–ø—ñ: –∞–Ω–æ–º–∞–ª—ñ—ó –ª–µ–≥—à–µ —ñ–∑–æ–ª—é–≤–∞—Ç–∏ –≤–∏–ø–∞–¥–∫–æ–≤–∏–º–∏ splits –Ω—ñ–∂ –Ω–æ—Ä–º–∞–ª—å–Ω—ñ —Ç–æ—á–∫–∏, —Ç–æ–º—É –º–∞—é—Ç—å shorter average path length –≤ isolation trees.

**–û—Å–Ω–æ–≤–Ω–∞ —ñ–¥–µ—è:**
- –ê–Ω–æ–º–∞–ª—ñ—ó = "few and different"
- –õ–µ–≥–∫–æ —ñ–∑–æ–ª—é–≤–∞—Ç–∏ random splits
- Short path ‚Üí high anomaly score

**–ê–ª–≥–æ—Ä–∏—Ç–º:**
1. –ü–æ–±—É–¥—É–≤–∞—Ç–∏ ensemble isolation trees (random features + splits)
2. –î–ª—è –∫–æ–∂–Ω–æ—ó —Ç–æ—á–∫–∏ –æ–±—á–∏—Å–ª–∏—Ç–∏ average path length
3. –ù–æ—Ä–º–∞–ª—ñ–∑—É–≤–∞—Ç–∏: $s(x) = 2^{-h(x)/c(n)}$
4. Threshold –¥–ª—è classification

**Anomaly score:**
- s ‚âà 1 ‚Üí Anomaly
- s ‚âà 0.5 ‚Üí Normal
- s < 0.5 ‚Üí Definitely normal

**–ü–µ—Ä–µ–≤–∞–≥–∏:**
- ‚ö° –î—É–∂–µ —à–≤–∏–¥–∫–∏–π (O(n log n))
- üìà –ú–∞—Å—à—Ç–∞–±—É—î—Ç—å—Å—è –Ω–∞ –≤–µ–ª–∏–∫—ñ –¥–∞–Ω—ñ
- üî¢ –î–æ–±—Ä–µ –Ω–∞ high-dimensional
- üíæ Memory efficient (subsampling)

**–ù–µ–¥–æ–ª—ñ–∫–∏:**
- ‚ùå –ü–æ—Ç—Ä–µ–±—É—î contamination parameter
- ‚ùå –ü–æ–≥–∞–Ω–æ –Ω–∞ categorical features
- ‚ùå –ú–æ–∂–µ –ø—Ä–æ–ø—É—Å–∫–∞—Ç–∏ –ª–æ–∫–∞–ª—å–Ω—ñ outliers

**–ö–ª—é—á–æ–≤—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏:**
- **contamination:** % –æ—á—ñ–∫—É–≤–∞–Ω–∏—Ö –∞–Ω–æ–º–∞–ª—ñ–π (CRITICAL!)
- **n_estimators:** –∫—ñ–ª—å–∫—ñ—Å—Ç—å –¥–µ—Ä–µ–≤ (100 default)
- **max_samples:** —Ä–æ–∑–º—ñ—Ä sample (256 default)

**–ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏:**
- –í–µ–ª–∏–∫—ñ –¥–∞–Ω—ñ + —à–≤–∏–¥–∫—ñ—Å—Ç—å = Isolation Forest ‚úì
- –õ–æ–∫–∞–ª—å–Ω—ñ outliers ‚Üí LOF ‚úì
- –°–∫–ª–∞–¥–Ω—ñ patterns ‚Üí Autoencoder ‚úì
- Labeled data ‚Üí Supervised ‚úì

**–ü—Ä–∞–∫—Ç–∏—á–Ω—ñ –ø–æ—Ä–∞–¥–∏:**
- –ù–∞–ª–∞—à—Ç—É–π contamination –ø—ñ–¥ domain
- –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π scores –¥–ª—è ranking
- –í–∞–ª—ñ–¥—É–π —è–∫—â–æ —î labels
- Ensemble –∑ LOF –¥–ª—è –∫—Ä–∞—â–∏—Ö results
- Incremental refit –¥–ª—è streaming

---

#ml #unsupervised-learning #anomaly-detection #outlier-detection #isolation-forest #fraud-detection #security #tree-based
