# Local Outlier Factor (LOF)

## –©–æ —Ü–µ?

**LOF (Local Outlier Factor)** ‚Äî —Ü–µ **density-based** –∞–ª–≥–æ—Ä–∏—Ç–º –¥–ª—è –≤–∏—è–≤–ª–µ–Ω–Ω—è **–ª–æ–∫–∞–ª—å–Ω–∏—Ö –∞–Ω–æ–º–∞–ª—ñ–π** (outliers), —â–æ –ø–æ—Ä—ñ–≤–Ω—é—î **–ª–æ–∫–∞–ª—å–Ω—É —â—ñ–ª—å–Ω—ñ—Å—Ç—å** —Ç–æ—á–∫–∏ –∑ —â—ñ–ª—å–Ω—ñ—Å—Ç—é —ó—ó —Å—É—Å—ñ–¥—ñ–≤. –¢–æ—á–∫–∏ –≤ —Ä–µ–≥—ñ–æ–Ω–∞—Ö –∑ –Ω–∏–∑—å–∫–æ—é —â—ñ–ª—å–Ω—ñ—Å—Ç—é –≤—ñ–¥–Ω–æ—Å–Ω–æ —Å—É—Å—ñ–¥—ñ–≤ –≤–≤–∞–∂–∞—é—Ç—å—Å—è –∞–Ω–æ–º–∞–ª—ñ—è–º–∏.

**–ì–æ–ª–æ–≤–Ω–∞ —ñ–¥–µ—è:** –∞–Ω–æ–º–∞–ª—ñ—è ‚Äî —Ü–µ —Ç–æ—á–∫–∞, —á–∏—è –ª–æ–∫–∞–ª—å–Ω–∞ —â—ñ–ª—å–Ω—ñ—Å—Ç—å –∑–Ω–∞—á–Ω–æ –Ω–∏–∂—á–∞ –∑–∞ —â—ñ–ª—å–Ω—ñ—Å—Ç—å —ó—ó —Å—É—Å—ñ–¥—ñ–≤. LOF –≤–∏–º—ñ—Ä—é—î –Ω–∞—Å–∫—ñ–ª—å–∫–∏ "—ñ–∑–æ–ª—å–æ–≤–∞–Ω–æ—é" —î —Ç–æ—á–∫–∞ –ø–æ—Ä—ñ–≤–Ω—è–Ω–æ –∑ –æ—Ç–æ—á–µ–Ω–Ω—è–º.

## –ù–∞–≤—ñ—â–æ –ø–æ—Ç—Ä—ñ–±–µ–Ω?

- üéØ **–õ–æ–∫–∞–ª—å–Ω—ñ outliers** ‚Äî –∞–Ω–æ–º–∞–ª—ñ—ó –≤ –ª–æ–∫–∞–ª—å–Ω–æ–º—É –∫–æ–Ω—Ç–µ–∫—Å—Ç—ñ
- üìä **Density-based** ‚Äî –≤—Ä–∞—Ö–æ–≤—É—î —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–∏—Ö
- üîç **Cluster outliers** ‚Äî —Ç–æ—á–∫–∏ –¥–∞–ª–µ–∫–æ –≤—ñ–¥ –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤
- üåê **Variable density** ‚Äî –ø—Ä–∞—Ü—é—î –∑ —Ä—ñ–∑–Ω–æ—é —â—ñ–ª—å–Ω—ñ—Å—Ç—é
- üìà **Degree of outlierness** ‚Äî –Ω–µ binary, –∞ score
- üè• **Medical diagnosis** ‚Äî –ª–æ–∫–∞–ª—å–Ω—ñ –∞–Ω–æ–º–∞–ª—ñ—ó –≤ –∞–Ω–∞–ª—ñ–∑–∞—Ö
- üè≠ **Sensor data** ‚Äî –Ω–µ—Å–ø—Ä–∞–≤–Ω–æ—Å—Ç—ñ –æ–±–ª–∞–¥–Ω–∞–Ω–Ω—è

## –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏?

**–ü–æ—Ç—Ä—ñ–±–Ω–æ:**
- **–õ–æ–∫–∞–ª—å–Ω—ñ outliers** ‚Äî –≤–∞–∂–ª–∏–≤–∞ –ª–æ–∫–∞–ª—å–Ω–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞
- **Variable density** ‚Äî —Ä—ñ–∑–Ω—ñ —â—ñ–ª—å–Ω–æ—Å—Ç—ñ –≤ –¥–∞–Ω–∏—Ö
- **Clusters** ‚Äî outliers –º—ñ–∂/–Ω–∞–≤–∫–æ–ª–æ –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤
- **Interpretable scores** ‚Äî LOF score –º–∞—î –∑–Ω–∞—á–µ–Ω–Ω—è
- **–°–µ—Ä–µ–¥–Ω—ñ –¥–∞–Ω—ñ** (100-50,000 –∑—Ä–∞–∑–∫—ñ–≤)
- **Numerical features** ‚Äî –Ω–µ–ø–µ—Ä–µ—Ä–≤–Ω—ñ –¥–∞–Ω—ñ

**–ù–µ –ø–æ—Ç—Ä—ñ–±–Ω–æ:**
- **–î—É–∂–µ –≤–µ–ª–∏–∫—ñ –¥–∞–Ω—ñ** (> 100K) ‚Üí Isolation Forest
- **High-dimensional** (> 50 features) ‚Üí Isolation Forest
- **Global outliers** —Ç—ñ–ª—å–∫–∏ ‚Üí Isolation Forest OK
- **Real-time streaming** ‚Üí –ø—Ä–æ—Å—Ç—ñ—à—ñ –º–µ—Ç–æ–¥–∏
- **–®–≤–∏–¥–∫—ñ—Å—Ç—å –∫—Ä–∏—Ç–∏—á–Ω–∞** ‚Üí Isolation Forest

---

## –Ü–Ω—Ç—É—ó—Ü—ñ—è

### –õ–æ–∫–∞–ª—å–Ω—ñ vs –ì–ª–æ–±–∞–ª—å–Ω—ñ outliers

**–ü—Ä–æ–±–ª–µ–º–∞ –∑ –≥–ª–æ–±–∞–ª—å–Ω–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏:**

```
Cluster 1 (dense):        Cluster 2 (sparse):
    ‚óè‚óè‚óè‚óè‚óè                     ‚óã  ‚óã
    ‚óè‚óè‚óè‚óè‚óè                     ‚óã  ‚óã
    ‚óè‚óè‚óè‚óè‚óè                   ‚óã  ‚òÖ  ‚óã
    ‚óè‚óè‚óè‚óè‚óè                     ‚óã  ‚óã
                              ‚óã  ‚óã

‚òÖ ‚Äî –ª–æ–∫–∞–ª—å–Ω–∏–π outlier –≤ Cluster 2
–ì–ª–æ–±–∞–ª—å–Ω–æ: –Ω–æ—Ä–º–∞–ª—å–Ω–∞ –≤—ñ–¥—Å—Ç–∞–Ω—å –≤—ñ–¥ —ñ–Ω—à–∏—Ö
–õ–æ–∫–∞–ª—å–Ω–æ: –¥–∞–ª–µ–∫–æ –≤—ñ–¥ —Å–≤–æ—ó—Ö —Å—É—Å—ñ–¥—ñ–≤ (Cluster 2)

–ó–≤–∏—á–∞–π–Ω—ñ –º–µ—Ç–æ–¥–∏: ‚úó –ø—Ä–æ–ø—É—Å—Ç—è—Ç—å
LOF: ‚úì –∑–Ω–∞–π–¥–µ!
```

### –ü—Ä–∏–∫–ª–∞–¥

```
Dense region:              Sparse region:
   ‚óè‚óè‚óè‚óè‚óè                     ‚óã    ‚óã
   ‚óè‚óè‚óè‚óè‚óè                   ‚óã  ‚òÖ    ‚óã
   ‚óè‚óè‚óè‚óè‚óè                     ‚óã    ‚óã
   ‚óè‚óè‚óè‚óè‚óè

–¢–æ—á–∫–∞ –≤ dense:             –¢–æ—á–∫–∞ ‚òÖ –≤ sparse:
- –ë–ª–∏–∑—å–∫–æ –¥–æ –±–∞–≥–∞—Ç—å–æ—Ö      - –î–∞–ª–µ–∫–æ –≤—ñ–¥ —Å—É—Å—ñ–¥—ñ–≤
- LOF ‚âà 1 (normal)         - LOF > 1 (outlier!)
```

---

## –ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞

### 1. k-distance

**k-distance(A)** ‚Äî –≤—ñ–¥—Å—Ç–∞–Ω—å –≤—ñ–¥ —Ç–æ—á–∫–∏ A –¥–æ k-–≥–æ –Ω–∞–π–±–ª–∏–∂—á–æ–≥–æ —Å—É—Å—ñ–¥–∞.

```python
k = 5  # parameter

Point A:
    ‚óè‚ÇÖ
  ‚óè‚ÇÑ  ‚óè‚ÇÉ
 A ‚óè‚ÇÇ ‚óè‚ÇÅ

k-distance(A) = distance to 5th neighbor
```

### 2. Reachability Distance

**–†–µ–∞–ª—å–Ω–∞ –≤—ñ–¥—Å—Ç–∞–Ω—å –∑ "–ø–æ—Ä–æ–≥–æ–º":**

$$\text{reach-dist}_k(A, B) = \max\{\text{k-distance}(B), d(A, B)\}$$

**–Ü–Ω—Ç—É—ó—Ü—ñ—è:**
- –Ø–∫—â–æ A –¥–∞–ª–µ–∫–æ –≤—ñ–¥ B ‚Üí –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π —Ä–µ–∞–ª—å–Ω—É –≤—ñ–¥—Å—Ç–∞–Ω—å
- –Ø–∫—â–æ A –±–ª–∏–∑—å–∫–æ ‚Üí –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π k-distance(B) —è–∫ –º—ñ–Ω—ñ–º—É–º

**–ß–æ–º—É?** –ó–≥–ª–∞–¥–∂—É—î —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–Ω—ñ —Ñ–ª—É–∫—Ç—É–∞—Ü—ñ—ó –¥–ª—è –±–ª–∏–∑—å–∫–∏—Ö —Ç–æ—á–æ–∫.

```
B's k-neighborhood:
    ‚óè‚óè‚óè‚óè‚óè
    ‚óè B ‚óè   A –¥–∞–ª–µ–∫–æ
    ‚óè‚óè‚óè‚óè‚óè     ‚Üì
              ‚óèA

reach-dist(A, B) = real distance (A –¥–∞–ª–µ–∫–æ)

    ‚óè‚óè‚óè‚óè‚óè
    ‚óè B ‚óè
    ‚óè‚óèA‚óè‚óè   A –±–ª–∏–∑—å–∫–æ

reach-dist(A, B) = k-distance(B) (–≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π –ø–æ—Ä—ñ–≥)
```

### 3. Local Reachability Density (LRD)

**–õ–æ–∫–∞–ª—å–Ω–∞ —â—ñ–ª—å–Ω—ñ—Å—Ç—å —Ç–æ—á–∫–∏ A:**

$$\text{LRD}_k(A) = \frac{1}{\frac{\sum_{B \in N_k(A)} \text{reach-dist}_k(A, B)}{|N_k(A)|}}$$

–¥–µ $N_k(A)$ ‚Äî k –Ω–∞–π–±–ª–∏–∂—á–∏—Ö —Å—É—Å—ñ–¥—ñ–≤ A.

**–Ü–Ω—Ç—É—ó—Ü—ñ—è:**
- LRD = 1 / (—Å–µ—Ä–µ–¥–Ω—è reachability distance –¥–æ —Å—É—Å—ñ–¥—ñ–≤)
- **–í–∏—Å–æ–∫–∞ —â—ñ–ª—å–Ω—ñ—Å—Ç—å** ‚Üí –º–∞–ª—ñ –≤—ñ–¥—Å—Ç–∞–Ω—ñ ‚Üí **–≤–∏—Å–æ–∫–∏–π LRD**
- **–ù–∏–∑—å–∫–∞ —â—ñ–ª—å–Ω—ñ—Å—Ç—å** ‚Üí –≤–µ–ª–∏–∫—ñ –≤—ñ–¥—Å—Ç–∞–Ω—ñ ‚Üí **–Ω–∏–∑—å–∫–∏–π LRD**

### 4. Local Outlier Factor (LOF)

**–ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è —â—ñ–ª—å–Ω–æ—Å—Ç—ñ A –∑ —Å—É—Å—ñ–¥–∞–º–∏:**

$$\text{LOF}_k(A) = \frac{\sum_{B \in N_k(A)} \frac{\text{LRD}_k(B)}{\text{LRD}_k(A)}}{|N_k(A)|}$$

**–°–ø—Ä–æ—â–µ–Ω–æ:**

$$\text{LOF}_k(A) = \frac{\text{Average LRD of neighbors}}{\text{LRD of A}}$$

**–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü—ñ—è:**

```
LOF ‚âà 1     ‚Üí Normal (—Å—Ö–æ–∂–∞ —â—ñ–ª—å–Ω—ñ—Å—Ç—å –∑ —Å—É—Å—ñ–¥–∞–º–∏)
LOF > 1     ‚Üí Outlier (–Ω–∏–∂—á–∞ —â—ñ–ª—å–Ω—ñ—Å—Ç—å)
LOF >> 1    ‚Üí Strong outlier (–Ω–∞–±–∞–≥–∞—Ç–æ –Ω–∏–∂—á–∞)

Typically:
LOF < 1.5   ‚Üí Normal
LOF > 2.0   ‚Üí Outlier
```

---

## –ê–ª–≥–æ—Ä–∏—Ç–º

### –ü–æ–∫—Ä–æ–∫–æ–≤–∏–π –ø—Ä–æ—Ü–µ—Å

```
–î–∞–Ω–æ: Dataset X, parameter k

FOR –∫–æ–∂–Ω–æ—ó —Ç–æ—á–∫–∏ A –≤ X:
    
    1. –ó–Ω–∞–π—Ç–∏ k-distance(A)
       - –í—ñ–¥—Å—Ç–∞–Ω—å –¥–æ k-–≥–æ –Ω–∞–π–±–ª–∏–∂—á–æ–≥–æ —Å—É—Å—ñ–¥–∞
    
    2. –ó–Ω–∞–π—Ç–∏ k-nearest neighbors N_k(A)
    
    3. –î–ª—è –∫–æ–∂–Ω–æ–≥–æ —Å—É—Å—ñ–¥–∞ B –≤ N_k(A):
       –û–±—á–∏—Å–ª–∏—Ç–∏ reach-dist_k(A, B) = max{k-distance(B), d(A, B)}
    
    4. –û–±—á–∏—Å–ª–∏—Ç–∏ LRD_k(A):
       LRD_k(A) = k / Œ£ reach-dist_k(A, B)
    
    5. –û–±—á–∏—Å–ª–∏—Ç–∏ LOF_k(A):
       LOF_k(A) = (Œ£ LRD_k(B) / LRD_k(A)) / k
       –¥–µ B –≤ N_k(A)

RETURN LOF scores –¥–ª—è –≤—Å—ñ—Ö —Ç–æ—á–æ–∫
```

### –ü—Å–µ–≤–¥–æ–∫–æ–¥

```python
def LOF(X, k):
    n = len(X)
    lof_scores = []
    
    # –û–±—á–∏—Å–ª–∏—Ç–∏ –≤—Å—ñ –≤—ñ–¥—Å—Ç–∞–Ω—ñ
    distances = compute_distances(X)
    
    FOR i in range(n):
        # 1. k-distance —Ç–∞ k-neighbors
        k_dist = k_distance(distances[i], k)
        neighbors = k_neighbors(distances[i], k)
        
        # 2. Reachability distances
        reach_dists = []
        FOR j in neighbors:
            rd = max(k_distance(distances[j], k), distances[i][j])
            reach_dists.append(rd)
        
        # 3. Local Reachability Density
        lrd_i = k / sum(reach_dists)
        
        # 4. LOF score
        lrd_neighbors = []
        FOR j in neighbors:
            lrd_j = compute_lrd(j, k)
            lrd_neighbors.append(lrd_j)
        
        lof_i = mean(lrd_neighbors) / lrd_i
        lof_scores.append(lof_i)
    
    RETURN lof_scores
```

---

## –ö–æ–¥ (Python + scikit-learn)

### –ë–∞–∑–æ–≤–∏–π –ø—Ä–∏–∫–ª–∞–¥

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.neighbors import LocalOutlierFactor
from sklearn.datasets import make_blobs

# 1. –ì–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ –¥–∞–Ω—ñ –∑ —Ä—ñ–∑–Ω–æ—é —â—ñ–ª—å–Ω—ñ—Å—Ç—é
np.random.seed(42)

# Dense cluster
X_dense, _ = make_blobs(
    n_samples=200,
    centers=[[0, 0]],
    cluster_std=0.3,
    random_state=42
)

# Sparse cluster
X_sparse, _ = make_blobs(
    n_samples=50,
    centers=[[4, 4]],
    cluster_std=0.8,
    random_state=42
)

# Combine
X_normal = np.vstack([X_dense, X_sparse])

# –î–æ–¥–∞—Ç–∏ outliers
X_outliers = np.array([
    [0, 3],      # Between clusters
    [-2, -2],    # Far from dense
    [6, 6],      # Far from sparse (–ª–æ–∫–∞–ª—å–Ω–∏–π outlier!)
    [2, 2]       # Between clusters
])

X = np.vstack([X_normal, X_outliers])
y_true = np.array([0]*250 + [1]*4)  # 0=normal, 1=outlier

print(f"Total points: {len(X)}")
print(f"Normal: {(y_true == 0).sum()}")
print(f"Outliers: {(y_true == 1).sum()}")

# 2. Fit LOF
clf = LocalOutlierFactor(
    n_neighbors=20,        # k parameter
    contamination=0.05,    # Expected fraction of outliers
    novelty=False          # False = fit_predict, True = fit + predict new
)

# fit_predict –Ω–∞ training data
y_pred = clf.fit_predict(X)  # 1=inlier, -1=outlier

# Negative outlier factor (—á–∏–º –º–µ–Ω—à–µ, —Ç–∏–º –±—ñ–ª—å—à–∞ –∞–Ω–æ–º–∞–ª—ñ—è)
lof_scores = -clf.negative_outlier_factor_

print(f"\n=== Predictions ===")
print(f"Predicted outliers: {(y_pred == -1).sum()}")

# 3. Metrics
from sklearn.metrics import classification_report, confusion_matrix

y_pred_binary = (y_pred == -1).astype(int)

print("\n=== Classification Report ===")
print(classification_report(y_true, y_pred_binary,
                           target_names=['Normal', 'Outlier']))

# 4. –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# Plot 1: True labels
axes[0].scatter(X[y_true == 0, 0], X[y_true == 0, 1],
               c='blue', s=20, alpha=0.6, label='Normal')
axes[0].scatter(X[y_true == 1, 0], X[y_true == 1, 1],
               c='red', s=100, marker='x', linewidths=2, label='True Outlier')
axes[0].set_title('True Labels', fontsize=13, fontweight='bold')
axes[0].set_xlabel('Feature 1')
axes[0].set_ylabel('Feature 2')
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# Plot 2: LOF scores
scatter = axes[1].scatter(X[:, 0], X[:, 1],
                         c=lof_scores, cmap='RdYlBu_r',
                         s=30, alpha=0.7, edgecolors='black', linewidths=0.5)
plt.colorbar(scatter, ax=axes[1], label='LOF Score\n(higher = more anomalous)')

# Mark predicted outliers
outlier_mask = y_pred == -1
axes[1].scatter(X[outlier_mask, 0], X[outlier_mask, 1],
               facecolors='none', edgecolors='red',
               s=150, linewidths=2, label='Predicted Outliers')

axes[1].set_title('LOF Scores and Predictions', fontsize=13, fontweight='bold')
axes[1].set_xlabel('Feature 1')
axes[1].set_ylabel('Feature 2')
axes[1].legend()
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Top outliers
top_outlier_indices = np.argsort(lof_scores)[-10:][::-1]

print("\n=== Top 10 Outliers ===")
for idx in top_outlier_indices:
    print(f"Index {idx}: LOF = {lof_scores[idx]:.3f}, "
          f"Point = {X[idx]}, "
          f"True label = {'Outlier' if y_true[idx] == 1 else 'Normal'}")
```

### Novelty Detection mode

```python
# LOF –¥–ª—è novelty detection (train –Ω–∞ normal, predict –Ω–∞ new)

# Train —Ç—ñ–ª—å–∫–∏ –Ω–∞ normal data
X_train = X_normal.copy()

# LOF –∑ novelty=True
clf_novelty = LocalOutlierFactor(
    n_neighbors=20,
    contamination=0.05,
    novelty=True  # ‚Üê Enable predict –¥–ª—è –Ω–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö
)

clf_novelty.fit(X_train)

# Test data (–Ω–æ–≤—ñ —Ç–æ—á–∫–∏)
X_test = np.vstack([
    X_sparse[:10],     # Normal from sparse cluster
    X_outliers         # Outliers
])

y_test_true = np.array([0]*10 + [1]*4)

# Predict –Ω–∞ –Ω–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö
y_test_pred = clf_novelty.predict(X_test)  # –¢–µ–ø–µ—Ä –ø—Ä–∞—Ü—é—î!
scores_test = clf_novelty.score_samples(X_test)

print("\n=== Novelty Detection ===")
print(f"Test set: {len(X_test)} samples")
print(f"Predicted outliers: {(y_test_pred == -1).sum()}")

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
plt.figure(figsize=(10, 7))

# Training data
plt.scatter(X_train[:, 0], X_train[:, 1],
           c='blue', s=20, alpha=0.3, label='Training (Normal)')

# Test normal
plt.scatter(X_test[y_test_true == 0, 0], X_test[y_test_true == 0, 1],
           c='green', s=50, marker='s', alpha=0.7, label='Test Normal')

# Test outliers
plt.scatter(X_test[y_test_true == 1, 0], X_test[y_test_true == 1, 1],
           c='red', s=100, marker='x', linewidths=2, label='Test Outlier')

# Predicted outliers
pred_outliers = y_test_pred == -1
plt.scatter(X_test[pred_outliers, 0], X_test[pred_outliers, 1],
           facecolors='none', edgecolors='orange',
           s=200, linewidths=2, label='Predicted Outlier')

plt.title('LOF Novelty Detection', fontsize=14, fontweight='bold')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

### –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è —Ä—ñ–∑–Ω–∏—Ö k

```python
# –í–ø–ª–∏–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ k (n_neighbors)

k_values = [5, 10, 20, 50]

fig, axes = plt.subplots(2, 2, figsize=(14, 12))
axes = axes.ravel()

for idx, k in enumerate(k_values):
    print(f"\nk = {k}")
    
    clf = LocalOutlierFactor(n_neighbors=k, contamination=0.05)
    y_pred = clf.fit_predict(X)
    lof_scores = -clf.negative_outlier_factor_
    
    # Plot
    scatter = axes[idx].scatter(X[:, 0], X[:, 1],
                               c=lof_scores, cmap='RdYlBu_r',
                               s=30, alpha=0.7, edgecolors='black',
                               linewidths=0.5)
    
    plt.colorbar(scatter, ax=axes[idx], label='LOF Score')
    
    # Mark predicted outliers
    outliers = y_pred == -1
    axes[idx].scatter(X[outliers, 0], X[outliers, 1],
                     facecolors='none', edgecolors='red',
                     s=150, linewidths=2)
    
    from sklearn.metrics import accuracy_score
    acc = accuracy_score(y_true, (y_pred == -1).astype(int))
    
    axes[idx].set_title(f'k = {k} (Accuracy: {acc:.2f})',
                       fontsize=12, fontweight='bold')
    axes[idx].set_xlabel('Feature 1')
    axes[idx].set_ylabel('Feature 2')
    axes[idx].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### Real example: Credit Card Fraud

```python
# –°–∏–º—É–ª—é—î–º–æ credit card transactions

np.random.seed(42)

# Normal transactions (2 —Ç–∏–ø–∏ –ø–æ–≤–µ–¥—ñ–Ω–∫–∏)
# Type 1: Regular small purchases
n_regular = 4000
amount_regular = np.random.lognormal(mean=3, sigma=0.5, size=n_regular)
time_regular = np.random.uniform(0, 24, size=n_regular)

# Type 2: Occasional large purchases
n_large = 1000
amount_large = np.random.lognormal(mean=5, sigma=0.3, size=n_large)
time_large = np.random.uniform(0, 24, size=n_large)

X_normal = np.vstack([
    np.column_stack([amount_regular, time_regular]),
    np.column_stack([amount_large, time_large])
])

# Fraudulent transactions (—Ä—ñ–∑–Ω—ñ patterns)
n_fraud = 100

# Type 1: Very high amounts at night
amount_fraud1 = np.random.uniform(1000, 5000, size=n_fraud//2)
time_fraud1 = np.random.uniform(2, 5, size=n_fraud//2)  # 2-5 AM

# Type 2: Rapid succession (–ª–æ–∫–∞–ª—å–Ω–∏–π outlier –∑–∞ —á–∞—Å–æ–º)
amount_fraud2 = np.random.uniform(300, 800, size=n_fraud//2)
time_fraud2 = np.random.choice([3.1, 3.2, 3.3, 3.4], size=n_fraud//2)  # Clustered in time

X_fraud = np.vstack([
    np.column_stack([amount_fraud1, time_fraud1]),
    np.column_stack([amount_fraud2, time_fraud2])
])

# Combine
X_transactions = np.vstack([X_normal, X_fraud])
y_true = np.array([0]*len(X_normal) + [1]*n_fraud)

print(f"Total transactions: {len(X_transactions)}")
print(f"Fraud rate: {(y_true == 1).sum() / len(y_true):.2%}")

# Feature scaling
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_transactions)

# LOF
clf = LocalOutlierFactor(
    n_neighbors=30,
    contamination=0.02,  # Expect 2% fraud
    novelty=False
)

y_pred = clf.fit_predict(X_scaled)
lof_scores = -clf.negative_outlier_factor_

# Metrics
from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score

y_pred_binary = (y_pred == -1).astype(int)

print("\n=== Fraud Detection Results ===")
print(f"Precision: {precision_score(y_true, y_pred_binary):.3f}")
print(f"Recall: {recall_score(y_true, y_pred_binary):.3f}")
print(f"F1-Score: {f1_score(y_true, y_pred_binary):.3f}")
print(f"ROC-AUC: {roc_auc_score(y_true, lof_scores):.3f}")

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# True labels
axes[0].scatter(X_transactions[y_true == 0, 0],
               X_transactions[y_true == 0, 1],
               c='green', s=5, alpha=0.3, label='Legitimate')
axes[0].scatter(X_transactions[y_true == 1, 0],
               X_transactions[y_true == 1, 1],
               c='red', s=30, marker='x', linewidths=1.5, label='Fraud')
axes[0].set_xlabel('Transaction Amount ($)')
axes[0].set_ylabel('Time of Day (hour)')
axes[0].set_title('True Labels', fontsize=13, fontweight='bold')
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# LOF predictions
scatter = axes[1].scatter(X_transactions[:, 0], X_transactions[:, 1],
                         c=lof_scores, cmap='RdYlBu_r',
                         s=5, alpha=0.5)
plt.colorbar(scatter, ax=axes[1], label='LOF Score')

# Predicted fraud
fraud_mask = y_pred == -1
axes[1].scatter(X_transactions[fraud_mask, 0],
               X_transactions[fraud_mask, 1],
               facecolors='none', edgecolors='red',
               s=50, linewidths=1.5, label='Predicted Fraud')

axes[1].set_xlabel('Transaction Amount ($)')
axes[1].set_ylabel('Time of Day (hour)')
axes[1].set_title('LOF Predictions', fontsize=13, fontweight='bold')
axes[1].legend()
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

---

## –ü–∞—Ä–∞–º–µ—Ç—Ä–∏

### –û—Å–Ω–æ–≤–Ω—ñ

```python
LocalOutlierFactor(
    n_neighbors=20,         # k parameter (–∫—Ä–∏—Ç–∏—á–Ω–∏–π!)
    contamination=0.1,      # Expected fraction outliers
    novelty=False,          # False=fit_predict, True=fit+predict
    algorithm='auto',       # 'ball_tree', 'kd_tree', 'brute'
    metric='minkowski',     # Distance metric
    p=2                     # Minkowski parameter (2=euclidean)
)
```

| –ü–∞—Ä–∞–º–µ—Ç—Ä | –û–ø–∏—Å | –¢–∏–ø–æ–≤—ñ –∑–Ω–∞—á–µ–Ω–Ω—è | –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó |
|----------|------|-----------------|--------------|
| **n_neighbors** | k (—Ä–æ–∑–º—ñ—Ä –æ–∫–æ–ª—É) | 10-50 | 20 (default) –¥–æ–±—Ä–µ –¥–ª—è –ø–æ—á–∞—Ç–∫—É |
| **contamination** | % outliers | 0.01-0.2 | –ë–∞–∑—É—î—Ç—å—Å—è –Ω–∞ domain knowledge |
| **novelty** | Mode | False/True | False –¥–ª—è fit_predict, True –¥–ª—è predict new |
| **metric** | Distance | 'euclidean', 'manhattan' | 'euclidean' –¥–ª—è –±—ñ–ª—å—à–æ—Å—Ç—ñ |

### n_neighbors (k) ‚Äî –Ω–∞–π–≤–∞–∂–ª–∏–≤—ñ—à–∏–π!

**–í–ø–ª–∏–≤:**

```python
k = 5     # –ú–∞–ª–∏–π k ‚Üí —á—É—Ç–ª–∏–≤–∏–π –¥–æ –ª–æ–∫–∞–ª—å–Ω–∏—Ö —Ñ–ª—É–∫—Ç—É–∞—Ü—ñ–π
k = 20    # –°–µ—Ä–µ–¥–Ω—ñ–π ‚Üí –±–∞–ª–∞–Ω—Å
k = 50    # –í–µ–ª–∏–∫–∏–π k ‚Üí –±—ñ–ª—å—à –≥–ª–æ–±–∞–ª—å–Ω–∞ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–∞
```

**–í—ñ–∑—É–∞–ª—å–Ω–æ:**

```
Small k (5):
    ‚óè‚óè‚óè
    ‚óè ‚óè ‚Üê –º–æ–∂–µ –≤–∏–∑–Ω–∞—á–∏—Ç–∏ —è–∫ outlier (–º–∞–ª–æ —Å—É—Å—ñ–¥—ñ–≤)
    ‚óè‚óè‚óè

Large k (50):
    ‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
    ‚óè       ‚óè
    ‚óè   ‚óè   ‚óè ‚Üê –Ω–æ—Ä–º–∞–ª—å–Ω–∏–π (–±–∞–≥–∞—Ç–æ —Å—É—Å—ñ–¥—ñ–≤ –≤ —Ä–∞–¥—ñ—É—Å—ñ)
    ‚óè       ‚óè
    ‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
```

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó:**
- **–©—ñ–ª—å–Ω—ñ –¥–∞–Ω—ñ:** k = 10-20
- **–†–æ–∑—Ä—ñ–¥–∂–µ–Ω—ñ –¥–∞–Ω—ñ:** k = 30-50
- **–ù–µ–≤–∏–∑–Ω–∞—á–µ–Ω—ñ—Å—Ç—å:** —Å–ø—Ä–æ–±—É–≤–∞—Ç–∏ –∫—ñ–ª—å–∫–∞ k —Ç–∞ –ø–æ—Ä—ñ–≤–Ω—è—Ç–∏

---

## –ü–µ—Ä–µ–≤–∞–≥–∏ —Ç–∞ –Ω–µ–¥–æ–ª—ñ–∫–∏

### –ü–µ—Ä–µ–≤–∞–≥–∏ ‚úì

| –ü–µ—Ä–µ–≤–∞–≥–∞ | –ü–æ—è—Å–Ω–µ–Ω–Ω—è |
|----------|-----------|
| **–õ–æ–∫–∞–ª—å–Ω—ñ outliers** | –ó–Ω–∞—Ö–æ–¥–∏—Ç—å outliers –≤ –ª–æ–∫–∞–ª—å–Ω–æ–º—É –∫–æ–Ω—Ç–µ–∫—Å—Ç—ñ |
| **Variable density** | –ü—Ä–∞—Ü—é—î –∑ —Ä—ñ–∑–Ω–æ—é —â—ñ–ª—å–Ω—ñ—Å—Ç—é |
| **Interpretable score** | LOF –º–∞—î —á—ñ—Ç–∫–µ –∑–Ω–∞—á–µ–Ω–Ω—è |
| **No assumptions** | –ù–µ –ø—Ä–∏–ø—É—Å–∫–∞—î —Ä–æ–∑–ø–æ–¥—ñ–ª—ñ–≤ |
| **Cluster outliers** | –î–æ–±—Ä–µ –¥–ª—è outliers –º—ñ–∂ –∫–ª–∞—Å—Ç–µ—Ä–∞–º–∏ |
| **Theory** | Solid mathematical foundation |

### –ù–µ–¥–æ–ª—ñ–∫–∏ ‚úó

| –ù–µ–¥–æ–ª—ñ–∫ | –ü–æ—è—Å–Ω–µ–Ω–Ω—è |
|---------|-----------|
| **–ü–æ–≤—ñ–ª—å–Ω–∏–π** | O(n¬≤) ‚Äî –æ–±—á–∏—Å–ª–µ–Ω–Ω—è –≤—Å—ñ—Ö –≤—ñ–¥—Å—Ç–∞–Ω–µ–π |
| **–ù–µ –º–∞—Å—à—Ç–∞–±—É—î—Ç—å—Å—è** | –ü–æ–≥–∞–Ω–æ –Ω–∞ > 50K |
| **High-dimensional** | Curse of dimensionality |
| **Parameter sensitive** | k —Å–∏–ª—å–Ω–æ –≤–ø–ª–∏–≤–∞—î |
| **Memory** | –ó–±–µ—Ä—ñ–≥–∞—î distance matrix |
| **No global view** | –ú–æ–∂–µ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç–∏ –≥–ª–æ–±–∞–ª—å–Ω—ñ patterns |

---

## –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –∑ —ñ–Ω—à–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏

| –ú–µ—Ç–æ–¥ | –õ–æ–∫–∞–ª—å–Ω—ñ outliers | –®–≤–∏–¥–∫—ñ—Å—Ç—å | Variable density | –ú–∞—Å—à—Ç–∞–±–æ–≤–∞–Ω—ñ—Å—Ç—å |
|-------|-------------------|-----------|------------------|-----------------|
| **LOF** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê |
| **Isolation Forest** | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **One-Class SVM** | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê |
| **DBSCAN** | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |

**–ö–æ–ª–∏ —â–æ:**
- **–õ–æ–∫–∞–ª—å–Ω—ñ outliers + variable density** ‚Üí LOF ‚úì
- **–í–µ–ª–∏–∫—ñ –¥–∞–Ω—ñ + —à–≤–∏–¥–∫—ñ—Å—Ç—å** ‚Üí Isolation Forest ‚úì
- **Global outliers** ‚Üí Isolation Forest ‚úì
- **Clustering + outliers** ‚Üí DBSCAN ‚úì

---

## –ü—Ä–∞–∫—Ç–∏—á–Ω—ñ –ø–æ—Ä–∞–¥–∏ üí°

### 1. –ï–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—É–π –∑ n_neighbors

```python
# –°–ø—Ä–æ–±—É–π —Ä—ñ–∑–Ω—ñ k
for k in [10, 20, 30, 50]:
    clf = LocalOutlierFactor(n_neighbors=k)
    y_pred = clf.fit_predict(X)
    
    # Evaluate
    score = evaluate(y_pred, y_true)
    print(f"k={k}: score={score:.3f}")
```

### 2. Feature scaling –≤–∞–∂–ª–∏–≤–∏–π

```python
# ‚úÖ LOF —á—É—Ç–ª–∏–≤–∏–π –¥–æ scale
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

clf = LocalOutlierFactor()
clf.fit_predict(X_scaled)
```

### 3. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π scores –¥–ª—è ranking

```python
# –ù–µ —Ç—ñ–ª—å–∫–∏ binary predictions
clf = LocalOutlierFactor(novelty=False)
y_pred = clf.fit_predict(X)

# LOF scores (negative_outlier_factor_)
lof_scores = -clf.negative_outlier_factor_

# Top-N –Ω–∞–π–±—ñ–ª—å—à –∞–Ω–æ–º–∞–ª—å–Ω–∏—Ö
top_n = 10
top_outliers = np.argsort(lof_scores)[-top_n:][::-1]

for idx in top_outliers:
    print(f"Rank {idx}: LOF = {lof_scores[idx]:.3f}")
```

### 4. novelty mode –¥–ª—è production

```python
# Train –Ω–∞ clean normal data
X_train_normal = clean_data

clf = LocalOutlierFactor(novelty=True)
clf.fit(X_train_normal)

# Predict –Ω–∞ –Ω–æ–≤–∏–π stream
for new_sample in data_stream:
    is_anomaly = clf.predict([new_sample])[0] == -1
    
    if is_anomaly:
        alert()
```

### 5. Combine –∑ —ñ–Ω—à–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏

```python
# Ensemble: LOF + Isolation Forest
from sklearn.ensemble import IsolationForest

# LOF
lof = LocalOutlierFactor(novelty=True)
lof.fit(X_train)
lof_scores = lof.score_samples(X_test)

# Isolation Forest
iforest = IsolationForest()
iforest.fit(X_train)
if_scores = iforest.score_samples(X_test)

# Normalize —Ç–∞ combine
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
lof_norm = scaler.fit_transform(lof_scores.reshape(-1, 1)).ravel()
if_norm = scaler.fit_transform(if_scores.reshape(-1, 1)).ravel()

# Voting –∞–±–æ averaging
ensemble_scores = (lof_norm + if_norm) / 2
```

### 6. Dimensionality reduction —Å–ø–æ—á–∞—Ç–∫—É

```python
# –î–ª—è high-dimensional data
from sklearn.decomposition import PCA

# PCA —Å–ø–æ—á–∞—Ç–∫—É
pca = PCA(n_components=10)
X_pca = pca.fit_transform(X)

# LOF –Ω–∞ reduced
clf = LocalOutlierFactor()
y_pred = clf.fit_predict(X_pca)
```

### 7. Cross-validation –¥–ª—è k

```python
# –Ø–∫—â–æ —î validation set
k_values = [10, 20, 30, 40, 50]
best_f1 = 0
best_k = None

for k in k_values:
    clf = LocalOutlierFactor(n_neighbors=k, novelty=False)
    y_pred = clf.fit_predict(X_val)
    
    from sklearn.metrics import f1_score
    f1 = f1_score(y_val_true, (y_pred == -1).astype(int))
    
    if f1 > best_f1:
        best_f1 = f1
        best_k = k

print(f"Best k: {best_k}")
```

### 8. Visualize neighborhoods

```python
# –î–ª—è debugging ‚Äî –ø–æ–¥–∏–≤–∏—Ç–∏—Å—å –Ω–∞ neighborhoods
from sklearn.neighbors import NearestNeighbors

nbrs = NearestNeighbors(n_neighbors=20)
nbrs.fit(X)

# –î–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ—ó —Ç–æ—á–∫–∏
point_idx = 0
distances, indices = nbrs.kneighbors([X[point_idx]])

plt.scatter(X[:, 0], X[:, 1], c='lightgray', s=20, alpha=0.5)
plt.scatter(X[point_idx, 0], X[point_idx, 1], 
           c='red', s=100, marker='*', label='Query point')
plt.scatter(X[indices[0], 0], X[indices[0], 1],
           c='blue', s=50, alpha=0.7, label='Neighbors')
plt.legend()
plt.show()
```

### 9. Incremental updates

```python
# LOF –Ω–µ –º–∞—î incremental learning
# –î–ª—è streaming: –ø–µ—Ä—ñ–æ–¥–∏—á–Ω–æ refit

class StreamingLOF:
    def __init__(self, window_size=1000, k=20):
        self.window_size = window_size
        self.k = k
        self.clf = LocalOutlierFactor(n_neighbors=k, novelty=True)
        self.buffer = []
        self.scaler = StandardScaler()
    
    def add_sample(self, x):
        self.buffer.append(x)
        
        if len(self.buffer) >= self.window_size:
            # Refit
            X_train = np.array(self.buffer[-self.window_size:])
            X_scaled = self.scaler.fit_transform(X_train)
            self.clf.fit(X_scaled)
            
            # Keep half
            self.buffer = self.buffer[-self.window_size//2:]
    
    def predict(self, x):
        x_scaled = self.scaler.transform([x])
        return self.clf.predict(x_scaled)[0]
```

### 10. Domain-specific distance metrics

```python
# –î–ª—è —Å–ø–µ—Ü–∏—Ñ—ñ—á–Ω–∏—Ö –¥–∞–Ω–∏—Ö ‚Äî custom metric
def custom_distance(x, y):
    # Domain-specific logic
    return np.sum((x - y) ** 2)

clf = LocalOutlierFactor(
    n_neighbors=20,
    metric=custom_distance
)
```

---

## –ü–æ—à–∏—Ä–µ–Ω—ñ –ø–æ–º–∏–ª–∫–∏ ‚ùå

### 1. –ù–µ scale features

```python
# ‚ùå Features –≤ —Ä—ñ–∑–Ω–∏—Ö scales
clf = LocalOutlierFactor()
clf.fit_predict(X_raw)

# ‚úÖ Scale —Å–ø–æ—á–∞—Ç–∫—É
X_scaled = StandardScaler().fit_transform(X_raw)
clf.fit_predict(X_scaled)
```

### 2. –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∏–π k

```python
# ‚ùå k = 5 (–∑–∞–Ω–∞–¥—Ç–æ –º–∞–ª–∏–π –¥–ª—è 10,000 points)
# ‚ùå k = 100 (–∑–∞–Ω–∞–¥—Ç–æ –≤–µ–ª–∏–∫–∏–π –¥–ª—è 200 points)

# ‚úÖ –†–æ–∑—É–º–Ω–∏–π k –≤—ñ–¥–Ω–æ—Å–Ω–æ dataset size
k = min(50, len(X) // 10)
```

### 3. novelty=False –¥–ª—è production

```python
# ‚ùå novelty=False –Ω–µ –º–æ–∂–µ predict –Ω–æ–≤—ñ –¥–∞–Ω—ñ
clf = LocalOutlierFactor(novelty=False)
clf.fit(X_train)
# clf.predict(X_test)  # Error!

# ‚úÖ novelty=True –¥–ª—è predict
clf = LocalOutlierFactor(novelty=True)
clf.fit(X_train)
clf.predict(X_test)  # OK!
```

### 4. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –Ω–∞ –¥—É–∂–µ –≤–µ–ª–∏–∫–∏—Ö –¥–∞–Ω–∏—Ö

```python
# ‚ùå 100,000+ points
# –î—É–∂–µ –ø–æ–≤—ñ–ª—å–Ω–æ!

# ‚úÖ Sample –∞–±–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–π Isolation Forest
if len(X) > 50000:
    # Use Isolation Forest
    from sklearn.ensemble import IsolationForest
```

### 5. –Ü–≥–Ω–æ—Ä—É–≤–∞—Ç–∏ high-dimensional curse

```python
# ‚ùå 100+ features
# LOF –ø–æ–≥–∞–Ω–æ –ø—Ä–∞—Ü—é—î —á–µ—Ä–µ–∑ curse of dimensionality

# ‚úÖ Dimensionality reduction
pca = PCA(n_components=20)
X_reduced = pca.fit_transform(X)
clf.fit_predict(X_reduced)
```

---

## –†–µ–∞–ª—å–Ω—ñ –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è

### 1. Network Intrusion Detection

```python
# –õ–æ–∫–∞–ª—å–Ω—ñ –∞–Ω–æ–º–∞–ª—ñ—ó –≤ network traffic
# Train –Ω–∞ normal traffic, detect attacks

clf = LocalOutlierFactor(n_neighbors=30, novelty=True)
clf.fit(normal_traffic_features)

# Real-time
for packet in stream:
    is_attack = clf.predict([packet_features])[0] == -1
    
    if is_attack:
        block_and_alert()
```

### 2. Medical Diagnosis

```python
# –õ–æ–∫–∞–ª—å–Ω—ñ –∞–Ω–æ–º–∞–ª—ñ—ó –≤ test results
# –í—Ä–∞—Ö–æ–≤—É—î —â–æ –Ω–æ—Ä–º–∞ —Ä—ñ–∑–Ω–∞ –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö –≥—Ä—É–ø

clf = LocalOutlierFactor(n_neighbors=20)
lof_scores = -clf.fit_predict(patient_data)

# Patients –∑ high LOF ‚Üí –¥–æ–¥–∞—Ç–∫–æ–≤—ñ –æ–±—Å—Ç–µ–∂–µ–Ω–Ω—è
high_risk = lof_scores > 2.0
flag_for_review(high_risk_patients)
```

### 3. Manufacturing Quality Control

```python
# –î–µ—Ñ–µ–∫—Ç–∏ –º–æ–∂—É—Ç—å –±—É—Ç–∏ –ª–æ–∫–∞–ª—å–Ω–∏–º–∏
# (—Ä—ñ–∑–Ω—ñ —Ç–∏–ø–∏ products –º–∞—é—Ç—å —Ä—ñ–∑–Ω—ñ –Ω–æ—Ä–º–∏)

clf = LocalOutlierFactor(n_neighbors=25)
y_pred = clf.fit_predict(sensor_readings)

defective = y_pred == -1
reject_items(defective_indices)
```

---

## –ü–æ–≤'—è–∑–∞–Ω—ñ —Ç–µ–º–∏

- [[01_Isolation_Forest]] ‚Äî —à–≤–∏–¥—à–∞ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞
- [[02_One_Class_SVM]] ‚Äî kernel-based approach
- [[04_Anomaly_Detection_Methods]] ‚Äî –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –≤—Å—ñ—Ö
- [[DBSCAN]] ‚Äî clustering –∑ outlier detection

## –†–µ—Å—É—Ä—Å–∏

- [Original Paper (Breunig et al., 2000)](https://www.dbs.ifi.lmu.de/Publikationen/Papers/LOF.pdf)
- [Scikit-learn: LOF](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html)
- [LOF Tutorial](https://scikit-learn.org/stable/auto_examples/neighbors/plot_lof_outlier_detection.html)

---

## –ö–ª—é—á–æ–≤—ñ –≤–∏—Å–Ω–æ–≤–∫–∏

> LOF (Local Outlier Factor) ‚Äî density-based –∞–ª–≥–æ—Ä–∏—Ç–º –¥–ª—è –≤–∏—è–≤–ª–µ–Ω–Ω—è –ª–æ–∫–∞–ª—å–Ω–∏—Ö –∞–Ω–æ–º–∞–ª—ñ–π, —â–æ –ø–æ—Ä—ñ–≤–Ω—é—î –ª–æ–∫–∞–ª—å–Ω—É —â—ñ–ª—å–Ω—ñ—Å—Ç—å —Ç–æ—á–∫–∏ –∑ —â—ñ–ª—å–Ω—ñ—Å—Ç—é —ó—ó k-–Ω–∞–π–±–ª–∏–∂—á–∏—Ö —Å—É—Å—ñ–¥—ñ–≤. –¢–æ—á–∫–∞ –∑ LOF >> 1 –≤–≤–∞–∂–∞—î—Ç—å—Å—è outlier.

**–û—Å–Ω–æ–≤–Ω–∞ —ñ–¥–µ—è:**
- –ê–Ω–æ–º–∞–ª—ñ—è = –Ω–∏–∑—å–∫–∞ –ª–æ–∫–∞–ª—å–Ω–∞ —â—ñ–ª—å–Ω—ñ—Å—Ç—å –≤—ñ–¥–Ω–æ—Å–Ω–æ —Å—É—Å—ñ–¥—ñ–≤
- –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è Local Reachability Density
- LOF score = ratio —â—ñ–ª—å–Ω–æ—Å—Ç–µ–π

**–ê–ª–≥–æ—Ä–∏—Ç–º:**
1. –î–ª—è –∫–æ–∂–Ω–æ—ó —Ç–æ—á–∫–∏ –∑–Ω–∞–π—Ç–∏ k-nearest neighbors
2. –û–±—á–∏—Å–ª–∏—Ç–∏ reachability distance –¥–æ —Å—É—Å—ñ–¥—ñ–≤
3. –û–±—á–∏—Å–ª–∏—Ç–∏ Local Reachability Density (LRD)
4. –û–±—á–∏—Å–ª–∏—Ç–∏ LOF = avg(LRD_neighbors) / LRD_point

**LOF Score —ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü—ñ—è:**
- LOF ‚âà 1 ‚Üí Normal (—Å—Ö–æ–∂–∞ —â—ñ–ª—å–Ω—ñ—Å—Ç—å)
- LOF > 1.5 ‚Üí Potential outlier
- LOF > 2.0 ‚Üí Strong outlier

**–ü–µ—Ä–µ–≤–∞–≥–∏:**
- ‚úÖ –ó–Ω–∞—Ö–æ–¥–∏—Ç—å –ª–æ–∫–∞–ª—å–Ω—ñ outliers
- ‚úÖ –ü—Ä–∞—Ü—é—î –∑ variable density
- ‚úÖ Interpretable score
- ‚úÖ No assumptions –ø—Ä–æ —Ä–æ–∑–ø–æ–¥—ñ–ª–∏

**–ù–µ–¥–æ–ª—ñ–∫–∏:**
- ‚ùå –ü–æ–≤—ñ–ª—å–Ω–∏–π (O(n¬≤))
- ‚ùå –ù–µ –º–∞—Å—à—Ç–∞–±—É—î—Ç—å—Å—è (>50K)
- ‚ùå –ß—É—Ç–ª–∏–≤–∏–π –¥–æ k
- ‚ùå High-dimensional –ø—Ä–æ–±–ª–µ–º–∏

**–ö–ª—é—á–æ–≤—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏:**
- **n_neighbors (k):** —Ä–æ–∑–º—ñ—Ä –æ–∫–æ–ª—É (10-50)
- **contamination:** expected % outliers
- **novelty:** False=fit_predict, True=fit+predict

**–ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏:**
- –õ–æ–∫–∞–ª—å–Ω—ñ outliers + variable density = LOF ‚úì
- –í–µ–ª–∏–∫—ñ –¥–∞–Ω—ñ + —à–≤–∏–¥–∫—ñ—Å—Ç—å ‚Üí Isolation Forest ‚úì
- Global outliers ‚Üí Isolation Forest ‚úì
- High-dimensional ‚Üí PCA + LOF ‚úì

**–ü—Ä–∞–∫—Ç–∏—á–Ω—ñ –ø–æ—Ä–∞–¥–∏:**
- –ó–ê–í–ñ–î–ò scale features
- –ï–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—É–π –∑ k (10-50)
- novelty=True –¥–ª—è production
- Combine –∑ Isolation Forest
- PCA –¥–ª—è high-dimensional
- Incremental refit –¥–ª—è streaming

---

#ml #unsupervised-learning #anomaly-detection #outlier-detection #lof #local-outlier-factor #density-based #nearest-neighbors
