# One-Class SVM

## –©–æ —Ü–µ?

**One-Class SVM (Support Vector Machine)** ‚Äî —Ü–µ –∞–ª–≥–æ—Ä–∏—Ç–º –¥–ª—è **novelty/anomaly detection**, —â–æ –Ω–∞–≤—á–∞—î—Ç—å—Å—è –Ω–∞ —Ç—ñ–ª—å–∫–∏ **–Ω–æ—Ä–º–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö** —ñ –±—É–¥—É—î –≥—Ä–∞–Ω–∏—Ü—é –Ω–∞–≤–∫–æ–ª–æ –Ω–∏—Ö. –í—Å–µ —â–æ –∑–∞ –º–µ–∂–∞–º–∏ —Ü—ñ—î—ó –≥—Ä–∞–Ω–∏—Ü—ñ –≤–≤–∞–∂–∞—î—Ç—å—Å—è –∞–Ω–æ–º–∞–ª—ñ—î—é.

**–ì–æ–ª–æ–≤–Ω–∞ —ñ–¥–µ—è:** –∑–Ω–∞–π—Ç–∏ –≥—ñ–ø–µ—Ä–ø–ª–æ—â–∏–Ω—É, —â–æ –≤—ñ–¥–æ–∫—Ä–µ–º–ª—é—î –Ω–æ—Ä–º–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ –≤—ñ–¥ –ø–æ—á–∞—Ç–∫—É –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç —É –≤–∏—Å–æ–∫–æ—Ä–æ–∑–º—ñ—Ä–Ω–æ–º—É feature space, –º–∞–∫—Å–∏–º—ñ–∑—É—é—á–∏ margin (–≤—ñ–¥—Å—Ç–∞–Ω—å –≤—ñ–¥ –≥—Ä–∞–Ω–∏—Ü—ñ –¥–æ —Ç–æ—á–æ–∫).

## –ù–∞–≤—ñ—â–æ –ø–æ—Ç—Ä—ñ–±–µ–Ω?

- üõ°Ô∏è **Novelty Detection** ‚Äî –≤–∏—è–≤–ª–µ–Ω–Ω—è –Ω–æ–≤–∏—Ö, –Ω–µ–∑–≤–∏—á–∞–π–Ω–∏—Ö –∑—Ä–∞–∑–∫—ñ–≤
- üîç **Outlier Detection** ‚Äî –∑–Ω–∞—Ö–æ–¥–∂–µ–Ω–Ω—è –≤–∏–∫–∏–¥—ñ–≤
- üè≠ **Quality Control** ‚Äî –¥–µ—Ñ–µ–∫—Ç–∏ –≤ –≤–∏—Ä–æ–±–Ω–∏—Ü—Ç–≤—ñ
- üåê **Network Security** ‚Äî intrusion detection
- üñºÔ∏è **Image Anomaly** ‚Äî –¥–µ—Ñ–µ–∫—Ç–∏ –Ω–∞ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è—Ö
- üìä **One-class classification** ‚Äî —Ç—ñ–ª—å–∫–∏ positive examples
- üß™ **Rare event detection** ‚Äî —Ä—ñ–¥–∫—ñ—Å–Ω—ñ –ø–æ–¥—ñ—ó

## –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏?

**–ü–æ—Ç—Ä—ñ–±–Ω–æ:**
- **–¢—ñ–ª—å–∫–∏ –Ω–æ—Ä–º–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ** –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è
- **Novelty detection** ‚Äî –Ω–æ–≤—ñ —Ç–∏–ø–∏ –∞–Ω–æ–º–∞–ª—ñ–π
- **Smooth decision boundary** ‚Äî –≥–ª–∞–¥–∫–∞ –≥—Ä–∞–Ω–∏—Ü—è
- **Kernel trick** ‚Äî –Ω–µ–ª—ñ–Ω—ñ–π–Ω—ñ patterns
- **Theoretical foundation** ‚Äî –º–∞—Ç–µ–º–∞—Ç–∏—á–Ω–æ –æ–±“ë—Ä—É–Ω—Ç–æ–≤–∞–Ω–æ
- **–°–µ—Ä–µ–¥–Ω—ñ –¥–∞–Ω—ñ** (100-10,000 –∑—Ä–∞–∑–∫—ñ–≤)

**–ù–µ –ø–æ—Ç—Ä—ñ–±–Ω–æ:**
- **–î—É–∂–µ –≤–µ–ª–∏–∫—ñ –¥–∞–Ω—ñ** (> 100K) ‚Üí Isolation Forest —à–≤–∏–¥—à–µ
- **High-dimensional + sparse** ‚Üí Isolation Forest
- **Real-time streaming** ‚Üí –ø—Ä–æ—Å—Ç—ñ—à—ñ –º–µ—Ç–æ–¥–∏
- **Interpretability –≤–∞–∂–ª–∏–≤–∞** ‚Üí Isolation Forest
- **–î—É–∂–µ –º–∞–ª—ñ –¥–∞–Ω—ñ** (< 50) ‚Üí statistical methods

---

## –ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞

### –û—Å–Ω–æ–≤–Ω–∞ —ñ–¥–µ—è

**–ú–µ—Ç–∞:** –∑–Ω–∞–π—Ç–∏ smallest hypersphere (–∞–±–æ hyperplane) —â–æ –º—ñ—Å—Ç–∏—Ç—å –±—ñ–ª—å—à—ñ—Å—Ç—å –Ω–æ—Ä–º–∞–ª—å–Ω–∏—Ö —Ç–æ—á–æ–∫.

**–í feature space:**

```
Original space:           Feature space (–ø—ñ—Å–ª—è kernel):
    ‚óè‚óè‚óè‚óè                      ‚óè‚óè‚óè‚óè‚óè
    ‚óè  ‚óè                      ‚óè   ‚óè
    ‚óè‚óè‚óè‚óè                      ‚óè‚óè‚óè‚óè‚óè
                              ‚Üì
                       Find boundary around data
```

### Optimization Problem

**Primal —Ñ–æ—Ä–º—É–ª—é–≤–∞–Ω–Ω—è:**

$$\min_{w, \rho, \xi} \frac{1}{2}\|w\|^2 - \rho + \frac{1}{\nu n}\sum_{i=1}^{n}\xi_i$$

**Subject to:**
$$w^T\phi(x_i) \geq \rho - \xi_i, \quad \xi_i \geq 0$$

–¥–µ:
- $w$ ‚Äî –Ω–æ—Ä–º–∞–ª—å –¥–æ hyperplane
- $\rho$ ‚Äî offset (–≤—ñ–¥—Å—Ç–∞–Ω—å –≤—ñ–¥ origin)
- $\xi_i$ ‚Äî slack variables (–¥–æ–∑–≤–æ–ª—è—é—Ç—å –ø–æ–º–∏–ª–∫–∏)
- $\nu \in (0,1]$ ‚Äî upper bound –Ω–∞ fraction outliers
- $\phi(x)$ ‚Äî kernel mapping

**–Ü–Ω—Ç—É—ó—Ü—ñ—è:**
- –ú–∞–∫—Å–∏–º—ñ–∑—É–≤–∞—Ç–∏ margin (–º—ñ–Ω—ñ–º—ñ–∑—É–≤–∞—Ç–∏ $\|w\|$)
- –ú–∞–∫—Å–∏–º—ñ–∑—É–≤–∞—Ç–∏ $\rho$ (–≤—ñ–¥—à—Ç–æ–≤—Ö–Ω—É—Ç–∏ –≤—ñ–¥ origin)
- –ú—ñ–Ω—ñ–º—ñ–∑—É–≤–∞—Ç–∏ –ø–æ—Ä—É—à–µ–Ω–Ω—è (slack $\xi_i$)

### Decision Function

**–î–ª—è –Ω–æ–≤–æ—ó —Ç–æ—á–∫–∏ $x$:**

$$f(x) = \text{sign}(w^T\phi(x) - \rho) = \text{sign}\left(\sum_{i}\alpha_i K(x_i, x) - \rho\right)$$

–¥–µ:
- $f(x) = +1$ ‚Üí Normal
- $f(x) = -1$ ‚Üí Anomaly
- $K(x_i, x)$ ‚Äî kernel function

### Kernel Functions

**Linear:**
$$K(x, x') = x^T x'$$

**RBF (Gaussian) ‚Äî –Ω–∞–π–ø–æ–ø—É–ª—è—Ä–Ω—ñ—à–∏–π:**
$$K(x, x') = \exp\left(-\gamma \|x - x'\|^2\right)$$

**Polynomial:**
$$K(x, x') = (x^T x' + c)^d$$

**Sigmoid:**
$$K(x, x') = \tanh(\gamma x^T x' + c)$$

---

## –í—ñ–∑—É–∞–ª—å–Ω–∞ —ñ–Ω—Ç—É—ó—Ü—ñ—è

### 2D –ø—Ä–∏–∫–ª–∞–¥

```
Linear kernel:
    y
    |  ‚óè‚óè‚óè‚óè‚óè
    |  ‚óè   ‚óè
    |  ‚óè‚óè‚óè‚óè‚óè
    |_________x
    
Decision boundary = straight line

RBF kernel:
    y
    |  ‚óè‚óè‚óè‚óè‚óè
    |  ‚óè ‚óã ‚óè  ‚Üê –º–æ–∂–µ –∑–∞—Ö–æ–ø–∏—Ç–∏ —Ç—É—Ç
    |  ‚óè‚óè‚óè‚óè‚óè
    |_________x
    
Decision boundary = curved (–≥–Ω—É—á–∫—ñ—à–∞)
```

### Margin visualization

```
        ‚óè‚óè‚óè‚óè‚óè
      ‚óè‚óè     ‚óè‚óè
     ‚óè         ‚óè
    ‚óè    ‚óã‚óã‚óã   ‚óè  ‚Üê Boundary
     ‚óè         ‚óè
      ‚óè‚óè     ‚óè‚óè
        ‚óè‚óè‚óè‚óè‚óè
        
    ‚Üë margin ‚Üë
```

---

## –ê–ª–≥–æ—Ä–∏—Ç–º

### Training

```
1. –î–∞–Ω–æ: —Ç—ñ–ª—å–∫–∏ NORMAL data X = {x‚ÇÅ, x‚ÇÇ, ..., x‚Çô}

2. –í–∏–±—Ä–∞—Ç–∏ kernel K(¬∑,¬∑) —Ç–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ (Œ≥ –¥–ª—è RBF, ŒΩ)

3. –†–æ–∑–≤'—è–∑–∞—Ç–∏ optimization problem:
   min 1/2||w||¬≤ - œÅ + (1/ŒΩn)Œ£Œæ·µ¢
   subject to: w·µÄœÜ(x·µ¢) ‚â• œÅ - Œæ·µ¢, Œæ·µ¢ ‚â• 0

4. –ó–Ω–∞–π—Ç–∏ support vectors (Œ±·µ¢ > 0)

5. –û–±—á–∏—Å–ª–∏—Ç–∏ œÅ

6. Decision function: f(x) = sign(Œ£Œ±·µ¢ K(x·µ¢, x) - œÅ)
```

### Prediction

```
–î–ª—è –Ω–æ–≤–æ—ó —Ç–æ—á–∫–∏ x:

1. –û–±—á–∏—Å–ª–∏—Ç–∏ kernel values –∑ support vectors:
   s(x) = Œ£ Œ±·µ¢ K(x·µ¢, x) - œÅ

2. Classify:
   IF s(x) ‚â• 0:
       x is NORMAL
   ELSE:
       x is ANOMALY
```

---

## –ö–æ–¥ (Python + scikit-learn)

### –ë–∞–∑–æ–≤–∏–π –ø—Ä–∏–∫–ª–∞–¥

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.svm import OneClassSVM
from sklearn.datasets import make_blobs

# 1. –ì–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ –¢–Ü–õ–¨–ö–ò –Ω–æ—Ä–º–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ –¥–ª—è training
np.random.seed(42)

X_train, _ = make_blobs(
    n_samples=200,
    centers=[[0, 0]],
    cluster_std=0.5,
    random_state=42
)

print(f"Training data: {X_train.shape}")
print("Training on NORMAL data only!")

# 2. –°—Ç–≤–æ—Ä–∏—Ç–∏ test set –∑ –Ω–æ—Ä–º–∞–ª—å–Ω–∏–º–∏ + –∞–Ω–æ–º–∞–ª—ñ—è–º–∏
X_test_normal, _ = make_blobs(
    n_samples=50,
    centers=[[0, 0]],
    cluster_std=0.5,
    random_state=100
)

X_test_anomalies = np.random.uniform(low=-4, high=4, size=(10, 2))

X_test = np.vstack([X_test_normal, X_test_anomalies])
y_test_true = np.array([1]*50 + [-1]*10)  # 1=normal, -1=anomaly

# 3. Fit One-Class SVM
clf = OneClassSVM(
    kernel='rbf',       # RBF kernel (default)
    gamma='auto',       # 1 / (n_features * X.var())
    nu=0.1              # Upper bound –Ω–∞ fraction outliers
)

clf.fit(X_train)

# 4. Predict
y_train_pred = clf.predict(X_train)
y_test_pred = clf.predict(X_test)

print(f"\n=== Training Set ===")
print(f"Predicted outliers: {(y_train_pred == -1).sum()} / {len(y_train_pred)}")
print(f"Expected: ~{int(0.1 * len(y_train_pred))} (nu=0.1)")

print(f"\n=== Test Set ===")
print(f"Predicted anomalies: {(y_test_pred == -1).sum()}")
print(f"True anomalies: {(y_test_true == -1).sum()}")

# 5. Metrics
from sklearn.metrics import classification_report, confusion_matrix

print("\n=== Classification Report ===")
print(classification_report(y_test_true, y_test_pred,
                           target_names=['Anomaly', 'Normal']))

# 6. –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# Training data + decision boundary
xx, yy = np.meshgrid(
    np.linspace(-4, 4, 200),
    np.linspace(-4, 4, 200)
)

Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

# Plot 1: Training
axes[0].contourf(xx, yy, Z, levels=20, cmap='RdYlBu_r', alpha=0.6)
axes[0].contour(xx, yy, Z, levels=[0], linewidths=2, colors='black')
axes[0].scatter(X_train[:, 0], X_train[:, 1],
               c='blue', s=20, alpha=0.6, edgecolors='black',
               linewidths=0.5, label='Training (Normal)')

# Support vectors
support_vectors = clf.support_vectors_
axes[0].scatter(support_vectors[:, 0], support_vectors[:, 1],
               s=100, facecolors='none', edgecolors='red',
               linewidths=2, label='Support Vectors')

axes[0].set_title('Training: One-Class SVM', fontsize=13, fontweight='bold')
axes[0].set_xlabel('Feature 1')
axes[0].set_ylabel('Feature 2')
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# Plot 2: Test
axes[1].contourf(xx, yy, Z, levels=20, cmap='RdYlBu_r', alpha=0.6)
axes[1].contour(xx, yy, Z, levels=[0], linewidths=2, colors='black')

# Normal test points
axes[1].scatter(X_test[y_test_true == 1, 0], X_test[y_test_true == 1, 1],
               c='blue', s=30, alpha=0.7, label='Normal')

# Anomalies
axes[1].scatter(X_test[y_test_true == -1, 0], X_test[y_test_true == -1, 1],
               c='red', s=50, marker='x', linewidths=2, label='Anomaly')

axes[1].set_title('Test: Predictions', fontsize=13, fontweight='bold')
axes[1].set_xlabel('Feature 1')
axes[1].set_ylabel('Feature 2')
axes[1].legend()
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è —Ä—ñ–∑–Ω–∏—Ö kernels

```python
# –†—ñ–∑–Ω—ñ kernels –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è
kernels = ['linear', 'rbf', 'poly', 'sigmoid']

fig, axes = plt.subplots(2, 2, figsize=(14, 12))
axes = axes.ravel()

for idx, kernel in enumerate(kernels):
    print(f"\nTraining {kernel} kernel...")
    
    clf = OneClassSVM(kernel=kernel, nu=0.1, gamma='auto')
    clf.fit(X_train)
    
    y_pred = clf.predict(X_test)
    
    # Decision function
    xx, yy = np.meshgrid(
        np.linspace(-4, 4, 200),
        np.linspace(-4, 4, 200)
    )
    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    
    # Plot
    axes[idx].contourf(xx, yy, Z, levels=20, cmap='RdYlBu_r', alpha=0.6)
    axes[idx].contour(xx, yy, Z, levels=[0], linewidths=2, colors='black')
    
    axes[idx].scatter(X_test[y_test_true == 1, 0], X_test[y_test_true == 1, 1],
                     c='blue', s=20, alpha=0.6, label='Normal')
    axes[idx].scatter(X_test[y_test_true == -1, 0], X_test[y_test_true == -1, 1],
                     c='red', s=40, marker='x', linewidths=2, label='Anomaly')
    
    # Support vectors
    sv = clf.support_vectors_
    axes[idx].scatter(sv[:, 0], sv[:, 1],
                     s=80, facecolors='none', edgecolors='red',
                     linewidths=2, label='Support Vectors')
    
    # Metrics
    from sklearn.metrics import accuracy_score
    acc = accuracy_score(y_test_true, y_pred)
    
    axes[idx].set_title(f'{kernel.upper()} kernel (Acc: {acc:.2f})',
                       fontsize=12, fontweight='bold')
    axes[idx].set_xlabel('Feature 1')
    axes[idx].set_ylabel('Feature 2')
    axes[idx].legend()
    axes[idx].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### –ü–∞—Ä–∞–º–µ—Ç—Ä tuning (gamma —Ç–∞ nu)

```python
# Grid search –¥–ª—è –Ω–∞–π–∫—Ä–∞—â–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤

from sklearn.model_selection import ParameterGrid

# –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –¥–ª—è –ø–æ—à—É–∫—É
param_grid = {
    'gamma': [0.001, 0.01, 0.1, 1.0],
    'nu': [0.01, 0.05, 0.1, 0.2]
}

best_score = -np.inf
best_params = None
results = []

for params in ParameterGrid(param_grid):
    clf = OneClassSVM(kernel='rbf', **params)
    clf.fit(X_train)
    
    y_pred = clf.predict(X_test)
    
    # Score (F1 –¥–ª—è anomaly class)
    from sklearn.metrics import f1_score
    score = f1_score(y_test_true, y_pred, pos_label=-1)
    
    results.append({
        'gamma': params['gamma'],
        'nu': params['nu'],
        'f1_score': score
    })
    
    if score > best_score:
        best_score = score
        best_params = params

print("\n=== Parameter Tuning Results ===")
print(f"Best params: {best_params}")
print(f"Best F1-score: {best_score:.4f}")

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è results
import pandas as pd

df_results = pd.DataFrame(results)
pivot = df_results.pivot(index='nu', columns='gamma', values='f1_score')

plt.figure(figsize=(10, 6))
import seaborn as sns
sns.heatmap(pivot, annot=True, fmt='.3f', cmap='viridis')
plt.title('F1-Score Heatmap (gamma vs nu)', fontsize=14, fontweight='bold')
plt.ylabel('nu')
plt.xlabel('gamma')
plt.tight_layout()
plt.show()
```

### Real example: Network Intrusion Detection

```python
# –°–∏–º—É–ª—é—î–º–æ network traffic data

np.random.seed(42)

# Normal traffic (training)
n_normal_train = 1000

packet_size_normal = np.random.normal(500, 100, n_normal_train)
duration_normal = np.random.exponential(2, n_normal_train)
packets_per_sec_normal = np.random.poisson(10, n_normal_train)

X_train_network = np.column_stack([
    packet_size_normal,
    duration_normal,
    packets_per_sec_normal
])

print(f"Training set: {X_train_network.shape}")

# Test data (normal + attacks)
n_normal_test = 200
n_attacks = 50

# Normal
packet_size_test = np.random.normal(500, 100, n_normal_test)
duration_test = np.random.exponential(2, n_normal_test)
packets_test = np.random.poisson(10, n_normal_test)

X_test_normal = np.column_stack([
    packet_size_test,
    duration_test,
    packets_test
])

# Attacks (different patterns)
# Type 1: DDoS (high packets per sec)
packet_size_attack1 = np.random.normal(100, 20, n_attacks//2)
duration_attack1 = np.random.uniform(0.1, 0.5, n_attacks//2)
packets_attack1 = np.random.poisson(100, n_attacks//2)  # Very high!

# Type 2: Large file transfer (suspicious)
packet_size_attack2 = np.random.normal(5000, 500, n_attacks//2)
duration_attack2 = np.random.uniform(10, 20, n_attacks//2)
packets_attack2 = np.random.poisson(5, n_attacks//2)

X_test_attacks = np.vstack([
    np.column_stack([packet_size_attack1, duration_attack1, packets_attack1]),
    np.column_stack([packet_size_attack2, duration_attack2, packets_attack2])
])

# Combine test
X_test_network = np.vstack([X_test_normal, X_test_attacks])
y_test_network = np.array([1]*n_normal_test + [-1]*n_attacks)

print(f"Test set: {X_test_network.shape}")
print(f"  Normal: {n_normal_test}")
print(f"  Attacks: {n_attacks}")

# Feature scaling (–í–ê–ñ–õ–ò–í–û!)
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_network)
X_test_scaled = scaler.transform(X_test_network)

# Train One-Class SVM
clf = OneClassSVM(
    kernel='rbf',
    gamma=0.1,
    nu=0.05  # Expect ~5% outliers in training
)

clf.fit(X_train_scaled)

# Predict
y_pred = clf.predict(X_test_scaled)

print("\n=== Intrusion Detection Results ===")
print(classification_report(y_test_network, y_pred,
                           target_names=['Attack', 'Normal']))

# ROC curve
from sklearn.metrics import roc_curve, auc

decision_scores = clf.decision_function(X_test_scaled)
fpr, tpr, thresholds = roc_curve(y_test_network, decision_scores, pos_label=1)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, linewidth=2, label=f'ROC curve (AUC = {roc_auc:.3f})')
plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random')
plt.xlabel('False Positive Rate', fontsize=12)
plt.ylabel('True Positive Rate', fontsize=12)
plt.title('ROC Curve: Network Intrusion Detection', fontsize=14, fontweight='bold')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# Feature importance (—á–µ—Ä–µ–∑ perturbation)
def feature_importance_ocsvm(clf, X, feature_names):
    """Approximate feature importance"""
    base_scores = clf.decision_function(X)
    importances = []
    
    for feature_idx in range(X.shape[1]):
        X_perm = X.copy()
        np.random.shuffle(X_perm[:, feature_idx])
        
        scores_perm = clf.decision_function(X_perm)
        diff = np.mean(np.abs(base_scores - scores_perm))
        importances.append(diff)
    
    return np.array(importances)

feature_names = ['Packet Size', 'Duration', 'Packets/sec']
importances = feature_importance_ocsvm(clf, X_test_scaled, feature_names)

plt.figure(figsize=(10, 6))
plt.barh(feature_names, importances)
plt.xlabel('Importance (perturbation effect)', fontsize=12)
plt.title('Feature Importance for Intrusion Detection', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.show()
```

---

## –ü–∞—Ä–∞–º–µ—Ç—Ä–∏

### –û—Å–Ω–æ–≤–Ω—ñ

```python
OneClassSVM(
    kernel='rbf',           # Kernel type: 'linear', 'poly', 'rbf', 'sigmoid'
    gamma='scale',          # Kernel coefficient
    nu=0.5,                 # Upper bound –Ω–∞ fraction outliers
    degree=3,               # Degree for poly kernel
    coef0=0.0,              # Independent term for poly/sigmoid
    tol=1e-3,               # Tolerance for stopping
    max_iter=-1             # Max iterations (-1 = no limit)
)
```

| –ü–∞—Ä–∞–º–µ—Ç—Ä | –û–ø–∏—Å | –¢–∏–ø–æ–≤—ñ –∑–Ω–∞—á–µ–Ω–Ω—è | –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó |
|----------|------|-----------------|--------------|
| **kernel** | –¢–∏–ø kernel | 'rbf', 'linear', 'poly' | 'rbf' (default) –Ω–∞–π–∫—Ä–∞—â–∏–π |
| **gamma** | RBF kernel width | 'scale', 'auto', 0.001-1.0 | 'scale' = 1/(n_features * X.var()) |
| **nu** | Upper bound outliers | 0.01-0.5 | Lower = —Å—Ç—Ä–æ–≥—ñ—à–µ (fewer outliers) |
| **degree** | Poly degree | 2-5 | –¢—ñ–ª—å–∫–∏ –¥–ª—è poly kernel |

### nu parameter (–∫—Ä–∏—Ç–∏—á–Ω–∏–π!)

**–©–æ —Ü–µ:** Upper bound –Ω–∞:
1. Fraction training errors (outliers –≤ training set)
2. Lower bound –Ω–∞ fraction support vectors

```python
nu = 0.01  # –î—É–∂–µ —Å—Ç—Ä–æ–≥–æ (1% outliers max)
nu = 0.1   # –°–µ—Ä–µ–¥–Ω—å–æ (10%)
nu = 0.5   # –ú'—è–∫–æ (50%)
```

**–í–ø–ª–∏–≤:**

```
nu = 0.01:  –í—É–∑—å–∫–∞ boundary
    ‚óè‚óè‚óè
    ‚óè‚óè‚óè   ‚Üí Tight fit

nu = 0.5:   –®–∏—Ä–æ–∫–∞ boundary
  ‚óè‚óè‚óè‚óè‚óè
 ‚óè     ‚óè  ‚Üí Loose fit
  ‚óè‚óè‚óè‚óè‚óè
```

### gamma parameter (–¥–ª—è RBF)

**–©–æ —Ü–µ:** –ö–æ–Ω—Ç—Ä–æ–ª—é—î "—Ä–∞–¥—ñ—É—Å –≤–ø–ª–∏–≤—É" support vectors.

$$K(x, x') = \exp(-\gamma \|x - x'\|^2)$$

**–í–ø–ª–∏–≤:**

```python
gamma = 0.001  # Wide influence (smooth boundary)
gamma = 0.1    # Medium
gamma = 1.0    # Narrow influence (complex boundary)
```

**–í—ñ–∑—É–∞–ª—å–Ω–æ:**

```
Low gamma (0.01):
    ‚óè‚óè‚óè‚óè‚óè‚óè‚óè
    ‚óè     ‚óè
    ‚óè‚óè‚óè‚óè‚óè‚óè‚óè
    Smooth boundary

High gamma (1.0):
    ‚óè‚óè‚óè‚óè‚óè
    ‚óè ‚óã ‚óè  ‚Üê –ú–æ–∂–µ –∑–∞—Ö–æ–ø–∏—Ç–∏ —ñ–Ω–¥–∏–≤—ñ–¥—É–∞–ª—å–Ω—ñ points
    ‚óè‚óè‚óè‚óè‚óè
    Complex boundary (–º–æ–∂–µ overfit!)
```

---

## –ü–µ—Ä–µ–≤–∞–≥–∏ —Ç–∞ –Ω–µ–¥–æ–ª—ñ–∫–∏

### –ü–µ—Ä–µ–≤–∞–≥–∏ ‚úì

| –ü–µ—Ä–µ–≤–∞–≥–∞ | –ü–æ—è—Å–Ω–µ–Ω–Ω—è |
|----------|-----------|
| **Kernel trick** | –ù–µ–ª—ñ–Ω—ñ–π–Ω—ñ boundaries |
| **Theoretical foundation** | Solid math theory |
| **Novelty detection** | –î–æ–±—Ä–µ –¥–ª—è –Ω–æ–≤–∏—Ö —Ç–∏–ø—ñ–≤ –∞–Ω–æ–º–∞–ª—ñ–π |
| **Smooth boundary** | –ù–µ —Ç–∞–∫–∏–π —á—É—Ç–ª–∏–≤–∏–π –¥–æ noise |
| **Few parameters** | nu, gamma (–ø—Ä–æ—Å—Ç–æ –Ω–∞–ª–∞—à—Ç—É–≤–∞—Ç–∏) |
| **Effective** | –ü—Ä–∞—Ü—é—î –¥–æ–±—Ä–µ –Ω–∞ –º–∞–ª–∏—Ö-—Å–µ—Ä–µ–¥–Ω—ñ—Ö –¥–∞–Ω–∏—Ö |

### –ù–µ–¥–æ–ª—ñ–∫–∏ ‚úó

| –ù–µ–¥–æ–ª—ñ–∫ | –ü–æ—è—Å–Ω–µ–Ω–Ω—è |
|---------|-----------|
| **–ü–æ–≤—ñ–ª—å–Ω–∏–π** | O(n¬≤) –¥–æ O(n¬≥) |
| **–ù–µ –º–∞—Å—à—Ç–∞–±—É—î—Ç—å—Å—è** | –ü–æ–≥–∞–Ω–æ –Ω–∞ >10K –∑—Ä–∞–∑–∫—ñ–≤ |
| **Memory intensive** | –ü–æ—Ç—Ä–µ–±—É—î –±–∞–≥–∞—Ç–æ –ø–∞–º'—è—Ç—ñ |
| **Sensitive to scaling** | –ü–æ—Ç—Ä–µ–±—É—î normalization |
| **Parameter tuning** | nu —Ç–∞ gamma —Ç—Ä–µ–±–∞ –ø—ñ–¥–±–∏—Ä–∞—Ç–∏ |
| **Binary output** | –¢—ñ–ª—å–∫–∏ anomaly/normal (–Ω–µ score) |

---

## –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –∑ —ñ–Ω—à–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏

| –ú–µ—Ç–æ–¥ | –®–≤–∏–¥–∫—ñ—Å—Ç—å | –ú–∞—Å—à—Ç–∞–±–æ–≤–∞–Ω—ñ—Å—Ç—å | Kernel trick | Interpretability |
|-------|-----------|-----------------|--------------|------------------|
| **One-Class SVM** | ‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚úÖ | ‚≠ê‚≠ê |
| **Isolation Forest** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚ùå | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **LOF** | ‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚ùå | ‚≠ê‚≠ê‚≠ê |
| **Autoencoder** | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö†Ô∏è | ‚≠ê |

**–ö–æ–ª–∏ —â–æ:**
- **–ú–∞–ª—ñ –¥–∞–Ω—ñ + –Ω–µ–ª—ñ–Ω—ñ–π–Ω—ñ patterns** ‚Üí One-Class SVM ‚úì
- **–í–µ–ª–∏–∫—ñ –¥–∞–Ω—ñ + —à–≤–∏–¥–∫—ñ—Å—Ç—å** ‚Üí Isolation Forest ‚úì
- **–õ–æ–∫–∞–ª—å–Ω—ñ outliers** ‚Üí LOF ‚úì
- **–°–∫–ª–∞–¥–Ω—ñ patterns** ‚Üí Autoencoder ‚úì

---

## –ü—Ä–∞–∫—Ç–∏—á–Ω—ñ –ø–æ—Ä–∞–¥–∏ üí°

### 1. –ó–ê–í–ñ–î–ò scale features

```python
# ‚ùå –ë–µ–∑ scaling
clf = OneClassSVM()
clf.fit(X)  # –ü–æ–≥–∞–Ω–æ!

# ‚úÖ –ó—ñ scaling
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

clf = OneClassSVM()
clf.fit(X_scaled)

# –î–ª—è –Ω–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö
X_new_scaled = scaler.transform(X_new)
```

### 2. –ü–æ—á–Ω–∏ –∑ RBF kernel —Ç–∞ default –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤

```python
# ‚úÖ –î–æ–±—Ä–∏–π –ø–æ—á–∞—Ç–æ–∫
clf = OneClassSVM(kernel='rbf', gamma='scale', nu=0.1)

# –ü–æ—Ç—ñ–º tune —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ
```

### 3. Grid search –¥–ª—è nu —Ç–∞ gamma

```python
from sklearn.model_selection import GridSearchCV

# Wrapper –¥–ª—è cross-validation
param_grid = {
    'gamma': [0.001, 0.01, 0.1, 1.0],
    'nu': [0.01, 0.05, 0.1, 0.2]
}

# –Ø–∫—â–æ —î validation set –∑ labels
best_f1 = 0
best_params = None

for gamma in param_grid['gamma']:
    for nu in param_grid['nu']:
        clf = OneClassSVM(gamma=gamma, nu=nu)
        clf.fit(X_train)
        y_pred = clf.predict(X_val)
        
        from sklearn.metrics import f1_score
        f1 = f1_score(y_val_true, y_pred, pos_label=-1)
        
        if f1 > best_f1:
            best_f1 = f1
            best_params = {'gamma': gamma, 'nu': nu}

print(f"Best params: {best_params}")
```

### 4. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π decision_function –¥–ª—è ranking

```python
# –ó–∞–º—ñ—Å—Ç—å binary predict
scores = clf.decision_function(X_test)

# Negative scores = more anomalous
sorted_indices = np.argsort(scores)

print("Most anomalous samples:")
for idx in sorted_indices[:10]:
    print(f"  Index {idx}: score = {scores[idx]:.4f}")
```

### 5. Ensemble –∑ —ñ–Ω—à–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏

```python
# Combine –∑ Isolation Forest
from sklearn.ensemble import IsolationForest

# One-Class SVM
ocsvm = OneClassSVM(nu=0.1)
ocsvm.fit(X_train)
ocsvm_scores = ocsvm.decision_function(X_test)

# Isolation Forest
iforest = IsolationForest(contamination=0.1)
iforest.fit(X_train)
if_scores = iforest.score_samples(X_test)

# Normalize scores
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()

ocsvm_scores_norm = scaler.fit_transform(ocsvm_scores.reshape(-1, 1)).ravel()
if_scores_norm = scaler.fit_transform(if_scores.reshape(-1, 1)).ravel()

# Combine (average)
combined_scores = (ocsvm_scores_norm + if_scores_norm) / 2

# Threshold
threshold = np.percentile(combined_scores, 10)  # Bottom 10%
y_pred_ensemble = (combined_scores < threshold).astype(int)
```

### 6. Cross-validation –¥–ª—è model selection

```python
# –Ø–∫—â–æ —î labeled validation set
from sklearn.model_selection import cross_val_score

# Wrapper –¥–ª—è scoring
class OCSVMWrapper:
    def __init__(self, nu=0.1, gamma='scale'):
        self.clf = OneClassSVM(nu=nu, gamma=gamma)
    
    def fit(self, X, y=None):
        self.clf.fit(X)
        return self
    
    def score(self, X, y):
        y_pred = self.clf.predict(X)
        from sklearn.metrics import f1_score
        return f1_score(y, y_pred, pos_label=-1)

# Cross-validation
wrapper = OCSVMWrapper(nu=0.1)
scores = cross_val_score(wrapper, X_train, y_train, cv=3)
print(f"CV F1-scores: {scores}")
print(f"Mean: {scores.mean():.3f} (+/- {scores.std():.3f})")
```

### 7. Incremental learning (streaming)

```python
# One-Class SVM –Ω–µ –º–∞—î incremental learning
# –î–ª—è streaming: –ø–µ—Ä—ñ–æ–¥–∏—á–Ω–æ refit

class StreamingOCSVM:
    def __init__(self, window_size=1000, nu=0.1):
        self.window_size = window_size
        self.clf = OneClassSVM(nu=nu)
        self.buffer = []
        self.scaler = StandardScaler()
    
    def add_sample(self, x):
        self.buffer.append(x)
        
        if len(self.buffer) >= self.window_size:
            # Refit
            X_train = np.array(self.buffer[-self.window_size:])
            X_scaled = self.scaler.fit_transform(X_train)
            self.clf.fit(X_scaled)
            
            # Keep half
            self.buffer = self.buffer[-self.window_size//2:]
    
    def predict(self, x):
        x_scaled = self.scaler.transform([x])
        return self.clf.predict(x_scaled)[0]

# Usage
detector = StreamingOCSVM()

for sample in data_stream:
    detector.add_sample(sample)
    
    if len(detector.buffer) >= detector.window_size:
        is_anomaly = detector.predict(sample)
        
        if is_anomaly == -1:
            alert("Anomaly detected!")
```

### 8. Visualize decision boundary (2D)

```python
if X.shape[1] == 2:
    # Mesh
    xx, yy = np.meshgrid(
        np.linspace(X[:, 0].min()-1, X[:, 0].max()+1, 200),
        np.linspace(X[:, 1].min()-1, X[:, 1].max()+1, 200)
    )
    
    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    
    plt.figure(figsize=(10, 7))
    plt.contourf(xx, yy, Z, levels=20, cmap='RdYlBu_r', alpha=0.6)
    plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors='black')
    plt.scatter(X[:, 0], X[:, 1], c='blue', s=20)
    plt.colorbar(label='Decision Function')
    plt.title('One-Class SVM Decision Boundary')
    plt.show()
```

### 9. Handle imbalanced data

```python
# –Ø–∫—â–æ training set –º—ñ—Å—Ç–∏—Ç—å —Ç—Ä–æ—Ö–∏ outliers
# Adjust nu –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–æ

estimated_outlier_fraction = 0.05  # 5% outliers –≤ train
clf = OneClassSVM(nu=estimated_outlier_fraction)
```

### 10. Combine –∑ dimensionality reduction

```python
# –î–ª—è high-dimensional data
from sklearn.decomposition import PCA

# PCA —Å–ø–æ—á–∞—Ç–∫—É
pca = PCA(n_components=10)
X_pca = pca.fit_transform(X_train)

# One-Class SVM –Ω–∞ reduced space
clf = OneClassSVM(nu=0.1)
clf.fit(X_pca)

# For new data
X_new_pca = pca.transform(X_new)
y_pred = clf.predict(X_new_pca)
```

---

## –ü–æ—à–∏—Ä–µ–Ω—ñ –ø–æ–º–∏–ª–∫–∏ ‚ùå

### 1. –ù–µ —Ä–æ–±–∏—Ç–∏ scaling

```python
# ‚ùå –ö–†–ò–¢–ò–ß–ù–ê –ø–æ–º–∏–ª–∫–∞
clf = OneClassSVM()
clf.fit(X_raw)  # Features –≤ —Ä—ñ–∑–Ω–∏—Ö scales!

# ‚úÖ –ó–ê–í–ñ–î–ò scale
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_raw)
clf.fit(X_scaled)
```

### 2. –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∏–π nu

```python
# ‚ùå nu = 0.5 (50% outliers?)
# –ó–∞–Ω–∞–¥—Ç–æ –º'—è–∫–æ!

# ‚úÖ –†–æ–∑—É–º–Ω–∏–π nu –±–∞–∑—É—é—á–∏—Å—å –Ω–∞ domain
nu = 0.01  # 1% –¥–ª—è fraud
nu = 0.05  # 5% –¥–ª—è defects
```

### 3. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –Ω–∞ –≤–µ–ª–∏–∫–∏—Ö –¥–∞–Ω–∏—Ö

```python
# ‚ùå 100,000 –∑—Ä–∞–∑–∫—ñ–≤
# –î—É–∂–µ –ø–æ–≤—ñ–ª—å–Ω–æ! –ì–æ–¥–∏–Ω–∏!

# ‚úÖ Sample —Å–ø–æ—á–∞—Ç–∫—É –∞–±–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–π Isolation Forest
if len(X) > 10000:
    # Use Isolation Forest instead
    from sklearn.ensemble import IsolationForest
    clf = IsolationForest()
```

### 4. –ó–∞–±—É—Ç–∏ –ø—Ä–æ kernel choice

```python
# ‚ùå Linear kernel –¥–ª—è –Ω–µ–ª—ñ–Ω—ñ–π–Ω–∏—Ö patterns

# ‚úÖ RBF –¥–ª—è –±—ñ–ª—å—à–æ—Å—Ç—ñ –≤–∏–ø–∞–¥–∫—ñ–≤
clf = OneClassSVM(kernel='rbf')
```

### 5. –ù–µ tune gamma

```python
# ‚ùå Default gamma –º–æ–∂–µ –±—É—Ç–∏ –ø–æ–≥–∞–Ω–∏–º

# ‚úÖ Grid search
for gamma in [0.001, 0.01, 0.1, 1.0]:
    clf = OneClassSVM(gamma=gamma)
    # Test and compare
```

---

## –†–µ–∞–ª—å–Ω—ñ –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è

### 1. Manufacturing Quality Control

```python
# Train –Ω–∞ GOOD products only
# Detect defects

clf = OneClassSVM(nu=0.01)  # 1% defect rate
clf.fit(sensor_readings_good)

# New product
is_defect = clf.predict([new_product_readings])[0] == -1

if is_defect:
    reject_product()
```

### 2. Medical Anomaly Detection

```python
# Train –Ω–∞ healthy patients
# Detect diseases

clf = OneClassSVM(kernel='rbf', nu=0.05)
clf.fit(healthy_patient_data)

# New patient
is_abnormal = clf.predict([patient_readings])[0] == -1

if is_abnormal:
    flag_for_doctor_review()
```

### 3. Video Surveillance

```python
# Train –Ω–∞ normal behavior
# Detect suspicious activity

clf = OneClassSVM(nu=0.1)
clf.fit(normal_activity_features)

# Real-time
for frame in video_stream:
    features = extract_features(frame)
    is_suspicious = clf.predict([features])[0] == -1
    
    if is_suspicious:
        alert_security()
```

---

## –ü–æ–≤'—è–∑–∞–Ω—ñ —Ç–µ–º–∏

- [[01_Isolation_Forest]] ‚Äî —à–≤–∏–¥—à–∞ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞
- [[03_Local_Outlier_Factor]] ‚Äî density-based
- [[04_Anomaly_Detection_Methods]] ‚Äî –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –≤—Å—ñ—Ö
- [[SVM_Classification]] ‚Äî two-class SVM

## –†–µ—Å—É—Ä—Å–∏

- [Original Paper (Sch√∂lkopf et al., 2001)](http://users.cecs.anu.edu.au/~williams/papers/P132.pdf)
- [Scikit-learn: One-Class SVM](https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html)
- [One-Class SVM Tutorial](https://scikit-learn.org/stable/auto_examples/svm/plot_oneclass.html)

---

## –ö–ª—é—á–æ–≤—ñ –≤–∏—Å–Ω–æ–≤–∫–∏

> One-Class SVM ‚Äî —Ü–µ kernel-based –∞–ª–≥–æ—Ä–∏—Ç–º –¥–ª—è novelty/anomaly detection, —â–æ –Ω–∞–≤—á–∞—î—Ç—å—Å—è —Ç—ñ–ª—å–∫–∏ –Ω–∞ –Ω–æ—Ä–º–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö —ñ –±—É–¥—É—î decision boundary, —è–∫–∞ –≤—ñ–¥–æ–∫—Ä–µ–º–ª—é—î –Ω–æ—Ä–º–∞–ª—å–Ω—ñ —Ç–æ—á–∫–∏ –≤—ñ–¥ origin —É feature space, –º–∞–∫—Å–∏–º—ñ–∑—É—é—á–∏ margin.

**–û—Å–Ω–æ–≤–Ω–∞ —ñ–¥–µ—è:**
- Train —Ç—ñ–ª—å–∫–∏ –Ω–∞ NORMAL data
- –ó–Ω–∞–π—Ç–∏ hyperplane/hypersphere –Ω–∞–≤–∫–æ–ª–æ –¥–∞–Ω–∏—Ö
- Kernel trick –¥–ª—è –Ω–µ–ª—ñ–Ω—ñ–π–Ω–∏—Ö boundaries

**–ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞:**
- Optimization: min ||w||¬≤ - œÅ + penalties
- Decision: f(x) = sign(Œ£Œ±·µ¢ K(x·µ¢, x) - œÅ)
- Kernel: RBF –Ω–∞–π–ø–æ–ø—É–ª—è—Ä–Ω—ñ—à–∏–π

**–ö–ª—é—á–æ–≤—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏:**
- **nu:** upper bound –Ω–∞ outliers (0.01-0.2)
- **gamma:** RBF width (0.001-1.0)
- **kernel:** 'rbf' (default), 'linear', 'poly'

**–ü–µ—Ä–µ–≤–∞–≥–∏:**
- ‚úÖ Kernel trick (–Ω–µ–ª—ñ–Ω—ñ–π–Ω—ñ boundaries)
- ‚úÖ Solid theory
- ‚úÖ Novelty detection
- ‚úÖ Smooth boundaries

**–ù–µ–¥–æ–ª—ñ–∫–∏:**
- ‚ùå –ü–æ–≤—ñ–ª—å–Ω–∏–π (O(n¬≤-n¬≥))
- ‚ùå –ù–µ –º–∞—Å—à—Ç–∞–±—É—î—Ç—å—Å—è (>10K)
- ‚ùå –ü–æ—Ç—Ä–µ–±—É—î scaling
- ‚ùå Parameter tuning

**–ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏:**
- –ú–∞–ª—ñ-—Å–µ—Ä–µ–¥–Ω—ñ –¥–∞–Ω—ñ + –Ω–µ–ª—ñ–Ω—ñ–π–Ω—ñ = One-Class SVM ‚úì
- –í–µ–ª–∏–∫—ñ –¥–∞–Ω—ñ ‚Üí Isolation Forest ‚úì
- –õ–æ–∫–∞–ª—å–Ω—ñ outliers ‚Üí LOF ‚úì
- –®–≤–∏–¥–∫—ñ—Å—Ç—å –∫—Ä–∏—Ç–∏—á–Ω–∞ ‚Üí Isolation Forest ‚úì

**–ü—Ä–∞–∫—Ç–∏—á–Ω—ñ –ø–æ—Ä–∞–¥–∏:**
- **–ó–ê–í–ñ–î–ò scale features** (StandardScaler)
- –ü–æ—á–Ω–∏ –∑ RBF kernel —Ç–∞ default
- Grid search –¥–ª—è nu —Ç–∞ gamma
- –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π decision_function –¥–ª—è ranking
- Ensemble –∑ Isolation Forest
- Sample —è–∫—â–æ >10K –∑—Ä–∞–∑–∫—ñ–≤

---

#ml #unsupervised-learning #anomaly-detection #one-class-svm #novelty-detection #kernel-methods #support-vector-machines #outlier-detection
