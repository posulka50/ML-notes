# Autoencoders (–ê–≤—Ç–æ–∫–æ–¥—É–≤–∞–ª—å–Ω–∏–∫–∏)

## –©–æ —Ü–µ?

**Autoencoder** ‚Äî —Ü–µ **neural network**, —è–∫–∞ –Ω–∞–≤—á–∞—î—Ç—å—Å—è —Å—Ç–∏—Å–∫–∞—Ç–∏ (encode) –¥–∞–Ω—ñ –≤ –Ω–∏–∑—å–∫–æ—Ä–æ–∑–º—ñ—Ä–Ω–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—è, –∞ –ø–æ—Ç—ñ–º –≤—ñ–¥–Ω–æ–≤–ª—é–≤–∞—Ç–∏ (decode) —ó—Ö –Ω–∞–∑–∞–¥. –¶–µ **unsupervised** –º–µ—Ç–æ–¥ –¥–ª—è dimensionality reduction —Ç–∞ feature learning.

**–ì–æ–ª–æ–≤–Ω–∞ —ñ–¥–µ—è:** –Ω–∞–≤—á–∏—Ç–∏ –º–µ—Ä–µ–∂—É –≤—ñ–¥—Ç–≤–æ—Ä—é–≤–∞—Ç–∏ –≤—Ö—ñ–¥ –Ω–∞ –≤–∏—Ö–æ–¥—ñ, –∑–º—É—à—É—é—á–∏ —ó—ó –ø—Ä–æ–ø—É—Å—Ç–∏—Ç–∏ –¥–∞–Ω—ñ —á–µ—Ä–µ–∑ "–≤—É–∑—å–∫–µ –≥–æ—Ä–ª–µ—á–∫–æ" (bottleneck), —â–æ —Å—Ç–≤–æ—Ä—é—î compressed representation.

## –ù–∞–≤—ñ—â–æ –ø–æ—Ç—Ä—ñ–±–Ω—ñ?

- üóúÔ∏è **Dimensionality reduction** ‚Äî –Ω–µ–ª—ñ–Ω—ñ–π–Ω–∞ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ PCA
- üé® **Feature learning** ‚Äî –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–µ –≤–∏—è–≤–ª–µ–Ω–Ω—è –∫–æ—Ä–∏—Å–Ω–∏—Ö features
- üñºÔ∏è **Image compression** ‚Äî —Å—Ç–∏—Å–Ω–µ–Ω–Ω—è –∑–æ–±—Ä–∞–∂–µ–Ω—å
- üîç **Anomaly detection** ‚Äî –≤–∏—è–≤–ª–µ–Ω–Ω—è –∞–Ω–æ–º–∞–ª—ñ–π
- üé≠ **Denoising** ‚Äî –≤–∏–¥–∞–ª–µ–Ω–Ω—è —à—É–º—É –∑ –¥–∞–Ω–∏—Ö
- üß¨ **Generative models** ‚Äî —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –Ω–æ–≤–∏—Ö –∑—Ä–∞–∑–∫—ñ–≤ (VAE)
- üìä **Visualization** ‚Äî 2D/3D embedding –¥–ª—è –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó

## –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏?

**–ü–æ—Ç—Ä—ñ–±–Ω–æ:**
- **–ù–µ–ª—ñ–Ω—ñ–π–Ω—ñ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏** ‚Äî —Å–∫–ª–∞–¥–Ω—ñ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ –≤ –¥–∞–Ω–∏—Ö
- **Unsupervised learning** ‚Äî –Ω–µ–º–∞—î labels
- **Deep features** ‚Äî –ø–æ—Ç—Ä—ñ–±–Ω—ñ abstract representations
- **–í–µ–ª–∏–∫—ñ –¥–∞–Ω—ñ** ‚Äî –±–∞–≥–∞—Ç–æ –∑—Ä–∞–∑–∫—ñ–≤ –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è NN
- **–ì–Ω—É—á–∫—ñ—Å—Ç—å** ‚Äî —Ä—ñ–∑–Ω—ñ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏ –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö –∑–∞–¥–∞—á
- **Anomaly detection** ‚Äî reconstruction error —è–∫ –º—ñ—Ä–∞

**–ù–µ –ø–æ—Ç—Ä—ñ–±–Ω–æ:**
- **–ú–∞–ª—ñ –¥–∞–Ω—ñ** (< 1000 –∑—Ä–∞–∑–∫—ñ–≤) ‚Üí PCA, t-SNE
- **–õ—ñ–Ω—ñ–π–Ω—ñ –¥–∞–Ω—ñ** ‚Üí PCA –ø—Ä–æ—Å—Ç—ñ—à–∏–π
- **–®–≤–∏–¥–∫—ñ—Å—Ç—å** –∫—Ä–∏—Ç–∏—á–Ω–∞ ‚Üí PCA —à–≤–∏–¥—à–µ
- **–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å** ‚Üí PCA –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∏ –∑—Ä–æ–∑—É–º—ñ–ª—ñ—à—ñ
- **–ü—Ä–æ—Å—Ç–æ—Ç–∞** –≤–∞–∂–ª–∏–≤–∞ ‚Üí traditional methods

---

## –ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞

### –ë–∞–∑–æ–≤–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞

```
Input ‚Üí Encoder ‚Üí Bottleneck ‚Üí Decoder ‚Üí Output
                     ‚Üì
               (latent space)
               (compressed)
```

**–ö–æ–º–ø–æ–Ω–µ–Ω—Ç–∏:**

1. **Encoder:** —Å—Ç–∏—Å–∫–∞—î –≤—Ö—ñ–¥ –≤ latent representation
2. **Bottleneck (latent space):** –Ω–∏–∑—å–∫–æ—Ä–æ–∑–º—ñ—Ä–Ω–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—è
3. **Decoder:** –≤—ñ–¥–Ω–æ–≤–ª—é—î –≤—Ö—ñ–¥ –∑ latent representation

### –î—ñ–∞–≥—Ä–∞–º–∞

```
Input (784D)
     ‚Üì
  [Dense 128] ‚Üê Encoder
     ‚Üì
  [Dense 64]
     ‚Üì
  [Dense 32] ‚Üê Bottleneck (latent space)
     ‚Üì
  [Dense 64] ‚Üê Decoder
     ‚Üì
  [Dense 128]
     ‚Üì
Output (784D)

–ú–µ—Ç–∞: Output ‚âà Input
```

### –ú–∞—Ç–µ–º–∞—Ç–∏—á–Ω–æ

**Encoder:**
$$\mathbf{z} = f_{enc}(\mathbf{x})$$

**Decoder:**
$$\hat{\mathbf{x}} = f_{dec}(\mathbf{z})$$

**Loss (reconstruction error):**
$$L = \|\mathbf{x} - \hat{\mathbf{x}}\|^2 = \|\mathbf{x} - f_{dec}(f_{enc}(\mathbf{x}))\|^2$$

---

## –ü—Ä–æ—Å—Ç–∏–π –ø—Ä–∏–∫–ª–∞–¥: Linear Autoencoder

### –ö–æ–¥ (PyTorch)

```python
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_digits
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –¥–∞–Ω—ñ
digits = load_digits()
X = digits.data  # (1797, 64)
y = digits.target

# Scaling
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# To tensor
X_tensor = torch.FloatTensor(X_scaled)

# Split
X_train, X_test = train_test_split(X_tensor, test_size=0.2, random_state=42)

print(f"Train shape: {X_train.shape}")  # (1437, 64)
print(f"Test shape: {X_test.shape}")    # (360, 64)

# –ü—Ä–æ—Å—Ç–∏–π Autoencoder
class LinearAutoencoder(nn.Module):
    def __init__(self, input_dim=64, latent_dim=2):
        super(LinearAutoencoder, self).__init__()
        
        # Encoder
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 32),
            nn.ReLU(),
            nn.Linear(32, latent_dim)  # Bottleneck
        )
        
        # Decoder
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, 32),
            nn.ReLU(),
            nn.Linear(32, input_dim)
        )
    
    def forward(self, x):
        z = self.encoder(x)  # Encode
        x_reconstructed = self.decoder(z)  # Decode
        return x_reconstructed
    
    def encode(self, x):
        return self.encoder(x)

# –°—Ç–≤–æ—Ä–∏—Ç–∏ –º–æ–¥–µ–ª—å
model = LinearAutoencoder(input_dim=64, latent_dim=2)
print(model)

# Loss —Ç–∞ optimizer
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# –ù–∞–≤—á–∞–Ω–Ω—è
num_epochs = 50
train_losses = []

for epoch in range(num_epochs):
    # Forward pass
    outputs = model(X_train)
    loss = criterion(outputs, X_train)
    
    # Backward pass
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    train_losses.append(loss.item())
    
    if (epoch + 1) % 10 == 0:
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è loss
plt.figure(figsize=(10, 5))
plt.plot(train_losses)
plt.xlabel('Epoch')
plt.ylabel('MSE Loss')
plt.title('Training Loss')
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# –û—Ü—ñ–Ω–∫–∞ –Ω–∞ test
with torch.no_grad():
    test_outputs = model(X_test)
    test_loss = criterion(test_outputs, X_test)
    print(f'\nTest Loss: {test_loss.item():.4f}')

# Encode –≤ 2D
with torch.no_grad():
    z_train = model.encode(X_train).numpy()
    z_test = model.encode(X_test).numpy()

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è latent space
plt.figure(figsize=(10, 7))
scatter = plt.scatter(z_train[:, 0], z_train[:, 1], 
                     c=y[:len(z_train)], cmap='tab10', s=20, alpha=0.6)
plt.colorbar(scatter, label='Digit')
plt.xlabel('Latent Dimension 1')
plt.ylabel('Latent Dimension 2')
plt.title('2D Latent Space (Autoencoder)')
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏—Ö —Ç–∞ –≤—ñ–¥–Ω–æ–≤–ª–µ–Ω–∏—Ö –∑–æ–±—Ä–∞–∂–µ–Ω—å
n_display = 10
fig, axes = plt.subplots(2, n_display, figsize=(15, 3))

with torch.no_grad():
    reconstructed = model(X_test[:n_display]).numpy()

for i in range(n_display):
    # Original
    axes[0, i].imshow(X_test[i].numpy().reshape(8, 8), cmap='gray')
    axes[0, i].axis('off')
    if i == 0:
        axes[0, i].set_title('Original', fontsize=10)
    
    # Reconstructed
    axes[1, i].imshow(reconstructed[i].reshape(8, 8), cmap='gray')
    axes[1, i].axis('off')
    if i == 0:
        axes[1, i].set_title('Reconstructed', fontsize=10)

plt.tight_layout()
plt.show()
```

---

## –¢–∏–ø–∏ Autoencoders

### 1. Undercomplete Autoencoder

**–©–æ —Ü–µ:** Latent dimension < input dimension (—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∏–π).

```python
class UndercompleteAE(nn.Module):
    def __init__(self):
        super(UndercompleteAE, self).__init__()
        
        # 784 ‚Üí 128 ‚Üí 64 ‚Üí 32 (bottleneck)
        self.encoder = nn.Sequential(
            nn.Linear(784, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 32)
        )
        
        # 32 ‚Üí 64 ‚Üí 128 ‚Üí 784
        self.decoder = nn.Sequential(
            nn.Linear(32, 64),
            nn.ReLU(),
            nn.Linear(64, 128),
            nn.ReLU(),
            nn.Linear(128, 784),
            nn.Sigmoid()  # –î–ª—è pixel values [0,1]
        )
    
    def forward(self, x):
        z = self.encoder(x)
        x_recon = self.decoder(z)
        return x_recon
```

**–í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è:** Dimensionality reduction, compression.

### 2. Denoising Autoencoder (DAE)

**–©–æ —Ü–µ:** –ù–∞–≤—á–∞—î—Ç—å—Å—è –≤—ñ–¥–Ω–æ–≤–ª—é–≤–∞—Ç–∏ —á–∏—Å—Ç–∏–π –≤—Ö—ñ–¥ –∑ –∑–∞—à—É–º–ª–µ–Ω–æ–≥–æ.

```python
class DenoisingAE(nn.Module):
    def __init__(self, input_dim=784, latent_dim=32):
        super(DenoisingAE, self).__init__()
        
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.ReLU(),
            nn.Linear(128, latent_dim)
        )
        
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, 128),
            nn.ReLU(),
            nn.Linear(128, input_dim),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        z = self.encoder(x)
        x_recon = self.decoder(z)
        return x_recon

# –ù–∞–≤—á–∞–Ω–Ω—è
def add_noise(x, noise_factor=0.3):
    noisy = x + noise_factor * torch.randn_like(x)
    return torch.clamp(noisy, 0., 1.)

model = DenoisingAE()
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

for epoch in range(num_epochs):
    # –î–æ–¥–∞—Ç–∏ —à—É–º –¥–æ –≤—Ö–æ–¥—É
    noisy_input = add_noise(X_train)
    
    # Forward (–≤—ñ–¥–Ω–æ–≤–∏—Ç–∏ —á–∏—Å—Ç–∏–π –≤—Ö—ñ–¥ –∑ –∑–∞—à—É–º–ª–µ–Ω–æ–≥–æ)
    outputs = model(noisy_input)
    loss = criterion(outputs, X_train)  # –ü–æ—Ä—ñ–≤–Ω—è—Ç–∏ –∑ —á–∏—Å—Ç–∏–º!
    
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
```

**–ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è:**
- –í–∏–¥–∞–ª–µ–Ω–Ω—è —à—É–º—É –∑ –∑–æ–±—Ä–∞–∂–µ–Ω—å
- Robust feature learning

### 3. Sparse Autoencoder

**–©–æ —Ü–µ:** –î–æ–¥–∞—î sparsity constraint –Ω–∞ latent representation.

```python
class SparseAE(nn.Module):
    def __init__(self, input_dim=784, latent_dim=64):
        super(SparseAE, self).__init__()
        
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.ReLU(),
            nn.Linear(128, latent_dim),
            nn.ReLU()
        )
        
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, 128),
            nn.ReLU(),
            nn.Linear(128, input_dim),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        z = self.encoder(x)
        x_recon = self.decoder(z)
        return x_recon, z

# Sparse loss
def sparse_loss(z, sparsity_target=0.05, sparsity_weight=1e-3):
    # L1 regularization –Ω–∞ activation
    sparsity = torch.mean(torch.abs(z))
    return sparsity_weight * torch.abs(sparsity - sparsity_target)

# –ù–∞–≤—á–∞–Ω–Ω—è
for epoch in range(num_epochs):
    outputs, z = model(X_train)
    
    recon_loss = criterion(outputs, X_train)
    sparse_penalty = sparse_loss(z)
    
    total_loss = recon_loss + sparse_penalty
    
    optimizer.zero_grad()
    total_loss.backward()
    optimizer.step()
```

**–ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è:**
- Feature learning (–º–∞–ª–æ –∞–∫—Ç–∏–≤–Ω–∏—Ö –Ω–µ–π—Ä–æ–Ω—ñ–≤)
- Interpretable features

### 4. Variational Autoencoder (VAE)

**–©–æ —Ü–µ:** Generative model, —è–∫–∏–π –≤—á–∏—Ç—å probabilistic latent space.

```python
class VAE(nn.Module):
    def __init__(self, input_dim=784, latent_dim=20):
        super(VAE, self).__init__()
        
        # Encoder (outputs mean and log_var)
        self.fc1 = nn.Linear(input_dim, 400)
        self.fc_mu = nn.Linear(400, latent_dim)
        self.fc_logvar = nn.Linear(400, latent_dim)
        
        # Decoder
        self.fc3 = nn.Linear(latent_dim, 400)
        self.fc4 = nn.Linear(400, input_dim)
    
    def encode(self, x):
        h = torch.relu(self.fc1(x))
        return self.fc_mu(h), self.fc_logvar(h)
    
    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std
    
    def decode(self, z):
        h = torch.relu(self.fc3(z))
        return torch.sigmoid(self.fc4(h))
    
    def forward(self, x):
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar)
        return self.decode(z), mu, logvar

# VAE loss
def vae_loss(recon_x, x, mu, logvar):
    # Reconstruction loss
    BCE = nn.functional.binary_cross_entropy(recon_x, x, reduction='sum')
    
    # KL divergence
    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
    
    return BCE + KLD

# –ù–∞–≤—á–∞–Ω–Ω—è
model = VAE()
optimizer = optim.Adam(model.parameters(), lr=1e-3)

for epoch in range(num_epochs):
    recon_batch, mu, logvar = model(X_train)
    loss = vae_loss(recon_batch, X_train, mu, logvar)
    
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

# –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –Ω–æ–≤–∏—Ö –∑—Ä–∞–∑–∫—ñ–≤
with torch.no_grad():
    z = torch.randn(64, 20)  # Sample from N(0,1)
    generated = model.decode(z).numpy()
    
    # –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–∏—Ö —Ü–∏—Ñ—Ä
    fig, axes = plt.subplots(8, 8, figsize=(10, 10))
    for i, ax in enumerate(axes.flat):
        ax.imshow(generated[i].reshape(28, 28), cmap='gray')
        ax.axis('off')
    plt.tight_layout()
    plt.show()
```

**–ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è:**
- –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –Ω–æ–≤–∏—Ö –∑–æ–±—Ä–∞–∂–µ–Ω—å
- Interpolation –≤ latent space
- Probabilistic modeling

### 5. Convolutional Autoencoder

**–©–æ —Ü–µ:** –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î CNN layers –¥–ª—è –∑–æ–±—Ä–∞–∂–µ–Ω—å.

```python
class ConvAutoencoder(nn.Module):
    def __init__(self):
        super(ConvAutoencoder, self).__init__()
        
        # Encoder
        self.encoder = nn.Sequential(
            nn.Conv2d(1, 16, 3, stride=2, padding=1),  # 28x28 ‚Üí 14x14
            nn.ReLU(),
            nn.Conv2d(16, 32, 3, stride=2, padding=1), # 14x14 ‚Üí 7x7
            nn.ReLU(),
            nn.Conv2d(32, 64, 7)  # 7x7 ‚Üí 1x1 (bottleneck)
        )
        
        # Decoder
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(64, 32, 7),  # 1x1 ‚Üí 7x7
            nn.ReLU(),
            nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1),  # 7x7 ‚Üí 14x14
            nn.ReLU(),
            nn.ConvTranspose2d(16, 1, 3, stride=2, padding=1, output_padding=1),   # 14x14 ‚Üí 28x28
            nn.Sigmoid()
        )
    
    def forward(self, x):
        z = self.encoder(x)
        x_recon = self.decoder(z)
        return x_recon

# –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è
from torchvision import datasets, transforms

# MNIST dataset
transform = transforms.ToTensor()
train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)

model = ConvAutoencoder()
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# –ù–∞–≤—á–∞–Ω–Ω—è
for epoch in range(10):
    for data, _ in train_loader:
        outputs = model(data)
        loss = criterion(outputs, data)
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    
    print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}')
```

**–ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è:**
- Image compression
- Feature extraction –¥–ª—è CNN
- Image-to-image tasks

---

## –ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è

### 1. Dimensionality Reduction

```python
# Autoencoder —è–∫ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ PCA

# 1. –ù–∞–≤—á–∏—Ç–∏ autoencoder
model = LinearAutoencoder(input_dim=784, latent_dim=50)
# ... train ...

# 2. Extract features
with torch.no_grad():
    X_reduced = model.encode(X_tensor).numpy()

print(f"Original: {X_tensor.shape}")   # (1797, 784)
print(f"Reduced: {X_reduced.shape}")    # (1797, 50)

# 3. –í–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ –¥–ª—è downstream tasks
from sklearn.ensemble import RandomForestClassifier

clf = RandomForestClassifier()
clf.fit(X_reduced[:1000], y[:1000])
accuracy = clf.score(X_reduced[1000:], y[1000:])
print(f"Accuracy: {accuracy:.4f}")
```

### 2. Anomaly Detection

```python
# –ù–∞–≤—á–∏—Ç–∏ autoencoder –Ω–∞ –Ω–æ—Ä–º–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö
# –ê–Ω–æ–º–∞–ª—ñ—ó –º–∞—Ç–∏–º—É—Ç—å –≤–∏—Å–æ–∫–∏–π reconstruction error

class AnomalyDetector:
    def __init__(self, model, threshold=None):
        self.model = model
        self.threshold = threshold
    
    def fit(self, X_normal):
        # Train autoencoder
        # ... training loop ...
        
        # –û–±—á–∏—Å–ª–∏—Ç–∏ threshold –Ω–∞ –Ω–æ—Ä–º–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö
        with torch.no_grad():
            recon = self.model(X_normal)
            errors = torch.mean((X_normal - recon) ** 2, dim=1)
            self.threshold = torch.quantile(errors, 0.95)  # 95th percentile
    
    def predict(self, X):
        with torch.no_grad():
            recon = self.model(X)
            errors = torch.mean((X - recon) ** 2, dim=1)
            
            # –ê–Ω–æ–º–∞–ª—ñ—è —è–∫—â–æ error > threshold
            is_anomaly = errors > self.threshold
            
        return is_anomaly.numpy(), errors.numpy()

# –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è
detector = AnomalyDetector(model)
detector.fit(X_normal_train)

is_anomaly, errors = detector.predict(X_test)

print(f"Anomalies detected: {is_anomaly.sum()}")

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
plt.hist(errors[~is_anomaly], bins=50, alpha=0.5, label='Normal')
plt.hist(errors[is_anomaly], bins=50, alpha=0.5, label='Anomaly')
plt.axvline(detector.threshold, color='red', linestyle='--', label='Threshold')
plt.xlabel('Reconstruction Error')
plt.ylabel('Count')
plt.legend()
plt.show()
```

### 3. Denoising

```python
# –í–∏–¥–∞–ª–∏—Ç–∏ —à—É–º –∑ –∑–æ–±—Ä–∞–∂–µ–Ω—å

# 1. –ù–∞–≤—á–∏—Ç–∏ denoising autoencoder
dae = DenoisingAE()
# ... train on (noisy_input, clean_target) ...

# 2. Denoise –Ω–æ–≤–∏—Ö –∑–æ–±—Ä–∞–∂–µ–Ω—å
with torch.no_grad():
    noisy_images = add_noise(clean_images)
    denoised = dae(noisy_images)

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
fig, axes = plt.subplots(3, 10, figsize=(15, 5))
for i in range(10):
    axes[0, i].imshow(clean_images[i].reshape(28, 28), cmap='gray')
    axes[0, i].axis('off')
    if i == 0:
        axes[0, i].set_ylabel('Clean', fontsize=12)
    
    axes[1, i].imshow(noisy_images[i].reshape(28, 28), cmap='gray')
    axes[1, i].axis('off')
    if i == 0:
        axes[1, i].set_ylabel('Noisy', fontsize=12)
    
    axes[2, i].imshow(denoised[i].reshape(28, 28), cmap='gray')
    axes[2, i].axis('off')
    if i == 0:
        axes[2, i].set_ylabel('Denoised', fontsize=12)

plt.tight_layout()
plt.show()
```

### 4. Image Compression

```python
# –°—Ç–∏—Å–Ω–µ–Ω–Ω—è –∑–æ–±—Ä–∞–∂–µ–Ω—å

# Original: 28x28 = 784 pixels
# Latent: 32 dimensions

compression_ratio = 784 / 32  # 24.5x

# Encode
with torch.no_grad():
    latent = model.encode(images)  # (N, 32)

# Store only latent representation (compressed)
# Decode when needed
with torch.no_grad():
    reconstructed = model.decode(latent)  # (N, 784)

print(f"Compression ratio: {compression_ratio:.1f}x")
print(f"MSE: {torch.mean((images - reconstructed)**2).item():.6f}")
```

### 5. Feature Learning

```python
# –í–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ encoder —è–∫ feature extractor

# 1. –ù–∞–≤—á–∏—Ç–∏ autoencoder (unsupervised)
model = ConvAutoencoder()
# ... train ...

# 2. Freeze encoder
for param in model.encoder.parameters():
    param.requires_grad = False

# 3. –î–æ–¥–∞—Ç–∏ classifier
class Classifier(nn.Module):
    def __init__(self, encoder):
        super(Classifier, self).__init__()
        self.encoder = encoder
        self.classifier = nn.Linear(64, 10)  # 10 classes
    
    def forward(self, x):
        features = self.encoder(x)
        features = features.view(features.size(0), -1)  # Flatten
        logits = self.classifier(features)
        return logits

clf_model = Classifier(model.encoder)

# 4. Fine-tune classifier (supervised)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(clf_model.classifier.parameters(), lr=0.001)

# ... training loop ...
```

---

## –ü–µ—Ä–µ–≤–∞–≥–∏ —Ç–∞ –Ω–µ–¥–æ–ª—ñ–∫–∏

### –ü–µ—Ä–µ–≤–∞–≥–∏ ‚úì

| –ü–µ—Ä–µ–≤–∞–≥–∞ | –ü–æ—è—Å–Ω–µ–Ω–Ω—è |
|----------|-----------|
| **–ù–µ–ª—ñ–Ω—ñ–π–Ω—ñ—Å—Ç—å** | –í–ª–æ–≤–ª—é—î —Å–∫–ª–∞–¥–Ω—ñ –Ω–µ–ª—ñ–Ω—ñ–π–Ω—ñ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏ |
| **Flexibility** | –†—ñ–∑–Ω—ñ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏ –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö –∑–∞–¥–∞—á |
| **Deep features** | –Ü—î—Ä–∞—Ä—Ö—ñ—á–Ω—ñ abstract representations |
| **Unsupervised** | –ù–µ –ø–æ—Ç—Ä–µ–±—É—î labels |
| **Versatility** | Reduction, denoising, generation, anomaly detection |
| **Scalability** | –ü—Ä–∞—Ü—é—î –Ω–∞ –≤–µ–ª–∏–∫–∏—Ö –¥–∞–Ω–∏—Ö |

### –ù–µ–¥–æ–ª—ñ–∫–∏ ‚úó

| –ù–µ–¥–æ–ª—ñ–∫ | –ü–æ—è—Å–Ω–µ–Ω–Ω—è |
|---------|-----------|
| **–°–∫–ª–∞–¥–Ω—ñ—Å—Ç—å** | –ü–æ—Ç—Ä–µ–±—É—î hyperparameter tuning |
| **–û–±—á–∏—Å–ª–µ–Ω–Ω—è** | –ü–æ–≤—ñ–ª—å–Ω—ñ—à–µ –∑–∞ PCA |
| **–î–∞–Ω—ñ** | –ü–æ—Ç—Ä–µ–±—É—î –±–∞–≥–∞—Ç–æ –∑—Ä–∞–∑–∫—ñ–≤ –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è |
| **–ù–µ —ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–Ω—ñ** | Latent dimensions –≤–∞–∂–∫–æ –∑—Ä–æ–∑—É–º—ñ—Ç–∏ |
| **–õ–æ–∫–∞–ª—å–Ω—ñ –º—ñ–Ω—ñ–º—É–º–∏** | –ú–æ–∂–µ –∑–∞—Å—Ç—Ä—è–≥—Ç–∏ –ø—Ä–∏ –Ω–∞–≤—á–∞–Ω–Ω—ñ |
| **Overfitting** | –õ–µ–≥–∫–æ –ø–µ—Ä–µ–æ–±—É—á–∏—Ç–∏—Å—å –Ω–∞ –º–∞–ª–∏—Ö –¥–∞–Ω–∏—Ö |

---

## –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –∑ —ñ–Ω—à–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏

| –ú–µ—Ç–æ–¥ | –õ—ñ–Ω—ñ–π–Ω–∏–π | Supervised | –®–≤–∏–¥–∫—ñ—Å—Ç—å | –î–ª—è ML | –ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∏–π |
|-------|----------|------------|-----------|--------|--------------|
| **PCA** | ‚úÖ | ‚ùå | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ | ‚ùå |
| **Autoencoder** | ‚ùå | ‚ùå | ‚≠ê‚≠ê | ‚úÖ | ‚ö†Ô∏è (VAE) |
| **VAE** | ‚ùå | ‚ùå | ‚≠ê‚≠ê | ‚ö†Ô∏è | ‚úÖ |
| **t-SNE** | ‚ùå | ‚ùå | ‚≠ê | ‚ùå | ‚ùå |
| **UMAP** | ‚ùå | ‚ùå | ‚≠ê‚≠ê‚≠ê | ‚úÖ | ‚ùå |

---

## –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ Autoencoders

### –Ü–¥–µ–∞–ª—å–Ω–æ –ø—ñ–¥—Ö–æ–¥–∏—Ç—å ‚úì

- **–ù–µ–ª—ñ–Ω—ñ–π–Ω—ñ –¥–∞–Ω—ñ** ‚Äî —Å–∫–ª–∞–¥–Ω—ñ manifolds
- **–í–µ–ª–∏–∫—ñ –¥–∞–Ω—ñ** ‚Äî –±–∞–≥–∞—Ç–æ –∑—Ä–∞–∑–∫—ñ–≤ –¥–ª—è NN
- **Image data** ‚Äî convolutional autoencoders
- **Denoising** ‚Äî –≤–∏–¥–∞–ª–µ–Ω–Ω—è —à—É–º—É
- **Anomaly detection** ‚Äî reconstruction error
- **Generation** ‚Äî VAE –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó
- **Deep features** ‚Äî –¥–ª—è downstream tasks

### –ö—Ä–∞—â–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ —ñ–Ω—à–µ ‚úó

- **–ú–∞–ª—ñ –¥–∞–Ω—ñ** (< 1000) ‚Üí PCA, t-SNE
- **–õ—ñ–Ω—ñ–π–Ω—ñ –¥–∞–Ω—ñ** ‚Üí PCA –ø—Ä–æ—Å—Ç—ñ—à–∏–π
- **–®–≤–∏–¥–∫—ñ—Å—Ç—å –∫—Ä–∏—Ç–∏—á–Ω–∞** ‚Üí PCA
- **–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å** ‚Üí PCA
- **–¢—ñ–ª—å–∫–∏ –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è** ‚Üí t-SNE, UMAP —à–≤–∏–¥—à–µ

---

## –ü—Ä–∞–∫—Ç–∏—á–Ω—ñ –ø–æ—Ä–∞–¥–∏ üí°

### 1. –ü–æ—á–Ω–∏ –∑ –ø—Ä–æ—Å—Ç–æ—ó –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏

```python
# ‚úÖ –ü—Ä–æ—Å—Ç–∞ ‚Üí —Å–∫–ª–∞–¥–Ω–∞
# –°–ø–æ—á–∞—Ç–∫—É linear layers
model = LinearAutoencoder(input_dim=784, latent_dim=32)

# –Ø–∫—â–æ –Ω–µ –≤–∏—Å—Ç–∞—á–∞—î ‚Üí –¥–æ–¥–∞–π layers
# –Ø–∫—â–æ overfitting ‚Üí –∑–º–µ–Ω—à capacity
```

### 2. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π –ø—Ä–∞–≤–∏–ª—å–Ω—É activation

```python
# –î–ª—è pixel values [0, 1]
self.decoder = nn.Sequential(
    ...,
    nn.Sigmoid()  # ‚Üê output [0, 1]
)

# –î–ª—è normalized data [-1, 1]
self.decoder = nn.Sequential(
    ...,
    nn.Tanh()  # ‚Üê output [-1, 1]
)

# –î–ª—è –±—É–¥—å-—è–∫–∏—Ö –∑–Ω–∞—á–µ–Ω—å
self.decoder = nn.Sequential(
    ...,
    # No activation
)
```

### 3. Regularization –¥–ª—è overfitting

```python
# Dropout
self.encoder = nn.Sequential(
    nn.Linear(784, 128),
    nn.ReLU(),
    nn.Dropout(0.2),  # ‚Üê Dropout
    nn.Linear(128, 32)
)

# L2 regularization
optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)
```

### 4. Batch Normalization –¥–ª—è stability

```python
self.encoder = nn.Sequential(
    nn.Linear(784, 128),
    nn.BatchNorm1d(128),  # ‚Üê BatchNorm
    nn.ReLU(),
    nn.Linear(128, 32)
)
```

### 5. –†—ñ–∑–Ω—ñ loss functions

```python
# MSE –¥–ª—è regression-like
criterion = nn.MSELoss()

# Binary Cross-Entropy –¥–ª—è binary inputs
criterion = nn.BCELoss()

# –ö–æ–º–±—ñ–Ω–∞—Ü—ñ—è
def combined_loss(recon, target, z):
    mse = nn.MSELoss()(recon, target)
    l1 = torch.mean(torch.abs(z))  # Sparsity
    return mse + 0.01 * l1
```

### 6. Learning rate scheduling

```python
optimizer = optim.Adam(model.parameters(), lr=0.001)
scheduler = optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, 'min', patience=5
)

for epoch in range(num_epochs):
    # Train...
    loss = train_epoch()
    
    # Update learning rate
    scheduler.step(loss)
```

### 7. Early stopping

```python
best_loss = float('inf')
patience = 10
patience_counter = 0

for epoch in range(num_epochs):
    val_loss = validate()
    
    if val_loss < best_loss:
        best_loss = val_loss
        patience_counter = 0
        # Save model
        torch.save(model.state_dict(), 'best_model.pth')
    else:
        patience_counter += 1
    
    if patience_counter >= patience:
        print("Early stopping!")
        break
```

### 8. Visualize latent space

```python
# –î–ª—è 2D latent space
with torch.no_grad():
    z = model.encode(X_test)
    
plt.scatter(z[:, 0], z[:, 1], c=y_test, cmap='tab10')
plt.colorbar()
plt.title('2D Latent Space')
plt.show()
```

### 9. Interpolation

```python
# –Ü–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü—ñ—è –º—ñ–∂ –¥–≤–æ–º–∞ –∑—Ä–∞–∑–∫–∞–º–∏
def interpolate(model, x1, x2, steps=10):
    with torch.no_grad():
        z1 = model.encode(x1.unsqueeze(0))
        z2 = model.encode(x2.unsqueeze(0))
        
        # Linear interpolation in latent space
        alphas = torch.linspace(0, 1, steps)
        interpolated = []
        
        for alpha in alphas:
            z = (1 - alpha) * z1 + alpha * z2
            x = model.decode(z)
            interpolated.append(x.squeeze())
    
    return torch.stack(interpolated)

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
interpolated = interpolate(model, img1, img2, steps=10)
fig, axes = plt.subplots(1, 10, figsize=(15, 2))
for i, ax in enumerate(axes):
    ax.imshow(interpolated[i].reshape(28, 28), cmap='gray')
    ax.axis('off')
plt.show()
```

### 10. Transfer learning

```python
# –í–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ pretrained encoder
pretrained = torch.load('pretrained_autoencoder.pth')
encoder = pretrained.encoder

# Freeze
for param in encoder.parameters():
    param.requires_grad = False

# Use for new task
```

---

## –ü–æ—à–∏—Ä–µ–Ω—ñ –ø–æ–º–∏–ª–∫–∏ ‚ùå

### 1. –ó–∞–Ω–∞–¥—Ç–æ —Å–∫–ª–∞–¥–Ω–∞ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è –º–∞–ª–∏—Ö –¥–∞–Ω–∏—Ö

```python
# ‚ùå Overfitting
model = nn.Sequential(
    nn.Linear(100, 1000),  # –ó–∞–Ω–∞–¥—Ç–æ –±–∞–≥–∞—Ç–æ –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤!
    nn.ReLU(),
    nn.Linear(1000, 500),
    nn.ReLU(),
    nn.Linear(500, 10)
)
# –ù–∞ 100 –∑—Ä–∞–∑–∫–∞—Ö ‚Üí overfitting

# ‚úÖ –ü—Ä–æ—Å—Ç–∞ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞
model = nn.Sequential(
    nn.Linear(100, 50),
    nn.ReLU(),
    nn.Linear(50, 10)
)
```

### 2. –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞ activation function

```python
# ‚ùå –î–ª—è pixel values [0, 1]
self.decoder = nn.Sequential(
    ...,
    # No activation ‚Üí –º–æ–∂–µ –±—É—Ç–∏ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ñ!
)

# ‚úÖ
self.decoder = nn.Sequential(
    ...,
    nn.Sigmoid()  # ‚Üê [0, 1]
)
```

### 3. –ù–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ validation set

```python
# ‚ùå –¢—ñ–ª—å–∫–∏ train
for epoch in range(100):
    train_loss = train()

# ‚úÖ Train + validation
for epoch in range(100):
    train_loss = train()
    val_loss = validate()
    
    if val_loss increasing:
        early_stop()
```

### 4. –ó–∞–±—É—Ç–∏ normalize inputs

```python
# ‚ùå –ë–µ–∑ normalization
X_tensor = torch.FloatTensor(X)  # [0, 255]

# ‚úÖ Normalize
X_normalized = X / 255.0  # [0, 1]
X_tensor = torch.FloatTensor(X_normalized)
```

### 5. –ó–∞–Ω–∞–¥—Ç–æ –≤–∏—Å–æ–∫–∏–π learning rate

```python
# ‚ùå Divergence
optimizer = optim.Adam(model.parameters(), lr=0.1)

# ‚úÖ Conservative
optimizer = optim.Adam(model.parameters(), lr=0.001)
```

---

## –†–µ—Å—É—Ä—Å–∏

- [PyTorch Autoencoder Tutorial](https://pytorch.org/tutorials/beginner/basics/autoencoders_tutorial.html)
- [VAE Paper (Kingma & Welling, 2013)](https://arxiv.org/abs/1312.6114)
- [Building Autoencoders in Keras](https://blog.keras.io/building-autoencoders-in-keras.html)
- [Stanford CS231n: Autoencoders](http://cs231n.stanford.edu/)

---

## –ö–ª—é—á–æ–≤—ñ –≤–∏—Å–Ω–æ–≤–∫–∏

> Autoencoders ‚Äî —Ü–µ neural networks —â–æ –Ω–∞–≤—á–∞—é—Ç—å—Å—è —Å—Ç–∏—Å–∫–∞—Ç–∏ –¥–∞–Ω—ñ —á–µ—Ä–µ–∑ bottleneck, —Å—Ç–≤–æ—Ä—é—é—á–∏ compact representations –¥–ª—è dimensionality reduction, denoising, anomaly detection —Ç–∞ generation.

**–û—Å–Ω–æ–≤–Ω—ñ –ø—Ä–∏–Ω—Ü–∏–ø–∏:**
- **Encode-Decode:** Input ‚Üí Compressed ‚Üí Reconstructed
- **Unsupervised:** –ù–∞–≤—á–∞—î—Ç—å—Å—è –Ω–∞ unlabeled –¥–∞–Ω–∏—Ö
- **Reconstruction:** –ú—ñ–Ω—ñ–º—ñ–∑—É—î ||x - xÃÇ||¬≤
- **Bottleneck:** –ó–º—É—à—É—î –º–µ—Ä–µ–∂—É –≤—á–∏—Ç–∏ compressed representation

**–ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞:**
- **Encoder:** –°—Ç–∏—Å–∫–∞—î –≤ latent space
- **Latent space:** Low-dimensional representation
- **Decoder:** –í—ñ–¥–Ω–æ–≤–ª—é—î –∑ latent

**–¢–∏–ø–∏:**
- **Undercomplete:** Compression (latent < input)
- **Denoising:** –®—É–º ‚Üí —á–∏—Å—Ç–∏–π
- **Sparse:** L1 regularization
- **VAE:** Generative, probabilistic
- **Convolutional:** –î–ª—è –∑–æ–±—Ä–∞–∂–µ–Ω—å

**–ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è:**
- Dimensionality reduction
- Anomaly detection (reconstruction error)
- Denoising
- Feature learning
- Generation (VAE)

**–ü–µ—Ä–µ–≤–∞–≥–∏:**
- ‚úÖ –ù–µ–ª—ñ–Ω—ñ–π–Ω—ñ transformations
- ‚úÖ –ì–Ω—É—á–∫—ñ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏
- ‚úÖ Deep features

**–ù–µ–¥–æ–ª—ñ–∫–∏:**
- ‚ùå –ü–æ—Ç—Ä–µ–±—É—î –±–∞–≥–∞—Ç–æ –¥–∞–Ω–∏—Ö
- ‚ùå –°–∫–ª–∞–¥–Ω–æ –Ω–∞–ª–∞—à—Ç—É–≤–∞—Ç–∏
- ‚ùå –ü–æ–≤—ñ–ª—å–Ω—ñ—à–µ –∑–∞ PCA

**–ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏:**
- –ù–µ–ª—ñ–Ω—ñ–π–Ω—ñ –¥–∞–Ω—ñ + –±–∞–≥–∞—Ç–æ –∑—Ä–∞–∑–∫—ñ–≤ = Autoencoders ‚úì
- –õ—ñ–Ω—ñ–π–Ω—ñ –¥–∞–Ω—ñ / –º–∞–ª—ñ –¥–∞–Ω—ñ ‚Üí PCA ‚úì
- Generation ‚Üí VAE ‚úì
- –¢—ñ–ª—å–∫–∏ –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è ‚Üí t-SNE/UMAP ‚úì

**–ù–∞–π–≤–∞–∂–ª–∏–≤—ñ—à–µ:**
- –ü–æ—á–Ω–∏ –ø—Ä–æ—Å—Ç–æ, —É—Å–∫–ª–∞–¥–Ω—é–π –ø–æ—Å—Ç—É–ø–æ–≤–æ
- Regularization –ø—Ä–æ—Ç–∏ overfitting
- Validation set –æ–±–æ–≤'—è–∑–∫–æ–≤–∏–π
- Normalize inputs
- Visualize latent space

---

#ml #unsupervised-learning #dimensionality-reduction #autoencoders #neural-networks #deep-learning #vae #denoising #anomaly-detection
