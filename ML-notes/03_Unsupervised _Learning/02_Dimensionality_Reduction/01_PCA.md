# PCA (Principal Component Analysis)

## –©–æ —Ü–µ?

**PCA (Principal Component Analysis)** ‚Äî —Ü–µ –∞–ª–≥–æ—Ä–∏—Ç–º **dimensionality reduction** (–∑–º–µ–Ω—à–µ–Ω–Ω—è —Ä–æ–∑–º—ñ—Ä–Ω–æ—Å—Ç—ñ), —è–∫–∏–π –ø–µ—Ä–µ—Ç–≤–æ—Ä—é—î –¥–∞–Ω—ñ –≤ –Ω–æ–≤–∏–π –ø—Ä–æ—Å—Ç—ñ—Ä –∑ –º–µ–Ω—à–æ—é –∫—ñ–ª—å–∫—ñ—Å—Ç—é –≤–∏–º—ñ—Ä—ñ–≤, –∑–±–µ—Ä—ñ–≥–∞—é—á–∏ **–º–∞–∫—Å–∏–º—É–º —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó** (–≤–∞—Ä—ñ–∞—Ü—ñ—ó).

**–ì–æ–ª–æ–≤–Ω–∞ —ñ–¥–µ—è:** –∑–Ω–∞–π—Ç–∏ –Ω–æ–≤—ñ –æ—Å—ñ (principal components), –≤–∑–¥–æ–≤–∂ —è–∫–∏—Ö –¥–∞–Ω—ñ –º–∞—é—Ç—å –Ω–∞–π–±—ñ–ª—å—à—É –¥–∏—Å–ø–µ—Ä—Å—ñ—é, —Ç–∞ —Å–ø—Ä–æ–µ–∫—Ç—É–≤–∞—Ç–∏ –¥–∞–Ω—ñ –Ω–∞ —Ü—ñ –æ—Å—ñ.

## –ù–∞–≤—ñ—â–æ –ø–æ—Ç—Ä—ñ–±–µ–Ω?

- üìä **–ó–º–µ–Ω—à–µ–Ω–Ω—è —Ä–æ–∑–º—ñ—Ä–Ω–æ—Å—Ç—ñ** ‚Äî –∑ 100 features –¥–æ 10 –±–µ–∑ –≤–µ–ª–∏–∫–æ—ó –≤—Ç—Ä–∞—Ç–∏ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó
- üé® **–í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è** ‚Äî –≤—ñ–∑—É–∞–ª—ñ–∑—É–≤–∞—Ç–∏ –≤–∏—Å–æ–∫–æ —Ä–æ–∑–º—ñ—Ä–Ω—ñ –¥–∞–Ω—ñ –≤ 2D/3D
- ‚ö° **–ü—Ä–∏—Å–∫–æ—Ä–µ–Ω–Ω—è –Ω–∞–≤—á–∞–Ω–Ω—è** ‚Äî –º–µ–Ω—à–µ features ‚Üí —à–≤–∏–¥—à–µ ML –∞–ª–≥–æ—Ä–∏—Ç–º–∏
- üßπ **–í–∏–¥–∞–ª–µ–Ω–Ω—è —à—É–º—É** ‚Äî –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∏ –∑ –Ω–∏–∑—å–∫–æ—é variance = —à—É–º
- üîç **Feature extraction** ‚Äî –∑–Ω–∞–π—Ç–∏ –Ω–∞–π–≤–∞–∂–ª–∏–≤—ñ—à—ñ –Ω–∞–ø—Ä—è–º–∫–∏ –≤–∞—Ä—ñ–∞—Ü—ñ—ó
- üíæ **–°—Ç–∏—Å–Ω–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö** ‚Äî –∑–±–µ—Ä—ñ–≥–∞—Ç–∏ –º–µ–Ω—à–µ –¥–∞–Ω–∏—Ö
- üéØ **Multicollinearity** ‚Äî –¥–µ–∫–æ—Ä–µ–ª—é–≤–∞—Ç–∏ —Å–∏–ª—å–Ω–æ –∫–æ—Ä–µ–ª—å–æ–≤–∞–Ω—ñ features

## –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏?

**–ü–æ—Ç—Ä—ñ–±–Ω–æ

- **–ë–∞–≥–∞—Ç–æ features** (>10-20) ‚Äî curse of dimensionality
- Features **–∫–æ—Ä–µ–ª—å–æ–≤–∞–Ω—ñ** –º—ñ–∂ —Å–æ–±–æ—é
- –ü–æ—Ç—Ä—ñ–±–Ω–∞ **–≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è** –≤–∏—Å–æ–∫–æ —Ä–æ–∑–º—ñ—Ä–Ω–∏—Ö –¥–∞–Ω–∏—Ö
- **–ß–∏—Å–ª–æ–≤—ñ features** (–Ω–µ–ø–µ—Ä–µ—Ä–≤–Ω—ñ)
- –î–∞–Ω—ñ **–ø—Ä–∏–±–ª–∏–∑–Ω–æ –ª—ñ–Ω—ñ–π–Ω—ñ** (–ª—ñ–Ω—ñ–π–Ω—ñ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ)
- –ü–æ—Ç—Ä—ñ–±–Ω–∞ **—ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å** ‚Äî —â–æ –æ–∑–Ω–∞—á–∞—é—Ç—å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∏
- **Preprocessing** –ø–µ—Ä–µ–¥ supervised learning

**–ù–µ –ø–æ—Ç—Ä—ñ–±–Ω–æ:**

- **–ú–∞–ª–æ features** (3-5) ‚Äî –Ω–µ –ø–æ—Ç—Ä—ñ–±–Ω–æ –∑–º–µ–Ω—à—É–≤–∞—Ç–∏
- **Categorical features** ‚Üí One-hot encoding —Å–ø–æ—á–∞—Ç–∫—É
- **–°–∏–ª—å–Ω–æ –Ω–µ–ª—ñ–Ω—ñ–π–Ω—ñ –¥–∞–Ω—ñ** ‚Üí t-SNE, UMAP, Autoencoders
- –ü–æ—Ç—Ä—ñ–±–Ω–∞ **—Ä–æ–∑—Ä—ñ–¥–∂–µ–Ω—ñ—Å—Ç—å** (sparsity) ‚Üí NMF
- **Supervised task** –¥–µ features –≤–∞–∂–ª–∏–≤—ñ ‚Üí Feature selection

---

### –ú–∞—Ç–µ–º–∞—Ç–∏—á–Ω–∏–π –ø—Ä–æ—Ü–µ—Å

**–í—Ö—ñ–¥:** –º–∞—Ç—Ä–∏—Ü—è –¥–∞–Ω–∏—Ö $X$ —Ä–æ–∑–º—ñ—Ä—É $n \times d$ (n —Ç–æ—á–æ–∫, d features)

**–ö—Ä–æ–∫ 1: –¶–µ–Ω—Ç—Ä—É–≤–∞–Ω–Ω—è –¥–∞–Ω–∏—Ö**

- –í—ñ–¥–Ω—è—Ç–∏ —Å–µ—Ä–µ–¥–Ω—î –≤—ñ–¥ –∫–æ–∂–Ω–æ—ó –æ–∑–Ω–∞–∫–∏:
$$X_{centered} = X - \bar{X}$$

**–ö—Ä–æ–∫ 2: –û–±—á–∏—Å–ª–∏—Ç–∏ –∫–æ–≤–∞—Ä—ñ–∞—Ü—ñ–π–Ω—É –º–∞—Ç—Ä–∏—Ü—é**
$$C = \frac{1}{n-1} X_{centered}^T X_{centered}$$

**–ö—Ä–æ–∫ 3: –ó–Ω–∞–π—Ç–∏ –≤–ª–∞—Å–Ω—ñ –≤–µ–∫—Ç–æ—Ä–∏ —Ç–∞ –≤–ª–∞—Å–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è**
- –†–æ–∑–≤'—è–∑–∞—Ç–∏: $C \mathbf{v} = \lambda \mathbf{v}$
- $\mathbf{v}$ ‚Äî –≤–ª–∞—Å–Ω–∏–π –≤–µ–∫—Ç–æ—Ä (principal component)
- $\lambda$ ‚Äî –≤–ª–∞—Å–Ω–µ –∑–Ω–∞—á–µ–Ω–Ω—è (variance –≤–∑–¥–æ–≤–∂ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞)

**–ö—Ä–æ–∫ 4: –í—ñ–¥—Å–æ—Ä—Ç—É–≤–∞—Ç–∏ –∑–∞ –≤–ª–∞—Å–Ω–∏–º–∏ –∑–Ω–∞—á–µ–Ω–Ω—è–º–∏**
- $\lambda_1 > \lambda_2 > \lambda_3 > ...$

**–ö—Ä–æ–∫ 5: –í–∏–±—Ä–∞—Ç–∏ top k –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ñ–≤**
- –ü–µ—Ä—à—ñ $k$ –≤–ª–∞—Å–Ω–∏—Ö –≤–µ–∫—Ç–æ—Ä—ñ–≤ ‚Üí –º–∞—Ç—Ä–∏—Ü—è $W$ —Ä–æ–∑–º—ñ—Ä—É $d \times k$

**–ö—Ä–æ–∫ 6: –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º—É–≤–∞—Ç–∏ –¥–∞–Ω—ñ**
$$X_{transformed} = X_{centered} \cdot W$$

**–í–∏—Ö—ñ–¥:** –Ω–æ–≤—ñ –¥–∞–Ω—ñ —Ä–æ–∑–º—ñ—Ä—É $n \times k$ (–∑–º–µ–Ω—à–µ–Ω–∞ —Ä–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å!)

---

## –ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞

### –ö–æ–≤–∞—Ä—ñ–∞—Ü—ñ–π–Ω–∞ –º–∞—Ç—Ä–∏—Ü—è

**–î–ª—è 2 features:**

$$C = \begin{bmatrix} 
\text{Var}(X_1) & \text{Cov}(X_1, X_2) \\
\text{Cov}(X_2, X_1) & \text{Var}(X_2)
\end{bmatrix}$$

**Variance (–¥–∏—Å–ø–µ—Ä—Å—ñ—è):**
$$\text{Var}(X) = \frac{1}{n-1} \sum_{i=1}^{n} (x_i - \bar{x})^2$$

**Covariance (–∫–æ–≤–∞—Ä—ñ–∞—Ü—ñ—è):**
$$\text{Cov}(X, Y) = \frac{1}{n-1} \sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})$$

### –í–ª–∞—Å–Ω—ñ –≤–µ–∫—Ç–æ—Ä–∏ —Ç–∞ –≤–ª–∞—Å–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è

**Eigenvalue equation:**
$$C \mathbf{v} = \lambda \mathbf{v}$$

–¥–µ:
- $C$ ‚Äî –∫–æ–≤–∞—Ä—ñ–∞—Ü—ñ–π–Ω–∞ –º–∞—Ç—Ä–∏—Ü—è
- $\mathbf{v}$ ‚Äî –≤–ª–∞—Å–Ω–∏–π –≤–µ–∫—Ç–æ—Ä (–Ω–∞–ø—Ä—è–º–æ–∫ principal component)
- $\lambda$ ‚Äî –≤–ª–∞—Å–Ω–µ –∑–Ω–∞—á–µ–Ω–Ω—è (variance –≤–∑–¥–æ–≤–∂ —Ü—å–æ–≥–æ –Ω–∞–ø—Ä—è–º–∫—É)

### –ü–æ—è—Å–Ω–µ–Ω–∞ –≤–∞—Ä—ñ–∞—Ü—ñ—è (Explained Variance)

**–ß–∞—Å—Ç–∫–∞ variance –ø–æ—è—Å–Ω–µ–Ω–æ—ó k-–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–º:**

$$\text{Explained Variance Ratio}_k = \frac{\lambda_k}{\sum_{i=1}^{d} \lambda_i}$$

**Cumulative explained variance:**

$$\text{Cumulative}_k = \frac{\sum_{i=1}^{k} \lambda_i}{\sum_{i=1}^{d} \lambda_i}$$

**–ü—Ä–∏–∫–ª–∞–¥:**
```
PC1: 65% variance
PC2: 20% variance
PC3: 10% variance
PC4: 5% variance

Cumulative:
PC1: 65%
PC1+PC2: 85%
PC1+PC2+PC3: 95%  ‚Üê –∑–±–µ—Ä—ñ–≥–∞—î 95% —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó!
```

---

## –ü—Ä–æ—Å—Ç–∏–π –ø—Ä–∏–∫–ª–∞–¥: 2D ‚Üí 1D

### –î–∞–Ω—ñ

–°—Ç—É–¥–µ–Ω—Ç–∏: –æ—Ü—ñ–Ω–∫–∏ –∑ Math —Ç–∞ Physics (—Å–∏–ª—å–Ω–æ –∫–æ—Ä–µ–ª—å–æ–≤–∞–Ω—ñ)

| Student | Math | Physics |
|---------|------|---------|
| A | 90 | 85 |
| B | 80 | 78 |
| C | 70 | 72 |
| D | 60 | 58 |
| E | 50 | 52 |

### –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è

```
Physics
  100|
     |
   80|    A ‚Ä¢
     |       B ‚Ä¢
   60|          C ‚Ä¢
     |             D ‚Ä¢
   40|                E ‚Ä¢
     |
    0|_________________
      0  20  40  60  80 100  Math
```

**–ü—Ä–æ–±–ª–µ–º–∞:** 2 features, –∞–ª–µ –≤–æ–Ω–∏ –¥—É–∂–µ —Å—Ö–æ–∂—ñ (—è–∫—â–æ Math –≤–∏—Å–æ–∫–∏–π ‚Üí Physics –≤–∏—Å–æ–∫–∏–π)

### PCA Process

**1. –¶–µ–Ω—Ç—Ä—É–≤–∞–Ω–Ω—è:**

```
Mean(Math) = 70, Mean(Physics) = 69

Centered data:
Student | Math  | Physics
A       | +20   | +16
B       | +10   | +9
C       | 0     | +3
D       | -10   | -11
E       | -20   | -17
```

**2. –ö–æ–≤–∞—Ä—ñ–∞—Ü—ñ–π–Ω–∞ –º–∞—Ç—Ä–∏—Ü—è:**

```
C = [100   98]
    [98    96]
```

**3. –í–ª–∞—Å–Ω—ñ –≤–µ–∫—Ç–æ—Ä–∏:**

```
PC1 = [0.71, 0.70]  Œª‚ÇÅ = 196  (98% variance)
PC2 = [-0.70, 0.71] Œª‚ÇÇ = 4    (2% variance)
```

**4. –ü—Ä–æ–µ–∫—Ü—ñ—è –Ω–∞ PC1:**

```
Student | PC1 (–Ω–æ–≤–∞ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞)
A       | +25.4
B       | +13.4
C       | +2.1
D       | -14.8
E       | -26.1
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç:** 2D ‚Üí 1D, –∑–±–µ—Ä—ñ–≥–∞—é—á–∏ 98% —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó!

**PC1 –º–æ–∂–Ω–∞ —ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç—É–≤–∞—Ç–∏ —è–∫:** "–∑–∞–≥–∞–ª—å–Ω–∞ —É—Å–ø—ñ—à–Ω—ñ—Å—Ç—å" (Math + Physics)

---

## –°–∫–ª–∞–¥–Ω–∏–π –ø—Ä–∏–∫–ª–∞–¥: Iris Dataset

### –ó–∞–¥–∞—á–∞

Iris dataset: 150 –∫–≤—ñ—Ç—ñ–≤, 4 features (sepal/petal length/width), 3 –≤–∏–¥–∏.

**–ú–µ—Ç–∞:** –ó–º–µ–Ω—à–∏—Ç–∏ –∑ 4D –¥–æ 2D –¥–ª—è –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó.

### –†–µ–∑—É–ª—å—Ç–∞—Ç PCA

**Explained variance:**
```
PC1: 72.96%
PC2: 22.85%
PC3: 3.67%
PC4: 0.52%

PC1+PC2: 95.81% ‚Üê 2 –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∏ –∑–±–µ—Ä—ñ–≥–∞—é—Ç—å 96% —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó!
```

**–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü—ñ—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ñ–≤:**

**PC1** = 0.52√ósepal_length + 0.37√ósepal_width + 0.72√ópetal_length + 0.26√ópetal_width
- –ù–∞–π–±—ñ–ª—å—à–∏–π –≤–∫–ª–∞–¥: petal_length
- **–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü—ñ—è:** "—Ä–æ–∑–º—ñ—Ä –∫–≤—ñ—Ç–∫–∏ –∑–∞–≥–∞–ª–æ–º"

**PC2** = 0.38√ósepal_length - 0.86√ósepal_width + 0.17√ópetal_length + 0.08√ópetal_width
- –ù–∞–π–±—ñ–ª—å—à–∏–π –≤–∫–ª–∞–¥: sepal_width (–Ω–µ–≥–∞—Ç–∏–≤–Ω–∏–π)
- **–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü—ñ—è:** "—Ñ–æ—Ä–º–∞ –∫–≤—ñ—Ç–∫–∏" (—à–∏—Ä–æ–∫–∏–π vs –≤—É–∑—å–∫–∏–π)

---

## –ö–æ–¥ (Python + scikit-learn)

### –ë–∞–∑–æ–≤–∏–π –ø—Ä–∏–∫–ª–∞–¥

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import load_iris

# 1. –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –¥–∞–Ω—ñ
iris = load_iris()
X = iris.data  # 150 samples, 4 features
y = iris.target

print(f"Original shape: {X.shape}")  # (150, 4)

# 2. –û–ë–û–í'–Ø–ó–ö–û–í–û: –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü—ñ—è
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 3. PCA
pca = PCA(n_components=2)  # –ó–º–µ–Ω—à–∏—Ç–∏ –¥–æ 2 –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ñ–≤
X_pca = pca.fit_transform(X_scaled)

print(f"Transformed shape: {X_pca.shape}")  # (150, 2)

# 4. Explained variance
print("\n=== Explained Variance ===")
print(f"PC1: {pca.explained_variance_ratio_[0]:.4f}")
print(f"PC2: {pca.explained_variance_ratio_[1]:.4f}")
print(f"Total: {pca.explained_variance_ratio_.sum():.4f}")

# 5. Components (loadings)
print("\n=== Principal Components ===")
components_df = pd.DataFrame(
    pca.components_,
    columns=iris.feature_names,
    index=['PC1', 'PC2']
)
print(components_df)

# 6. –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# –î–æ PCA (2 –∑ 4 features)
axes[0].scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', s=50, alpha=0.6)
axes[0].set_xlabel(iris.feature_names[0], fontsize=11)
axes[0].set_ylabel(iris.feature_names[1], fontsize=11)
axes[0].set_title('Original Data (2 of 4 features)', 
                 fontsize=13, fontweight='bold')
axes[0].grid(True, alpha=0.3)

# –ü—ñ—Å–ª—è PCA (–≤—Å—ñ 4 features ‚Üí 2 PC)
scatter = axes[1].scatter(X_pca[:, 0], X_pca[:, 1], 
                         c=y, cmap='viridis', s=50, alpha=0.6)
axes[1].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)', 
                  fontsize=11)
axes[1].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)', 
                  fontsize=11)
axes[1].set_title('After PCA (all 4 features ‚Üí 2 PCs)', 
                 fontsize=13, fontweight='bold')
axes[1].grid(True, alpha=0.3)

# –õ–µ–≥–µ–Ω–¥–∞
plt.colorbar(scatter, ax=axes[1], label='Species', 
            ticks=[0, 1, 2])

plt.tight_layout()
plt.show()
```

### –í–∏–±—ñ—Ä –∫—ñ–ª—å–∫–æ—Å—Ç—ñ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ñ–≤

```python
# –ú–µ—Ç–æ–¥ 1: Explained variance ratio
pca = PCA()  # –í—Å—ñ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∏
pca.fit(X_scaled)

# Cumulative explained variance
cumsum = np.cumsum(pca.explained_variance_ratio_)

plt.figure(figsize=(12, 5))

# Scree plot
plt.subplot(1, 2, 1)
plt.bar(range(1, len(pca.explained_variance_ratio_) + 1),
        pca.explained_variance_ratio_, alpha=0.7, edgecolor='black')
plt.xlabel('Principal Component', fontsize=12)
plt.ylabel('Explained Variance Ratio', fontsize=12)
plt.title('Scree Plot', fontsize=14, fontweight='bold')
plt.grid(True, alpha=0.3, axis='y')

# Cumulative variance
plt.subplot(1, 2, 2)
plt.plot(range(1, len(cumsum) + 1), cumsum, 'o-', linewidth=2, markersize=8)
plt.axhline(y=0.95, color='red', linestyle='--', 
           label='95% threshold', linewidth=2)
plt.xlabel('Number of Components', fontsize=12)
plt.ylabel('Cumulative Explained Variance', fontsize=12)
plt.title('Cumulative Explained Variance', fontsize=14, fontweight='bold')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# –ó–Ω–∞–π—Ç–∏ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ñ–≤ –¥–ª—è 95% variance
n_components_95 = np.argmax(cumsum >= 0.95) + 1
print(f"\n–ö—ñ–ª—å–∫—ñ—Å—Ç—å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ñ–≤ –¥–ª—è 95% variance: {n_components_95}")

# –ú–µ—Ç–æ–¥ 2: –ó–∞–¥–∞—Ç–∏ –±–∞–∂–∞–Ω—É variance
pca_95 = PCA(n_components=0.95)  # 95% variance
X_pca_95 = pca_95.fit_transform(X_scaled)
print(f"–í–∏–±—Ä–∞–Ω–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ñ–≤: {pca_95.n_components_}")
```

### –ü–æ–≤–Ω–∏–π –ø—Ä–∏–∫–ª–∞–¥: MNIST

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import load_digits

# –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –¥–∞–Ω—ñ (8x8 = 64 features)
digits = load_digits()
X = digits.data  # (1797, 64)
y = digits.target

print(f"Original shape: {X.shape}")
print(f"Features: 64 pixels (8x8 image)")

# –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü—ñ—è
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# PCA: –≤—Å—ñ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∏ –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É
pca_full = PCA()
pca_full.fit(X_scaled)

# –ê–Ω–∞–ª—ñ–∑ explained variance
cumsum = np.cumsum(pca_full.explained_variance_ratio_)

print("\n=== Explained Variance Analysis ===")
for threshold in [0.80, 0.90, 0.95, 0.99]:
    n_comp = np.argmax(cumsum >= threshold) + 1
    print(f"{threshold:.0%} variance: {n_comp} components "
          f"(compression: {64/n_comp:.1f}x)")

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è explained variance
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# Scree plot
axes[0, 0].bar(range(1, 21), 
              pca_full.explained_variance_ratio_[:20],
              alpha=0.7, edgecolor='black')
axes[0, 0].set_xlabel('Principal Component', fontsize=11)
axes[0, 0].set_ylabel('Explained Variance Ratio', fontsize=11)
axes[0, 0].set_title('Scree Plot (first 20 PCs)', fontsize=12, fontweight='bold')
axes[0, 0].grid(True, alpha=0.3, axis='y')

# Cumulative variance
axes[0, 1].plot(range(1, 65), cumsum, 'o-', linewidth=2, markersize=4)
axes[0, 1].axhline(y=0.95, color='red', linestyle='--', 
                  label='95% threshold', linewidth=2)
axes[0, 1].axhline(y=0.90, color='orange', linestyle='--',
                  label='90% threshold', linewidth=2, alpha=0.7)
axes[0, 1].set_xlabel('Number of Components', fontsize=11)
axes[0, 1].set_ylabel('Cumulative Explained Variance', fontsize=11)
axes[0, 1].set_title('Cumulative Variance', fontsize=12, fontweight='bold')
axes[0, 1].legend()
axes[0, 1].grid(True, alpha=0.3)

# PCA 2D –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
pca_2d = PCA(n_components=2)
X_pca_2d = pca_2d.fit_transform(X_scaled)

scatter = axes[1, 0].scatter(X_pca_2d[:, 0], X_pca_2d[:, 1],
                            c=y, cmap='tab10', s=20, alpha=0.6)
axes[1, 0].set_xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]:.1%})', 
                     fontsize=11)
axes[1, 0].set_ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]:.1%})', 
                     fontsize=11)
axes[1, 0].set_title('2D Projection (64D ‚Üí 2D)', fontsize=12, fontweight='bold')
axes[1, 0].grid(True, alpha=0.3)
plt.colorbar(scatter, ax=axes[1, 0], label='Digit')

# –ü–µ—Ä—à—ñ 10 principal components (—è–∫ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è)
axes[1, 1].axis('off')
axes[1, 1].set_title('First 10 Principal Components', 
                    fontsize=12, fontweight='bold')

# –°—Ç–≤–æ—Ä–∏—Ç–∏ sub-grid –¥–ª—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ñ–≤
for i in range(10):
    ax = plt.subplot(4, 5, i + 11)
    component = pca_full.components_[i].reshape(8, 8)
    ax.imshow(component, cmap='RdBu_r', aspect='auto')
    ax.set_title(f'PC{i+1}', fontsize=9)
    ax.axis('off')

plt.tight_layout()
plt.show()

# –†–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—è –∑–æ–±—Ä–∞–∂–µ–Ω—å –∑ —Ä—ñ–∑–Ω–æ—é –∫—ñ–ª—å–∫—ñ—Å—Ç—é –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ñ–≤
fig, axes = plt.subplots(2, 5, figsize=(15, 6))
fig.suptitle('Image Reconstruction with Different Numbers of PCs', 
            fontsize=14, fontweight='bold')

sample_idx = 0  # –ü–µ—Ä—à–µ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è (—Ü–∏—Ñ—Ä–∞)
original = X_scaled[sample_idx]

n_components_list = [1, 2, 5, 10, 20, 30, 40, 50, 60, 64]

for idx, n_comp in enumerate(n_components_list):
    ax = axes[idx // 5, idx % 5]
    
    if n_comp == 64:
        # –û—Ä–∏–≥—ñ–Ω–∞–ª
        reconstructed = original
        mse = 0
    else:
        # PCA —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—è
        pca_temp = PCA(n_components=n_comp)
        X_temp = pca_temp.fit_transform(X_scaled)
        reconstructed = pca_temp.inverse_transform(X_temp)[sample_idx]
        mse = np.mean((original - reconstructed) ** 2)
    
    # –í—ñ–¥–æ–±—Ä–∞–∑–∏—Ç–∏
    img = scaler.inverse_transform(reconstructed.reshape(1, -1)).reshape(8, 8)
    ax.imshow(img, cmap='gray', aspect='auto')
    ax.set_title(f'{n_comp} PCs\nMSE: {mse:.4f}', fontsize=9)
    ax.axis('off')

plt.tight_layout()
plt.show()

# Compression vs Quality
n_comp_range = range(1, 65)
mse_scores = []

for n_comp in n_comp_range:
    pca_temp = PCA(n_components=n_comp)
    X_temp = pca_temp.fit_transform(X_scaled)
    X_reconstructed = pca_temp.inverse_transform(X_temp)
    
    mse = np.mean((X_scaled - X_reconstructed) ** 2)
    mse_scores.append(mse)

plt.figure(figsize=(10, 6))
plt.plot(n_comp_range, mse_scores, linewidth=2)
plt.xlabel('Number of Components', fontsize=12)
plt.ylabel('Mean Squared Error', fontsize=12)
plt.title('Reconstruction Error vs Number of Components', 
         fontsize=14, fontweight='bold')
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

print("\n=== Reconstruction Quality ===")
for n_comp in [5, 10, 20, 30, 40]:
    idx = n_comp - 1
    print(f"{n_comp:2d} components: MSE = {mse_scores[idx]:.6f}, "
          f"Compression = {64/n_comp:.1f}x")
```

### Inverse Transform (—Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—è)

```python
# PCA forward transform
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

# Inverse transform (—Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—è)
X_reconstructed_scaled = pca.inverse_transform(X_pca)

# Inverse scaling
X_reconstructed = scaler.inverse_transform(X_reconstructed_scaled)

# –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è
original_sample = X[0]
reconstructed_sample = X_reconstructed[0]

print("Original:", original_sample[:4])
print("Reconstructed:", reconstructed_sample[:4])
print(f"MSE: {np.mean((original_sample - reconstructed_sample)**2):.6f}")
```

---

## Kernel PCA (–¥–ª—è –Ω–µ–ª—ñ–Ω—ñ–π–Ω–∏—Ö –¥–∞–Ω–∏—Ö)

### –ü—Ä–æ–±–ª–µ–º–∞

**–õ—ñ–Ω—ñ–π–Ω–∏–π PCA** –Ω–µ –ø—Ä–∞—Ü—é—î –∑ –Ω–µ–ª—ñ–Ω—ñ–π–Ω–∏–º–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞–º–∏:

```
Before (–Ω–µ–ª—ñ–Ω—ñ–π–Ω–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞):
    y
    |  ‚óè‚óè‚óè
    | ‚óè   ‚óè
    |‚óè     ‚óè
    |‚óè     ‚óè
    | ‚óè   ‚óè
    |  ‚óè‚óè‚óè
    |_______ x
    –ö–æ–ª–æ

After –ª—ñ–Ω—ñ–π–Ω–∏–π PCA:
PC2|
   |‚óè‚óè‚óè‚óè‚óè‚óè‚óè
   |
   |_______ PC1
   
–ù–µ —Ä–æ–∑–¥—ñ–ª—è—î! ‚ùå
```

### –†—ñ—à–µ–Ω–Ω—è: Kernel PCA

**–Ü–¥–µ—è:** –í—ñ–¥–æ–±—Ä–∞–∑–∏—Ç–∏ –¥–∞–Ω—ñ –≤ –≤–∏—â—É —Ä–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å —á–µ—Ä–µ–∑ kernel trick, –ø–æ—Ç—ñ–º PCA.

```python
from sklearn.decomposition import KernelPCA

# Kernel PCA –∑ RBF kernel
kpca = KernelPCA(
    n_components=2,
    kernel='rbf',      # 'linear', 'poly', 'rbf', 'sigmoid'
    gamma=15,          # –ü–∞—Ä–∞–º–µ—Ç—Ä kernel
    fit_inverse_transform=True
)

X_kpca = kpca.fit_transform(X)

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# –û—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ
axes[0].scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', s=50)
axes[0].set_title('Original Data', fontsize=13, fontweight='bold')
axes[0].grid(True, alpha=0.3)

# –õ—ñ–Ω—ñ–π–Ω–∏–π PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)
axes[1].scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', s=50)
axes[1].set_title('Linear PCA', fontsize=13, fontweight='bold')
axes[1].grid(True, alpha=0.3)

# Kernel PCA
axes[2].scatter(X_kpca[:, 0], X_kpca[:, 1], c=y, cmap='viridis', s=50)
axes[2].set_title('Kernel PCA (RBF)', fontsize=13, fontweight='bold')
axes[2].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### Kernel types

| Kernel | –§–æ—Ä–º—É–ª–∞ | –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è |
|--------|---------|--------------|
| **Linear** | $\mathbf{x}^T \mathbf{y}$ | –õ—ñ–Ω—ñ–π–Ω—ñ –¥–∞–Ω—ñ (= –∑–≤–∏—á–∞–π–Ω–∏–π PCA) |
| **Polynomial** | $(\gamma \mathbf{x}^T \mathbf{y} + c)^d$ | –ü–æ–ª—ñ–Ω–æ–º—ñ–∞–ª—å–Ω—ñ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ |
| **RBF (Gaussian)** | $\exp(-\gamma \|\mathbf{x} - \mathbf{y}\|^2)$ | –°–∫–ª–∞–¥–Ω—ñ –Ω–µ–ª—ñ–Ω—ñ–π–Ω—ñ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏ |
| **Sigmoid** | $\tanh(\gamma \mathbf{x}^T \mathbf{y} + c)$ | –ù–µ–π—Ä–æ–Ω–Ω—ñ –º–µ—Ä–µ–∂—ñ |

---

## Incremental PCA (–¥–ª—è –≤–µ–ª–∏–∫–∏—Ö –¥–∞–Ω–∏—Ö)

### –ü—Ä–æ–±–ª–µ–º–∞

**–ó–≤–∏—á–∞–π–Ω–∏–π PCA:** –ø–æ—Ç—Ä–µ–±—É—î –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –≤—Å—ñ –¥–∞–Ω—ñ –≤ –ø–∞–º'—è—Ç—å.

**–í–µ–ª–∏–∫—ñ –¥–∞–Ω—ñ** (–Ω–µ –≤–º—ñ—â—É—é—Ç—å—Å—è –≤ RAM) ‚Üí –ø—Ä–æ–±–ª–µ–º–∞!

### –†—ñ—à–µ–Ω–Ω—è: Incremental PCA

**–Ü–¥–µ—è:** –û–±—Ä–æ–±–ª—è—Ç–∏ –¥–∞–Ω—ñ –±–∞—Ç—á–∞–º–∏ (–ø–æ —á–∞—Å—Ç–∏–Ω–∞—Ö).

```python
from sklearn.decomposition import IncrementalPCA

# Incremental PCA
n_batches = 10
inc_pca = IncrementalPCA(n_components=50)

# –ù–∞–≤—á–∞–Ω–Ω—è –ø–æ –±–∞—Ç—á–∞–º
batch_size = len(X) // n_batches

for i in range(n_batches):
    start = i * batch_size
    end = start + batch_size
    
    batch = X_scaled[start:end]
    inc_pca.partial_fit(batch)

# Transform
X_inc_pca = inc_pca.transform(X_scaled)

print(f"Incremental PCA shape: {X_inc_pca.shape}")
print(f"Explained variance: {inc_pca.explained_variance_ratio_.sum():.4f}")
```

**–ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏:**
- –î–∞–Ω—ñ –Ω–µ –≤–º—ñ—â—É—é—Ç—å—Å—è –≤ RAM
- Streaming data (–¥–∞–Ω—ñ –ø—Ä–∏—Ö–æ–¥—è—Ç—å –ø–æ—Å—Ç—É–ø–æ–≤–æ)
- –î—É–∂–µ –≤–µ–ª–∏–∫—ñ –¥–∞—Ç–∞—Å–µ—Ç–∏ (GB+)

---

## Sparse PCA

### –ü—Ä–æ–±–ª–µ–º–∞

**–ó–≤–∏—á–∞–π–Ω–∏–π PCA:** principal components - —Ü–µ –ª—ñ–Ω—ñ–π–Ω—ñ –∫–æ–º–±—ñ–Ω–∞—Ü—ñ—ó **–≤—Å—ñ—Ö** features.

**–í–∞–∂–∫–æ —ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç—É–≤–∞—Ç–∏:** –∫–æ–∂–µ–Ω PC –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î –≤—Å—ñ features.

### –†—ñ—à–µ–Ω–Ω—è: Sparse PCA

**–Ü–¥–µ—è:** –ó–º—É—Å–∏—Ç–∏ components –±—É—Ç–∏ **—Ä–æ–∑—Ä—ñ–¥–∂–µ–Ω–∏–º–∏** (–±–∞–≥–∞—Ç–æ –Ω—É–ª—ñ–≤).

```python
from sklearn.decomposition import SparsePCA

# Sparse PCA
spca = SparsePCA(
    n_components=5,
    alpha=1.0,        # Regularization (–±—ñ–ª—å—à–µ = –±—ñ–ª—å—à–µ sparsity)
    max_iter=100,
    random_state=42
)

X_spca = spca.fit_transform(X_scaled)

# –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –∑ –∑–≤–∏—á–∞–π–Ω–∏–º PCA
pca = PCA(n_components=5)
X_pca = pca.fit_transform(X_scaled)

# Sparsity comparison
print("=== Sparsity Comparison ===")
print(f"PCA zeros: {np.sum(pca.components_ == 0)}")
print(f"Sparse PCA zeros: {np.sum(spca.components_ == 0)}")

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ñ–≤
fig, axes = plt.subplots(2, 5, figsize=(15, 6))

for i in range(5):
    # PCA
    axes[0, i].bar(range(len(pca.components_[i])), 
                   np.abs(pca.components_[i]))
    axes[0, i].set_title(f'PCA PC{i+1}', fontsize=10)
    axes[0, i].set_ylim(0, 0.6)
    
    # Sparse PCA
    axes[1, i].bar(range(len(spca.components_[i])), 
                   np.abs(spca.components_[i]))
    axes[1, i].set_title(f'Sparse PC{i+1}', fontsize=10)
    axes[1, i].set_ylim(0, 0.6)

axes[0, 0].set_ylabel('PCA\nAbsolute Weight', fontsize=11)
axes[1, 0].set_ylabel('Sparse PCA\nAbsolute Weight', fontsize=11)

plt.tight_layout()
plt.show()
```

**–ü–µ—Ä–µ–≤–∞–≥–∏:**
- ‚úÖ –õ–µ–≥—à–µ —ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç—É–≤–∞—Ç–∏ (–º–µ–Ω—à–µ features –≤ –∫–æ–∂–Ω–æ–º—É PC)
- ‚úÖ Feature selection (–∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –≤–∏–±–∏—Ä–∞—î –≤–∞–∂–ª–∏–≤—ñ features)

**–ù–µ–¥–æ–ª—ñ–∫–∏:**
- ‚ùå –ü–æ–≤—ñ–ª—å–Ω—ñ—à–µ
- ‚ùå –ú–µ–Ω—à–µ explained variance

---

## Preprocessing –¥–ª—è PCA

### 1. Scaling (–ö–†–ò–¢–ò–ß–ù–û! ‚ö†Ô∏è)

**PCA –¥—É–∂–µ —á—É—Ç–ª–∏–≤–∏–π –¥–æ –º–∞—Å—à—Ç–∞–±—É!**

```python
# ‚ùå –ë–ï–ó SCALING - –ù–ï–ü–†–ê–í–ò–õ–¨–ù–û!
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

# Feature –∑ –±—ñ–ª—å—à–∏–º –¥—ñ–∞–ø–∞–∑–æ–Ω–æ–º –¥–æ–º—ñ–Ω—É—î!
# –ù–∞–ø—Ä–∏–∫–ª–∞–¥: –≤—ñ–∫ (0-100) vs –∑–∞—Ä–ø–ª–∞—Ç–∞ (0-150000)
# PCA –±—É–¥–µ –º–∞–π–∂–µ –ø–æ–≤–Ω—ñ—Å—Ç—é –±–∞–∑—É–≤–∞—Ç–∏—Å—è –Ω–∞ –∑–∞—Ä–ø–ª–∞—Ç—ñ!

# ‚úÖ –ó–Ü SCALING - –ü–†–ê–í–ò–õ–¨–ù–û!
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)
```

**–ü—Ä–∏–∫–ª–∞–¥ –±–µ–∑/–∑—ñ scaling:**

```python
import numpy as np
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# –î–∞–Ω—ñ: –≤—ñ–∫ (20-80) —Ç–∞ –∑–∞—Ä–ø–ª–∞—Ç–∞ (20000-150000)
np.random.seed(42)
age = np.random.uniform(20, 80, 100)
salary = np.random.uniform(20000, 150000, 100)

X = np.column_stack([age, salary])

# –ë–µ–∑ scaling
pca_no_scale = PCA(n_components=2)
pca_no_scale.fit(X)

print("=== WITHOUT SCALING ===")
print("PC1 loadings:", pca_no_scale.components_[0])
print("Explained variance:", pca_no_scale.explained_variance_ratio_)
# PC1 –º–∞–π–∂–µ –ø–æ–≤–Ω—ñ—Å—Ç—é = salary (–±–æ –±—ñ–ª—å—à–∏–π –º–∞—Å—à—Ç–∞–±)

# –ó—ñ scaling
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

pca_with_scale = PCA(n_components=2)
pca_with_scale.fit(X_scaled)

print("\n=== WITH SCALING ===")
print("PC1 loadings:", pca_with_scale.components_[0])
print("Explained variance:", pca_with_scale.explained_variance_ratio_)
# –¢–µ–ø–µ—Ä –æ–±–∏–¥–≤—ñ features –≤–ø–ª–∏–≤–∞—é—Ç—å —Ä—ñ–≤–Ω–æ–º—ñ—Ä–Ω–æ
```

### 2. Missing Values

**PCA –Ω–µ –ø—Ä–∞—Ü—é—î –∑ NaN!**

```python
# –û–±—Ä–æ–±–∫–∞ missing values
from sklearn.impute import SimpleImputer

# –ó–∞–ø–æ–≤–Ω–∏—Ç–∏ —Å–µ—Ä–µ–¥–Ω—ñ–º
imputer = SimpleImputer(strategy='mean')
X_imputed = imputer.fit_transform(X)

# –ü–æ—Ç—ñ–º scaling —Ç–∞ PCA
X_scaled = scaler.fit_transform(X_imputed)
X_pca = pca.fit_transform(X_scaled)
```

### 3. Outliers

**Outliers —Å–∏–ª—å–Ω–æ –≤–ø–ª–∏–≤–∞—é—Ç—å –Ω–∞ PCA!**

```python
# –í–∏–¥–∞–ª–∏—Ç–∏ –µ–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ñ outliers
from scipy import stats

z_scores = np.abs(stats.zscore(X))
mask = (z_scores < 3).all(axis=1)
X_clean = X[mask]

# –ê–±–æ robust scaling
from sklearn.preprocessing import RobustScaler

robust_scaler = RobustScaler()
X_robust = robust_scaler.fit_transform(X)
```

---

## –ü–µ—Ä–µ–≤–∞–≥–∏ —Ç–∞ –Ω–µ–¥–æ–ª—ñ–∫–∏

### –ü–µ—Ä–µ–≤–∞–≥–∏ ‚úì

| –ü–µ—Ä–µ–≤–∞–≥–∞ | –ü–æ—è—Å–Ω–µ–Ω–Ω—è |
|----------|-----------|
| **–ó–º–µ–Ω—à–µ–Ω–Ω—è —Ä–æ–∑–º—ñ—Ä–Ω–æ—Å—Ç—ñ** | 100 features ‚Üí 10 –±–µ–∑ –≤–µ–ª–∏–∫–æ—ó –≤—Ç—Ä–∞—Ç–∏ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó |
| **–í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è** | –í–∏—Å–æ–∫–æ—Ä–æ–∑–º—ñ—Ä–Ω—ñ –¥–∞–Ω—ñ ‚Üí 2D/3D |
| **–î–µ–∫–æ—Ä—Ä–µ–ª—è—Ü—ñ—è features** | –ö–æ–º–ø–æ–Ω–µ–Ω—Ç–∏ –Ω–µ–∑–∞–ª–µ–∂–Ω—ñ (–æ—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω—ñ) |
| **–í–∏–¥–∞–ª–µ–Ω–Ω—è —à—É–º—É** | –ö–æ–º–ø–æ–Ω–µ–Ω—Ç–∏ –∑ –Ω–∏–∑—å–∫–æ—é variance = —à—É–º |
| **–ü—Ä–∏—Å–∫–æ—Ä–µ–Ω–Ω—è ML** | –ú–µ–Ω—à–µ features ‚Üí —à–≤–∏–¥—à–µ –Ω–∞–≤—á–∞–Ω–Ω—è |
| **–ú–∞—Ç–µ–º–∞—Ç–∏—á–Ω–æ –æ–±“ë—Ä—É–Ω—Ç–æ–≤–∞–Ω–∏–π** | –û–ø—Ç–∏–º–∞–ª—å–Ω–∞ –ø—Ä–æ–µ–∫—Ü—ñ—è (–º–∞–∫—Å–∏–º—É–º variance) |
| **–î–µ—Ç–µ—Ä–º—ñ–Ω—ñ—Å—Ç–∏—á–Ω–∏–π** | –û–¥–Ω–∞–∫–æ–≤—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ (–±–µ–∑ random) |

### –ù–µ–¥–æ–ª—ñ–∫–∏ ‚úó

| –ù–µ–¥–æ–ª—ñ–∫ | –ü–æ—è—Å–Ω–µ–Ω–Ω—è |
|---------|-----------|
| **–õ—ñ–Ω—ñ–π–Ω—ñ—Å—Ç—å** | –ü—Ä–∞—Ü—é—î —Ç—ñ–ª—å–∫–∏ –∑ –ª—ñ–Ω—ñ–π–Ω–∏–º–∏ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—è–º–∏ |
| **–ß—É—Ç–ª–∏–≤—ñ—Å—Ç—å –¥–æ –º–∞—Å—à—Ç–∞–±—É** | –û–ë–û–í'–Ø–ó–ö–û–í–ò–ô scaling |
| **–í–∞–∂–∫–æ —ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç—É–≤–∞—Ç–∏ PC** | –õ—ñ–Ω—ñ–π–Ω—ñ –∫–æ–º–±—ñ–Ω–∞—Ü—ñ—ó –≤—Å—ñ—Ö features |
| **–í—Ç—Ä–∞—Ç–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó** | –ó–º–µ–Ω—à–µ–Ω–Ω—è —Ä–æ–∑–º—ñ—Ä–Ω–æ—Å—Ç—ñ = –≤—Ç—Ä–∞—Ç–∞ –¥–µ—Ç–∞–ª–µ–π |
| **–ß—É—Ç–ª–∏–≤—ñ—Å—Ç—å –¥–æ outliers** | –í–∏–∫–∏–¥–∏ –≤–ø–ª–∏–≤–∞—é—Ç—å –Ω–∞ PC |
| **–ü–æ—Ç—Ä–µ–±—É—î –±–∞–≥–∞—Ç–æ –ø–∞–º'—è—Ç—ñ** | –ö–æ–≤–∞—Ä—ñ–∞—Ü—ñ–π–Ω–∞ –º–∞—Ç—Ä–∏—Ü—è d√ód |
| **Supervised info –≤—Ç—Ä–∞—á–µ–Ω–∞** | –ù–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î labels (—è–∫—â–æ —î) |

---

## –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –∑ —ñ–Ω—à–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏

| –ú–µ—Ç–æ–¥ | –õ—ñ–Ω—ñ–π–Ω–∏–π? | Supervised? | –Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å | –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è |
|-------|-----------|-------------|-------------------|--------------|
| **PCA** | ‚úÖ –¢–∞–∫ | ‚ùå –ù—ñ | ‚≠ê‚≠ê‚≠ê | –ó–∞–≥–∞–ª—å–Ω–µ –∑–º–µ–Ω—à–µ–Ω–Ω—è —Ä–æ–∑–º—ñ—Ä–Ω–æ—Å—Ç—ñ |
| **t-SNE** | ‚ùå –ù—ñ | ‚ùå –ù—ñ | ‚≠ê | –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è (—Ç—ñ–ª—å–∫–∏ 2D/3D) |
| **UMAP** | ‚ùå –ù—ñ | ‚ùå –ù—ñ | ‚≠ê‚≠ê | –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è + downstream tasks |
| **LDA** | ‚úÖ –¢–∞–∫ | ‚úÖ –¢–∞–∫ | ‚≠ê‚≠ê‚≠ê‚≠ê | –ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è (supervised) |
| **Autoencoders** | ‚ùå –ù—ñ | ‚ùå –ù—ñ | ‚≠ê | –°–∫–ª–∞–¥–Ω—ñ –Ω–µ–ª—ñ–Ω—ñ–π–Ω—ñ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏ |

### PCA vs t-SNE

**PCA:**
- ‚úÖ –®–≤–∏–¥–∫–∏–π
- ‚úÖ –î–µ—Ç–µ—Ä–º—ñ–Ω—ñ—Å—Ç–∏—á–Ω–∏–π
- ‚úÖ –ü—Ä–∞—Ü—é—î –∑ –±—É–¥—å-—è–∫–æ—é —Ä–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—é
- ‚ùå –¢—ñ–ª—å–∫–∏ –ª—ñ–Ω—ñ–π–Ω–∏–π
- ‚ùå –ì–ª–æ–±–∞–ª—å–Ω–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ (–º–æ–∂–µ –≤—Ç—Ä–∞—Ç–∏—Ç–∏ –ª–æ–∫–∞–ª—å–Ω—ñ –ø–∞—Ç—Ç–µ—Ä–Ω–∏)

**t-SNE:**
- ‚úÖ –ù–µ–ª—ñ–Ω—ñ–π–Ω–∏–π (–∑–Ω–∞—Ö–æ–¥–∏—Ç—å —Å–∫–ª–∞–¥–Ω—ñ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏)
- ‚úÖ –ö—Ä–∞—â–∞ –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤
- ‚ùå –ü–æ–≤—ñ–ª—å–Ω–∏–π
- ‚ùå –¢—ñ–ª—å–∫–∏ –¥–ª—è –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó (2D/3D)
- ‚ùå –ù–µ–¥–µ—Ç–µ—Ä–º—ñ–Ω—ñ—Å—Ç–∏—á–Ω–∏–π

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—è:** PCA —Å–ø–æ—á–∞—Ç–∫—É (100D ‚Üí 50D), –ø–æ—Ç—ñ–º t-SNE (50D ‚Üí 2D) ‚úì

---

## –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ PCA

### –Ü–¥–µ–∞–ª—å–Ω–æ –ø—ñ–¥—Ö–æ–¥–∏—Ç—å ‚úì

- **–ë–∞–≥–∞—Ç–æ features** (>10-20) ‚Äî curse of dimensionality
- **–ö–æ—Ä–µ–ª—å–æ–≤–∞–Ω—ñ features** ‚Äî PCA –¥–µ–∫–æ—Ä–µ–ª—é—î
- **–í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è** –≤–∏—Å–æ–∫–æ—Ä–æ–∑–º—ñ—Ä–Ω–∏—Ö –¥–∞–Ω–∏—Ö
- **Preprocessing** –ø–µ—Ä–µ–¥ ML (–ø—Ä–∏—Å–∫–æ—Ä–µ–Ω–Ω—è)
- **–í–∏–¥–∞–ª–µ–Ω–Ω—è —à—É–º—É** ‚Äî –≤–∏–∫–∏–Ω—É—Ç–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∏ –∑ –Ω–∏–∑—å–∫–æ—é variance
- **–°—Ç–∏—Å–Ω–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö** ‚Äî –∑–±–µ—Ä—ñ–≥–∞—Ç–∏ –º–µ–Ω—à–µ
- **–õ—ñ–Ω—ñ–π–Ω—ñ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ** –º—ñ–∂ features
- **–ß–∏—Å–ª–æ–≤—ñ features** (–Ω–µ–ø–µ—Ä–µ—Ä–≤–Ω—ñ)

### –ö—Ä–∞—â–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ —ñ–Ω—à–µ ‚úó

- **–ù–µ–ª—ñ–Ω—ñ–π–Ω—ñ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏** ‚Üí Kernel PCA, Autoencoders, UMAP
- **Supervised task** –¥–µ labels –≤–∞–∂–ª–∏–≤—ñ ‚Üí LDA
- **–¢—ñ–ª—å–∫–∏ –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è** –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤ ‚Üí t-SNE, UMAP
- **Categorical features** ‚Üí MCA (Multiple Correspondence Analysis)
- **–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å** –∫—Ä–∏—Ç–∏—á–Ω–∞ ‚Üí Feature selection
- **–†–æ–∑—Ä—ñ–¥–∂–µ–Ω—ñ—Å—Ç—å** (sparsity) –≤–∞–∂–ª–∏–≤–∞ ‚Üí Sparse PCA, NMF

---

## –ü—Ä–∞–∫—Ç–∏—á–Ω—ñ –ø–æ—Ä–∞–¥–∏ üí°

### 1. –ó–ê–í–ñ–î–ò —Ä–æ–±–∏—Ç–∏ scaling!

```python
# ‚ùå –ö–†–ò–¢–ò–ß–ù–ê –ü–û–ú–ò–õ–ö–ê
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

# ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
X_pca = pca.fit_transform(X_scaled)
```

### 2. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π 95% variance —è–∫ threshold

```python
# Cumulative variance
pca = PCA()
pca.fit(X_scaled)
cumsum = np.cumsum(pca.explained_variance_ratio_)

# –ó–Ω–∞–π—Ç–∏ n_components –¥–ª—è 95%
n_components = np.argmax(cumsum >= 0.95) + 1
print(f"95% variance: {n_components} components")

# –ê–±–æ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ
pca = PCA(n_components=0.95)  # –ó–±–µ—Ä–µ–≥—Ç–∏ 95% variance
X_pca = pca.fit_transform(X_scaled)
```

### 3. –í—ñ–∑—É–∞–ª—ñ–∑—É–π scree plot

```python
# Scree plot –¥–ª—è –≤–∏–±–æ—Ä—É n_components
pca = PCA()
pca.fit(X_scaled)

plt.figure(figsize=(10, 6))
plt.bar(range(1, len(pca.explained_variance_ratio_) + 1),
        pca.explained_variance_ratio_)
plt.xlabel('Principal Component')
plt.ylabel('Explained Variance Ratio')
plt.title('Scree Plot')
plt.show()

# –®—É–∫–∞–π "–ª—ñ–∫–æ—Ç—å" (elbow)
```

### 4. –Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç—É–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∏

```python
# –Ø–∫—ñ features –Ω–∞–π–±—ñ–ª—å—à–µ –≤–ø–ª–∏–≤–∞—é—Ç—å –Ω–∞ –∫–æ–∂–µ–Ω PC?
components_df = pd.DataFrame(
    pca.components_,
    columns=feature_names,
    index=[f'PC{i+1}' for i in range(pca.n_components_)]
)

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
plt.figure(figsize=(12, 6))
sns.heatmap(components_df, cmap='RdBu_r', center=0, 
           annot=True, fmt='.2f')
plt.title('Principal Component Loadings')
plt.tight_layout()
plt.show()

# –ù–∞–π–≤–∞–∂–ª–∏–≤—ñ—à—ñ features –¥–ª—è PC1
pc1_loadings = np.abs(components_df.iloc[0])
top_features = pc1_loadings.nlargest(5)
print("Top 5 features for PC1:")
print(top_features)
```

### 5. PCA –¥–ª—è preprocessing –ø–µ—Ä–µ–¥ ML

```python
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression

# Pipeline: scaling ‚Üí PCA ‚Üí classifier
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('pca', PCA(n_components=0.95)),
    ('classifier', LogisticRegression())
])

pipeline.fit(X_train, y_train)
score = pipeline.score(X_test, y_test)

print(f"Accuracy: {score:.4f}")
print(f"Used {pipeline.named_steps['pca'].n_components_} components")
```

### 6. –í–∏–¥–∞–ª–∏ outliers –ø–µ—Ä–µ–¥ PCA

```python
# Outliers —Å–∏–ª—å–Ω–æ –≤–ø–ª–∏–≤–∞—é—Ç—å –Ω–∞ PC
from scipy import stats

z_scores = np.abs(stats.zscore(X))
mask = (z_scores < 3).all(axis=1)
X_clean = X[mask]

print(f"Removed {np.sum(~mask)} outliers")
```

### 7. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π Incremental PCA –¥–ª—è –≤–µ–ª–∏–∫–∏—Ö –¥–∞–Ω–∏—Ö

```python
# –Ø–∫—â–æ –¥–∞–Ω—ñ –Ω–µ –≤–º—ñ—â—É—é—Ç—å—Å—è –≤ RAM
from sklearn.decomposition import IncrementalPCA

inc_pca = IncrementalPCA(n_components=50, batch_size=1000)

# –ù–∞–≤—á–∞–Ω–Ω—è –ø–æ –±–∞—Ç—á–∞–º
for batch in data_batches:
    inc_pca.partial_fit(batch)
```

### 8. Kernel PCA –¥–ª—è –Ω–µ–ª—ñ–Ω—ñ–π–Ω–∏—Ö –¥–∞–Ω–∏—Ö

```python
# –Ø–∫—â–æ –ª—ñ–Ω—ñ–π–Ω–∏–π PCA –Ω–µ –ø—Ä–∞—Ü—é—î
from sklearn.decomposition import KernelPCA

kpca = KernelPCA(n_components=2, kernel='rbf', gamma=10)
X_kpca = kpca.fit_transform(X_scaled)
```

### 9. –ó–±–µ—Ä—ñ–≥–∞–π scaler —Ç–∞ PCA —Ä–∞–∑–æ–º

```python
import joblib

# –ó–±–µ—Ä–µ–≥—Ç–∏
joblib.dump(scaler, 'scaler.pkl')
joblib.dump(pca, 'pca.pkl')

# –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ —Ç–∞ –∑–∞—Å—Ç–æ—Å—É–≤–∞—Ç–∏
scaler = joblib.load('scaler.pkl')
pca = joblib.load('pca.pkl')

X_new_scaled = scaler.transform(X_new)
X_new_pca = pca.transform(X_new_scaled)
```

### 10. –ü–µ—Ä–µ–≤—ñ—Ä—è–π reconstruction error

```python
# –Ø–∫—â–æ reconstruction error –≤–µ–ª–∏–∫–∏–π ‚Üí –ø–æ—Ç—Ä—ñ–±–Ω–æ –±—ñ–ª—å—à–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ñ–≤
X_reconstructed = pca.inverse_transform(X_pca)
mse = np.mean((X_scaled - X_reconstructed) ** 2)

print(f"Reconstruction MSE: {mse:.6f}")

# –ê–±–æ –¥–ª—è –æ–∫—Ä–µ–º–∏—Ö –∑—Ä–∞–∑–∫—ñ–≤
errors = np.sum((X_scaled - X_reconstructed) ** 2, axis=1)
worst_samples = np.argsort(errors)[-5:]
print(f"Samples with worst reconstruction: {worst_samples}")
```

---

## –†–µ–∞–ª—å–Ω—ñ –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è

### 1. Image Compression

**–ó–∞–¥–∞—á–∞:** –ó–º–µ–Ω—à–∏—Ç–∏ —Ä–æ–∑–º—ñ—Ä –∑–æ–±—Ä–∞–∂–µ–Ω—å –±–µ–∑ –≤–µ–ª–∏–∫–æ—ó –≤—Ç—Ä–∞—Ç–∏ —è–∫–æ—Å—Ç—ñ.

**–ü—ñ–¥—Ö—ñ–¥:**
- –ö–æ–∂–µ–Ω –ø—ñ–∫—Å–µ–ª—å = feature
- PCA –Ω–∞ –ø—ñ–∫—Å–µ–ª—è—Ö
- –ó–±–µ—Ä—ñ–≥–∞—Ç–∏ —Ç—ñ–ª—å–∫–∏ top k –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ñ–≤

**–†–µ–∑—É–ª—å—Ç–∞—Ç:**
- 1000√ó1000 RGB = 3,000,000 values
- PCA (500 –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ñ–≤) = 500 values
- Compression: 6000x!

```python
from sklearn.decomposition import PCA
import numpy as np
from PIL import Image

# –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è
img = Image.open('image.jpg').convert('RGB')
img_array = np.array(img)  # (height, width, 3)

# –û–±—Ä–æ–±–∏—Ç–∏ –∫–æ–∂–µ–Ω –∫–∞–Ω–∞–ª –æ–∫—Ä–µ–º–æ
compressed_channels = []

for channel in range(3):  # R, G, B
    channel_data = img_array[:, :, channel]
    
    # PCA
    pca = PCA(n_components=50)  # –ó–±–µ—Ä–µ–≥—Ç–∏ 50 –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ñ–≤
    transformed = pca.fit_transform(channel_data)
    
    compressed_channels.append({
        'pca': pca,
        'transformed': transformed
    })

# –†–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—è
reconstructed_img = np.zeros_like(img_array)

for channel in range(3):
    pca = compressed_channels[channel]['pca']
    transformed = compressed_channels[channel]['transformed']
    
    reconstructed = pca.inverse_transform(transformed)
    reconstructed_img[:, :, channel] = np.clip(reconstructed, 0, 255)

reconstructed_img = reconstructed_img.astype(np.uint8)

# –ó–±–µ—Ä–µ–≥—Ç–∏
Image.fromarray(reconstructed_img).save('compressed.jpg')

# Compression ratio
original_size = img_array.size
compressed_size = sum(
    ch['transformed'].size + ch['pca'].components_.size 
    for ch in compressed_channels
)

print(f"Compression ratio: {original_size / compressed_size:.1f}x")
```

### 2. Face Recognition (Eigenfaces)

**–ó–∞–¥–∞—á–∞:** –†–æ–∑–ø—ñ–∑–Ω–∞–≤–∞–Ω–Ω—è –æ–±–ª–∏—á.

**–ü—ñ–¥—Ö—ñ–¥:**
- –ö–æ–∂–Ω–µ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –æ–±–ª–∏—á—á—è = —Ç–æ—á–∫–∞ –≤ –≤–∏—Å–æ–∫–æ—Ä–æ–∑–º—ñ—Ä–Ω–æ–º—É –ø—Ä–æ—Å—Ç–æ—Ä—ñ
- PCA –∑–Ω–∞—Ö–æ–¥–∏—Ç—å "eigenfaces" (–≥–æ–ª–æ–≤–Ω—ñ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∏ –æ–±–ª–∏—á)
- –ù–æ–≤–µ –æ–±–ª–∏—á—á—è = –ª—ñ–Ω—ñ–π–Ω–∞ –∫–æ–º–±—ñ–Ω–∞—Ü—ñ—è eigenfaces

**–ü–µ—Ä–µ–≤–∞–≥–∏:**
- –ó–º–µ–Ω—à—É—î —Ä–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å (100√ó100 = 10,000D ‚Üí 50D)
- –í–∏–¥–∞–ª—è—î —à—É–º
- –®–≤–∏–¥–∫–µ —Ä–æ–∑–ø—ñ–∑–Ω–∞–≤–∞–Ω–Ω—è

### 3. Recommender Systems

**–ó–∞–¥–∞—á–∞:** –†–µ–∫–æ–º–µ–Ω–¥—É–≤–∞—Ç–∏ —Ñ—ñ–ª—å–º–∏ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞–º.

**–ü—ñ–¥—Ö—ñ–¥:**
- –ú–∞—Ç—Ä–∏—Ü—è users √ó movies (–¥—É–∂–µ —Ä–æ–∑—Ä—ñ–¥–∂–µ–Ω–∞!)
- PCA –∑–Ω–∞—Ö–æ–¥–∏—Ç—å –ª–∞—Ç–µ–Ω—Ç–Ω—ñ —Ñ–∞–∫—Ç–æ—Ä–∏ (–∂–∞–Ω—Ä–∏, —Å—Ç–∏–ª—ñ)
- User preferences = –∫–æ–º–±—ñ–Ω–∞—Ü—ñ—è –ª–∞—Ç–µ–Ω—Ç–Ω–∏—Ö —Ñ–∞–∫—Ç–æ—Ä—ñ–≤

**–ü—Ä–∏–∫–ª–∞–¥:**
```
Original: 10,000 users √ó 5,000 movies = 50M features
PCA: 10,000 users √ó 20 factors = 200K features
Compression: 250x!
```

### 4. Gene Expression Analysis

**–ó–∞–¥–∞—á–∞:** –ü—Ä–æ–∞–Ω–∞–ª—ñ–∑—É–≤–∞—Ç–∏ –¥–∞–Ω—ñ –µ–∫—Å–ø—Ä–µ—Å—ñ—ó –≥–µ–Ω—ñ–≤.

**–î–∞–Ω—ñ:**
- –ó—Ä–∞–∑–∫–∏ –ø–∞—Ü—ñ—î–Ω—Ç—ñ–≤ √ó –≥–µ–Ω–∏ (1000 √ó 20,000)
- –î—É–∂–µ –≤–∏—Å–æ–∫–æ—Ä–æ–∑–º—ñ—Ä–Ω—ñ!

**–ü—ñ–¥—Ö—ñ–¥:**
- PCA –¥–ª—è –∑–º–µ–Ω—à–µ–Ω–Ω—è –¥–æ 50-100 –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ñ–≤
- –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –≤ 2D/3D
- –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—è –ø–∞—Ü—ñ—î–Ω—Ç—ñ–≤

**–†–µ–∑—É–ª—å—Ç–∞—Ç:**
- –í–∏—è–≤–ª–µ–Ω–Ω—è –ø—ñ–¥—Ç–∏–ø—ñ–≤ –∑–∞—Ö–≤–æ—Ä—é–≤–∞–Ω—å
- –ü—Ä–æ–≥–Ω–æ—Å—Ç–∏—á–Ω—ñ –º–∞—Ä–∫–µ—Ä–∏

### 5. Financial Portfolio Analysis

**–ó–∞–¥–∞—á–∞:** –ê–Ω–∞–ª—ñ–∑ –∫–æ—Ä–µ–ª—è—Ü—ñ–π –º—ñ–∂ –∞–∫—Ü—ñ—è–º–∏.

**–ü—ñ–¥—Ö—ñ–¥:**
- Features = —Ü—ñ–Ω–∏ —Ä—ñ–∑–Ω–∏—Ö –∞–∫—Ü—ñ–π
- PCA –∑–Ω–∞—Ö–æ–¥–∏—Ç—å "—Ñ–∞–∫—Ç–æ—Ä–Ω—ñ –ø–æ—Ä—Ç—Ñ–µ–ª—ñ"
- PC1 —á–∞—Å—Ç–æ = "—Ä–∏–Ω–∫–æ–≤–∏–π —Ñ–∞–∫—Ç–æ—Ä" (–∑–∞–≥–∞–ª—å–Ω–∏–π —Ç—Ä–µ–Ω–¥ —Ä–∏–Ω–∫—É)

**–ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è:**
- –î–∏–≤–µ—Ä—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è –ø–æ—Ä—Ç—Ñ–µ–ª—è
- Risk management
- Factor investing

---

## –ü–æ—à–∏—Ä–µ–Ω—ñ –ø–æ–º–∏–ª–∫–∏ ‚ùå

### 1. –ù–µ —Ä–æ–±–∏—Ç–∏ scaling

```python
# ‚ùå –ë–ï–ó SCALING
# Features: –≤—ñ–∫ (20-80), –∑–∞—Ä–ø–ª–∞—Ç–∞ (20K-150K), —Å—Ç–∞–∂ (0-40)
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)
# –ó–∞—Ä–ø–ª–∞—Ç–∞ –ø–æ–≤–Ω—ñ—Å—Ç—é –¥–æ–º—ñ–Ω—É—î!

# ‚úÖ –ó–Ü SCALING
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
X_pca = pca.fit_transform(X_scaled)
```

### 2. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ PCA –Ω–∞ categorical features

```python
# ‚ùå PCA –Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä—ñ—è—Ö
# Features: –∫—Ä–∞—ó–Ω–∞ (UK, US, FR), —Å—Ç–∞—Ç—å (M, F)
pca = PCA(n_components=2)
pca.fit(X_categorical)  # –ë–µ–∑–≥–ª—É–∑–¥–æ!

# ‚úÖ One-hot encoding —Å–ø–æ—á–∞—Ç–∫—É
X_encoded = pd.get_dummies(df, drop_first=True)
X_scaled = scaler.fit_transform(X_encoded)
X_pca = pca.fit_transform(X_scaled)
```

### 3. –Ü–≥–Ω–æ—Ä—É–≤–∞—Ç–∏ explained variance

```python
# ‚ùå –ü—Ä–æ—Å—Ç–æ –≤–∏–±—Ä–∞—Ç–∏ n_components=2 –¥–ª—è –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)
# –ú–æ–∂–ª–∏–≤–æ –≤—Ç—Ä–∞—á–µ–Ω–æ 95% —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó!

# ‚úÖ –ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ explained variance
print(f"Explained variance: {pca.explained_variance_ratio_.sum():.2%}")

# –Ø–∫—â–æ < 80% ‚Üí –¥–æ–¥–∞—Ç–∏ –±—ñ–ª—å—à–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ñ–≤ –∞–±–æ PCA –Ω–µ –ø—ñ–¥—Ö–æ–¥–∏—Ç—å
```

### 4. –ó–∞—Å—Ç–æ—Å–æ–≤—É–≤–∞—Ç–∏ PCA –Ω–∞ train+test —Ä–∞–∑–æ–º

```python
# ‚ùå FIT –Ω–∞ –≤—Å—ñ—Ö –¥–∞–Ω–∏—Ö (–≤–∫–ª—é—á–∞—é—á–∏ test)
X_all = np.vstack([X_train, X_test])
pca = PCA(n_components=10)
pca.fit(X_all)  # DATA LEAKAGE!

X_train_pca = pca.transform(X_train)
X_test_pca = pca.transform(X_test)

# ‚úÖ FIT —Ç—ñ–ª—å–∫–∏ –Ω–∞ train
pca = PCA(n_components=10)
pca.fit(X_train)

X_train_pca = pca.transform(X_train)
X_test_pca = pca.transform(X_test)
```

### 5. –Ü–≥–Ω–æ—Ä—É–≤–∞—Ç–∏ outliers

```python
# ‚ùå PCA –∑ outliers
# –í–∏–∫–∏–¥–∏ —Å–∏–ª—å–Ω–æ –≤–ø–ª–∏–≤–∞—é—Ç—å –Ω–∞ principal components!

# ‚úÖ –í–∏–¥–∞–ª–∏—Ç–∏ –∞–±–æ robust scaling
from sklearn.preprocessing import RobustScaler

robust_scaler = RobustScaler()
X_robust = robust_scaler.fit_transform(X)
```

### 6. –û—á—ñ–∫—É–≤–∞—Ç–∏ —â–æ PCA –ø–æ–∫—Ä–∞—â–∏—Ç—å –º–æ–¥–µ–ª—å

```python
# ‚ùå "PCA –∑–∞–≤–∂–¥–∏ –ø–æ–∫—Ä–∞—â—É—î accuracy"
# PCA –≤–∏–¥–∞–ª—è—î —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é! –ù–µ –≥–∞—Ä–∞–Ω—Ç—É—î –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è.

# ‚úÖ –ü–æ—Ä—ñ–≤–Ω—è–π –∑/–±–µ–∑ PCA
# –ó PCA: —à–≤–∏–¥—à–µ, –º–µ–Ω—à–µ overfitting
# –ë–µ–∑ PCA: –±—ñ–ª—å—à–µ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó, –º–æ–∂–ª–∏–≤–æ –∫—Ä–∞—â–∞ accuracy
```

### 7. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ PCA –¥–ª—è feature selection

```python
# ‚ùå "PCA –≤–∏–±–∏—Ä–∞—î –Ω–∞–π–∫—Ä–∞—â—ñ features"
# PCA —Å—Ç–≤–æ—Ä—é—î –ù–û–í–Ü features (–∫–æ–º–±—ñ–Ω–∞—Ü—ñ—ó —Å—Ç–∞—Ä–∏—Ö)!

# ‚úÖ –î–ª—è feature selection –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π:
from sklearn.feature_selection import SelectKBest
# –∞–±–æ Recursive Feature Elimination
```

### 8. –ó–∞–±—É—Ç–∏ inverse_transform –ø—Ä–∏ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—ó

```python
# ‚ùå –ó–∞–±—É—Ç–∏ –ø—Ä–æ scaler –ø—Ä–∏ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—ó
X_pca = pca.transform(X_scaled)
X_reconstructed = pca.inverse_transform(X_pca)
# X_reconstructed –≤—Å–µ —â–µ scaled!

# ‚úÖ Inverse scaling
X_reconstructed_original = scaler.inverse_transform(X_reconstructed)
```

---

## –ü–æ–≤'—è–∑–∞–Ω—ñ —Ç–µ–º–∏

- [[02_t-SNE]] ‚Äî –Ω–µ–ª—ñ–Ω—ñ–π–Ω–∞ –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
- [[03_UMAP]] ‚Äî —à–≤–∏–¥—à–∞ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ t-SNE
- [[04_LDA]] ‚Äî supervised dimensionality reduction
- [[05_Autoencoders]] ‚Äî neural network based
- [[06_Manifold_Learning]] ‚Äî –Ω–µ–ª—ñ–Ω—ñ–π–Ω—ñ –º–µ—Ç–æ–¥–∏
- [[Feature_Selection]] ‚Äî –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ –¥–æ PCA
- [[SVD]] ‚Äî –º–∞—Ç–µ–º–∞—Ç–∏—á–Ω–∞ –æ—Å–Ω–æ–≤–∞ PCA

## –†–µ—Å—É—Ä—Å–∏

- [Scikit-learn: PCA](https://scikit-learn.org/stable/modules/decomposition.html#pca)
- [Original Paper: Pearson (1901)](https://www.tandfonline.com/doi/abs/10.1080/14786440109462720)
- [StatQuest: PCA](https://www.youtube.com/watch?v=FgakZw6K1QQ)
- [A Tutorial on PCA (Shlens, 2014)](https://arxiv.org/abs/1404.1100)

---

## –ö–ª—é—á–æ–≤—ñ –≤–∏—Å–Ω–æ–≤–∫–∏

> PCA –∑–Ω–∞—Ö–æ–¥–∏—Ç—å –Ω–æ–≤—ñ –æ—Å—ñ (principal components) –≤–∑–¥–æ–≤–∂ —è–∫–∏—Ö –¥–∞–Ω—ñ –º–∞—é—Ç—å –Ω–∞–π–±—ñ–ª—å—à—É variance, –¥–æ–∑–≤–æ–ª—è—é—á–∏ –∑–º–µ–Ω—à–∏—Ç–∏ —Ä–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å –∑–±–µ—Ä—ñ–≥–∞—é—á–∏ –º–∞–∫—Å–∏–º—É–º —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó.

**–û—Å–Ω–æ–≤–Ω—ñ –ø—Ä–∏–Ω—Ü–∏–ø–∏:**

- **–õ—ñ–Ω—ñ–π–Ω–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ—è:** –Ω–æ–≤—ñ features = –ª—ñ–Ω—ñ–π–Ω—ñ –∫–æ–º–±—ñ–Ω–∞—Ü—ñ—ó —Å—Ç–∞—Ä–∏—Ö
- **–ú–∞–∫—Å–∏–º—É–º variance:** PC1 –∑–∞—Ö–æ–ø–ª—é—î –Ω–∞–π–±—ñ–ª—å—à—É variance, PC2 ‚Äî –¥—Ä—É–≥—É –Ω–∞–π–±—ñ–ª—å—à—É, —ñ —Ç.–¥.
- **–û—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω—ñ—Å—Ç—å:** principal components –ø–µ—Ä–ø–µ–Ω–¥–∏–∫—É–ª—è—Ä–Ω—ñ (–¥–µ–∫–æ—Ä–µ–ª—å–æ–≤–∞–Ω—ñ)
- **Unsupervised:** –Ω–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î labels (—è–∫—â–æ —î)

**–ü—Ä–æ—Ü–µ—Å:**

1. –¶–µ–Ω—Ç—Ä—É–≤–∞–Ω–Ω—è –¥–∞–Ω–∏—Ö (–≤—ñ–¥–Ω—è—Ç–∏ —Å–µ—Ä–µ–¥–Ω—î)
2. –û–±—á–∏—Å–ª–∏—Ç–∏ –∫–æ–≤–∞—Ä—ñ–∞—Ü—ñ–π–Ω—É –º–∞—Ç—Ä–∏—Ü—é
3. –ó–Ω–∞–π—Ç–∏ –≤–ª–∞—Å–Ω—ñ –≤–µ–∫—Ç–æ—Ä–∏ —Ç–∞ –∑–Ω–∞—á–µ–Ω–Ω—è
4. –í—ñ–¥—Å–æ—Ä—Ç—É–≤–∞—Ç–∏ –∑–∞ –≤–ª–∞—Å–Ω–∏–º–∏ –∑–Ω–∞—á–µ–Ω–Ω—è–º–∏
5. –í–∏–±—Ä–∞—Ç–∏ top k –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ñ–≤
6. –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º—É–≤–∞—Ç–∏ –¥–∞–Ω—ñ

**–í–∏–±—ñ—Ä –∫—ñ–ª—å–∫–æ—Å—Ç—ñ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ñ–≤:**

- **Explained variance:** –∑–±–µ—Ä–µ–≥—Ç–∏ 90-95% variance
- **Scree plot:** —à—É–∫–∞—Ç–∏ "–ª—ñ–∫–æ—Ç—å"
- **Domain knowledge:** —Å–∫—ñ–ª—å–∫–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ñ–≤ –º–∞—î —Å–µ–Ω—Å

**–ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏:**

- –ë–∞–≥–∞—Ç–æ features + –∫–æ—Ä–µ–ª—å–æ–≤–∞–Ω—ñ + –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è = PCA ‚úì
- –ù–µ–ª—ñ–Ω—ñ–π–Ω—ñ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏ ‚Üí t-SNE, UMAP ‚úì
- Supervised + labels –≤–∞–∂–ª–∏–≤—ñ ‚Üí LDA ‚úì

**–ù–∞–π–≤–∞–∂–ª–∏–≤—ñ—à–µ:**

- **–ó–ê–í–ñ–î–ò —Ä–æ–±–∏—Ç–∏ scaling** (StandardScaler)
- –ü–µ—Ä–µ–≤—ñ—Ä—è—Ç–∏ explained variance (‚â• 80-95%)
- –í—ñ–∑—É–∞–ª—ñ–∑—É–≤–∞—Ç–∏ scree plot
- FIT —Ç—ñ–ª—å–∫–∏ –Ω–∞ train (—É–Ω–∏–∫–∞—Ç–∏ data leakage)
- –Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç—É–≤–∞—Ç–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∏ (loadings)
- –í–∏–¥–∞–ª—è—Ç–∏ outliers –ø–µ—Ä–µ–¥ PCA

---

#ml #unsupervised-learning #dimensionality-reduction #pca #principal-component-analysis #feature-extraction #visualization #linear-transformation
