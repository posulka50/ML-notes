# LDA (Linear Discriminant Analysis)

## –©–æ —Ü–µ?

**LDA (Linear Discriminant Analysis)** ‚Äî —Ü–µ **supervised** –∞–ª–≥–æ—Ä–∏—Ç–º dimensionality reduction, —è–∫–∏–π –∑–Ω–∞—Ö–æ–¥–∏—Ç—å –ª—ñ–Ω—ñ–π–Ω—É –∫–æ–º–±—ñ–Ω–∞—Ü—ñ—é features, —â–æ –Ω–∞–π–∫—Ä–∞—â–µ **—Ä–æ–∑–¥—ñ–ª—è—î –∫–ª–∞—Å–∏**. –ù–∞ –≤—ñ–¥–º—ñ–Ω—É –≤—ñ–¥ PCA, LDA –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ labels.

**–ì–æ–ª–æ–≤–Ω–∞ —ñ–¥–µ—è:** –∑–Ω–∞–π—Ç–∏ –Ω–∞–ø—Ä—è–º–∫–∏ (discriminants), –≤–∑–¥–æ–≤–∂ —è–∫–∏—Ö —Ä—ñ–∑–Ω—ñ –∫–ª–∞—Å–∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Ä–æ–∑'—î–¥–Ω–∞–Ω—ñ, –∞ —Ç–æ—á–∫–∏ –≤—Å–µ—Ä–µ–¥–∏–Ω—ñ –∫–ª–∞—Å—É –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –∑–≥—Ä—É–ø–æ–≤–∞–Ω—ñ.

## –ù–∞–≤—ñ—â–æ –ø–æ—Ç—Ä—ñ–±–µ–Ω?

- üéØ **Supervised reduction** ‚Äî –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î labels –¥–ª—è –∫—Ä–∞—â–æ–≥–æ —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è
- üìä **Classification preprocessing** ‚Äî features –¥–ª—è –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä–∞
- üîç **Feature extraction** ‚Äî –∑–Ω–∞–π—Ç–∏ –Ω–∞–π–≤–∞–∂–ª–∏–≤—ñ—à—ñ –Ω–∞–ø—Ä—è–º–∫–∏ –¥–ª—è —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è
- üé® **–í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è** ‚Äî 2D/3D –ø—Ä–æ–µ–∫—Ü—ñ—è –∑ —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è–º –∫–ª–∞—Å—ñ–≤
- ‚öñÔ∏è **–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ —Å–µ–ø–∞—Ä–∞—Ü—ñ—è** ‚Äî between-class variance / within-class variance
- üìà **–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å** ‚Äî –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∏ –º–∞—é—Ç—å —á—ñ—Ç–∫–µ –∑–Ω–∞—á–µ–Ω–Ω—è

## –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏?

**–ü–æ—Ç—Ä—ñ–±–Ω–æ:**
- **–Ñ labels** (supervised task) ‚Äî –∫–ª—é—á–æ–≤–∞ –≤–∏–º–æ–≥–∞!
- **Classification** ‚Äî preprocessing –ø–µ—Ä–µ–¥ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä–æ–º
- **–ö–ª–∞—Å–∏ –∑–±–∞–ª–∞–Ω—Å–æ–≤–∞–Ω—ñ** ‚Äî –ø—Ä–∏–±–ª–∏–∑–Ω–æ –æ–¥–Ω–∞–∫–æ–≤–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∑—Ä–∞–∑–∫—ñ–≤
- **–ß–∏—Å–ª–æ–≤—ñ features** (–Ω–µ–ø–µ—Ä–µ—Ä–≤–Ω—ñ)
- –ü–æ—Ç—Ä—ñ–±–Ω–∞ **—Ä–æ–∑–¥—ñ–ª—è—é—á–∞ –∑–¥–∞—Ç–Ω—ñ—Å—Ç—å** –º—ñ–∂ –∫–ª–∞—Å–∞–º–∏
- **< 20 –∫–ª–∞—Å—ñ–≤** (–æ–±–º–µ–∂–µ–Ω–Ω—è: n_components ‚â§ n_classes - 1)

**–ù–µ –ø–æ—Ç—Ä—ñ–±–Ω–æ:**
- **Unsupervised** (–Ω–µ–º–∞—î labels) ‚Üí PCA, t-SNE, UMAP
- **–î—É–∂–µ –Ω–µ–±–∞–ª–∞–Ω—Å–æ–≤–∞–Ω—ñ –∫–ª–∞—Å–∏** ‚Üí weighted LDA
- **–ù–µ–ª—ñ–Ω—ñ–π–Ω–µ —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è** ‚Üí Kernel LDA –∞–±–æ Neural Networks
- **–ë–∞–≥–∞—Ç–æ –∫–ª–∞—Å—ñ–≤** (>100) ‚Üí —ñ–Ω—à—ñ –º–µ—Ç–æ–¥–∏
- **Categorical features** ‚Üí preprocessing —Å–ø–æ—á–∞—Ç–∫—É

---

## –Ø–∫ –ø—Ä–∞—Ü—é—î LDA?

### –Ü–Ω—Ç—É—ó—Ü—ñ—è

**–ü—Ä–æ–±–ª–µ–º–∞ PCA:** –ú–∞–∫—Å–∏–º—ñ–∑—É—î variance, –∞–ª–µ –Ω–µ –≤—Ä–∞—Ö–æ–≤—É—î –∫–ª–∞—Å–∏.

**LDA:** –ó–Ω–∞—Ö–æ–¥–∏—Ç—å –ø—Ä–æ–µ–∫—Ü—ñ—é, –¥–µ –∫–ª–∞—Å–∏ —Ä–æ–∑–¥—ñ–ª–µ–Ω—ñ.

### –ú–∞—Ç–µ–º–∞—Ç–∏—á–Ω–∞ –º–µ—Ç–∞

**–ú–∞–∫—Å–∏–º—ñ–∑—É–≤–∞—Ç–∏:**

$$J(w) = \frac{\text{between-class variance}}{\text{within-class variance}} = \frac{w^T S_B w}{w^T S_W w}$$

–¥–µ:
- $S_B$ ‚Äî between-class scatter matrix (–≤—ñ–¥–º—ñ–Ω–Ω–æ—Å—Ç—ñ –º—ñ–∂ –∫–ª–∞—Å–∞–º–∏)
- $S_W$ ‚Äî within-class scatter matrix (—Ä–æ–∑–∫–∏–¥ –≤—Å–µ—Ä–µ–¥–∏–Ω—ñ –∫–ª–∞—Å—ñ–≤)

**–Ü–Ω—Ç—É—ó—Ü—ñ—è:**
- **–ú—ñ–∂ –∫–ª–∞—Å–∞–º–∏** ‚Äî —Ö–æ—á–µ–º–æ –í–ï–õ–ò–ö–Ü –≤—ñ–¥—Å—Ç–∞–Ω—ñ
- **–í—Å–µ—Ä–µ–¥–∏–Ω—ñ –∫–ª–∞—Å—É** ‚Äî —Ö–æ—á–µ–º–æ –ú–ê–õ–ï–ù–¨–ö–ò–ô —Ä–æ–∑–∫–∏–¥

---

## –ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞

### 1. Within-class scatter matrix

**–†–æ–∑–∫–∏–¥ –≤—Å–µ—Ä–µ–¥–∏–Ω—ñ –∫–æ–∂–Ω–æ–≥–æ –∫–ª–∞—Å—É:**

$$S_W = \sum_{c=1}^{C} \sum_{x \in \text{class } c} (x - \mu_c)(x - \mu_c)^T$$

–¥–µ:
- $C$ ‚Äî –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∫–ª–∞—Å—ñ–≤
- $\mu_c$ ‚Äî —Å–µ—Ä–µ–¥–Ω—î –∫–ª–∞—Å—É $c$

**–Ü–Ω—Ç—É—ó—Ü—ñ—è:** –ù–∞—Å–∫—ñ–ª—å–∫–∏ —Ä–æ–∑–∫–∏–¥–∞–Ω—ñ —Ç–æ—á–∫–∏ –≤—Å–µ—Ä–µ–¥–∏–Ω—ñ –∫–æ–∂–Ω–æ–≥–æ –∫–ª–∞—Å—É.

### 2. Between-class scatter matrix

**–í—ñ–¥–º—ñ–Ω–Ω–æ—Å—Ç—ñ –º—ñ–∂ –∫–ª–∞—Å–∞–º–∏:**

$$S_B = \sum_{c=1}^{C} n_c (\mu_c - \mu)(\mu_c - \mu)^T$$

–¥–µ:
- $n_c$ ‚Äî –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∑—Ä–∞–∑–∫—ñ–≤ —É –∫–ª–∞—Å—ñ $c$
- $\mu$ ‚Äî –∑–∞–≥–∞–ª—å–Ω–µ —Å–µ—Ä–µ–¥–Ω—î –≤—Å—ñ—Ö –¥–∞–Ω–∏—Ö

**–Ü–Ω—Ç—É—ó—Ü—ñ—è:** –ù–∞—Å–∫—ñ–ª—å–∫–∏ –¥–∞–ª–µ–∫–æ —Ü–µ–Ω—Ç—Ä–∏ –∫–ª–∞—Å—ñ–≤ –≤—ñ–¥ –∑–∞–≥–∞–ª—å–Ω–æ–≥–æ —Ü–µ–Ω—Ç—Ä—É.

### 3. –†–æ–∑–≤'—è–∑–∞–Ω–Ω—è

**–ó–Ω–∞–π—Ç–∏ –≤–ª–∞—Å–Ω—ñ –≤–µ–∫—Ç–æ—Ä–∏:**

$$S_W^{-1} S_B w = \lambda w$$

**–ü—Ä–æ—Ü–µ—Å:**
1. –û–±—á–∏—Å–ª–∏—Ç–∏ $S_W$ —Ç–∞ $S_B$
2. –†–æ–∑–≤'—è–∑–∞—Ç–∏ $S_W^{-1} S_B w = \lambda w$
3. –í—ñ–¥—Å–æ—Ä—Ç—É–≤–∞—Ç–∏ –≤–ª–∞—Å–Ω—ñ –≤–µ–∫—Ç–æ—Ä–∏ –∑–∞ –≤–ª–∞—Å–Ω–∏–º–∏ –∑–Ω–∞—á–µ–Ω–Ω—è–º–∏
4. –í–∏–±—Ä–∞—Ç–∏ top k –≤–ª–∞—Å–Ω–∏—Ö –≤–µ–∫—Ç–æ—Ä—ñ–≤

**–ö—ñ–ª—å–∫—ñ—Å—Ç—å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ñ–≤:** –º–∞–∫—Å–∏–º—É–º $C - 1$ (–¥–µ $C$ ‚Äî –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∫–ª–∞—Å—ñ–≤)

---

## –ü—Ä–æ—Å—Ç–∏–π –ø—Ä–∏–∫–ª–∞–¥: 2 –∫–ª–∞—Å–∏ –≤ 2D

### –î–∞–Ω—ñ

```
–ö–ª–∞—Å A (—á–µ—Ä–≤–æ–Ω—ñ):    –ö–ª–∞—Å B (—Å–∏–Ω—ñ):
x = [1, 2]           x = [5, 6]
x = [2, 3]           x = [6, 7]
x = [1.5, 2.5]       x = [5.5, 6.5]
```

### –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è

```
    y
  8 |
  7 |        üîµ üîµ
  6 |      üîµ
  5 |
  4 |
  3 |  üî¥
  2 |üî¥  üî¥
  1 |
  0 |____________ x
    0  2  4  6  8
```

### LDA –ø—Ä–æ–µ–∫—Ü—ñ—è

**–ö—Ä–æ–∫ 1:** –û–±—á–∏—Å–ª–∏—Ç–∏ —Å–µ—Ä–µ–¥–Ω—ñ –∫–ª–∞—Å—ñ–≤
```
Œº_A = [1.5, 2.5]
Œº_B = [5.5, 6.5]
```

**–ö—Ä–æ–∫ 2:** Within-class scatter
```
S_W = scatter_A + scatter_B
```

**–ö—Ä–æ–∫ 3:** Between-class scatter
```
S_B = n_A * (Œº_A - Œº_overall)(Œº_A - Œº_overall)^T 
    + n_B * (Œº_B - Œº_overall)(Œº_B - Œº_overall)^T
```

**–ö—Ä–æ–∫ 4:** –ó–Ω–∞–π—Ç–∏ –Ω–∞–ø—Ä—è–º–æ–∫ —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è
```
LD1 –Ω–∞–ø—Ä—è–º–æ–∫: [1, 1] (diagonal)

–ü—Ä–æ–µ–∫—Ü—ñ—è –Ω–∞ LD1:
    
    LD1
     |
  üî¥üî¥|    üîµüîµ
     |
     
–Ü–¥–µ–∞–ª—å–Ω–µ —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è!
```

---

## –°–∫–ª–∞–¥–Ω–∏–π –ø—Ä–∏–∫–ª–∞–¥: Iris Dataset

### –ó–∞–¥–∞—á–∞

Iris: 150 –∫–≤—ñ—Ç—ñ–≤, 4 features, 3 –≤–∏–¥–∏ (setosa, versicolor, virginica).

**–ú–µ—Ç–∞:** –ó–º–µ–Ω—à–∏—Ç–∏ –¥–æ 2D –¥–ª—è –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó –∑ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∏–º —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è–º –∫–ª–∞—Å—ñ–≤.

### –†–µ–∑—É–ª—å—Ç–∞—Ç

**LDA –∑–Ω–∞—Ö–æ–¥–∏—Ç—å 2 –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∏** (3 –∫–ª–∞—Å–∏ - 1 = 2):
- **LD1:** –†–æ–∑–¥—ñ–ª—è—î setosa –≤—ñ–¥ (versicolor + virginica)
- **LD2:** –†–æ–∑–¥—ñ–ª—è—î versicolor –≤—ñ–¥ virginica

```
    LD2
     |
   2 |  versicolor
     |    ‚óè‚óè‚óè
   0 |‚óè‚óè‚óè     ‚óè‚óè‚óè  
     |setosa  virginica
  -2 |
     |_____________ LD1
    -8  -4  0  4  8
```

---

## –ö–æ–¥ (Python + scikit-learn)

### –ë–∞–∑–æ–≤–∏–π –ø—Ä–∏–∫–ª–∞–¥

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler

# 1. –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –¥–∞–Ω—ñ
iris = load_iris()
X = iris.data
y = iris.target

print(f"Original shape: {X.shape}")  # (150, 4)
print(f"Classes: {np.unique(y)}")    # [0, 1, 2]

# 2. Scaling (—Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–æ)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 3. LDA
lda = LDA(n_components=2)  # max = n_classes - 1 = 2
X_lda = lda.fit_transform(X_scaled, y)

print(f"LDA shape: {X_lda.shape}")  # (150, 2)

# 4. Explained variance ratio
print("\n=== Explained Variance ===")
print(f"LD1: {lda.explained_variance_ratio_[0]:.4f}")
print(f"LD2: {lda.explained_variance_ratio_[1]:.4f}")
print(f"Total: {lda.explained_variance_ratio_.sum():.4f}")

# 5. –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
plt.figure(figsize=(10, 7))

colors = ['red', 'green', 'blue']
target_names = iris.target_names

for color, i, target_name in zip(colors, [0, 1, 2], target_names):
    plt.scatter(
        X_lda[y == i, 0],
        X_lda[y == i, 1],
        alpha=0.8,
        color=color,
        label=target_name,
        s=50
    )

plt.xlabel(f'LD1 ({lda.explained_variance_ratio_[0]:.1%} variance)', fontsize=12)
plt.ylabel(f'LD2 ({lda.explained_variance_ratio_[1]:.1%} variance)', fontsize=12)
plt.title('LDA Projection of Iris Dataset', fontsize=14, fontweight='bold')
plt.legend(loc='best')
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

### –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è PCA vs LDA

```python
from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA

# Scaling
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# PCA (unsupervised)
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

# LDA (supervised)
lda = LDA(n_components=2)
X_lda = lda.fit_transform(X_scaled, y)

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è
fig, axes = plt.subplots(1, 2, figsize=(16, 7))

# PCA
for color, i, target_name in zip(colors, [0, 1, 2], target_names):
    axes[0].scatter(X_pca[y == i, 0], X_pca[y == i, 1],
                   alpha=0.8, color=color, label=target_name, s=50)

axes[0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})', fontsize=11)
axes[0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})', fontsize=11)
axes[0].set_title('PCA (Unsupervised)', fontsize=13, fontweight='bold')
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# LDA
for color, i, target_name in zip(colors, [0, 1, 2], target_names):
    axes[1].scatter(X_lda[y == i, 0], X_lda[y == i, 1],
                   alpha=0.8, color=color, label=target_name, s=50)

axes[1].set_xlabel(f'LD1 ({lda.explained_variance_ratio_[0]:.1%})', fontsize=11)
axes[1].set_ylabel(f'LD2 ({lda.explained_variance_ratio_[1]:.1%})', fontsize=11)
axes[1].set_title('LDA (Supervised)', fontsize=13, fontweight='bold')
axes[1].legend()
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("\n=== Comparison ===")
print("PCA: Maximizes variance (ignores classes)")
print("LDA: Maximizes class separation (uses labels)")
print("\nLDA shows better class separation!")
```

### LDA –¥–ª—è –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó

```python
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

# Split
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.3, random_state=42
)

# LDA —è–∫ classifier (–≤–±—É–¥–æ–≤–∞–Ω–∞ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è!)
lda_classifier = LDA()
lda_classifier.fit(X_train, y_train)

# Predictions
y_pred = lda_classifier.predict(X_test)

# Accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"LDA Classifier Accuracy: {accuracy:.4f}")

print("\nClassification Report:")
print(classification_report(y_test, y_pred, target_names=iris.target_names))

# LDA –º–æ–∂–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏—Å—å —ñ —è–∫ reducer, —ñ —è–∫ classifier!
```

### –ü–æ–≤–Ω–∏–π –ø—Ä–∏–∫–ª–∞–¥: Wine Dataset

```python
import pandas as pd
from sklearn.datasets import load_wine

# –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ Wine dataset
wine = load_wine()
X = wine.data
y = wine.target

print(f"Original shape: {X.shape}")  # (178, 13)
print(f"Classes: {np.unique(y)}")    # [0, 1, 2] - 3 types of wine

# Scaling
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.3, random_state=42, stratify=y
)

# LDA transformation (dimensionality reduction)
lda = LDA(n_components=2)
X_train_lda = lda.fit_transform(X_train, y_train)
X_test_lda = lda.transform(X_test)

print(f"\n=== LDA Transformation ===")
print(f"Original: {X_train.shape[1]} features")
print(f"LDA: {X_train_lda.shape[1]} components")
print(f"Explained variance: {lda.explained_variance_ratio_.sum():.2%}")

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
plt.figure(figsize=(12, 5))

# Train set
plt.subplot(1, 2, 1)
for i in range(3):
    mask = y_train == i
    plt.scatter(X_train_lda[mask, 0], X_train_lda[mask, 1],
               label=f'Class {i}', s=50, alpha=0.7)
plt.xlabel('LD1', fontsize=11)
plt.ylabel('LD2', fontsize=11)
plt.title('Train Set (LDA)', fontsize=13, fontweight='bold')
plt.legend()
plt.grid(True, alpha=0.3)

# Test set
plt.subplot(1, 2, 2)
for i in range(3):
    mask = y_test == i
    plt.scatter(X_test_lda[mask, 0], X_test_lda[mask, 1],
               label=f'Class {i}', s=50, alpha=0.7)
plt.xlabel('LD1', fontsize=11)
plt.ylabel('LD2', fontsize=11)
plt.title('Test Set (LDA)', fontsize=13, fontweight='bold')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Classification on LDA features
from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(n_neighbors=5)

# –ù–∞ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏—Ö features
knn.fit(X_train, y_train)
acc_original = knn.score(X_test, y_test)

# –ù–∞ LDA features
knn.fit(X_train_lda, y_train)
acc_lda = knn.score(X_test_lda, y_test)

print(f"\n=== KNN Classification ===")
print(f"Original features (13D): {acc_original:.4f}")
print(f"LDA features (2D): {acc_lda:.4f}")
print(f"Dimension reduction: {13/2:.1f}x")
```

### Coefficients (Feature Importance)

```python
# LDA coefficients –ø–æ–∫–∞–∑—É—é—Ç—å –≤–∞–∂–ª–∏–≤—ñ—Å—Ç—å features
lda = LDA(n_components=2)
lda.fit(X_scaled, y)

# Coefficients –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞
coefficients = lda.coef_

print("\n=== LDA Coefficients ===")
for i, coef in enumerate(coefficients):
    print(f"\nLD{i+1} coefficients:")
    
    # –ê–±—Å–æ–ª—é—Ç–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è –¥–ª—è –≤–∞–∂–ª–∏–≤–æ—Å—Ç—ñ
    abs_coef = np.abs(coef)
    
    # –¢–æ–ø-5 features
    top_indices = np.argsort(abs_coef)[-5:][::-1]
    
    for idx in top_indices:
        print(f"  {wine.feature_names[idx]}: {coef[idx]:.4f}")
```

### Decision Boundaries

```python
from matplotlib.colors import ListedColormap

# –í–∏–∫–æ—Ä–∏—Å—Ç–∞—î–º–æ —Ç—ñ–ª—å–∫–∏ 2 features –¥–ª—è –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó
X_2d = X_scaled[:, [0, 1]]  # Alcohol —Ç–∞ Malic acid

# LDA –Ω–∞ 2D
lda = LDA()
lda.fit(X_2d, y)

# Create mesh
h = 0.02
x_min, x_max = X_2d[:, 0].min() - 1, X_2d[:, 0].max() + 1
y_min, y_max = X_2d[:, 1].min() - 1, X_2d[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                     np.arange(y_min, y_max, h))

# Predict –Ω–∞ mesh
Z = lda.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

# Plot
plt.figure(figsize=(10, 7))
plt.contourf(xx, yy, Z, alpha=0.3, cmap=ListedColormap(['red', 'green', 'blue']))

# Scatter points
colors = ['red', 'green', 'blue']
for i in range(3):
    mask = y == i
    plt.scatter(X_2d[mask, 0], X_2d[mask, 1],
               c=colors[i], label=f'Class {i}',
               s=50, alpha=0.7, edgecolors='black')

plt.xlabel(wine.feature_names[0], fontsize=11)
plt.ylabel(wine.feature_names[1], fontsize=11)
plt.title('LDA Decision Boundaries', fontsize=13, fontweight='bold')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

---

## –í–∞—Ä—ñ–∞—Ü—ñ—ó LDA

### 1. Quadratic Discriminant Analysis (QDA)

**–©–æ —Ü–µ:** –î–æ–∑–≤–æ–ª—è—î —Ä—ñ–∑–Ω—ñ –∫–æ–≤–∞—Ä—ñ–∞—Ü—ñ–π–Ω—ñ –º–∞—Ç—Ä–∏—Ü—ñ –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –∫–ª–∞—Å—É (–∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω—ñ –≥—Ä–∞–Ω–∏—Ü—ñ).

```python
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA

# QDA (–±—ñ–ª—å—à –≥–Ω—É—á–∫—ñ –≥—Ä–∞–Ω–∏—Ü—ñ)
qda = QDA()
qda.fit(X_train, y_train)
qda_score = qda.score(X_test, y_test)

# LDA (–ª—ñ–Ω—ñ–π–Ω—ñ –≥—Ä–∞–Ω–∏—Ü—ñ)
lda = LDA()
lda.fit(X_train, y_train)
lda_score = lda.score(X_test, y_test)

print(f"LDA accuracy: {lda_score:.4f}")
print(f"QDA accuracy: {qda_score:.4f}")

# QDA –º–æ–∂–µ –±—É—Ç–∏ –∫—Ä–∞—â–∏–º —è–∫—â–æ –∫–ª–∞—Å–∏ –º–∞—é—Ç—å —Ä—ñ–∑–Ω—ñ —Ä–æ–∑–ø–æ–¥—ñ–ª–∏
```

**–ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏:**
- LDA: –ö–ª–∞—Å–∏ –º–∞—é—Ç—å –ø–æ–¥—ñ–±–Ω—ñ covariances
- QDA: –ö–ª–∞—Å–∏ –º–∞—é—Ç—å —Ä—ñ–∑–Ω—ñ covariances

### 2. Shrinkage LDA

**–©–æ —Ü–µ:** Regularization –¥–ª—è –º–∞–ª–∏—Ö datasets (–∫–æ–ª–∏ n_samples < n_features).

```python
# LDA –∑ shrinkage
lda_shrinkage = LDA(solver='lsqr', shrinkage='auto')
lda_shrinkage.fit(X_train, y_train)

print(f"Shrinkage parameter: {lda_shrinkage.covariance_.shape}")
```

**–ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏:**
- –ú–∞–ª–∏–π dataset (n < p)
- High-dimensional data
- Regularization –ø–æ—Ç—Ä—ñ–±–Ω–∞

---

## –ü—Ä–∏–ø—É—â–µ–Ω–Ω—è LDA

### 1. –ù–æ—Ä–º–∞–ª—å–Ω—ñ—Å—Ç—å

**–ö–æ–∂–µ–Ω –∫–ª–∞—Å –º–∞—î Gaussian —Ä–æ–∑–ø–æ–¥—ñ–ª.**

```python
# –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–æ—Ä–º–∞–ª—å–Ω–æ—Å—Ç—ñ
from scipy.stats import shapiro

feature_idx = 0  # –ü–µ—Ä—à–∞ feature
class_idx = 0    # –ü–µ—Ä—à–∏–π –∫–ª–∞—Å

data = X_scaled[y == class_idx, feature_idx]

stat, p_value = shapiro(data)

if p_value > 0.05:
    print(f"Feature {feature_idx} in class {class_idx} is normally distributed")
else:
    print(f"Not normally distributed (p={p_value:.4f})")
```

### 2. –û–¥–Ω–∞–∫–æ–≤–∞ –∫–æ–≤–∞—Ä—ñ–∞—Ü—ñ—è

**–í—Å—ñ –∫–ª–∞—Å–∏ –º–∞—é—Ç—å –æ–¥–Ω–∞–∫–æ–≤—É –∫–æ–≤–∞—Ä—ñ–∞—Ü—ñ–π–Ω—É –º–∞—Ç—Ä–∏—Ü—é.**

```python
# –í—ñ–∑—É–∞–ª—å–Ω–æ –ø–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏
for i in range(3):
    class_data = X_scaled[y == i]
    cov = np.cov(class_data.T)
    print(f"\nClass {i} covariance (first 2x2):")
    print(cov[:2, :2])

# –Ø–∫—â–æ –¥—É–∂–µ —Ä—ñ–∑–Ω—ñ ‚Üí —Ä–æ–∑–≥–ª—è–Ω—É—Ç–∏ QDA
```

### 3. –õ—ñ–Ω—ñ–π–Ω–∞ —Ä–æ–∑–¥—ñ–ª—é–≤–∞–Ω—ñ—Å—Ç—å

**–ö–ª–∞—Å–∏ –º–æ–∂–Ω–∞ —Ä–æ–∑–¥—ñ–ª–∏—Ç–∏ –ª—ñ–Ω—ñ–π–Ω–∏–º–∏ –≥—Ä–∞–Ω–∏—Ü—è–º–∏.**

```python
# –Ø–∫—â–æ –Ω—ñ ‚Üí Kernel LDA –∞–±–æ Neural Networks
```

---

## –ü–µ—Ä–µ–≤–∞–≥–∏ —Ç–∞ –Ω–µ–¥–æ–ª—ñ–∫–∏

### –ü–µ—Ä–µ–≤–∞–≥–∏ ‚úì

| –ü–µ—Ä–µ–≤–∞–≥–∞ | –ü–æ—è—Å–Ω–µ–Ω–Ω—è |
|----------|-----------|
| **Supervised** | –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î labels –¥–ª—è –∫—Ä–∞—â–æ–≥–æ —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è |
| **–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å** | Coefficients –ø–æ–∫–∞–∑—É—é—Ç—å –≤–∞–∂–ª–∏–≤—ñ—Å—Ç—å features |
| **Classifier + Reducer** | –ú–æ–∂–µ —ñ –∫–ª–∞—Å–∏—Ñ—ñ–∫—É–≤–∞—Ç–∏, —ñ –∑–º–µ–Ω—à—É–≤–∞—Ç–∏ —Ä–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å |
| **–®–≤–∏–¥–∫—ñ—Å—Ç—å** | –î—É–∂–µ —à–≤–∏–¥–∫–∏–π (closed-form solution) |
| **–î–µ—Ç–µ—Ä–º—ñ–Ω—ñ–∑–º** | –û–¥–Ω–∞–∫–æ–≤—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∑–∞–≤–∂–¥–∏ |
| **–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ —Å–µ–ø–∞—Ä–∞—Ü—ñ—è** | –û–ø—Ç–∏–º—ñ–∑—É—î —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –∫–ª–∞—Å—ñ–≤ |

### –ù–µ–¥–æ–ª—ñ–∫–∏ ‚úó

| –ù–µ–¥–æ–ª—ñ–∫ | –ü–æ—è—Å–Ω–µ–Ω–Ω—è |
|---------|-----------|
| **–ü–æ—Ç—Ä–µ–±—É—î labels** | –¢—ñ–ª—å–∫–∏ supervised tasks |
| **–õ—ñ–Ω—ñ–π–Ω—ñ—Å—Ç—å** | –¢—ñ–ª—å–∫–∏ –ª—ñ–Ω—ñ–π–Ω—ñ –≥—Ä–∞–Ω–∏—Ü—ñ |
| **–ü—Ä–∏–ø—É—â–µ–Ω–Ω—è** | Gaussian, –æ–¥–Ω–∞–∫–æ–≤–∞ covariance |
| **–û–±–º–µ–∂–µ–Ω–Ω—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ñ–≤** | –ú–∞–∫—Å–∏–º—É–º n_classes - 1 |
| **–ß—É—Ç–ª–∏–≤—ñ—Å—Ç—å –¥–æ outliers** | –í–ø–ª–∏–≤–∞—é—Ç—å –Ω–∞ –æ—Ü—ñ–Ω–∫—É covariance |
| **–ú–∞–ª—ñ –∫–ª–∞—Å–∏** | –ü–æ–≥–∞–Ω–æ —è–∫—â–æ –∫–ª–∞—Å –º–∞—î –º–∞–ª–æ –∑—Ä–∞–∑–∫—ñ–≤ |

---

## –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –∑ —ñ–Ω—à–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏

| –ö—Ä–∏—Ç–µ—Ä—ñ–π | PCA | LDA | t-SNE | UMAP |
|----------|-----|-----|-------|------|
| **Supervised** | ‚ùå | ‚úÖ | ‚ùå | ‚ö†Ô∏è (–æ–ø—Ü—ñ–π–Ω–æ) |
| **–õ—ñ–Ω—ñ–π–Ω–∏–π** | ‚úÖ | ‚úÖ | ‚ùå | ‚ùå |
| **–®–≤–∏–¥–∫—ñ—Å—Ç—å** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **–ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è** | ‚ùå | ‚úÖ | ‚ùå | ‚ùå |
| **Max –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ñ–≤** | min(n,p) | n_classes-1 | –±—É–¥—å-—è–∫–∏–π | –±—É–¥—å-—è–∫–∏–π |
| **–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å** | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê | ‚≠ê |

### LDA vs PCA (–¥–µ—Ç–∞–ª—å–Ω–æ)

**PCA:**
- ‚úÖ Unsupervised (–Ω–µ –ø–æ—Ç—Ä–µ–±—É—î labels)
- ‚úÖ –ú–∞–∫—Å–∏–º—ñ–∑—É—î variance
- ‚úÖ –ë—É–¥—å-—è–∫–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ñ–≤
- ‚ùå –Ü–≥–Ω–æ—Ä—É—î –∫–ª–∞—Å–∏

**LDA:**
- ‚úÖ Supervised (–≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î labels)
- ‚úÖ –ú–∞–∫—Å–∏–º—ñ–∑—É—î —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –∫–ª–∞—Å—ñ–≤
- ‚úÖ –ú–æ–∂–Ω–∞ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ —è–∫ classifier
- ‚ùå –ú–∞–∫—Å–∏–º—É–º n_classes - 1 –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ñ–≤

**–ö–æ–ª–∏ —â–æ:**
- **Classification task** ‚Üí LDA ‚úì
- **Exploratory analysis –±–µ–∑ labels** ‚Üí PCA ‚úì
- **–ë–∞–≥–∞—Ç–æ features, –º–∞–ª–æ –∫–ª–∞—Å—ñ–≤** ‚Üí PCA ‚Üí LDA ‚úì

---

## –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ LDA

### –Ü–¥–µ–∞–ª—å–Ω–æ –ø—ñ–¥—Ö–æ–¥–∏—Ç—å ‚úì

- **Classification task** –∑ labels
- **Preprocessing** –ø–µ—Ä–µ–¥ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä–æ–º
- **–í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è** –∑ —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è–º –∫–ª–∞—Å—ñ–≤
- **Feature extraction** –¥–ª—è —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è
- **–ú–∞–ª–∏–π –¥–∞—Ç–∞—Å–µ—Ç** (—à–≤–∏–¥—à–µ –∑–∞ neural networks)
- **–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å** –≤–∞–∂–ª–∏–≤–∞
- **2-10 –∫–ª–∞—Å—ñ–≤** (–æ–ø—Ç–∏–º–∞–ª—å–Ω–æ)

### –ö—Ä–∞—â–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ —ñ–Ω—à–µ ‚úó

- **Unsupervised** (–Ω–µ–º–∞—î labels) ‚Üí PCA, t-SNE, UMAP
- **–ù–µ–ª—ñ–Ω—ñ–π–Ω–µ —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è** ‚Üí Kernel LDA, Neural Networks
- **–ë–∞–≥–∞—Ç–æ –∫–ª–∞—Å—ñ–≤** (>100) ‚Üí —ñ–Ω—à—ñ –º–µ—Ç–æ–¥–∏
- **–ö–ª–∞—Å–∏ –Ω–µ Gaussian** ‚Üí —ñ–Ω—à—ñ –º–µ—Ç–æ–¥–∏
- **–î—É–∂–µ –Ω–µ–±–∞–ª–∞–Ω—Å–æ–≤–∞–Ω—ñ –∫–ª–∞—Å–∏** ‚Üí weighted LDA –∞–±–æ —ñ–Ω—à–µ

---

## –ü—Ä–∞–∫—Ç–∏—á–Ω—ñ –ø–æ—Ä–∞–¥–∏ üí°

### 1. –ó–∞–≤–∂–¥–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π –∑—ñ scaling

```python
# ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

lda = LDA()
X_lda = lda.fit_transform(X_scaled, y)
```

### 2. –ü–µ—Ä–µ–≤—ñ—Ä—è–π –±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—ñ–≤

```python
# –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞
unique, counts = np.unique(y, return_counts=True)
print("Class distribution:")
for cls, count in zip(unique, counts):
    print(f"  Class {cls}: {count} samples")

# –Ø–∫—â–æ –¥—É–∂–µ –Ω–µ–±–∞–ª–∞–Ω—Å–æ–≤–∞–Ω–æ ‚Üí —Ä–æ–∑–≥–ª—è–Ω—É—Ç–∏ resampling
```

### 3. PCA –ø–µ—Ä–µ–¥ LDA –¥–ª—è high-dimensional

```python
# –Ø–∫—â–æ n_features >> n_samples
from sklearn.pipeline import Pipeline

# Pipeline: PCA ‚Üí LDA
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('pca', PCA(n_components=50)),  # –ó–º–µ–Ω—à–∏—Ç–∏ —Å–ø–æ—á–∞—Ç–∫—É
    ('lda', LDA(n_components=2))    # –ü–æ—Ç—ñ–º LDA
])

X_reduced = pipeline.fit_transform(X, y)
```

### 4. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π —è–∫ classifier

```python
# LDA –º–æ–∂–µ –∫–ª–∞—Å–∏—Ñ—ñ–∫—É–≤–∞—Ç–∏!
lda = LDA()
lda.fit(X_train, y_train)

# Predictions
y_pred = lda.predict(X_test)

# Probabilities
y_proba = lda.predict_proba(X_test)
print(f"Probabilities for first sample: {y_proba[0]}")
```

### 5. –ü–µ—Ä–µ–≤—ñ—Ä—è–π explained variance

```python
lda = LDA()
lda.fit(X_scaled, y)

print("Explained variance ratio:")
for i, ratio in enumerate(lda.explained_variance_ratio_):
    print(f"  LD{i+1}: {ratio:.2%}")

cumsum = np.cumsum(lda.explained_variance_ratio_)
print(f"\nCumulative: {cumsum[-1]:.2%}")
```

### 6. QDA —è–∫—â–æ –∫–ª–∞—Å–∏ –º–∞—é—Ç—å —Ä—ñ–∑–Ω—ñ covariances

```python
# –ü–æ—Ä—ñ–≤–Ω—è–π LDA —Ç–∞ QDA
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis

lda = LDA()
qda = QDA()

lda_score = lda.fit(X_train, y_train).score(X_test, y_test)
qda_score = qda.fit(X_train, y_train).score(X_test, y_test)

if qda_score > lda_score + 0.05:  # –ó–Ω–∞—á–Ω–æ –∫—Ä–∞—â–µ
    print("Use QDA (classes have different covariances)")
else:
    print("Use LDA (simpler, similar performance)")
```

### 7. Cross-validation –¥–ª—è –æ—Ü—ñ–Ω–∫–∏

```python
from sklearn.model_selection import cross_val_score

lda = LDA()
scores = cross_val_score(lda, X_scaled, y, cv=5)

print(f"Cross-validation scores: {scores}")
print(f"Mean accuracy: {scores.mean():.4f} (+/- {scores.std():.4f})")
```

### 8. Feature importance —á–µ—Ä–µ–∑ coefficients

```python
lda = LDA(n_components=1)
lda.fit(X_scaled, y)

# Absolute coefficients
importance = np.abs(lda.coef_[0])

# Sort
sorted_idx = np.argsort(importance)[::-1]

print("Top 5 most important features:")
for i in sorted_idx[:5]:
    print(f"  {feature_names[i]}: {importance[i]:.4f}")
```

### 9. –í—ñ–∑—É–∞–ª—ñ–∑—É–π decision boundaries

```python
# –î–ª—è 2D –¥–∞–Ω–∏—Ö
from matplotlib.colors import ListedColormap

# Train LDA
lda = LDA()
lda.fit(X_2d, y)

# Create mesh
h = 0.02
x_min, x_max = X_2d[:, 0].min() - 1, X_2d[:, 0].max() + 1
y_min, y_max = X_2d[:, 1].min() - 1, X_2d[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                     np.arange(y_min, y_max, h))

Z = lda.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

plt.contourf(xx, yy, Z, alpha=0.3)
plt.scatter(X_2d[:, 0], X_2d[:, 1], c=y)
plt.show()
```

### 10. Shrinkage –¥–ª—è –º–∞–ª–∏—Ö datasets

```python
# –Ø–∫—â–æ n_samples –Ω–µ–≤–µ–ª–∏–∫–µ
lda = LDA(solver='lsqr', shrinkage='auto')
lda.fit(X_train, y_train)

# –ê–±–æ –∑–∞–¥–∞—Ç–∏ shrinkage –≤—Ä—É—á–Ω—É
lda = LDA(solver='lsqr', shrinkage=0.5)
```

---

## –†–µ–∞–ª—å–Ω—ñ –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è

### 1. Face Recognition

**–ó–∞–¥–∞—á–∞:** –†–æ–∑–ø—ñ–∑–Ω–∞–≤–∞–Ω–Ω—è –æ–±–ª–∏—á (Fisherfaces method).

**–ü—ñ–¥—Ö—ñ–¥:**
```python
# 1. –ö–æ–∂–Ω–∞ –æ—Å–æ–±–∞ = –∫–ª–∞—Å
# 2. LDA –∑–Ω–∞—Ö–æ–¥–∏—Ç—å –Ω–∞–ø—Ä—è–º–∫–∏, —â–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Ä–æ–∑–¥—ñ–ª—è—é—Ç—å –æ—Å—ñ–±
# 3. –ü—Ä–æ–µ–∫—Ç—É–≤–∞—Ç–∏ –Ω–æ–≤—ñ –æ–±–ª–∏—á—á—è –Ω–∞ —Ü—ñ –Ω–∞–ø—Ä—è–º–∫–∏

# Features: pixel values –∞–±–æ PCA features
faces_lda = lda.fit_transform(face_features, person_ids)

# –ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è: nearest neighbor –≤ LDA space
```

### 2. Medical Diagnosis

**–ó–∞–¥–∞—á–∞:** –î—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –∑–∞—Ö–≤–æ—Ä—é–≤–∞–Ω—å –∑–∞ —Å–∏–º–ø—Ç–æ–º–∞–º–∏.

**–ü—ñ–¥—Ö—ñ–¥:**
```python
# Classes: healthy, disease A, disease B
# Features: test results, symptoms

lda = LDA()
lda.fit(patient_features, diagnoses)

# –ù–æ–≤—ñ –ø–∞—Ü—ñ—î–Ω—Ç–∏
diagnosis_pred = lda.predict(new_patient_features)
probabilities = lda.predict_proba(new_patient_features)
```

### 3. Credit Scoring

**–ó–∞–¥–∞—á–∞:** –û—Ü—ñ–Ω–∫–∞ –∫—Ä–µ–¥–∏—Ç–æ—Å–ø—Ä–æ–º–æ–∂–Ω–æ—Å—Ç—ñ.

**–ü—ñ–¥—Ö—ñ–¥:**
```python
# Classes: good credit, bad credit
# Features: income, debt, history, etc.

lda = LDA(n_components=1)  # 1 –∫–æ–º–ø–æ–Ω–µ–Ω—Ç –¥–ª—è binary
credit_score = lda.fit_transform(applicant_features, credit_status)

# –í–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ —è–∫ credit score
threshold = find_optimal_threshold(credit_score, credit_status)
```

### 4. Document Classification

**–ó–∞–¥–∞—á–∞:** –ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è —Ç–µ–∫—Å—Ç—ñ–≤ –∑–∞ —Ç–µ–º–∞–º–∏.

**–ü—ñ–¥—Ö—ñ–¥:**
```python
from sklearn.feature_extraction.text import TfidfVectorizer

# TF-IDF features
vectorizer = TfidfVectorizer(max_features=1000)
X_tfidf = vectorizer.fit_transform(documents)

# PCA ‚Üí LDA (–¥–ª—è –∑–º–µ–Ω—à–µ–Ω–Ω—è —Ä–æ–∑–º—ñ—Ä–Ω–æ—Å—Ç—ñ)
pca = PCA(n_components=100)
X_pca = pca.fit_transform(X_tfidf.toarray())

lda = LDA(n_components=5)  # 6 topics ‚Üí 5 LDs
X_lda = lda.fit_transform(X_pca, topics)
```

### 5. Biometric Authentication

**–ó–∞–¥–∞—á–∞:** –ê—É—Ç–µ–Ω—Ç–∏—Ñ—ñ–∫–∞—Ü—ñ—è –∑–∞ –±—ñ–æ–º–µ—Ç—Ä—ñ—î—é (fingerprints, voice).

**–ü—ñ–¥—Ö—ñ–¥:**
```python
# Features: –±–∏–æ–º–µ—Ç—Ä–∏—á–Ω—ñ –æ–∑–Ω–∞–∫–∏
# Classes: —Ä—ñ–∑–Ω—ñ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ

lda = LDA()
lda.fit(biometric_features, user_ids)

# –í–µ—Ä–∏—Ñ—ñ–∫–∞—Ü—ñ—è: —á–∏ –Ω–æ–≤–∏–π –∑—Ä–∞–∑–æ–∫ –≤—ñ–¥ claimed user?
def verify(new_sample, claimed_user_id):
    proba = lda.predict_proba([new_sample])[0]
    user_proba = proba[claimed_user_id]
    
    return user_proba > threshold
```

---

## –ü–æ—à–∏—Ä–µ–Ω—ñ –ø–æ–º–∏–ª–∫–∏ ‚ùå

### 1. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –±–µ–∑ labels

```python
# ‚ùå LDA –ø–æ—Ç—Ä–µ–±—É—î labels!
lda = LDA()
lda.fit(X)  # TypeError!

# ‚úÖ –ü–µ—Ä–µ–¥–∞–π y
lda.fit(X, y)
```

### 2. –ù–µ —Ä–æ–±–∏—Ç–∏ scaling

```python
# ‚ùå –ë–µ–∑ scaling (features –≤ —Ä—ñ–∑–Ω–∏—Ö –º–∞—Å—à—Ç–∞–±–∞—Ö)
lda = LDA()
lda.fit(X, y)

# ‚úÖ –ó—ñ scaling
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
lda.fit(X_scaled, y)
```

### 3. –ë—ñ–ª—å—à–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ñ–≤ –Ω—ñ–∂ –∫–ª–∞—Å—ñ–≤ - 1

```python
# ‚ùå –ó–∞–Ω–∞–¥—Ç–æ –±–∞–≥–∞—Ç–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ñ–≤
lda = LDA(n_components=5)  # –∞–ª–µ —Ç—ñ–ª—å–∫–∏ 3 –∫–ª–∞—Å–∏!
# ValueError: n_components > n_classes - 1

# ‚úÖ –ú–∞–∫—Å–∏–º—É–º n_classes - 1
n_classes = len(np.unique(y))
lda = LDA(n_components=min(2, n_classes - 1))
```

### 4. –î—É–∂–µ –Ω–µ–±–∞–ª–∞–Ω—Å–æ–≤–∞–Ω—ñ –∫–ª–∞—Å–∏

```python
# ‚ùå –ö–ª–∞—Å 1: 1000 –∑—Ä–∞–∑–∫—ñ–≤, –ö–ª–∞—Å 2: 10 –∑—Ä–∞–∑–∫—ñ–≤
# LDA –±—É–¥–µ bias –¥–æ –≤–µ–ª–∏–∫–æ–≥–æ –∫–ª–∞—Å—É

# ‚úÖ Resampling –∞–±–æ weighted LDA
from imblearn.over_sampling import SMOTE

smote = SMOTE()
X_resampled, y_resampled = smote.fit_resample(X, y)

lda.fit(X_resampled, y_resampled)
```

### 5. –û—á—ñ–∫—É–≤–∞—Ç–∏ –Ω–µ–ª—ñ–Ω—ñ–π–Ω–µ —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è

```python
# ‚ùå LDA —Ç—ñ–ª—å–∫–∏ –ª—ñ–Ω—ñ–π–Ω–∏–π!
# –Ø–∫—â–æ –∫–ª–∞—Å–∏ –Ω–µ–ª—ñ–Ω—ñ–π–Ω–æ —Ä–æ–∑–¥—ñ–ª–µ–Ω—ñ ‚Üí –ø–æ–≥–∞–Ω—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏

# ‚úÖ Kernel LDA –∞–±–æ Neural Networks
# –ê–±–æ feature engineering —Å–ø–æ—á–∞—Ç–∫—É
```

### 6. –Ü–≥–Ω–æ—Ä—É–≤–∞—Ç–∏ –ø—Ä–∏–ø—É—â–µ–Ω–Ω—è

```python
# ‚ùå –ù–µ –ø–µ—Ä–µ–≤—ñ—Ä—è—Ç–∏ Gaussian assumption
# –Ø–∫—â–æ –¥–∞–Ω—ñ –¥—É–∂–µ –Ω–µ-Gaussian ‚Üí LDA –º–æ–∂–µ –ø–æ–≥–∞–Ω–æ –ø—Ä–∞—Ü—é–≤–∞—Ç–∏

# ‚úÖ –ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ –≤—ñ–∑—É–∞–ª—å–Ω–æ –∞–±–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–Ω–æ
import seaborn as sns

for cls in np.unique(y):
    class_data = X_scaled[y == cls, 0]  # –ü–µ—Ä—à–∞ feature
    sns.histplot(class_data, kde=True)
plt.show()
```

### 7. Fit –Ω–∞ –≤—Å—ñ—Ö –¥–∞–Ω–∏—Ö (train + test)

```python
# ‚ùå DATA LEAKAGE
X_all = np.vstack([X_train, X_test])
y_all = np.hstack([y_train, y_test])
lda.fit(X_all, y_all)  # ‚Üê Leakage!

# ‚úÖ Fit —Ç—ñ–ª—å–∫–∏ –Ω–∞ train
lda.fit(X_train, y_train)
X_test_lda = lda.transform(X_test)
```

---

## –ü–æ–≤'—è–∑–∞–Ω—ñ —Ç–µ–º–∏

- [[01_PCA]] ‚Äî unsupervised –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞
- [[02_t-SNE]] ‚Äî –Ω–µ–ª—ñ–Ω—ñ–π–Ω–∞ –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
- [[03_UMAP]] ‚Äî —à–≤–∏–¥–∫–∞ –Ω–µ–ª—ñ–Ω—ñ–π–Ω–∞ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞
- [[Linear_Regression]] ‚Äî —Ä–µ–≥—Ä–µ—Å—ñ—è –∑–∞–º—ñ—Å—Ç—å –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó
- [[Logistic_Regression]] ‚Äî —ñ–Ω—à–∏–π supervised classifier
- [[QDA]] ‚Äî –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–∞ –≤–µ—Ä—Å—ñ—è LDA

## –†–µ—Å—É—Ä—Å–∏

- [Scikit-learn: LDA](https://scikit-learn.org/stable/modules/lda_qda.html)
- [Original Paper: Fisher (1936)](https://digital.library.adelaide.edu.au/dspace/bitstream/2440/15227/1/138.pdf)
- [StatQuest: LDA](https://www.youtube.com/watch?v=azXCzI57Yfc)
- [Comparison: PCA vs LDA](https://sebastianraschka.com/Articles/2014_python_lda.html)

---

## –ö–ª—é—á–æ–≤—ñ –≤–∏—Å–Ω–æ–≤–∫–∏

> LDA ‚Äî —Ü–µ supervised –∞–ª–≥–æ—Ä–∏—Ç–º dimensionality reduction, —è–∫–∏–π –∑–Ω–∞—Ö–æ–¥–∏—Ç –Ω–∞–ø—Ä—è–º–∫–∏, —â–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Ä–æ–∑–¥—ñ–ª—è—é—Ç—å –∫–ª–∞—Å–∏, –º–∞–∫—Å–∏–º—ñ–∑—É—é—á–∏ –≤—ñ–¥–Ω–æ—à–µ–Ω–Ω—è between-class variance –¥–æ within-class variance.

**–û—Å–Ω–æ–≤–Ω—ñ –ø—Ä–∏–Ω—Ü–∏–ø–∏:**
- **Supervised:** –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î labels (–∫–ª—é—á–æ–≤–∞ –≤—ñ–¥–º—ñ–Ω–Ω—ñ—Å—Ç—å –≤—ñ–¥ PCA)
- **–õ—ñ–Ω—ñ–π–Ω–∏–π:** –∑–Ω–∞—Ö–æ–¥–∏—Ç—å –ª—ñ–Ω—ñ–π–Ω—ñ –∫–æ–º–±—ñ–Ω–∞—Ü—ñ—ó features
- **–û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è:** max(between-class var / within-class var)
- **Classifier + Reducer:** –º–æ–∂–µ —ñ –∫–ª–∞—Å–∏—Ñ—ñ–∫—É–≤–∞—Ç–∏, —ñ –∑–º–µ–Ω—à—É–≤–∞—Ç–∏ —Ä–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å

**–ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞:**
- **–ú–∞–∫—Å–∏–º—ñ–∑—É–≤–∞—Ç–∏:** $J(w) = \frac{w^T S_B w}{w^T S_W w}$
- **–†–æ–∑–≤'—è–∑–æ–∫:** –í–ª–∞—Å–Ω—ñ –≤–µ–∫—Ç–æ—Ä–∏ $S_W^{-1} S_B$
- **–û–±–º–µ–∂–µ–Ω–Ω—è:** –ú–∞–∫—Å–∏–º—É–º n_classes - 1 –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ñ–≤

**–ü—Ä–∏–ø—É—â–µ–Ω–Ω—è:**
- Gaussian —Ä–æ–∑–ø–æ–¥—ñ–ª –∫–æ–∂–Ω–æ–≥–æ –∫–ª–∞—Å—É
- –û–¥–Ω–∞–∫–æ–≤–∞ –∫–æ–≤–∞—Ä—ñ–∞—Ü—ñ–π–Ω–∞ –º–∞—Ç—Ä–∏—Ü—è
- –õ—ñ–Ω—ñ–π–Ω–∞ —Ä–æ–∑–¥—ñ–ª—é–≤–∞–Ω—ñ—Å—Ç—å

**–ü–µ—Ä–µ–≤–∞–≥–∏ –Ω–∞–¥ PCA:**
- ‚úÖ –ö—Ä–∞—â–µ —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –∫–ª–∞—Å—ñ–≤
- ‚úÖ Supervised (uses labels)
- ‚úÖ –ú–æ–∂–Ω–∞ –∫–ª–∞—Å–∏—Ñ—ñ–∫—É–≤–∞—Ç–∏
- ‚úÖ –Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å (coefficients)

**–ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏:**
- Classification + —î labels = LDA ‚úì
- Unsupervised ‚Üí PCA ‚úì
- –ù–µ–ª—ñ–Ω—ñ–π–Ω–µ ‚Üí Kernel LDA –∞–±–æ NN ‚úì
- –ë–∞–≥–∞—Ç–æ –∫–ª–∞—Å—ñ–≤ (>100) ‚Üí —ñ–Ω—à–µ ‚úì

**–ù–∞–π–≤–∞–∂–ª–∏–≤—ñ—à–µ:**
- **–ü–æ—Ç—Ä—ñ–±–Ω—ñ labels** ‚Äî supervised –º–µ—Ç–æ–¥
- **Scaling –∫—Ä–∏—Ç–∏—á–Ω–∏–π** ‚Äî –∑–∞–≤–∂–¥–∏ StandardScaler
- **Max n_classes - 1** –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ñ–≤
- **PCA ‚Üí LDA** pipeline –¥–ª—è high-dimensional
- **–ü–µ—Ä–µ–≤—ñ—Ä—è–π –±–∞–ª–∞–Ω—Å** –∫–ª–∞—Å—ñ–≤
- **QDA** —è–∫—â–æ —Ä—ñ–∑–Ω—ñ covariances

---

#ml #supervised-learning #dimensionality-reduction #lda #linear-discriminant-analysis #classification #feature-extraction #supervised
