# Market Basket Analysis

## –©–æ —Ü–µ?

**Market Basket Analysis (MBA)** ‚Äî —Ü–µ —Ç–µ—Ö–Ω—ñ–∫–∞ data mining –¥–ª—è –≤–∏—è–≤–ª–µ–Ω–Ω—è **–∞—Å–æ—Ü—ñ–∞—Ü—ñ–π –º—ñ–∂ —Ç–æ–≤–∞—Ä–∞–º–∏**, —è–∫—ñ –∫–ª—ñ—î–Ω—Ç–∏ –∫—É–ø—É—é—Ç—å —Ä–∞–∑–æ–º. –¶–µ –ø—Ä–∞–∫—Ç–∏—á–Ω–µ –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è association rules mining (Apriori, FP-Growth) —É retail —Ç–∞ e-commerce.

**–ì–æ–ª–æ–≤–Ω–∞ —ñ–¥–µ—è:** –∑–Ω–∞–π—Ç–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–∏ —Ç–∏–ø—É "–∫–ª—ñ—î–Ω—Ç–∏, —â–æ –∫—É–ø—É—é—Ç—å X, —Ç–∞–∫–æ–∂ –∫—É–ø—É—é—Ç—å Y", —â–æ–± –æ–ø—Ç–∏–º—ñ–∑—É–≤–∞—Ç–∏ –ø—Ä–æ–¥–∞–∂—ñ, —Ä–æ–∑—Ç–∞—à—É–≤–∞–Ω–Ω—è —Ç–æ–≤–∞—Ä—ñ–≤, —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó.

## –ù–∞–≤—ñ—â–æ –ø–æ—Ç—Ä—ñ–±–µ–Ω?

- üõí **Product placement** ‚Äî –¥–µ —Ä–æ–∑–º—ñ—â—É–≤–∞—Ç–∏ —Ç–æ–≤–∞—Ä–∏ –≤ –º–∞–≥–∞–∑–∏–Ω—ñ
- üí∞ **Cross-selling** ‚Äî —è–∫—ñ —Ç–æ–≤–∞—Ä–∏ –ø—Ä–æ–ø–æ–Ω—É–≤–∞—Ç–∏ —Ä–∞–∑–æ–º
- üéØ **Bundling** ‚Äî —è–∫—ñ bundle –ø—Ä–æ–ø–æ–∑–∏—Ü—ñ—ó —Å—Ç–≤–æ—Ä—é–≤–∞—Ç–∏
- üìß **Personalized marketing** ‚Äî targeted campaigns
- üìä **Inventory management** ‚Äî –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è –∑–∞–ø–∞—Å—ñ–≤
- üè™ **Store layout** ‚Äî –ø–ª–∞–Ω—É–≤–∞–Ω–Ω—è —Ä–æ–∑—Ç–∞—à—É–≤–∞–Ω–Ω—è –≤—ñ–¥–¥—ñ–ª—ñ–≤
- üí° **Product recommendations** ‚Äî "–ß–∞—Å—Ç–æ –∫—É–ø—É—é—Ç—å —Ä–∞–∑–æ–º"

## –ë—ñ–∑–Ω–µ—Å-—Ü—ñ–Ω–Ω—ñ—Å—Ç—å

### –¢–∏–ø–æ–≤—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏

**–ü—Ä–∏–∫–ª–∞–¥ 1: Diapers & Beer**
```
–ü—Ä–∞–≤–∏–ª–æ: {Diapers} ‚Üí {Beer}
Support: 15%
Confidence: 75%
Lift: 2.3

–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü—ñ—è:
- 15% —Ç—Ä–∞–Ω–∑–∞–∫—Ü—ñ–π –º—ñ—Å—Ç—è—Ç—å –æ–±–∏–¥–≤–∞ —Ç–æ–≤–∞—Ä–∏
- 75% —Ö—Ç–æ –∫—É–ø—É—î –ø—ñ–¥–≥—É–∑–∫–∏ —Ç–∞–∫–æ–∂ –∫—É–ø—É—î –ø–∏–≤–æ
- Lift 2.3 ‚Üí –≤ 2.3 —Ä–∞–∑–∏ –±—ñ–ª—å—à–∞ –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å –Ω—ñ–∂ –≤–∏–ø–∞–¥–∫–æ–≤–æ

–ë—ñ–∑–Ω–µ—Å –¥—ñ—è:
‚úì –†–æ–∑–º—ñ—Å—Ç–∏—Ç–∏ –ø–∏–≤–æ –ø–æ—Ä—è–¥ –∑ –¥–∏—Ç—è—á–∏–º–∏ —Ç–æ–≤–∞—Ä–∞–º–∏
‚úì Bundle –ø—Ä–æ–ø–æ–∑–∏—Ü—ñ—è: "Pampers + Budweiser"
‚úì Email campaign: "Bought diapers? Try our beer selection"
```

**–ü—Ä–∏–∫–ª–∞–¥ 2: Bread & Butter**
```
–ü—Ä–∞–≤–∏–ª–æ: {Bread} ‚Üí {Butter}
Support: 20%
Confidence: 60%
Lift: 1.5

–î—ñ—è:
‚úì –†–æ–∑–º—ñ—Å—Ç–∏—Ç–∏ –ø–æ—Ä—É—á –≤ –º–∞–≥–∞–∑–∏–Ω—ñ
‚úì –ó–Ω–∏–∂–∫–∞ –Ω–∞ butter –ø—Ä–∏ –∫—É–ø—ñ–≤–ª—ñ bread
‚úì Recipe suggestions (Bread + Butter recipes)
```

---

## –ü–æ–≤–Ω–∏–π –±—ñ–∑–Ω–µ—Å-–ø—Ä–æ—Ü–µ—Å MBA

### 1. –ë—ñ–∑–Ω–µ—Å-–ø–∏—Ç–∞–Ω–Ω—è

**–¢–∏–ø–æ–≤—ñ –ø–∏—Ç–∞–Ω–Ω—è:**
- –Ø–∫—ñ —Ç–æ–≤–∞—Ä–∏ –∫—É–ø—É—é—Ç—å —Ä–∞–∑–æ–º?
- –Ø–∫ –æ–ø—Ç–∏–º—ñ–∑—É–≤–∞—Ç–∏ —Ä–æ–∑—Ç–∞—à—É–≤–∞–Ω–Ω—è —Ç–æ–≤–∞—Ä—ñ–≤?
- –Ø–∫—ñ bundle –ø—Ä–æ–ø–æ–∑–∏—Ü—ñ—ó —Å—Ç–≤–æ—Ä–∏—Ç–∏?
- –ö–æ–º—É –≤—ñ–¥–ø—Ä–∞–≤–∏—Ç–∏ targeted marketing?
- –Ø–∫ –∑–±—ñ–ª—å–∏—Ç–∏ average basket size?

### 2. –ó–±—ñ—Ä –¥–∞–Ω–∏—Ö

**–î–∂–µ—Ä–µ–ª–∞:**
- POS (Point of Sale) systems
- E-commerce transaction logs
- Loyalty card data
- Online clickstream data

**–§–æ—Ä–º–∞—Ç:**
```
Transaction_ID, Date, Customer_ID, Products
T001, 2024-01-15, C123, [Milk, Bread, Butter]
T002, 2024-01-15, C456, [Beer, Chips, Diapers]
T003, 2024-01-15, C789, [Coffee, Sugar, Milk]
```

### 3. Data preprocessing

```python
import pandas as pd
import numpy as np
from mlxtend.preprocessing import TransactionEncoder
from mlxtend.frequent_patterns import apriori, fpgrowth, association_rules

# –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –¥–∞–Ω—ñ
df_raw = pd.read_csv('transactions.csv')

print(f"Total transactions: {len(df_raw)}")
print(f"Date range: {df_raw['Date'].min()} to {df_raw['Date'].max()}")
print(f"Unique customers: {df_raw['Customer_ID'].nunique()}")

# –ü–µ—Ä–µ—Ç–≤–æ—Ä–∏—Ç–∏ –≤ transaction format
transactions = df_raw.groupby('Transaction_ID')['Product'].apply(list).values.tolist()

print(f"\nSample transactions:")
for i, trans in enumerate(transactions[:5], 1):
    print(f"T{i}: {trans}")

# –û—á–∏—Å—Ç–∏—Ç–∏ –¥–∞–Ω—ñ
def clean_transactions(transactions):
    """–û—á–∏—Å—Ç–∏—Ç–∏ —Ç—Ä–∞–Ω–∑–∞–∫—Ü—ñ—ó"""
    clean = []
    
    for trans in transactions:
        # Remove nulls, empty strings
        trans_clean = [item.strip() for item in trans if item and str(item).strip()]
        
        # Remove duplicates in same transaction
        trans_clean = list(set(trans_clean))
        
        # Keep only non-empty
        if len(trans_clean) > 0:
            clean.append(trans_clean)
    
    return clean

transactions_clean = clean_transactions(transactions)

print(f"\nAfter cleaning: {len(transactions_clean)} transactions")

# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
transaction_sizes = [len(t) for t in transactions_clean]
print(f"\nTransaction size stats:")
print(f"  Mean: {np.mean(transaction_sizes):.2f}")
print(f"  Median: {np.median(transaction_sizes):.0f}")
print(f"  Max: {max(transaction_sizes)}")
print(f"  Min: {min(transaction_sizes)}")
```

### 4. Exploratory Data Analysis

```python
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter

# –¢–æ–ø —Ç–æ–≤–∞—Ä—ñ–≤
all_items = [item for trans in transactions_clean for item in trans]
item_counts = Counter(all_items)

top_items = item_counts.most_common(20)

plt.figure(figsize=(12, 6))
items, counts = zip(*top_items)
plt.barh(range(len(items)), counts)
plt.yticks(range(len(items)), items)
plt.xlabel('Frequency', fontsize=12)
plt.title('Top 20 Most Frequent Items', fontsize=14, fontweight='bold')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()

print("\n=== Top 10 Items ===")
for item, count in top_items[:10]:
    pct = count / len(transactions_clean) * 100
    print(f"{item:20s}: {count:5d} ({pct:5.2f}%)")

# –†–æ–∑–ø–æ–¥—ñ–ª —Ä–æ–∑–º—ñ—Ä—É —Ç—Ä–∞–Ω–∑–∞–∫—Ü—ñ–π
plt.figure(figsize=(10, 6))
plt.hist(transaction_sizes, bins=50, edgecolor='black', alpha=0.7)
plt.xlabel('Transaction Size (number of items)', fontsize=12)
plt.ylabel('Frequency', fontsize=12)
plt.title('Distribution of Transaction Sizes', fontsize=14, fontweight='bold')
plt.axvline(np.mean(transaction_sizes), color='red', 
           linestyle='--', label=f'Mean: {np.mean(transaction_sizes):.1f}')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# Temporal analysis
df_raw['Date'] = pd.to_datetime(df_raw['Date'])
df_raw['DayOfWeek'] = df_raw['Date'].dt.day_name()
df_raw['Hour'] = df_raw['Date'].dt.hour

# –¢—Ä–∞–Ω–∑–∞–∫—Ü—ñ—ó –ø–æ –¥–Ω—è—Ö —Ç–∏–∂–Ω—è
plt.figure(figsize=(10, 6))
day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 
            'Friday', 'Saturday', 'Sunday']
day_counts = df_raw.groupby('DayOfWeek').size().reindex(day_order)
day_counts.plot(kind='bar', color='steelblue', edgecolor='black')
plt.xlabel('Day of Week', fontsize=12)
plt.ylabel('Number of Transactions', fontsize=12)
plt.title('Transactions by Day of Week', fontsize=14, fontweight='bold')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()
```

### 5. Association Rules Mining

```python
# Transform –¥–æ binary matrix
te = TransactionEncoder()
te_ary = te.fit(transactions_clean).transform(transactions_clean)
df_encoded = pd.DataFrame(te_ary, columns=te.columns_)

print(f"Binary matrix shape: {df_encoded.shape}")

# FP-Growth (—à–≤–∏–¥—à–µ –Ω–∞ –≤–µ–ª–∏–∫–∏—Ö –¥–∞–Ω–∏—Ö)
print("\nMining frequent itemsets...")
frequent_itemsets = fpgrowth(
    df_encoded,
    min_support=0.01,  # 1% - adjust based on data
    use_colnames=True
)

print(f"Found {len(frequent_itemsets)} frequent itemsets")

# Association rules
print("\nGenerating association rules...")
rules = association_rules(
    frequent_itemsets,
    metric="confidence",
    min_threshold=0.3  # 30%
)

# –î–æ–¥–∞—Ç–∫–æ–≤—ñ –º–µ—Ç—Ä–∏–∫–∏
rules['antecedent_len'] = rules['antecedents'].apply(lambda x: len(x))
rules['consequent_len'] = rules['consequents'].apply(lambda x: len(x))

print(f"Generated {len(rules)} rules")
print()

# –§—ñ–ª—å—Ç—Ä—É–≤–∞—Ç–∏ strong rules
strong_rules = rules[
    (rules['confidence'] >= 0.5) &  # 50%+
    (rules['lift'] >= 1.2) &         # 20%+ lift
    (rules['support'] >= 0.01)       # 1%+ support
]

print(f"Strong rules: {len(strong_rules)}")
```

### 6. –Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü—ñ—è —Ç–∞ Insights

```python
# –¢–æ–ø –ø—Ä–∞–≤–∏–ª–∞ –∑–∞ lift
print("\n=== Top 10 Rules by Lift ===\n")
top_rules = rules.nlargest(10, 'lift')

for idx, rule in top_rules.iterrows():
    ant = ', '.join(list(rule['antecedents']))
    cons = ', '.join(list(rule['consequents']))
    
    print(f"{ant} ‚Üí {cons}")
    print(f"  Support: {rule['support']:.3f} ({rule['support']*100:.1f}%)")
    print(f"  Confidence: {rule['confidence']:.3f} ({rule['confidence']*100:.1f}%)")
    print(f"  Lift: {rule['lift']:.3f}")
    print()

# –ì—Ä—É–ø—É–≤–∞—Ç–∏ –∑–∞ –∫–∞—Ç–µ–≥–æ—Ä—ñ—è–º–∏ (—è–∫—â–æ —î)
def categorize_item(item):
    """–ö–∞—Ç–µ–≥–æ—Ä–∏–∑—É–≤–∞—Ç–∏ —Ç–æ–≤–∞—Ä"""
    # –ü—Ä–∏–∫–ª–∞–¥ –ª–æ–≥—ñ–∫–∏
    if any(word in item.lower() for word in ['milk', 'cheese', 'butter', 'yogurt']):
        return 'Dairy'
    elif any(word in item.lower() for word in ['bread', 'buns', 'bagel']):
        return 'Bakery'
    elif any(word in item.lower() for word in ['beer', 'wine', 'vodka']):
        return 'Alcohol'
    # ... more categories
    else:
        return 'Other'

# –ê–Ω–∞–ª—ñ–∑ cross-category rules
def get_category_cross_rules(rules):
    """–ó–Ω–∞–π—Ç–∏ –ø—Ä–∞–≤–∏–ª–∞ –º—ñ–∂ –∫–∞—Ç–µ–≥–æ—Ä—ñ—è–º–∏"""
    cross_rules = []
    
    for _, rule in rules.iterrows():
        ant_items = list(rule['antecedents'])
        cons_items = list(rule['consequents'])
        
        ant_cats = set(categorize_item(item) for item in ant_items)
        cons_cats = set(categorize_item(item) for item in cons_items)
        
        if ant_cats != cons_cats:  # Different categories
            cross_rules.append({
                'ant_category': ant_cats,
                'cons_category': cons_cats,
                'rule': rule
            })
    
    return cross_rules

cross_category = get_category_cross_rules(strong_rules)
print(f"\n=== Cross-Category Rules: {len(cross_category)} ===")
```

### 7. –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è

```python
# Network graph
import networkx as nx

def plot_rules_network(rules, top_n=15):
    """Network graph —Å–∏–ª—å–Ω–∏—Ö –ø—Ä–∞–≤–∏–ª"""
    
    top_rules = rules.nlargest(top_n, 'lift')
    
    G = nx.DiGraph()
    
    for _, rule in top_rules.iterrows():
        for ant in rule['antecedents']:
            for cons in rule['consequents']:
                G.add_edge(
                    ant, cons,
                    weight=rule['lift'],
                    confidence=rule['confidence']
                )
    
    plt.figure(figsize=(14, 10))
    
    pos = nx.spring_layout(G, k=1.5, iterations=50)
    
    # Nodes
    nx.draw_networkx_nodes(G, pos, node_size=3000,
                          node_color='lightblue',
                          alpha=0.8, edgecolors='black', linewidths=2)
    
    # Labels
    nx.draw_networkx_labels(G, pos, font_size=9, font_weight='bold')
    
    # Edges
    edges = G.edges()
    weights = [G[u][v]['weight'] for u, v in edges]
    
    nx.draw_networkx_edges(
        G, pos,
        width=[w*2 for w in weights],
        alpha=0.5,
        edge_color='gray',
        arrows=True,
        arrowsize=20,
        arrowstyle='->',
        connectionstyle='arc3,rad=0.1'
    )
    
    plt.title('Product Association Network\n(Edge width = Lift)', 
             fontsize=14, fontweight='bold')
    plt.axis('off')
    plt.tight_layout()
    plt.show()

plot_rules_network(strong_rules)

# Scatter plot
plt.figure(figsize=(12, 8))

scatter = plt.scatter(
    rules['support'],
    rules['confidence'],
    s=rules['lift'] * 100,
    alpha=0.6,
    c=rules['lift'],
    cmap='viridis',
    edgecolors='black',
    linewidths=0.5
)

plt.colorbar(scatter, label='Lift')
plt.xlabel('Support', fontsize=12)
plt.ylabel('Confidence', fontsize=12)
plt.title('Association Rules: Support vs Confidence\n(Size and Color = Lift)',
         fontsize=14, fontweight='bold')
plt.grid(True, alpha=0.3)

# Annotate top rules
top_5 = rules.nlargest(5, 'lift')
for _, rule in top_5.iterrows():
    ant = ', '.join(list(rule['antecedents']))[:15]
    cons = ', '.join(list(rule['consequents']))[:15]
    plt.annotate(
        f"{ant}‚Üí{cons}",
        (rule['support'], rule['confidence']),
        fontsize=7,
        alpha=0.7
    )

plt.tight_layout()
plt.show()

# Heatmap –∫–∞—Ç–µ–≥–æ—Ä—ñ–π
def plot_category_heatmap(rules):
    """Heatmap –∞—Å–æ—Ü—ñ–∞—Ü—ñ–π –º—ñ–∂ –∫–∞—Ç–µ–≥–æ—Ä—ñ—è–º–∏"""
    
    # –ö–∞—Ç–µ–≥–æ—Ä—ñ—ó
    categories = set()
    for _, rule in rules.iterrows():
        for item in rule['antecedents']:
            categories.add(categorize_item(item))
        for item in rule['consequents']:
            categories.add(categorize_item(item))
    
    categories = sorted(list(categories))
    
    # –ú–∞—Ç—Ä–∏—Ü—è
    matrix = np.zeros((len(categories), len(categories)))
    
    for _, rule in rules.iterrows():
        for ant in rule['antecedents']:
            for cons in rule['consequents']:
                ant_cat = categorize_item(ant)
                cons_cat = categorize_item(cons)
                
                i = categories.index(ant_cat)
                j = categories.index(cons_cat)
                
                matrix[i, j] += rule['lift']
    
    # Plot
    plt.figure(figsize=(10, 8))
    sns.heatmap(
        matrix,
        annot=True,
        fmt='.1f',
        cmap='YlOrRd',
        xticklabels=categories,
        yticklabels=categories,
        cbar_kws={'label': 'Total Lift'}
    )
    plt.xlabel('Consequent Category', fontsize=12)
    plt.ylabel('Antecedent Category', fontsize=12)
    plt.title('Category Cross-Selling Heatmap', 
             fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.show()

plot_category_heatmap(strong_rules)
```

### 8. –ë—ñ–∑–Ω–µ—Å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó

```python
def generate_recommendations(rules, top_n=20):
    """–ì–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ –±—ñ–∑–Ω–µ—Å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó"""
    
    recommendations = []
    
    top_rules = rules.nlargest(top_n, 'lift')
    
    for _, rule in top_rules.iterrows():
        ant = ', '.join(list(rule['antecedents']))
        cons = ', '.join(list(rule['consequents']))
        
        # Store layout
        rec = {
            'type': 'Store Layout',
            'action': f"Place '{cons}' near '{ant}'",
            'reason': f"Lift: {rule['lift']:.2f}, Confidence: {rule['confidence']:.1%}",
            'expected_impact': f"{rule['support']*100:.1f}% of transactions affected"
        }
        recommendations.append(rec)
        
        # Bundle
        if rule['lift'] > 1.5 and rule['confidence'] > 0.6:
            rec = {
                'type': 'Bundle Offer',
                'action': f"Create bundle: '{ant}' + '{cons}'",
                'reason': f"Strong association (lift={rule['lift']:.2f})",
                'expected_impact': f"Potential {rule['confidence']*100:.0f}% conversion"
            }
            recommendations.append(rec)
        
        # Marketing campaign
        if rule['support'] > 0.05:  # High support
            rec = {
                'type': 'Email Campaign',
                'action': f"Send '{cons}' offer to customers who bought '{ant}'",
                'reason': f"{rule['confidence']*100:.0f}% likely to buy",
                'expected_impact': f"Target {rule['support']*100:.1f}% of customer base"
            }
            recommendations.append(rec)
    
    return recommendations

recs = generate_recommendations(strong_rules)

print("\n=== Business Recommendations ===\n")
for i, rec in enumerate(recs[:10], 1):
    print(f"{i}. [{rec['type']}] {rec['action']}")
    print(f"   Reason: {rec['reason']}")
    print(f"   Impact: {rec['expected_impact']}")
    print()
```

---

## –°–µ–≥–º–µ–Ω—Ç–∞—Ü—ñ—è –∫–ª—ñ—î–Ω—Ç—ñ–≤

### RFM Analysis + MBA

```python
# –†–æ–∑—Ä–∞—Ö—É–≤–∞—Ç–∏ RFM –º–µ—Ç—Ä–∏–∫–∏
def calculate_rfm(df):
    """Recency, Frequency, Monetary"""
    
    current_date = df['Date'].max() + pd.Timedelta(days=1)
    
    rfm = df.groupby('Customer_ID').agg({
        'Date': lambda x: (current_date - x.max()).days,  # Recency
        'Transaction_ID': 'count',                         # Frequency
        'Amount': 'sum'                                    # Monetary
    })
    
    rfm.columns = ['Recency', 'Frequency', 'Monetary']
    
    return rfm

rfm = calculate_rfm(df_raw)

# –°–µ–≥–º–µ–Ω—Ç—É–≤–∞—Ç–∏
def segment_customers(rfm):
    """–°–µ–≥–º–µ–Ω—Ç–∞—Ü—ñ—è –∑–∞ RFM"""
    
    # Quartiles
    r_labels = range(4, 0, -1)  # 4=best (recent), 1=worst
    f_labels = range(1, 5)       # 4=best (frequent), 1=worst
    m_labels = range(1, 5)
    
    rfm['R_Score'] = pd.qcut(rfm['Recency'], 4, labels=r_labels)
    rfm['F_Score'] = pd.qcut(rfm['Frequency'], 4, labels=f_labels)
    rfm['M_Score'] = pd.qcut(rfm['Monetary'], 4, labels=m_labels)
    
    rfm['RFM_Score'] = (
        rfm['R_Score'].astype(str) +
        rfm['F_Score'].astype(str) +
        rfm['M_Score'].astype(str)
    )
    
    # –°–µ–≥–º–µ–Ω—Ç–∏
    def assign_segment(row):
        if row['R_Score'] >= 3 and row['F_Score'] >= 3 and row['M_Score'] >= 3:
            return 'Champions'
        elif row['R_Score'] >= 3 and row['F_Score'] >= 2:
            return 'Loyal Customers'
        elif row['R_Score'] >= 3:
            return 'Potential Loyalists'
        elif row['F_Score'] <= 2 and row['R_Score'] <= 2:
            return 'At Risk'
        elif row['R_Score'] <= 2:
            return 'Lost'
        else:
            return 'Others'
    
    rfm['Segment'] = rfm.apply(assign_segment, axis=1)
    
    return rfm

rfm_segments = segment_customers(rfm)

print("=== Customer Segments ===")
print(rfm_segments['Segment'].value_counts())

# MBA –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞ –æ–∫—Ä–µ–º–æ
def mba_by_segment(df, segment_name):
    """Association rules –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞"""
    
    # –§—ñ–ª—å—Ç—Ä—É–≤–∞—Ç–∏ customers
    segment_customers = rfm_segments[
        rfm_segments['Segment'] == segment_name
    ].index
    
    segment_trans = df[
        df['Customer_ID'].isin(segment_customers)
    ]
    
    # MBA
    transactions = segment_trans.groupby('Transaction_ID')['Product'].apply(list).values.tolist()
    
    te = TransactionEncoder()
    te_ary = te.fit(transactions).transform(transactions)
    df_encoded = pd.DataFrame(te_ary, columns=te.columns_)
    
    frequent = fpgrowth(df_encoded, min_support=0.05, use_colnames=True)
    rules = association_rules(frequent, metric="confidence", min_threshold=0.4)
    
    return rules

# –ü—Ä–∞–≤–∏–ª–∞ –¥–ª—è Champions
rules_champions = mba_by_segment(df_raw, 'Champions')
print(f"\nChampions: {len(rules_champions)} rules")

# –ü—Ä–∞–≤–∏–ª–∞ –¥–ª—è At Risk
rules_atrisk = mba_by_segment(df_raw, 'At Risk')
print(f"At Risk: {len(rules_atrisk)} rules")

# –ü–µ—Ä—Å–æ–Ω–∞–ª—ñ–∑–æ–≤–∞–Ω—ñ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó –∑–∞ —Å–µ–≥–º–µ–Ω—Ç–æ–º!
```

---

## A/B Testing

### –í–∞–ª—ñ–¥–∞—Ü—ñ—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ–π

```python
# –ï–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç: –ø–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ —á–∏ –ø—Ä–∞—Ü—é—î recommendation

# 1. –ë–∞–∑–æ–≤–∏–π –ø–µ—Ä—ñ–æ–¥ (–¥–æ –∑–º—ñ–Ω–∏)
baseline_data = df_raw[df_raw['Date'] < '2024-02-01']

# 2. –ó–∞—Å—Ç–æ—Å—É–≤–∞—Ç–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—é
# –ù–∞–ø—Ä–∏–∫–ª–∞–¥: —Ä–æ–∑–º—ñ—Å—Ç–∏—Ç–∏ Beer –ø–æ—Ä—è–¥ –∑ Diapers

# 3. –¢–µ—Å—Ç–æ–≤–∏–π –ø–µ—Ä—ñ–æ–¥ (–ø—ñ—Å–ª—è –∑–º—ñ–Ω–∏)
test_data = df_raw[df_raw['Date'] >= '2024-02-01']

# 4. –í–∏–º—ñ—Ä—è—Ç–∏ –µ—Ñ–µ–∫—Ç
def measure_impact(baseline, test, itemA, itemB):
    """–í–∏–º—ñ—Ä—è—Ç–∏ –∑–º—ñ–Ω—É co-purchase rate"""
    
    def copurchase_rate(data, itemA, itemB):
        """% —Ç—Ä–∞–Ω–∑–∞–∫—Ü—ñ–π –∑ –æ–±–æ–º–∞ items"""
        trans = data.groupby('Transaction_ID')['Product'].apply(set)
        
        both = sum(1 for t in trans if itemA in t and itemB in t)
        total = len(trans)
        
        return both / total
    
    baseline_rate = copurchase_rate(baseline, itemA, itemB)
    test_rate = copurchase_rate(test, itemA, itemB)
    
    lift = (test_rate - baseline_rate) / baseline_rate
    
    print(f"Co-purchase rate: {itemA} & {itemB}")
    print(f"  Baseline: {baseline_rate:.2%}")
    print(f"  Test: {test_rate:.2%}")
    print(f"  Lift: {lift:+.1%}")
    
    return lift

lift = measure_impact(baseline_data, test_data, 'Diapers', 'Beer')

# –°—Ç–∞—Ç–∏—Å—Ç–∏—á–Ω–∞ –∑–Ω–∞—á—É—â—ñ—Å—Ç—å
from scipy.stats import chi2_contingency

def test_significance(baseline, test, itemA, itemB):
    """Chi-square test"""
    
    def get_contingency_table(data, itemA, itemB):
        trans = data.groupby('Transaction_ID')['Product'].apply(set)
        
        both = sum(1 for t in trans if itemA in t and itemB in t)
        only_a = sum(1 for t in trans if itemA in t and itemB not in t)
        only_b = sum(1 for t in trans if itemB in t and itemA not in t)
        neither = len(trans) - both - only_a - only_b
        
        return [[both, only_a], [only_b, neither]]
    
    table_baseline = get_contingency_table(baseline, itemA, itemB)
    table_test = get_contingency_table(test, itemA, itemB)
    
    # Chi-square
    _, p_value, _, _ = chi2_contingency(table_baseline + table_test)
    
    print(f"\nSignificance test: p-value = {p_value:.4f}")
    
    if p_value < 0.05:
        print("‚úì Statistically significant!")
    else:
        print("‚úó Not significant")
    
    return p_value

test_significance(baseline_data, test_data, 'Diapers', 'Beer')
```

---

## ROI –∫–∞–ª—å–∫—É–ª—è—Ü—ñ—è

### –û—Ü—ñ–Ω–∫–∞ –±—ñ–∑–Ω–µ—Å-–≤–ø–ª–∏–≤—É

```python
def calculate_roi(rule, avg_transaction_value, implementation_cost):
    """ROI —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó"""
    
    # Estimate
    affected_transactions = rule['support'] * total_transactions
    conversion_rate = rule['confidence']
    
    # –ü–æ—Ç–µ–Ω—Ü—ñ–π–Ω–∞ –¥–æ–¥–∞—Ç–∫–æ–≤–∞ –≤–∏—Ä—É—á–∫–∞
    additional_sales = affected_transactions * conversion_rate * avg_transaction_value
    
    # ROI
    roi = (additional_sales - implementation_cost) / implementation_cost
    
    return {
        'affected_transactions': affected_transactions,
        'additional_sales': additional_sales,
        'implementation_cost': implementation_cost,
        'net_profit': additional_sales - implementation_cost,
        'roi': roi
    }

# –ü—Ä–∏–∫–ª–∞–¥
top_rule = strong_rules.iloc[0]

avg_transaction_value = 50  # $50
implementation_cost = 5000   # $5000 (store reorganization)
total_transactions = 100000  # –∑–∞ –º—ñ—Å—è—Ü—å

roi_calc = calculate_roi(top_rule, avg_transaction_value, implementation_cost)

print("\n=== ROI Calculation ===")
print(f"Rule: {list(top_rule['antecedents'])} ‚Üí {list(top_rule['consequents'])}")
print(f"Affected transactions: {roi_calc['affected_transactions']:.0f}")
print(f"Additional sales: ${roi_calc['additional_sales']:,.0f}")
print(f"Implementation cost: ${roi_calc['implementation_cost']:,.0f}")
print(f"Net profit: ${roi_calc['net_profit']:,.0f}")
print(f"ROI: {roi_calc['roi']*100:.1f}%")
```

---

## –ü—Ä–∞–∫—Ç–∏—á–Ω—ñ –ø–æ—Ä–∞–¥–∏ üí°

### 1. –ü–æ—á–Ω–∏ –∑ exploratory analysis

```python
# –ó—Ä–æ–∑—É–º—ñ–π –¥–∞–Ω—ñ —Å–ø–æ—á–∞—Ç–∫—É!
# - –Ø–∫—ñ —Ç–æ–ø —Ç–æ–≤–∞—Ä–∏?
# - –†–æ–∑–ø–æ–¥—ñ–ª —Ä–æ–∑–º—ñ—Ä—ñ–≤ —Ç—Ä–∞–Ω–∑–∞–∫—Ü—ñ–π?
# - Temporal patterns?
# - Customer segments?
```

### 2. –ù–∞–ª–∞—à—Ç—É–π –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –ø—ñ–¥ –±—ñ–∑–Ω–µ—Å

```python
# min_support: –∑–∞–ª–µ–∂–∏—Ç—å –≤—ñ–¥ –º–µ—Ç–∏
# - Cross-selling: 1-5% (rare but valuable)
# - Store layout: 10%+ (affect many customers)

# min_confidence: 
# - Aggressive campaigns: 50%+
# - Conservative: 70%+

# min_lift:
# - Must have: > 1.2
```

### 3. –§—ñ–ª—å—Ç—Ä—É–π —Ç—Ä–∏–≤—ñ–∞–ª—å–Ω—ñ –ø—Ä–∞–≤–∏–ª–∞

```python
# –í–∏–¥–∞–ª–∏—Ç–∏ –æ—á–µ–≤–∏–¥–Ω—ñ –∞—Å–æ—Ü—ñ–∞—Ü—ñ—ó
trivial_pairs = [
    ('Coffee', 'Sugar'),
    ('Bread', 'Butter'),
    ('Chips', 'Salsa'),
    # ... domain knowledge
]

def is_trivial(ant, cons):
    for a, c in trivial_pairs:
        if a in ant and c in cons:
            return True
    return False

rules_filtered = rules[
    ~rules.apply(lambda r: is_trivial(r['antecedents'], r['consequents']), axis=1)
]
```

### 4. Segment-specific analysis

```python
# –†—ñ–∑–Ω—ñ —Å–µ–≥–º–µ–Ω—Ç–∏ ‚Üí —Ä—ñ–∑–Ω—ñ –ø–∞—Ç—Ç–µ—Ä–Ω–∏
# Champions vs Budget Shoppers
# Weekday vs Weekend
# Morning vs Evening
```

### 5. Temporal analysis

```python
# –ü–∞—Ç—Ç–µ—Ä–Ω–∏ –∑–º—ñ–Ω—é—é—Ç—å—Å—è –∑ —á–∞—Å–æ–º!
# Seasonal products
# Trending items
# Holiday effects

# Rolling window analysis
for month in ['Jan', 'Feb', 'Mar']:
    monthly_data = filter_by_month(df, month)
    rules_monthly = run_mba(monthly_data)
    # Compare changes
```

### 6. Category-level analysis

```python
# –ù–µ —Ç—ñ–ª—å–∫–∏ products, –∞–ª–µ –π categories
# "Dairy ‚Üí Bakery"
# –î–æ–ø–æ–º–∞–≥–∞—î –≤ macro store layout
```

### 7. Validate –∑ A/B testing

```python
# –ù–µ –≤—Å—ñ –ø—Ä–∞–≤–∏–ª–∞ –ø—Ä–∞—Ü—é—é—Ç—å –Ω–∞ –ø—Ä–∞–∫—Ç–∏—Ü—ñ!
# Test –ø–µ—Ä–µ–¥ –ø–æ–≤–Ω–∏–º rollout
```

### 8. Monitor continuously

```python
# Dashboard –∑ key metrics
# - Top rules
# - Changes over time
# - ROI tracking
```

### 9. Combine –∑ —ñ–Ω—à–∏–º–∏ –¥–∞–Ω–∏–º–∏

```python
# MBA + Weather data
# MBA + Events (holidays, sports)
# MBA + Customer demographics
```

### 10. Communicate results

```python
# Business stakeholders –Ω–µ –∑–Ω–∞—é—Ç—å lift/support
# Translate –≤ –±—ñ–∑–Ω–µ—Å-–º–æ–≤—É:
# "75% –ª—é–¥–µ–π —â–æ –∫—É–ø—É—é—Ç—å X —Ç–∞–∫–æ–∂ –∫—É–ø—É—é—Ç—å Y"
# "–ü–æ—Ç–µ–Ω—Ü—ñ–π–Ω–∞ –¥–æ–¥–∞—Ç–∫–æ–≤–∞ –≤–∏—Ä—É—á–∫–∞ $50K/–º—ñ—Å—è—Ü—å"
```

---

## –ö–µ–π—Å–∏ –∑ –ø—Ä–∞–∫—Ç–∏–∫–∏

### Case 1: Amazon "Frequently Bought Together"

**–ü—Ä–æ–±–ª–µ–º–∞:** –ó–±—ñ–ª—å—à–∏—Ç–∏ average order value.

**–†—ñ—à–µ–Ω–Ω—è:**
- MBA –Ω–∞ purchase history
- Real-time recommendations
- "Add both to cart" button

**–†–µ–∑—É–ª—å—Ç–∞—Ç:**
- +35% –¥–æ–¥–∞—Ç–∫–æ–≤–∏—Ö –ø—Ä–æ–¥–∞–∂—ñ–≤
- +20% average basket size

### Case 2: Walmart Store Layout

**–ü—Ä–æ–±–ª–µ–º–∞:** –û–ø—Ç–∏–º—ñ–∑—É–≤–∞—Ç–∏ —Ä–æ–∑—Ç–∞—à—É–≤–∞–Ω–Ω—è —Ç–æ–≤–∞—Ä—ñ–≤.

**–†—ñ—à–µ–Ω–Ω—è:**
- MBA –Ω–∞ POS data
- –†–æ–∑–º—ñ—Å—Ç–∏—Ç–∏ –∞—Å–æ—Ü—ñ–π–æ–≤–∞–Ω—ñ —Ç–æ–≤–∞—Ä–∏ —Ä–∞–∑–æ–º
- "End cap" displays –¥–ª—è bundles

**–†–µ–∑—É–ª—å—Ç–∞—Ç:**
- +15% sales –¥–ª—è paired products
- –ü–æ–∫—Ä–∞—â–µ–Ω–∏–π customer flow

### Case 3: Starbucks Food Pairing

**–ü—Ä–æ–±–ª–µ–º–∞:** –ó–±—ñ–ª—å—à–∏—Ç–∏ food sales.

**–†—ñ—à–µ–Ω–Ω—è:**
- MBA: Coffee types ‚Üí Food items
- "Pairs well with" recommendations
- Training baristas –Ω–∞ upsell

**–†–µ–∑—É–ª—å—Ç–∞—Ç:**
- +25% food attachment rate
- Higher customer satisfaction

---

## –ü–æ—à–∏—Ä–µ–Ω—ñ –ø–æ–º–∏–ª–∫–∏ ‚ùå

### 1. –Ü–≥–Ω–æ—Ä—É–≤–∞—Ç–∏ domain knowledge

```python
# ‚ùå –ü—Ä–∏–π–º–∞—Ç–∏ –≤—Å—ñ –ø—Ä–∞–≤–∏–ª–∞ literally
# –î–µ—è–∫—ñ –æ—á–µ–≤–∏–¥–Ω—ñ, –¥–µ—è–∫—ñ nonsensical

# ‚úÖ Filter —á–µ—Ä–µ–∑ business logic
```

### 2. –ù–µ —Ç–µ—Å—Ç—É–≤–∞—Ç–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó

```python
# ‚ùå Implement –±–µ–∑ validation
# –ú–æ–∂–µ –Ω–µ –ø—Ä–∞—Ü—é–≤–∞—Ç–∏ –≤ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç—ñ

# ‚úÖ A/B test —Å–ø–æ—á–∞—Ç–∫—É
```

### 3. –ó–∞–Ω–∞–¥—Ç–æ –Ω–∏–∑—å–∫–∏–π min_support

```python
# ‚ùå min_support=0.001
# –ú—ñ–ª—å–π–æ–Ω–∏ —Å–ª–∞–±–∫–∏—Ö –ø—Ä–∞–≤–∏–ª

# ‚úÖ –†–æ–∑—É–º–Ω–∏–π –ø–æ—Ä—ñ–≥ –Ω–∞ –æ—Å–Ω–æ–≤—ñ –±—ñ–∑–Ω–µ—Å—É
```

### 4. –ó–∞–±—É—Ç–∏ –ø—Ä–æ causality

```python
# ‚ùå "A causes B"
# Correlation ‚â† Causation!

# ‚úÖ "A and B —á–∞—Å—Ç–æ —Ä–∞–∑–æ–º" (–º–æ–∂–µ –±—É—Ç–∏ 3rd factor)
```

### 5. Static analysis

```python
# ‚ùå –û–¥–Ω–æ—Ä–∞–∑–æ–≤–∏–π –∞–Ω–∞–ª—ñ–∑
# –ü–∞—Ç—Ç–µ—Ä–Ω–∏ –∑–º—ñ–Ω—é—é—Ç—å—Å—è!

# ‚úÖ Regular updates (monthly/quarterly)
```

---

## –Ü–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∏ —Ç–∞ —Ç–µ—Ö–Ω–æ–ª–æ–≥—ñ—ó

### Python Libraries
- **mlxtend** ‚Äî Apriori, FP-Growth
- **pandas** ‚Äî Data manipulation
- **networkx** ‚Äî Graph visualization
- **matplotlib/seaborn** ‚Äî Plotting

### Business Intelligence
- **Tableau** ‚Äî Dashboards
- **Power BI** ‚Äî Reporting
- **Looker** ‚Äî Analytics

### Big Data
- **Spark MLlib** ‚Äî Distributed MBA
- **Apache Mahout** ‚Äî Scalable algorithms

---

## –ü–æ–≤'—è–∑–∞–Ω—ñ —Ç–µ–º–∏

- [[01_Apriori]] ‚Äî –æ—Å–Ω–æ–≤–Ω–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º
- [[02_FP-Growth]] ‚Äî —à–≤–∏–¥—à–∞ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞
- [[Recommendation_Systems]] ‚Äî –ø–µ—Ä—Å–æ–Ω–∞–ª—ñ–∑–æ–≤–∞–Ω—ñ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó
- [[Customer_Segmentation]] ‚Äî RFM analysis
- [[A_B_Testing]] ‚Äî –≤–∞–ª—ñ–¥–∞—Ü—ñ—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ–π

## –†–µ—Å—É—Ä—Å–∏

- [mlxtend Market Basket Analysis](http://rasbt.github.io/mlxtend/user_guide/frequent_patterns/association_rules/)
- [Market Basket Analysis Tutorial](https://www.kaggle.com/code/datatheque/market-basket-analysis-tutorial)
- [Practical Guide to Market Basket Analysis](https://towardsdatascience.com/market-basket-analysis-978ac064d8c6)

---

## –ö–ª—é—á–æ–≤—ñ –≤–∏—Å–Ω–æ–≤–∫–∏

> Market Basket Analysis ‚Äî –ø—Ä–∞–∫—Ç–∏—á–Ω–µ –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è association rules mining –¥–ª—è –≤–∏—è–≤–ª–µ–Ω–Ω—è –ø–∞—Ç—Ç–µ—Ä–Ω—ñ–≤ –ø–æ–∫—É–ø–æ–∫ —Ç–∞ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó retail/e-commerce —Å—Ç—Ä–∞—Ç–µ–≥—ñ–π —á–µ—Ä–µ–∑ product placement, cross-selling, bundling —Ç–∞ personalized marketing.

**–ë—ñ–∑–Ω–µ—Å-—Ü—ñ–Ω–Ω—ñ—Å—Ç—å:**
- üõí –û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è store layout
- üí∞ –ó–±—ñ–ª—å—à–µ–Ω–Ω—è sales —á–µ—Ä–µ–∑ cross-selling
- üéØ Targeted marketing campaigns
- üì¶ Smart bundling strategies
- üìä Inventory optimization

**–ü—Ä–æ—Ü–µ—Å:**
1. **–ó–±—ñ—Ä –¥–∞–Ω–∏—Ö** ‚Äî POS, e-commerce logs
2. **Preprocessing** ‚Äî –æ—á–∏—Å—Ç–∏—Ç–∏, transform
3. **EDA** ‚Äî –∑—Ä–æ–∑—É–º—ñ—Ç–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–∏
4. **Mining** ‚Äî Apriori/FP-Growth
5. **Filtering** ‚Äî strong + non-trivial rules
6. **Interpretation** ‚Äî –±—ñ–∑–Ω–µ—Å insights
7. **Recommendations** ‚Äî actionable advice
8. **Validation** ‚Äî A/B testing
9. **Implementation** ‚Äî rollout
10. **Monitor** ‚Äî track impact

**–ö–ª—é—á–æ–≤—ñ –º–µ—Ç—Ä–∏–∫–∏:**
- **Support:** —è–∫ —á–∞—Å—Ç–æ —Ä–∞–∑–æ–º
- **Confidence:** –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å –∫—É–ø—ñ–≤–ª—ñ
- **Lift:** —Å–∏–ª–∞ –∑–≤'—è–∑–∫—É (>1 = –ø–æ–∑–∏—Ç–∏–≤–Ω–∞ –∫–æ—Ä–µ–ª—è—Ü—ñ—è)

**–ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è:**
- Product placement –≤ –º–∞–≥–∞–∑–∏–Ω—ñ
- "Frequently bought together" recommendations
- Bundle –ø—Ä–æ–ø–æ–∑–∏—Ü—ñ—ó
- Email campaigns (–ø–µ—Ä—Å–æ–Ω–∞–ª—ñ–∑–æ–≤–∞–Ω—ñ)
- Inventory management

**Best Practices:**
- Segment-specific analysis (RFM)
- Temporal patterns (seasons, trends)
- Domain knowledge filtering
- A/B testing validation
- ROI calculation
- Continuous monitoring

**–ù–∞–π–≤–∞–∂–ª–∏–≤—ñ—à–µ:**
- Translate technical metrics ‚Üí business value
- Test –ø–µ—Ä–µ–¥ implementation
- Combine –∑ —ñ–Ω—à–∏–º–∏ –¥–∞–Ω–∏–º–∏
- Monitor continuously
- Update regularly

---

#ml #unsupervised-learning #market-basket-analysis #association-rules #retail #e-commerce #cross-selling #recommendations #business-analytics
