# Apriori Algorithm

## –©–æ —Ü–µ?

**Apriori** ‚Äî —Ü–µ –∫–ª–∞—Å–∏—á–Ω–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º –¥–ª—è –ø–æ—à—É–∫—É **association rules** (–∞—Å–æ—Ü—ñ–∞—Ç–∏–≤–Ω–∏—Ö –ø—Ä–∞–≤–∏–ª) —É —Ç—Ä–∞–Ω–∑–∞–∫—Ü—ñ–π–Ω–∏—Ö –¥–∞–Ω–∏—Ö. –í—ñ–Ω –∑–Ω–∞—Ö–æ–¥–∏—Ç—å –ø–∞—Ç—Ç–µ—Ä–Ω–∏ —Ç–∏–ø—É "—è–∫—â–æ –∫—É–ø—É—é—Ç—å A, —Ç–æ —á–∞—Å—Ç–æ –∫—É–ø—É—é—Ç—å —ñ B".

**–ì–æ–ª–æ–≤–Ω–∞ —ñ–¥–µ—è:** —è–∫—â–æ itemset —á–∞—Å—Ç–∏–π, —Ç–æ –≤—Å—ñ –π–æ–≥–æ –ø—ñ–¥–º–Ω–æ–∂–∏–Ω–∏ —Ç–∞–∫–æ–∂ —á–∞—Å—Ç—ñ (Apriori principle). –¶–µ –¥–æ–∑–≤–æ–ª—è—î –µ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –ø—Ä–∏—Å–∫–æ—Ä–∏—Ç–∏ –ø–æ—à—É–∫, –≤—ñ–¥—Å—ñ—é—é—á–∏ –Ω–µ—á–∞—Å—Ç—ñ –∫–æ–º–±—ñ–Ω–∞—Ü—ñ—ó.

## –ù–∞–≤—ñ—â–æ –ø–æ—Ç—Ä—ñ–±–µ–Ω?

- üõí **Market Basket Analysis** ‚Äî —â–æ –∫—É–ø—É—é—Ç—å —Ä–∞–∑–æ–º
- üí° **Recommendation Systems** ‚Äî —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó —Ç–æ–≤–∞—Ä—ñ–≤
- üîç **Pattern Discovery** ‚Äî –≤–∏—è–≤–ª–µ–Ω–Ω—è –ø—Ä–∏—Ö–æ–≤–∞–Ω–∏—Ö –∑–≤'—è–∑–∫—ñ–≤
- üìä **Cross-selling** ‚Äî —Å—Ç—Ä–∞—Ç–µ–≥—ñ—ó –ø—Ä–æ–¥–∞–∂—É
- üè• **Medical Diagnosis** ‚Äî symptom co-occurrence
- üìö **Web Usage Mining** ‚Äî —è–∫—ñ —Å—Ç–æ—Ä—ñ–Ω–∫–∏ –≤—ñ–¥–≤—ñ–¥—É—é—Ç—å —Ä–∞–∑–æ–º

## –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏?

**–ü–æ—Ç—Ä—ñ–±–Ω–æ:**
- **–¢—Ä–∞–Ω–∑–∞–∫—Ü—ñ–π–Ω—ñ –¥–∞–Ω—ñ** ‚Äî —Å–ø–∏—Å–∫–∏ –ø–æ–∫—É–ø–æ–∫, –∫–ª—ñ–∫–∏, —Ç–æ—â–æ
- **Categorical items** ‚Äî –¥–∏—Å–∫—Ä–µ—Ç–Ω—ñ —Ç–æ–≤–∞—Ä–∏/–ø–æ–¥—ñ—ó
- **Pattern mining** ‚Äî –∑–Ω–∞–π—Ç–∏ —â–æ –π–¥–µ —Ä–∞–∑–æ–º
- **Interpretable rules** ‚Äî –∑—Ä–æ–∑—É–º—ñ–ª—ñ –ø—Ä–∞–≤–∏–ª–∞ –¥–ª—è –±—ñ–∑–Ω–µ—Å—É
- **–°–µ—Ä–µ–¥–Ω—ñ –¥–∞–Ω—ñ** (1000-100,000 —Ç—Ä–∞–Ω–∑–∞–∫—Ü—ñ–π)

**–ù–µ –ø–æ—Ç—Ä—ñ–±–Ω–æ:**
- **–î—É–∂–µ –≤–µ–ª–∏–∫—ñ –¥–∞–Ω—ñ** (>1M —Ç—Ä–∞–Ω–∑–∞–∫—Ü—ñ–π) ‚Üí FP-Growth —à–≤–∏–¥—à–µ
- **Numerical data** ‚Üí Clustering, Regression
- **Sequence –≤–∞–∂–ª–∏–≤–∞** ‚Üí Sequential Pattern Mining
- **Real-time** ‚Üí Streaming algorithms

---

## –û—Å–Ω–æ–≤–Ω—ñ –∫–æ–Ω—Ü–µ–ø—Ü—ñ—ó

### 1. Itemset

**Itemset** ‚Äî –Ω–∞–±—ñ—Ä —Ç–æ–≤–∞—Ä—ñ–≤.

```
1-itemset: {Milk}
2-itemset: {Milk, Bread}
3-itemset: {Milk, Bread, Butter}
```

### 2. Support (–ü—ñ–¥—Ç—Ä–∏–º–∫–∞)

**Support** ‚Äî —è–∫ —á–∞—Å—Ç–æ itemset –∑—É—Å—Ç—Ä—ñ—á–∞—î—Ç—å—Å—è –≤ –¥–∞–Ω–∏—Ö.

$$\text{Support}(A) = \frac{\text{–ö—ñ–ª—å–∫—ñ—Å—Ç—å —Ç—Ä–∞–Ω–∑–∞–∫—Ü—ñ–π –∑ } A}{\text{–í—Å—å–æ–≥–æ —Ç—Ä–∞–Ω–∑–∞–∫—Ü—ñ–π}}$$

**–ü—Ä–∏–∫–ª–∞–¥:**
```
–¢—Ä–∞–Ω–∑–∞–∫—Ü—ñ—ó:
1: {Milk, Bread, Butter}
2: {Milk, Bread}
3: {Milk, Eggs}
4: {Bread, Butter}
5: {Bread, Eggs}

Support({Milk}) = 3/5 = 0.6 (60%)
Support({Bread}) = 4/5 = 0.8 (80%)
Support({Milk, Bread}) = 2/5 = 0.4 (40%)
```

### 3. Confidence (–í–ø–µ–≤–Ω–µ–Ω—ñ—Å—Ç—å)

**Confidence** ‚Äî —è–∫—â–æ –∫—É–ø—É—é—Ç—å A, —è–∫–∞ –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å —â–æ –∫—É–ø—É—é—Ç—å B?

$$\text{Confidence}(A \Rightarrow B) = \frac{\text{Support}(A \cup B)}{\text{Support}(A)}$$

**–ü—Ä–∏–∫–ª–∞–¥:**
```
–ü—Ä–∞–≤–∏–ª–æ: {Milk} ‚Üí {Bread}

Support({Milk, Bread}) = 0.4
Support({Milk}) = 0.6

Confidence({Milk} ‚Üí {Bread}) = 0.4 / 0.6 = 0.67 (67%)

–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü—ñ—è: 67% –ª—é–¥–µ–π, —â–æ –∫—É–ø—É—é—Ç—å –º–æ–ª–æ–∫–æ, —Ç–∞–∫–æ–∂ –∫—É–ø—É—é—Ç—å —Ö–ª—ñ–±.
```

### 4. Lift (–ü—ñ–¥–π–æ–º)

**Lift** ‚Äî —á–∏ A —Ç–∞ B –Ω–µ–∑–∞–ª–µ–∂–Ω—ñ, —á–∏ —î –∑–≤'—è–∑–æ–∫?

$$\text{Lift}(A \Rightarrow B) = \frac{\text{Support}(A \cup B)}{\text{Support}(A) \times \text{Support}(B)}$$

**–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü—ñ—è:**
```
Lift = 1  ‚Üí A —Ç–∞ B –Ω–µ–∑–∞–ª–µ–∂–Ω—ñ (–≤–∏–ø–∞–¥–∫–æ–≤–∏–π –∑–≤'—è–∑–æ–∫)
Lift > 1  ‚Üí –ü–æ–∑–∏—Ç–∏–≤–Ω–∞ –∫–æ—Ä–µ–ª—è—Ü—ñ—è (–∫—É–ø—É—é—Ç—å —Ä–∞–∑–æ–º)
Lift < 1  ‚Üí –ù–µ–≥–∞—Ç–∏–≤–Ω–∞ –∫–æ—Ä–µ–ª—è—Ü—ñ—è (–Ω–µ –∫—É–ø—É—é—Ç—å —Ä–∞–∑–æ–º)
```

**–ü—Ä–∏–∫–ª–∞–¥:**
```
Support({Milk, Bread}) = 0.4
Support({Milk}) = 0.6
Support({Bread}) = 0.8

Lift = 0.4 / (0.6 √ó 0.8) = 0.4 / 0.48 = 0.83

Lift < 1 ‚Üí Weak negative correlation
```

---

## Apriori Principle

**–ö–ª—é—á–æ–≤–∞ —ñ–¥–µ—è:** –Ø–∫—â–æ itemset –Ω–µ—á–∞—Å—Ç–∏–π, —Ç–æ –≤—Å—ñ –π–æ–≥–æ —Å—É–ø–µ—Ä–º–Ω–æ–∂–∏–Ω–∏ —Ç–∞–∫–æ–∂ –Ω–µ—á–∞—Å—Ç—ñ.

```
{Milk, Bread, Butter} ‚Äî –Ω–µ—á–∞—Å—Ç–∏–π
         ‚Üì
{Milk, Bread, Butter, Eggs} ‚Äî –¢–û–ß–ù–û –Ω–µ—á–∞—Å—Ç–∏–π!

–ú–æ–∂–Ω–∞ –Ω–µ –ø–µ—Ä–µ–≤—ñ—Ä—è—Ç–∏!
```

**–ü—Ä–æ—Ç–∏–ª–µ–∂–Ω–æ:**

```
{Milk, Bread} ‚Äî —á–∞—Å—Ç–∏–π
         ‚Üì
{Milk} —Ç–∞ {Bread} ‚Äî –û–ë–ê —á–∞—Å—Ç—ñ

–í—Å—ñ –ø—ñ–¥–º–Ω–æ–∂–∏–Ω–∏ —á–∞—Å—Ç—ñ!
```

### –Ü–ª—é—Å—Ç—Ä–∞—Ü—ñ—è

```
Level 1:  {A}  {B}  {C}  {D}
           ‚úì    ‚úì    ‚úó    ‚úì
           
Level 2:  {A,B} {A,C} {A,D} {B,C} {B,D} {C,D}
           ‚úì     ‚úó     ‚úì     ‚úó     ‚úì     ‚úó
                 ‚Üë                 ‚Üë     ‚Üë
         –ù–µ –ø–µ—Ä–µ–≤—ñ—Ä—è—Ç–∏ (C –Ω–µ—á–∞—Å—Ç–∏–π)

Level 3:  {A,B,D}  {A,C,D}  {B,C,D}  {A,B,C}
           ‚úì        ‚úó        ‚úó        ‚úó
                    ‚Üë        ‚Üë        ‚Üë
            –ù–µ –ø–µ—Ä–µ–≤—ñ—Ä—è—Ç–∏ ({A,C}, {B,C}, {C,D} –Ω–µ—á–∞—Å—Ç—ñ)
```

---

## –ê–ª–≥–æ—Ä–∏—Ç–º Apriori

### –ü—Å–µ–≤–¥–æ–∫–æ–¥

```
1. –ó–Ω–∞–π—Ç–∏ –≤—Å—ñ —á–∞—Å—Ç—ñ 1-itemsets (L‚ÇÅ)
   - –ü—ñ–¥—Ä–∞—Ö—É–≤–∞—Ç–∏ support –∫–æ–∂–Ω–æ–≥–æ item
   - –ó–∞–ª–∏—à–∏—Ç–∏ —Ç—ñ–ª—å–∫–∏ –∑ support ‚â• min_support

2. FOR k = 2, 3, 4, ... WHILE L_{k-1} –Ω–µ –ø—É—Å—Ç–µ:
   
   a) –ì–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ –∫–∞–Ω–¥–∏–¥–∞—Ç—ñ–≤ C_k
      - –ö–æ–º–±—ñ–Ω—É–≤–∞—Ç–∏ —á–∞—Å—Ç—ñ (k-1)-itemsets
      - –ó–∞—Å—Ç–æ—Å—É–≤–∞—Ç–∏ Apriori principle (pruning)
   
   b) –ü—ñ–¥—Ä–∞—Ö—É–≤–∞—Ç–∏ support –¥–ª—è –∫–∞–Ω–¥–∏–¥–∞—Ç—ñ–≤
      - –°–∫–∞–Ω—É–≤–∞—Ç–∏ —Ç—Ä–∞–Ω–∑–∞–∫—Ü—ñ—ó
      - –ü—ñ–¥—Ä–∞—Ö—É–≤–∞—Ç–∏ —Å–∫—ñ–ª—å–∫–∏ —Ä–∞–∑—ñ–≤ –∑—É—Å—Ç—Ä—ñ—á–∞—î—Ç—å—Å—è
   
   c) –í—ñ–¥—Ñ—ñ–ª—å—Ç—Ä—É–≤–∞—Ç–∏ –Ω–µ—á–∞—Å—Ç—ñ
      - –ó–∞–ª–∏—à–∏—Ç–∏ —Ç—ñ–ª—å–∫–∏ –∑ support ‚â• min_support
      - L_k = —á–∞—Å—Ç—ñ k-itemsets

3. –ü–æ–≤–µ—Ä–Ω—É—Ç–∏ –≤—Å—ñ —á–∞—Å—Ç—ñ itemsets (L‚ÇÅ ‚à™ L‚ÇÇ ‚à™ ...)

4. –ó–≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ association rules
   - –î–ª—è –∫–æ–∂–Ω–æ–≥–æ —á–∞—Å—Ç–æ–≥–æ itemset
   - –†–æ–∑–¥—ñ–ª–∏—Ç–∏ –Ω–∞ antecedent ‚Üí consequent
   - –û–±—á–∏—Å–ª–∏—Ç–∏ confidence
   - –ó–∞–ª–∏—à–∏—Ç–∏ rules –∑ confidence ‚â• min_confidence
```

### –ü—Ä–∏–∫–ª–∞–¥ –≤–∏–∫–æ–Ω–∞–Ω–Ω—è

**–î–∞–Ω—ñ:**
```
Transactions:
T1: {Milk, Bread, Butter}
T2: {Milk, Bread}
T3: {Milk, Eggs}
T4: {Bread, Butter}
T5: {Bread, Eggs}

min_support = 0.4 (40%)
min_confidence = 0.6 (60%)
```

**–ö—Ä–æ–∫ 1: –ó–Ω–∞–π—Ç–∏ L‚ÇÅ (—á–∞—Å—Ç—ñ 1-itemsets)**

```
Item      Count   Support   Frequent?
Milk        3       0.6        ‚úì
Bread       4       0.8        ‚úì
Butter      2       0.4        ‚úì
Eggs        2       0.4        ‚úì

L‚ÇÅ = {{Milk}, {Bread}, {Butter}, {Eggs}}
```

**–ö—Ä–æ–∫ 2: –ó–Ω–∞–π—Ç–∏ L‚ÇÇ (—á–∞—Å—Ç—ñ 2-itemsets)**

```
–ö–∞–Ω–¥–∏–¥–∞—Ç–∏ C‚ÇÇ (–≤—Å—ñ –∫–æ–º–±—ñ–Ω–∞—Ü—ñ—ó L‚ÇÅ):
{Milk, Bread}
{Milk, Butter}
{Milk, Eggs}
{Bread, Butter}
{Bread, Eggs}
{Butter, Eggs}

–ü—ñ–¥—Ä–∞—Ö—É–≤–∞—Ç–∏ support:
{Milk, Bread}    Count=2  Support=0.4  ‚úì
{Milk, Butter}   Count=1  Support=0.2  ‚úó
{Milk, Eggs}     Count=1  Support=0.2  ‚úó
{Bread, Butter}  Count=2  Support=0.4  ‚úì
{Bread, Eggs}    Count=1  Support=0.2  ‚úó
{Butter, Eggs}   Count=0  Support=0.0  ‚úó

L‚ÇÇ = {{Milk, Bread}, {Bread, Butter}}
```

**–ö—Ä–æ–∫ 3: –ó–Ω–∞–π—Ç–∏ L‚ÇÉ (—á–∞—Å—Ç—ñ 3-itemsets)**

```
–ö–∞–Ω–¥–∏–¥–∞—Ç–∏ C‚ÇÉ:
{Milk, Bread, Butter} ‚Äî –∑ L‚ÇÇ

–ü—ñ–¥—Ä–∞—Ö—É–≤–∞—Ç–∏ support:
{Milk, Bread, Butter}  Count=1  Support=0.2  ‚úó

L‚ÇÉ = {} (–ø—É—Å—Ç–æ)

STOP ‚Äî –Ω–µ–º–∞—î –±—ñ–ª—å—à–µ —á–∞—Å—Ç–∏—Ö itemsets
```

**–ö—Ä–æ–∫ 4: –ì–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ –ø—Ä–∞–≤–∏–ª–∞**

```
–ß–∞—Å—Ç—ñ itemsets: {{Milk}, {Bread}, {Butter}, {Eggs},
                 {Milk, Bread}, {Bread, Butter}}

–ü—Ä–∞–≤–∏–ª–∞ –∑ {Milk, Bread}:
1. {Milk} ‚Üí {Bread}
   Confidence = Support({Milk,Bread}) / Support({Milk})
              = 0.4 / 0.6 = 0.67 ‚úì (‚â• 0.6)
   Lift = 0.4 / (0.6 √ó 0.8) = 0.83

2. {Bread} ‚Üí {Milk}
   Confidence = 0.4 / 0.8 = 0.5 ‚úó (< 0.6)

–ü—Ä–∞–≤–∏–ª–∞ –∑ {Bread, Butter}:
3. {Bread} ‚Üí {Butter}
   Confidence = 0.4 / 0.8 = 0.5 ‚úó

4. {Butter} ‚Üí {Bread}
   Confidence = 0.4 / 0.4 = 1.0 ‚úì
   Lift = 0.4 / (0.4 √ó 0.8) = 1.25

–§—ñ–Ω–∞–ª—å–Ω—ñ –ø—Ä–∞–≤–∏–ª–∞:
‚úì {Milk} ‚Üí {Bread}      (conf=0.67, lift=0.83)
‚úì {Butter} ‚Üí {Bread}    (conf=1.0, lift=1.25)
```

---

## –ö–æ–¥ (Python)

### –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è mlxtend

```python
import pandas as pd
import numpy as np
from mlxtend.frequent_patterns import apriori, association_rules
from mlxtend.preprocessing import TransactionEncoder

# 1. –î–∞–Ω—ñ (—Å–ø–∏—Å–æ–∫ —Ç—Ä–∞–Ω–∑–∞–∫—Ü—ñ–π)
transactions = [
    ['Milk', 'Bread', 'Butter'],
    ['Milk', 'Bread'],
    ['Milk', 'Eggs'],
    ['Bread', 'Butter'],
    ['Bread', 'Eggs'],
    ['Milk', 'Bread', 'Butter', 'Eggs'],
    ['Bread', 'Butter', 'Eggs'],
    ['Milk', 'Bread', 'Cheese'],
]

print(f"Total transactions: {len(transactions)}\n")

# 2. –ü–µ—Ä–µ—Ç–≤–æ—Ä–∏—Ç–∏ –≤ binary matrix
te = TransactionEncoder()
te_ary = te.fit(transactions).transform(transactions)
df = pd.DataFrame(te_ary, columns=te.columns_)

print("Binary transaction matrix:")
print(df)
print()

# 3. –ó–Ω–∞–π—Ç–∏ —á–∞—Å—Ç—ñ itemsets (Apriori)
frequent_itemsets = apriori(
    df, 
    min_support=0.3,    # 30%
    use_colnames=True,
    verbose=1
)

print("\n=== Frequent Itemsets ===")
print(frequent_itemsets.sort_values('support', ascending=False))
print()

# 4. –ì–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ association rules
rules = association_rules(
    frequent_itemsets,
    metric="confidence",
    min_threshold=0.6,   # 60%
    num_itemsets=len(frequent_itemsets)
)

print("\n=== Association Rules ===")
print(rules[['antecedents', 'consequents', 'support', 
             'confidence', 'lift']].sort_values('lift', ascending=False))
```

**–í–∏–≤—ñ–¥:**
```
Binary transaction matrix:
   Bread  Butter  Cheese   Eggs   Milk
0   True    True   False  False   True
1   True   False   False  False   True
2  False   False   False   True   True
3   True    True   False  False  False
4   True   False   False   True  False
5   True    True   False   True   True
6   True    True   False   True  False
7   True   False    True  False   True

=== Frequent Itemsets ===
    support              itemsets
7  0.875                {Bread}
2  0.500                 {Milk}
1  0.500               {Butter}
0  0.500                 {Eggs}
4  0.500         {Bread, Milk}
5  0.500       {Bread, Butter}
3  0.375         {Bread, Eggs}
6  0.375   {Bread, Butter, Eggs}

=== Association Rules ===
  antecedents consequents  support  confidence   lift
1   {Butter}     {Bread}    0.500        1.00   1.14
0      {Milk}     {Bread}    0.500        1.00   1.14
2      {Eggs}     {Bread}    0.375        0.75   0.86
```

### –í–ª–∞—Å–Ω–∞ —Ä–µ–∞–ª—ñ–∑–∞—Ü—ñ—è

```python
from itertools import combinations
from collections import defaultdict

class AprioriAlgorithm:
    def __init__(self, min_support=0.3, min_confidence=0.6):
        self.min_support = min_support
        self.min_confidence = min_confidence
        self.frequent_itemsets = []
        self.rules = []
    
    def fit(self, transactions):
        """–ó–Ω–∞–π—Ç–∏ —á–∞—Å—Ç—ñ itemsets —Ç–∞ –ø—Ä–∞–≤–∏–ª–∞"""
        self.transactions = transactions
        n_transactions = len(transactions)
        
        # 1. –ó–Ω–∞–π—Ç–∏ –≤—Å—ñ —É–Ω—ñ–∫–∞–ª—å–Ω—ñ items
        all_items = set()
        for transaction in transactions:
            all_items.update(transaction)
        
        # 2. Level 1: —á–∞—Å—Ç—ñ 1-itemsets
        itemsets = [frozenset([item]) for item in all_items]
        frequent_itemsets = self._get_frequent_itemsets(
            itemsets, transactions, n_transactions
        )
        
        all_frequent = frequent_itemsets.copy()
        k = 2
        
        # 3. Level k: –≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ —Ç–∞ —Ñ—ñ–ª—å—Ç—Ä—É–≤–∞—Ç–∏
        while frequent_itemsets:
            print(f"Level {k}: {len(frequent_itemsets)} frequent itemsets")
            
            # –ì–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ –∫–∞–Ω–¥–∏–¥–∞—Ç—ñ–≤
            candidates = self._generate_candidates(frequent_itemsets, k)
            
            # –§—ñ–ª—å—Ç—Ä—É–≤–∞—Ç–∏ —á–∞—Å—Ç—ñ
            frequent_itemsets = self._get_frequent_itemsets(
                candidates, transactions, n_transactions
            )
            
            all_frequent.extend(frequent_itemsets)
            k += 1
        
        self.frequent_itemsets = all_frequent
        
        # 4. –ì–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ –ø—Ä–∞–≤–∏–ª–∞
        self._generate_rules()
        
        return self
    
    def _get_frequent_itemsets(self, itemsets, transactions, n_transactions):
        """–ü—ñ–¥—Ä–∞—Ö—É–≤–∞—Ç–∏ support —Ç–∞ –≤—ñ–¥—Ñ—ñ–ª—å—Ç—Ä—É–≤–∞—Ç–∏"""
        itemset_counts = defaultdict(int)
        
        # –ü—ñ–¥—Ä–∞—Ö—É–≤–∞—Ç–∏
        for itemset in itemsets:
            for transaction in transactions:
                if itemset.issubset(transaction):
                    itemset_counts[itemset] += 1
        
        # –§—ñ–ª—å—Ç—Ä—É–≤–∞—Ç–∏
        frequent = []
        for itemset, count in itemset_counts.items():
            support = count / n_transactions
            if support >= self.min_support:
                frequent.append((itemset, support))
        
        return frequent
    
    def _generate_candidates(self, frequent_itemsets, k):
        """–ì–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ k-itemsets –∑ (k-1)-itemsets"""
        candidates = []
        n = len(frequent_itemsets)
        
        for i in range(n):
            for j in range(i + 1, n):
                itemset1 = frequent_itemsets[i][0]
                itemset2 = frequent_itemsets[j][0]
                
                # Join —è–∫—â–æ –ø–µ—Ä—à—ñ k-2 –µ–ª–µ–º–µ–Ω—Ç–∏ –æ–¥–Ω–∞–∫–æ–≤—ñ
                union = itemset1 | itemset2
                if len(union) == k:
                    # –ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ Apriori principle
                    if self._has_frequent_subsets(union, frequent_itemsets):
                        candidates.append(union)
        
        return candidates
    
    def _has_frequent_subsets(self, itemset, frequent_itemsets):
        """–ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ —á–∏ –≤—Å—ñ –ø—ñ–¥–º–Ω–æ–∂–∏–Ω–∏ —á–∞—Å—Ç—ñ"""
        frequent_sets = {fs[0] for fs in frequent_itemsets}
        
        # –í—Å—ñ (k-1)-–ø—ñ–¥–º–Ω–æ–∂–∏–Ω–∏
        for item in itemset:
            subset = itemset - frozenset([item])
            if subset not in frequent_sets:
                return False
        
        return True
    
    def _generate_rules(self):
        """–ì–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ association rules"""
        rules = []
        
        for itemset, support in self.frequent_itemsets:
            if len(itemset) < 2:
                continue
            
            # –í—Å—ñ –º–æ–∂–ª–∏–≤—ñ —Ä–æ–∑–±–∏—Ç—Ç—è –Ω–∞ antecedent ‚Üí consequent
            for i in range(1, len(itemset)):
                for antecedent in combinations(itemset, i):
                    antecedent = frozenset(antecedent)
                    consequent = itemset - antecedent
                    
                    # –û–±—á–∏—Å–ª–∏—Ç–∏ confidence
                    antecedent_support = self._get_support(antecedent)
                    if antecedent_support > 0:
                        confidence = support / antecedent_support
                        
                        if confidence >= self.min_confidence:
                            # –û–±—á–∏—Å–ª–∏—Ç–∏ lift
                            consequent_support = self._get_support(consequent)
                            lift = support / (antecedent_support * consequent_support)
                            
                            rules.append({
                                'antecedent': antecedent,
                                'consequent': consequent,
                                'support': support,
                                'confidence': confidence,
                                'lift': lift
                            })
        
        self.rules = sorted(rules, key=lambda x: x['lift'], reverse=True)
    
    def _get_support(self, itemset):
        """–ó–Ω–∞–π—Ç–∏ support –¥–ª—è itemset"""
        for fs, support in self.frequent_itemsets:
            if fs == itemset:
                return support
        return 0
    
    def print_rules(self, top_n=10):
        """–í–∏–≤–µ—Å—Ç–∏ —Ç–æ–ø –ø—Ä–∞–≤–∏–ª–∞"""
        print(f"\n=== Top {top_n} Association Rules ===\n")
        
        for i, rule in enumerate(self.rules[:top_n], 1):
            ant = ', '.join(rule['antecedent'])
            cons = ', '.join(rule['consequent'])
            
            print(f"{i}. {{{ant}}} ‚Üí {{{cons}}}")
            print(f"   Support: {rule['support']:.3f}")
            print(f"   Confidence: {rule['confidence']:.3f}")
            print(f"   Lift: {rule['lift']:.3f}")
            print()

# –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è
transactions = [
    ['Milk', 'Bread', 'Butter'],
    ['Milk', 'Bread'],
    ['Milk', 'Eggs'],
    ['Bread', 'Butter'],
    ['Bread', 'Eggs'],
]

apriori = AprioriAlgorithm(min_support=0.4, min_confidence=0.6)
apriori.fit(transactions)
apriori.print_rules()
```

---

## –ü—Ä–∏–∫–ª–∞–¥: Supermarket Data

### –†–µ–∞–ª—å–Ω—ñ—à—ñ –¥–∞–Ω—ñ

```python
# –ë—ñ–ª—å—à —Ä–µ–∞–ª—ñ—Å—Ç–∏—á–Ω–∏–π –ø—Ä–∏–∫–ª–∞–¥
supermarket_transactions = [
    ['Beer', 'Diapers', 'Milk'],
    ['Beer', 'Diapers'],
    ['Beer', 'Chips'],
    ['Diapers', 'Milk', 'Bread'],
    ['Beer', 'Diapers', 'Chips'],
    ['Beer', 'Chips'],
    ['Diapers', 'Milk'],
    ['Beer', 'Diapers', 'Milk', 'Bread'],
    ['Beer', 'Diapers'],
    ['Chips', 'Cookies'],
]

# Transform
te = TransactionEncoder()
te_ary = te.fit(supermarket_transactions).transform(supermarket_transactions)
df_super = pd.DataFrame(te_ary, columns=te.columns_)

# Apriori
frequent = apriori(df_super, min_support=0.3, use_colnames=True)
rules = association_rules(frequent, metric="confidence", min_threshold=0.6)

# –í–∏–≤—ñ–¥
print("=== Frequent Itemsets ===")
print(frequent.sort_values('support', ascending=False))

print("\n=== Strong Association Rules ===")
rules_display = rules[['antecedents', 'consequents', 'support', 
                       'confidence', 'lift']].sort_values('lift', ascending=False)
print(rules_display)
```

**–Ü–Ω—Å–∞–π—Ç–∏:**
```
Top Rule: {Diapers} ‚Üí {Beer}
- Support: 0.5 (50% —Ç—Ä–∞–Ω–∑–∞–∫—Ü—ñ–π)
- Confidence: 0.83 (83% —Ö—Ç–æ –∫—É–ø—É—î –ø—ñ–¥–≥—É–∑–∫–∏ –∫—É–ø—É—î –ø–∏–≤–æ)
- Lift: 1.39 (—Å–∏–ª—å–Ω–∏–π –∑–≤'—è–∑–æ–∫!)

–ë—ñ–∑–Ω–µ—Å –¥—ñ—è: –†–æ–∑–º—ñ—Å—Ç–∏—Ç–∏ –ø–∏–≤–æ –ø–æ—Ä—è–¥ –∑ –ø—ñ–¥–≥—É–∑–∫–∞–º–∏!
```

---

## –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è

### Network Graph –ø—Ä–∞–≤–∏–ª

```python
import networkx as nx
import matplotlib.pyplot as plt

def plot_rules_network(rules, top_n=10):
    """–í—ñ–∑—É–∞–ª—ñ–∑—É–≤–∞—Ç–∏ –ø—Ä–∞–≤–∏–ª–∞ —è–∫ –≥—Ä–∞—Ñ"""
    
    # Top rules
    top_rules = rules.nlargest(top_n, 'lift')
    
    # –°—Ç–≤–æ—Ä–∏—Ç–∏ –≥—Ä–∞—Ñ
    G = nx.DiGraph()
    
    for _, rule in top_rules.iterrows():
        antecedents = ', '.join(list(rule['antecedents']))
        consequents = ', '.join(list(rule['consequents']))
        
        # –î–æ–¥–∞—Ç–∏ edge –∑ –≤–∞–≥–æ—é = lift
        G.add_edge(
            antecedents, 
            consequents, 
            weight=rule['lift'],
            confidence=rule['confidence']
        )
    
    # Plot
    plt.figure(figsize=(12, 8))
    
    pos = nx.spring_layout(G, k=0.5, iterations=50)
    
    # Nodes
    nx.draw_networkx_nodes(G, pos, node_size=3000, 
                          node_color='lightblue',
                          alpha=0.7)
    
    # Labels
    nx.draw_networkx_labels(G, pos, font_size=10, 
                           font_weight='bold')
    
    # Edges –∑ —Ä—ñ–∑–Ω–æ—é —Ç–æ–≤—â–∏–Ω–æ—é (–∑–∞ lift)
    edges = G.edges()
    weights = [G[u][v]['weight'] for u, v in edges]
    
    nx.draw_networkx_edges(G, pos, width=weights,
                          alpha=0.6, edge_color='gray',
                          arrows=True, arrowsize=20,
                          connectionstyle='arc3,rad=0.1')
    
    # Edge labels (confidence)
    edge_labels = {(u, v): f"{G[u][v]['confidence']:.2f}" 
                   for u, v in G.edges()}
    nx.draw_networkx_edge_labels(G, pos, edge_labels, 
                                font_size=8)
    
    plt.title('Association Rules Network\n(Edge width = Lift)', 
             fontsize=14, fontweight='bold')
    plt.axis('off')
    plt.tight_layout()
    plt.show()

# –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è
plot_rules_network(rules, top_n=10)
```

### Heatmap –º–µ—Ç—Ä–∏–∫

```python
import seaborn as sns

def plot_rules_heatmap(rules, top_n=20):
    """Heatmap support, confidence, lift"""
    
    top_rules = rules.nlargest(top_n, 'lift')
    
    # –°—Ç–≤–æ—Ä–∏—Ç–∏ labels –¥–ª—è –ø—Ä–∞–≤–∏–ª
    rule_labels = []
    for _, rule in top_rules.iterrows():
        ant = ', '.join(list(rule['antecedents']))
        cons = ', '.join(list(rule['consequents']))
        rule_labels.append(f"{ant} ‚Üí {cons}")
    
    # –ú–∞—Ç—Ä–∏—Ü—è –º–µ—Ç—Ä–∏–∫
    metrics = top_rules[['support', 'confidence', 'lift']].values
    
    # Plot
    plt.figure(figsize=(8, 12))
    
    sns.heatmap(
        metrics,
        annot=True,
        fmt='.2f',
        cmap='YlOrRd',
        yticklabels=rule_labels,
        xticklabels=['Support', 'Confidence', 'Lift'],
        cbar_kws={'label': 'Value'}
    )
    
    plt.title('Association Rules Metrics Heatmap', 
             fontsize=14, fontweight='bold')
    plt.xlabel('Metric', fontsize=12)
    plt.ylabel('Rule', fontsize=12)
    plt.tight_layout()
    plt.show()

plot_rules_heatmap(rules)
```

### Scatter Plot

```python
def plot_rules_scatter(rules):
    """Scatter plot: Support vs Confidence (—Ä–æ–∑–º—ñ—Ä = Lift)"""
    
    plt.figure(figsize=(10, 7))
    
    scatter = plt.scatter(
        rules['support'],
        rules['confidence'],
        s=rules['lift'] * 100,  # –†–æ–∑–º—ñ—Ä –∑–∞ lift
        alpha=0.6,
        c=rules['lift'],
        cmap='viridis',
        edgecolors='black',
        linewidths=0.5
    )
    
    plt.colorbar(scatter, label='Lift')
    plt.xlabel('Support', fontsize=12)
    plt.ylabel('Confidence', fontsize=12)
    plt.title('Association Rules: Support vs Confidence\n(Size = Lift)', 
             fontsize=14, fontweight='bold')
    plt.grid(True, alpha=0.3)
    
    # –î–æ–¥–∞—Ç–∏ labels –¥–ª—è —Ç–æ–ø –ø—Ä–∞–≤–∏–ª
    top_rules = rules.nlargest(5, 'lift')
    for _, rule in top_rules.iterrows():
        ant = ', '.join(list(rule['antecedents']))
        cons = ', '.join(list(rule['consequents']))
        plt.annotate(
            f"{ant}‚Üí{cons}",
            (rule['support'], rule['confidence']),
            fontsize=8,
            alpha=0.7
        )
    
    plt.tight_layout()
    plt.show()

plot_rules_scatter(rules)
```

---

## –û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è —Ç–∞ –ø—Ä–∏—Å–∫–æ—Ä–µ–Ω–Ω—è

### 1. Hash-based pruning

```python
def hash_based_apriori(transactions, min_support):
    """–í–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ hash table –¥–ª—è pruning"""
    
    # Hash –∫–∞–Ω–¥–∏–¥–∞—Ç—ñ–≤ –≤ buckets
    # –Ø–∫—â–æ bucket count < min_support ‚Üí –≤—Å—ñ itemsets –≤ –Ω—å–æ–º—É –Ω–µ—á–∞—Å—Ç—ñ
    # –ù–µ —Ä–µ–∞–ª—ñ–∑–æ–≤–∞–Ω–æ –ø–æ–≤–Ω—ñ—Å—Ç—é, –∞–ª–µ —ñ–¥–µ—è
    pass
```

### 2. Sampling

```python
# –î–ª—è –¥—É–∂–µ –≤–µ–ª–∏–∫–∏—Ö –¥–∞–Ω–∏—Ö
# –°–ø–æ—á–∞—Ç–∫—É –Ω–∞ sample, –ø–æ—Ç—ñ–º verify –Ω–∞ –≤—Å—ñ—Ö –¥–∞–Ω–∏—Ö

sample_size = min(10000, len(transactions))
sample_indices = np.random.choice(len(transactions), sample_size, replace=False)
sample_transactions = [transactions[i] for i in sample_indices]

# Apriori –Ω–∞ sample
frequent = apriori(df_sample, min_support=0.3)

# Verify –Ω–∞ –ø–æ–≤–Ω–∏—Ö –¥–∞–Ω–∏—Ö
```

### 3. Parallel Apriori

```python
from multiprocessing import Pool

def count_support_parallel(itemset_chunk, transactions):
    """–ü—ñ–¥—Ä–∞—Ö—É–≤–∞—Ç–∏ support –ø–∞—Ä–∞–ª–µ–ª—å–Ω–æ"""
    counts = {}
    for itemset in itemset_chunk:
        count = sum(1 for t in transactions if itemset.issubset(t))
        counts[itemset] = count
    return counts

# –†–æ–∑–¥—ñ–ª–∏—Ç–∏ itemsets –Ω–∞ chunks —Ç–∞ –æ–±—Ä–æ–±–∏—Ç–∏ –ø–∞—Ä–∞–ª–µ–ª—å–Ω–æ
```

---

## –ü–µ—Ä–µ–≤–∞–≥–∏ —Ç–∞ –Ω–µ–¥–æ–ª—ñ–∫–∏

### –ü–µ—Ä–µ–≤–∞–≥–∏ ‚úì

| –ü–µ—Ä–µ–≤–∞–≥–∞ | –ü–æ—è—Å–Ω–µ–Ω–Ω—è |
|----------|-----------|
| **–ü—Ä–æ—Å—Ç–∏–π** | –õ–µ–≥–∫–æ –∑—Ä–æ–∑—É–º—ñ—Ç–∏ —Ç–∞ —Ä–µ–∞–ª—ñ–∑—É–≤–∞—Ç–∏ |
| **–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω–∏–π** | –ü—Ä–∞–≤–∏–ª–∞ –∑—Ä–æ–∑—É–º—ñ–ª—ñ –¥–ª—è –±—ñ–∑–Ω–µ—Å—É |
| **–ì–∞—Ä–∞–Ω—Ç–æ–≤–∞–Ω–æ –ø–æ–≤–Ω–∏–π** | –ó–Ω–∞—Ö–æ–¥–∏—Ç—å –í–°–Ü —á–∞—Å—Ç—ñ itemsets |
| **Apriori principle** | –ï—Ñ–µ–∫—Ç–∏–≤–Ω–µ pruning |
| **–ú–∞—Å—à—Ç–∞–±—É—î—Ç—å—Å—è** | –ü—Ä–∞—Ü—é—î –Ω–∞ —Å–µ—Ä–µ–¥–Ω—ñ—Ö –¥–∞–Ω–∏—Ö |

### –ù–µ–¥–æ–ª—ñ–∫–∏ ‚úó

| –ù–µ–¥–æ–ª—ñ–∫ | –ü–æ—è—Å–Ω–µ–Ω–Ω—è |
|---------|-----------|
| **–ü–æ–≤—ñ–ª—å–Ω–∏–π** | –ë–∞–≥–∞—Ç–æ —Å–∫–∞–Ω—É–≤–∞–Ω—å –ë–î |
| **–ë–∞–≥–∞—Ç–æ –∫–∞–Ω–¥–∏–¥–∞—Ç—ñ–≤** | –ï–∫—Å–ø–æ–Ω–µ–Ω—Ü—ñ–π–Ω–∏–π —Ä—ñ—Å—Ç |
| **–ù–µ –¥–ª—è –≤–µ–ª–∏–∫–∏—Ö –¥–∞–Ω–∏—Ö** | FP-Growth —à–≤–∏–¥—à–µ |
| **–¢—ñ–ª—å–∫–∏ categorical** | –ù–µ –ø—Ä–∞—Ü—é—î –∑ numerical |
| **–°—Ç–∞—Ç–∏—á–Ω–∏–π** | –ù–µ –¥–ª—è streaming data |

---

## –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –∑ —ñ–Ω—à–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏

| –ö—Ä–∏—Ç–µ—Ä—ñ–π | Apriori | FP-Growth | Eclat |
|----------|---------|-----------|-------|
| **–®–≤–∏–¥–∫—ñ—Å—Ç—å** | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **–ü–∞–º'—è—Ç—å** | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |
| **–ü—Ä–æ—Å—Ç–æ—Ç–∞** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **–ú–∞—Å—à—Ç–∞–±–æ–≤–∞–Ω—ñ—Å—Ç—å** | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |

**–ö–æ–ª–∏ —â–æ:**
- **–ú–∞–ª—ñ/—Å–µ—Ä–µ–¥–Ω—ñ –¥–∞–Ω—ñ + –ø—Ä–æ—Å—Ç–æ—Ç–∞** ‚Üí Apriori ‚úì
- **–í–µ–ª–∏–∫—ñ –¥–∞–Ω—ñ + —à–≤–∏–¥–∫—ñ—Å—Ç—å** ‚Üí FP-Growth ‚úì
- **Vertical format** ‚Üí Eclat ‚úì

---

## –ü—Ä–∞–∫—Ç–∏—á–Ω—ñ –ø–æ—Ä–∞–¥–∏ üí°

### 1. –ü–æ—á–Ω–∏ –∑ –≤–∏—Å–æ–∫–æ–≥–æ min_support

```python
# ‚úÖ –°–ø–æ—á–∞—Ç–∫—É –≤–∏—Å–æ–∫–∏–π
frequent = apriori(df, min_support=0.5)  # 50%

# –Ø–∫—â–æ –º–∞–ª–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ ‚Üí –∑–º–µ–Ω—à—É–π
frequent = apriori(df, min_support=0.3)  # 30%
frequent = apriori(df, min_support=0.1)  # 10%
```

### 2. –§—ñ–ª—å—Ç—Ä—É–π –∑–∞ lift > 1

```python
# –ó–∞–ª–∏—à–∏—Ç–∏ —Ç—ñ–ª—å–∫–∏ –ø–æ–∑–∏—Ç–∏–≤–Ω—ñ –∫–æ—Ä–µ–ª—è—Ü—ñ—ó
rules_filtered = rules[rules['lift'] > 1]

# –°–∏–ª—å–Ω—ñ –ø—Ä–∞–≤–∏–ª–∞
strong_rules = rules[(rules['confidence'] > 0.7) & (rules['lift'] > 1.2)]
```

### 3. –û–±–º–µ–∂ –¥–æ–≤–∂–∏–Ω—É itemsets

```python
# –¢—ñ–ª—å–∫–∏ 2-itemsets (–Ω–∞–π–±—ñ–ª—å—à —ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ)
frequent = apriori(df, min_support=0.3, max_len=2)
```

### 4. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π domain knowledge

```python
# –í–∏–∫–ª—é—á–∏—Ç–∏ –æ—á–µ–≤–∏–¥–Ω—ñ –ø—Ä–∞–≤–∏–ª–∞
# –ù–∞–ø—Ä–∏–∫–ª–∞–¥: {–ö–∞–≤–∞} ‚Üí {–¶—É–∫–æ—Ä} (–∑–∞–Ω–∞–¥—Ç–æ –æ—á–µ–≤–∏–¥–Ω–æ)

rules_interesting = rules[
    ~rules.apply(lambda x: 
        ('Coffee' in x['antecedents'] and 'Sugar' in x['consequents']),
        axis=1
    )
]
```

### 5. –ì—Ä—É–ø—É–π —Å—Ö–æ–∂—ñ items

```python
# –ó–∞–º—ñ—Å—Ç—å –æ–∫—Ä–µ–º–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤ ‚Üí –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó
# {Milk_1L, Milk_2L} ‚Üí {Milk}
# {Bread_White, Bread_Wheat} ‚Üí {Bread}
```

### 6. Temporal analysis

```python
# –†–æ–∑–¥—ñ–ª–∏—Ç–∏ –∑–∞ —á–∞—Å–æ–º
transactions_morning = [t for t in transactions if t['time'] == 'morning']
transactions_evening = [t for t in transactions if t['time'] == 'evening']

# –û–∫—Ä–µ–º–∏–π Apriori –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ
rules_morning = apriori(...)
rules_evening = apriori(...)

# –ü–æ—Ä—ñ–≤–Ω—è—Ç–∏ —Ä—ñ–∑–Ω–∏—Ü—é
```

### 7. –ü–µ—Ä–µ–≤—ñ—Ä—è–π —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–Ω—É –∑–Ω–∞—á—É—â—ñ—Å—Ç—å

```python
# Chi-square test –¥–ª—è –Ω–µ–∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ
from scipy.stats import chi2_contingency

def is_significant(rule, transactions, alpha=0.05):
    """–ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ —á–∏ –ø—Ä–∞–≤–∏–ª–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–Ω–æ –∑–Ω–∞—á—É—â–µ"""
    
    ant = rule['antecedents']
    cons = rule['consequents']
    
    # –ü–æ–±—É–¥—É–≤–∞—Ç–∏ contingency table
    # ... (–ø—ñ–¥—Ä–∞—Ö—É–≤–∞—Ç–∏ a, b, c, d)
    
    # Chi-square test
    _, p_value, _, _ = chi2_contingency([[a, b], [c, d]])
    
    return p_value < alpha

# –§—ñ–ª—å—Ç—Ä—É–≤–∞—Ç–∏
significant_rules = rules[rules.apply(
    lambda r: is_significant(r, transactions), axis=1
)]
```

### 8. –í—ñ–∑—É–∞–ª—ñ–∑—É–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏

```python
# –ì—Ä–∞—Ñ—ñ–∫–∏ –¥–æ–ø–æ–º–∞–≥–∞—é—Ç—å –∑–Ω–∞–π—Ç–∏ —ñ–Ω—Å–∞–π—Ç–∏
plot_rules_network(rules)
plot_rules_scatter(rules)
```

### 9. A/B testing –¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏

```python
# –ó–Ω–∞–π—à–ª–∏ –ø—Ä–∞–≤–∏–ª–æ: {Chips} ‚Üí {Beer}
# –¢–µ—Å—Ç: –ø–æ–∫–ª–∞—Å—Ç–∏ chips –ø–æ—Ä—è–¥ –∑ beer –≤ –ø–æ–ª–æ–≤–∏–Ω—ñ –º–∞–≥–∞–∑–∏–Ω—ñ–≤
# –í–∏–º—ñ—Ä—è—Ç–∏ —á–∏ –∑–±—ñ–ª—å—à–∏–ª–∏—Å—å –ø—Ä–æ–¥–∞–∂—ñ
```

### 10. Combine –∑ —ñ–Ω—à–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏

```python
# Apriori + Clustering
# 1. –ö–ª–∞—Å—Ç–µ—Ä–∏–∑—É–≤–∞—Ç–∏ –∫–ª—ñ—î–Ω—Ç—ñ–≤
# 2. –û–∫—Ä–µ–º–∏–π Apriori –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞
# 3. –ü–µ—Ä—Å–æ–Ω–∞–ª—ñ–∑–æ–≤–∞–Ω—ñ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó
```

---

## –ü–æ—à–∏—Ä–µ–Ω—ñ –ø–æ–º–∏–ª–∫–∏ ‚ùå

### 1. –ó–∞–Ω–∞–¥—Ç–æ –Ω–∏–∑—å–∫–∏–π min_support

```python
# ‚ùå –ó–∞–Ω–∞–¥—Ç–æ –±–∞–≥–∞—Ç–æ –ø—Ä–∞–≤–∏–ª (–º—ñ–ª—å–π–æ–Ω–∏!)
frequent = apriori(df, min_support=0.01)  # 1%

# ‚úÖ –†–æ–∑—É–º–Ω–∏–π –ø–æ—Ä—ñ–≥
frequent = apriori(df, min_support=0.1)   # 10%
```

### 2. –Ü–≥–Ω–æ—Ä—É–≤–∞—Ç–∏ lift

```python
# ‚ùå –¢—ñ–ª—å–∫–∏ confidence
rules_bad = rules[rules['confidence'] > 0.8]
# –ú–æ–∂–µ –≤–∫–ª—é—á–∞—Ç–∏ –≤–∏–ø–∞–¥–∫–æ–≤—ñ –∫–æ—Ä–µ–ª—è—Ü—ñ—ó!

# ‚úÖ Confidence + Lift
rules_good = rules[
    (rules['confidence'] > 0.6) & 
    (rules['lift'] > 1.2)
]
```

### 3. –ù–µ –ø–µ—Ä–µ–≤—ñ—Ä—è—Ç–∏ data quality

```python
# ‚ùå –ü—Ä–æ–ø—É—â–µ–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è, –¥—É–±–ª—ñ–∫–∞—Ç–∏
# –ú–æ–∂—É—Ç—å —Å–ø–æ—Ç–≤–æ—Ä–∏—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏

# ‚úÖ –û—á–∏—Å—Ç–∏—Ç–∏ –¥–∞–Ω—ñ —Å–ø–æ—á–∞—Ç–∫—É
transactions_clean = remove_duplicates(transactions)
transactions_clean = remove_empty(transactions_clean)
```

### 4. –Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç—É–≤–∞—Ç–∏ correlation —è–∫ causation

```python
# ‚ùå "Chips –í–ò–ö–õ–ò–ö–ê–Æ–¢–¨ –∫—É–ø—ñ–≤–ª—é Beer"
# –ú–æ–∂–µ –±—É—Ç–∏ —Ç—Ä–µ—Ç—ñ–π —Ñ–∞–∫—Ç–æ—Ä (—Å–ø–æ—Ä—Ç–∏–≤–Ω—ñ –ø–æ–¥—ñ—ó)

# ‚úÖ "Chips —Ç–∞ Beer —á–∞—Å—Ç–æ –∫—É–ø—É—é—Ç—å —Ä–∞–∑–æ–º"
```

### 5. –ù–µ –≤—Ä–∞—Ö–æ–≤—É–≤–∞—Ç–∏ —Ä–æ–∑–º—ñ—Ä —Ç—Ä–∞–Ω–∑–∞–∫—Ü—ñ–π

```python
# –í–µ–ª–∏–∫—ñ —Ç—Ä–∞–Ω–∑–∞–∫—Ü—ñ—ó ‚Üí –±–∞–≥–∞—Ç–æ itemsets ‚Üí —Å–ø–æ—Ç–≤–æ—Ä–µ–Ω—ñ –º–µ—Ç—Ä–∏–∫–∏
# –ù–æ—Ä–º–∞–ª—ñ–∑—É–≤–∞—Ç–∏ –∞–±–æ —Ñ—ñ–ª—å—Ç—Ä—É–≤–∞—Ç–∏
```

---

## –†–µ–∞–ª—å–Ω—ñ –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è

### 1. Retail / Supermarkets

```python
# –ó–Ω–∞–π—Ç–∏:
# - –©–æ –∫—É–ø—É—é—Ç—å —Ä–∞–∑–æ–º
# - Cross-selling opportunities
# - Store layout optimization

# –ü—Ä–∏–∫–ª–∞–¥ —ñ–Ω—Å–∞–π—Ç—É:
# {Diapers} ‚Üí {Beer} (lift=1.4)
# ‚Üí –†–æ–∑–º—ñ—Å—Ç–∏—Ç–∏ –ø–∏–≤–æ –±—ñ–ª—è –¥–∏—Ç—è—á–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤
```

### 2. E-commerce

```python
# –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó:
# "Customers who bought X also bought Y"

# –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è:
user_cart = {'Laptop', 'Mouse'}
# –ó–Ω–∞–π—Ç–∏ –ø—Ä–∞–≤–∏–ª–∞: {Laptop, Mouse} ‚Üí {???}
recommendations = find_recommendations(user_cart, rules)
```

### 3. Healthcare

```python
# Symptom co-occurrence
# {Fever, Cough} ‚Üí {Flu} (confidence=0.85)

# Drug interactions
# {DrugA, DrugB} ‚Üí {Side Effect}
```

### 4. Web Usage Mining

```python
# –Ø–∫—ñ —Å—Ç–æ—Ä—ñ–Ω–∫–∏ –≤—ñ–¥–≤—ñ–¥—É—é—Ç—å —Ä–∞–∑–æ–º
# {HomePage, Products} ‚Üí {Checkout} (path optimization)

# –ü–µ—Ä—Å–æ–Ω–∞–ª—ñ–∑–∞—Ü—ñ—è –∫–æ–Ω—Ç–µ–Ω—Ç—É
```

### 5. Telecommunications

```python
# Churn prediction
# {CallDrop, HighBill, LowUsage} ‚Üí {Churn}

# Bundle recommendations
# {MobileData} ‚Üí {Streaming} (lift=1.3)
```

---

## –ü–æ–≤'—è–∑–∞–Ω—ñ —Ç–µ–º–∏

- [[02_FP-Growth]] ‚Äî —à–≤–∏–¥—à–∞ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞
- [[03_Market_Basket_Analysis]] ‚Äî –ø—Ä–∞–∫—Ç–∏—á–Ω–µ –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è
- [[Clustering]] ‚Äî —Å–µ–≥–º–µ–Ω—Ç–∞—Ü—ñ—è –ø–µ—Ä–µ–¥ association mining
- [[Recommendation_Systems]] ‚Äî –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –ø—Ä–∞–≤–∏–ª

## –†–µ—Å—É—Ä—Å–∏

- [mlxtend Documentation](http://rasbt.github.io/mlxtend/user_guide/frequent_patterns/apriori/)
- [Original Apriori Paper (Agrawal & Srikant, 1994)](https://www.vldb.org/conf/1994/P487.PDF)
- [Introduction to Data Mining (Tan et al.)](https://www-users.cs.umn.edu/~kumar001/dmbook/index.php)

---

## –ö–ª—é—á–æ–≤—ñ –≤–∏—Å–Ω–æ–≤–∫–∏

> Apriori ‚Äî –∫–ª–∞—Å–∏—á–Ω–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º –¥–ª—è –∑–Ω–∞—Ö–æ–¥–∂–µ–Ω–Ω—è —á–∞—Å—Ç–∏—Ö itemsets —Ç–∞ association rules —É —Ç—Ä–∞–Ω–∑–∞–∫—Ü—ñ–π–Ω–∏—Ö –¥–∞–Ω–∏—Ö, —â–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î Apriori principle (—è–∫—â–æ itemset –Ω–µ—á–∞—Å—Ç–∏–π, —Ç–æ –≤—Å—ñ –π–æ–≥–æ —Å—É–ø–µ—Ä–º–Ω–æ–∂–∏–Ω–∏ —Ç–∞–∫–æ–∂ –Ω–µ—á–∞—Å—Ç—ñ) –¥–ª—è –µ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ pruning –∫–∞–Ω–¥–∏–¥–∞—Ç—ñ–≤.

**–û—Å–Ω–æ–≤–Ω—ñ –∫–æ–Ω—Ü–µ–ø—Ü—ñ—ó:**
- **Support:** —è–∫ —á–∞—Å—Ç–æ –∑—É—Å—Ç—Ä—ñ—á–∞—î—Ç—å—Å—è
- **Confidence:** –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å consequent –ø—Ä–∏ antecedent
- **Lift:** —Å–∏–ª–∞ –∑–≤'—è–∑–∫—É (>1 = –ø–æ–∑–∏—Ç–∏–≤–Ω–∞ –∫–æ—Ä–µ–ª—è—Ü—ñ—è)

**–ê–ª–≥–æ—Ä–∏—Ç–º:**
1. –ó–Ω–∞–π—Ç–∏ —á–∞—Å—Ç—ñ 1-itemsets (L‚ÇÅ)
2. –î–ª—è k=2,3,... –≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ –∫–∞–Ω–¥–∏–¥–∞—Ç—ñ–≤ C_k
3. Pruning —á–µ—Ä–µ–∑ Apriori principle
4. –ü—ñ–¥—Ä–∞—Ö—É–≤–∞—Ç–∏ support, –≤—ñ–¥—Ñ—ñ–ª—å—Ç—Ä—É–≤–∞—Ç–∏
5. –ì–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ –ø—Ä–∞–≤–∏–ª–∞ –∑ confidence ‚â• threshold

**–ü–µ—Ä–µ–≤–∞–≥–∏:**
- ‚úÖ –ü—Ä–æ—Å—Ç–∏–π —Ç–∞ —ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω–∏–π
- ‚úÖ –ì–∞—Ä–∞–Ω—Ç–æ–≤–∞–Ω–æ –∑–Ω–∞—Ö–æ–¥–∏—Ç—å –≤—Å—ñ —á–∞—Å—Ç—ñ itemsets
- ‚úÖ –ï—Ñ–µ–∫—Ç–∏–≤–Ω–µ pruning

**–ù–µ–¥–æ–ª—ñ–∫–∏:**
- ‚ùå –ü–æ–≤—ñ–ª—å–Ω–∏–π –Ω–∞ –≤–µ–ª–∏–∫–∏—Ö –¥–∞–Ω–∏—Ö
- ‚ùå –ë–∞–≥–∞—Ç–æ —Å–∫–∞–Ω—É–≤–∞–Ω—å –ë–î
- ‚ùå –ï–∫—Å–ø–æ–Ω–µ–Ω—Ü—ñ–π–Ω–∏–π —Ä—ñ—Å—Ç –∫–∞–Ω–¥–∏–¥–∞—Ç—ñ–≤

**–ú–µ—Ç—Ä–∏–∫–∏:**
- **High support:** –ß–∞—Å—Ç–æ –∑—É—Å—Ç—Ä—ñ—á–∞—î—Ç—å—Å—è
- **High confidence:** –°–∏–ª—å–Ω–µ –ø—Ä–∞–≤–∏–ª–æ
- **High lift > 1:** –ü–æ–∑–∏—Ç–∏–≤–Ω–∞ –∫–æ—Ä–µ–ª—è—Ü—ñ—è
- **–í—Å—ñ —Ç—Ä–∏ –≤–∏—Å–æ–∫—ñ:** –ù–∞–π–∫—Ä–∞—â—ñ –ø—Ä–∞–≤–∏–ª–∞!

**–ü—Ä–∞–∫—Ç–∏—á–Ω—ñ –ø–æ—Ä–∞–¥–∏:**
- –ü–æ—á–Ω–∏ –∑ –≤–∏—Å–æ–∫–æ–≥–æ min_support (30-50%)
- –§—ñ–ª—å—Ç—Ä—É–π –∑–∞ lift > 1
- –í—ñ–∑—É–∞–ª—ñ–∑—É–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
- –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π domain knowledge
- –ü–µ—Ä–µ–≤—ñ—Ä—è–π —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–Ω—É –∑–Ω–∞—á—É—â—ñ—Å—Ç—å

**–ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏:**
- Market basket analysis ‚úì
- Cross-selling ‚úì
- Recommendation systems ‚úì
- –°–µ—Ä–µ–¥–Ω—ñ –¥–∞–Ω—ñ (1K-100K) ‚úì
- –í–µ–ª–∏–∫—ñ –¥–∞–Ω—ñ (>1M) ‚Üí FP-Growth ‚úì

---

#ml #unsupervised-learning #association-rules #apriori #market-basket-analysis #pattern-mining #frequent-itemsets #data-mining
