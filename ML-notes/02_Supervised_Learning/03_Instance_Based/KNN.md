# K-Nearest Neighbors (KNN)

## –©–æ —Ü–µ?

**K-Nearest Neighbors (KNN)** ‚Äî —Ü–µ –ø—Ä–æ—Å—Ç–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º supervised learning, —è–∫–∏–π –∫–ª–∞—Å–∏—Ñ—ñ–∫—É—î –Ω–æ–≤–∏–π –æ–±'—î–∫—Ç –Ω–∞ –æ—Å–Ω–æ–≤—ñ **k –Ω–∞–π–±–ª–∏–∂—á–∏—Ö —Å—É—Å—ñ–¥—ñ–≤** —É –ø—Ä–æ—Å—Ç–æ—Ä—ñ –æ–∑–Ω–∞–∫.

**–ì–æ–ª–æ–≤–Ω–∞ —ñ–¥–µ—è:** "–°–∫–∞–∂–∏ –º–µ–Ω—ñ, —Ö—Ç–æ —Ç–≤—ñ–π –¥—Ä—É–≥, —ñ —è —Å–∫–∞–∂—É, —Ö—Ç–æ —Ç–∏" ‚Äî –æ–±'—î–∫—Ç–∏, —è–∫—ñ –∑–Ω–∞—Ö–æ–¥—è—Ç—å—Å—è –ø–æ—Ä—É—á —É –ø—Ä–æ—Å—Ç–æ—Ä—ñ –æ–∑–Ω–∞–∫, –Ω–∞–π—ñ–º–æ–≤—ñ—Ä–Ω—ñ—à–µ –Ω–∞–ª–µ–∂–∞—Ç—å –¥–æ –æ–¥–Ω–æ–≥–æ –∫–ª–∞—Å—É.

## –ù–∞–≤—ñ—â–æ –ø–æ—Ç—Ä—ñ–±–µ–Ω?

- üéØ **–ü—Ä–æ—Å—Ç–æ—Ç–∞** ‚Äî –æ–¥–∏–Ω –∑ –Ω–∞–π–ø—Ä–æ—Å—Ç—ñ—à–∏—Ö ML –∞–ª–≥–æ—Ä–∏—Ç–º—ñ–≤
- üìä **–£–Ω—ñ–≤–µ—Ä—Å–∞–ª—å–Ω—ñ—Å—Ç—å** ‚Äî –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è —Ç–∞ —Ä–µ–≥—Ä–µ—Å—ñ—è
- üîß **–ù–µ–ª—ñ–Ω—ñ–π–Ω—ñ boundaries** ‚Äî —Å–∫–ª–∞–¥–Ω—ñ decision boundaries
- üí° **–Ü–Ω—Ç—É—ó—Ç–∏–≤–Ω—ñ—Å—Ç—å** ‚Äî –ª–µ–≥–∫–æ –∑—Ä–æ–∑—É–º—ñ—Ç–∏ —Ç–∞ –ø–æ—è—Å–Ω–∏—Ç–∏
- ‚ö° **Lazy learning** ‚Äî –Ω–µ –ø–æ—Ç—Ä–µ–±—É—î —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è (instance-based)
- üé® **Baseline** ‚Äî —à–≤–∏–¥–∫–∏–π —Å—Ç–∞—Ä—Ç –¥–ª—è —Ä–æ–∑—É–º—ñ–Ω–Ω—è –¥–∞–Ω–∏—Ö

## –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏?

**–ü–æ—Ç—Ä—ñ–±–Ω–æ:**

- **–ú–∞–ª—ñ/—Å–µ—Ä–µ–¥–Ω—ñ –¥–∞—Ç–∞—Å–µ—Ç–∏** ‚Äî —à–≤–∏–¥–∫—ñ –æ–±—á–∏—Å–ª–µ–Ω–Ω—è
- –ù–µ–ª—ñ–Ω—ñ–π–Ω—ñ decision boundaries
- –ü–æ—Ç—Ä—ñ–±–µ–Ω **–ø—Ä–æ—Å—Ç–∏–π baseline**
- **–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å** –≤–∞–∂–ª–∏–≤–∞
- –î–∞–Ω—ñ –¥–æ–±—Ä–µ –º–∞—Å—à—Ç–∞–±–æ–≤–∞–Ω—ñ
- –ù–µ–º–∞—î –±–∞–≥–∞—Ç–æ –æ–∑–Ω–∞–∫ (curse of dimensionality)

**–ù–µ –ø–æ—Ç—Ä—ñ–±–Ω–æ:**

- **–í–µ–ª–∏–∫—ñ –¥–∞—Ç–∞—Å–µ—Ç–∏** (>100k –∑—Ä–∞–∑–∫—ñ–≤) ‚Üí –¥—É–∂–µ –ø–æ–≤—ñ–ª—å–Ω–æ
- **–í–∏—Å–æ–∫–æ—Ä–æ–∑–º—ñ—Ä–Ω—ñ –¥–∞–Ω—ñ** (>50 –æ–∑–Ω–∞–∫) ‚Üí curse of dimensionality
- –ü–æ—Ç—Ä—ñ–±–Ω–∞ **—à–≤–∏–¥–∫—ñ—Å—Ç—å inference** ‚Üí tree-based, linear models
- –î–∞–Ω—ñ –Ω–µ –Ω–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω—ñ
- –ë–∞–≥–∞—Ç–æ irrelevant features

---

## –Ø–∫ –ø—Ä–∞—Ü—é—î KNN?

### –ê–ª–≥–æ—Ä–∏—Ç–º (–ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è)

**Training phase:** –ù–ï –†–û–ë–ò–¢–¨ –ù–Ü–ß–û–ì–û! –ü—Ä–æ—Å—Ç–æ –∑–±–µ—Ä—ñ–≥–∞—î –≤—Å—ñ —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ.

**Prediction phase:**

1. –û–±—á–∏—Å–ª–∏—Ç–∏ **–≤—ñ–¥—Å—Ç–∞–Ω—å** –≤—ñ–¥ –Ω–æ–≤–æ–≥–æ –∑—Ä–∞–∑–∫–∞ –¥–æ –≤—Å—ñ—Ö —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∏—Ö –∑—Ä–∞–∑–∫—ñ–≤
2. –í–∏–±—Ä–∞—Ç–∏ **k –Ω–∞–π–±–ª–∏–∂—á–∏—Ö —Å—É—Å—ñ–¥—ñ–≤**
3. **–ì–æ–ª–æ—Å—É–≤–∞–Ω–Ω—è –±—ñ–ª—å—à–æ—Å—Ç—ñ** (majority vote) —Å–µ—Ä–µ–¥ k —Å—É—Å—ñ–¥—ñ–≤
4. –ü–æ–≤–µ—Ä–Ω—É—Ç–∏ –Ω–∞–π–ø–æ–ø—É–ª—è—Ä–Ω—ñ—à–∏–π –∫–ª–∞—Å

### –ü—Ä–∏–∫–ª–∞–¥ –∑ —á–∏—Å–ª–∞–º–∏

**–î–∞–Ω—ñ:**
```
Train:
  Point 1: [1, 1] ‚Üí Class A
  Point 2: [2, 2] ‚Üí Class A
  Point 3: [3, 1] ‚Üí Class B
  Point 4: [6, 5] ‚Üí Class B
  Point 5: [7, 7] ‚Üí Class B

New point: [3, 3] ‚Üí ?
```

**k=3, Euclidean distance:**

1. –í—ñ–¥—Å—Ç–∞–Ω—ñ –≤—ñ–¥ [3, 3]:
   - Point 1: ‚àö((3-1)¬≤ + (3-1)¬≤) = ‚àö8 = 2.83
   - Point 2: ‚àö((3-2)¬≤ + (3-2)¬≤) = ‚àö2 = 1.41 ‚Üê 1st
   - Point 3: ‚àö((3-3)¬≤ + (3-1)¬≤) = ‚àö4 = 2.00 ‚Üê 2nd
   - Point 4: ‚àö((3-6)¬≤ + (3-5)¬≤) = ‚àö13 = 3.61
   - Point 5: ‚àö((3-7)¬≤ + (3-7)¬≤) = ‚àö32 = 5.66

2. 3 –Ω–∞–π–±–ª–∏–∂—á—ñ: Point 2 (A), Point 3 (B), Point 1 (A)

3. –ì–æ–ª–æ—Å—É–≤–∞–Ω–Ω—è: A=2, B=1

4. **Prediction: Class A** ‚úì

---

## Distance Metrics (–ú–µ—Ç—Ä–∏–∫–∏ –≤—ñ–¥—Å—Ç–∞–Ω—ñ)

### 1. Euclidean Distance (–ï–≤–∫–ª—ñ–¥–æ–≤–∞)

**–§–æ—Ä–º—É–ª–∞:**
$$d(x, y) = \sqrt{\sum_{i=1}^{n} (x_i - y_i)^2}$$

**–ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏:**
- ‚úÖ –ó–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º (–Ω–∞–π–ø–æ–ø—É–ª—è—Ä–Ω—ñ—à–∞)
- ‚úÖ Continuous features
- ‚úÖ –†—ñ–≤–Ω–æ–º—ñ—Ä–Ω–∏–π –º–∞—Å—à—Ç–∞–± –æ–∑–Ω–∞–∫

**–ü—Ä–∏–∫–ª–∞–¥:**
```python
x = [1, 2, 3]
y = [4, 5, 6]
d = sqrt((1-4)¬≤ + (2-5)¬≤ + (3-6)¬≤) = sqrt(27) = 5.20
```

### 2. Manhattan Distance (–ú–∞–Ω—Ö–µ—Ç—Ç–µ–Ω—Å—å–∫–∞)

**–§–æ—Ä–º—É–ª–∞:**
$$d(x, y) = \sum_{i=1}^{n} |x_i - y_i|$$

**–ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏:**
- ‚úÖ Grid-like —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–∏—Ö
- ‚úÖ –ú–µ–Ω—à —á—É—Ç–ª–∏–≤–∞ –¥–æ outliers
- ‚úÖ High-dimensional data

**–ü—Ä–∏–∫–ª–∞–¥:**
```python
x = [1, 2, 3]
y = [4, 5, 6]
d = |1-4| + |2-5| + |3-6| = 3 + 3 + 3 = 9
```

### 3. Minkowski Distance (–ó–∞–≥–∞–ª—å–Ω–∞ —Ñ–æ—Ä–º–∞)

**–§–æ—Ä–º—É–ª–∞:**
$$d(x, y) = \left(\sum_{i=1}^{n} |x_i - y_i|^p\right)^{1/p}$$

–¥–µ:
- $p=1$ ‚Üí Manhattan
- $p=2$ ‚Üí Euclidean
- $p=\infty$ ‚Üí Chebyshev

### 4. Cosine Distance (–¥–ª—è —Ç–µ–∫—Å—Ç—ñ–≤, sparse data)

**–§–æ—Ä–º—É–ª–∞:**
$$\text{similarity}(x, y) = \frac{x \cdot y}{||x|| \cdot ||y||}$$

$$\text{distance}(x, y) = 1 - \text{similarity}(x, y)$$

**–ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏:**
- ‚úÖ Text data (TF-IDF vectors)
- ‚úÖ High-dimensional sparse data
- ‚úÖ –ù–∞–ø—Ä—è–º–æ–∫ –≤–∞–∂–ª–∏–≤—ñ—à–∏–π –∑–∞ magnitude

### 5. Hamming Distance (–¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä—ñ–∞–ª—å–Ω–∏—Ö)

**–§–æ—Ä–º—É–ª–∞:**
$$d(x, y) = \frac{1}{n}\sum_{i=1}^{n} \mathbb{1}[x_i \neq y_i]$$

**–ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏:**
- ‚úÖ Categorical features
- ‚úÖ Binary data

---

## –í–∏–±—ñ—Ä k (–∫—ñ–ª—å–∫—ñ—Å—Ç—å —Å—É—Å—ñ–¥—ñ–≤)

### –ï—Ñ–µ–∫—Ç —Ä—ñ–∑–Ω–∏—Ö k

```
k=1 (–∑–∞–Ω–∞–¥—Ç–æ –º–∞–ª–∏–π):        k=optimal:               k=n (–∑–∞–Ω–∞–¥—Ç–æ –≤–µ–ª–∏–∫–∏–π):
    High variance               Balanced                  High bias
    Overfitting                 ‚úì –ù–∞–π–∫—Ä–∞—â–µ                Underfitting
    
Decision boundary:          Decision boundary:        Decision boundary:
–î—É–∂–µ –Ω–µ—Ä—ñ–≤–Ω–∞ –∫—Ä–∏–≤–∞         –ó–≥–ª–∞–¥–∂–µ–Ω–∞ –∫—Ä–∏–≤–∞           –ú–∞–π–∂–µ –ø—Ä—è–º–∞ –ª—ñ–Ω—ñ—è
```

### –Ø–∫ –≤–∏–±—Ä–∞—Ç–∏ k?

#### 1. –ü—Ä–∞–≤–∏–ª–æ –≤–µ–ª–∏–∫–æ–≥–æ –ø–∞–ª—å—Ü—è

$$k \approx \sqrt{n}$$

–¥–µ $n$ ‚Äî –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∏—Ö –∑—Ä–∞–∑–∫—ñ–≤.

**–ü—Ä–∏–∫–ª–∞–¥:**
- n=100 ‚Üí k ‚âà 10
- n=1000 ‚Üí k ‚âà 32

#### 2. –ù–µ–ø–∞—Ä–Ω–µ k –¥–ª—è –±—ñ–Ω–∞—Ä–Ω–æ—ó –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó

**–ß–æ–º—É?** –£–Ω–∏–∫–Ω—É—Ç–∏ –Ω—ñ—á–∏—ó (tie) –ø—Ä–∏ –≥–æ–ª–æ—Å—É–≤–∞–Ω–Ω—ñ.

```python
# ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û
k = 3, 5, 7, 9, ...

# ‚ùå –ú–û–ñ–ï –ë–£–¢–ò TIE
k = 2, 4, 6, 8, ...
# –ü—Ä–∏ k=2: A=1, B=1 ‚Üí —è–∫ –æ–±—Ä–∞—Ç–∏?
```

#### 3. Cross-validation

**–ù–∞–π–∫—Ä–∞—â–∏–π –º–µ—Ç–æ–¥:**

```python
from sklearn.model_selection import cross_val_score

k_values = [1, 3, 5, 7, 9, 11, 15, 21, 31]
cv_scores = []

for k in k_values:
    knn = KNeighborsClassifier(n_neighbors=k)
    scores = cross_val_score(knn, X_train, y_train, cv=5)
    cv_scores.append(scores.mean())

optimal_k = k_values[np.argmax(cv_scores)]
```

### –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –≤–ø–ª–∏–≤—É k

```
CV Score
    |
0.9 |      ‚ï±‚îÄ‚îÄ‚îÄ‚îÄ‚ï≤
    |     ‚ï±      ‚ï≤
0.8 |    ‚ï±        ‚ï≤
    |   ‚ï±          ‚ï≤___
0.7 |  ‚ï±
    |_‚ï±_____________________ k
      1  5  10  15  20  25

Overfitting ‚Üê Optimal ‚Üí Underfitting
```

---

## KNN –¥–ª—è –†–µ–≥—Ä–µ—Å—ñ—ó

### –ê–ª–≥–æ—Ä–∏—Ç–º

–ó–∞–º—ñ—Å—Ç—å –≥–æ–ª–æ—Å—É–≤–∞–Ω–Ω—è ‚Äî **—É—Å–µ—Ä–µ–¥–Ω–µ–Ω–Ω—è** –∑–Ω–∞—á–µ–Ω—å k —Å—É—Å—ñ–¥—ñ–≤:

$$\hat{y} = \frac{1}{k} \sum_{i=1}^{k} y_i$$

### Weighted KNN Regression

–°—É—Å—ñ–¥–∏, —è–∫—ñ –±–ª–∏–∂—á–µ, –º–∞—é—Ç—å –±—ñ–ª—å—à—É –≤–∞–≥—É:

$$\hat{y} = \frac{\sum_{i=1}^{k} w_i \cdot y_i}{\sum_{i=1}^{k} w_i}$$

–¥–µ $w_i = \frac{1}{d_i}$ –∞–±–æ $w_i = \frac{1}{d_i^2}$

### –ü—Ä–∏–∫–ª–∞–¥

**–î–∞–Ω—ñ:**
```
Point 1: [1, 1] ‚Üí y=10
Point 2: [2, 2] ‚Üí y=20
Point 3: [3, 1] ‚Üí y=15
Point 4: [6, 5] ‚Üí y=50

New point: [2.5, 2.5] ‚Üí ?
```

**k=3:**
1. –í—ñ–¥—Å—Ç–∞–Ω—ñ: Point 2 (d=0.71), Point 3 (d=1.58), Point 1 (d=2.12)
2. k=3 —Å—É—Å—ñ–¥–∏: y=[20, 15, 10]
3. –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è: $\hat{y} = \frac{20+15+10}{3} = 15$

**Weighted (w = 1/d):**
$$\hat{y} = \frac{20 \cdot \frac{1}{0.71} + 15 \cdot \frac{1}{1.58} + 10 \cdot \frac{1}{2.12}}{\frac{1}{0.71} + \frac{1}{1.58} + \frac{1}{2.12}} \approx 17.5$$

Weighted –¥–∞—î –±—ñ–ª—å—à—É –≤–∞–≥—É –±–ª–∏–∂—á–∏–º —Å—É—Å—ñ–¥–∞–º ‚úì

---

## –ö–æ–¥ (Python + scikit-learn)

### –ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.neighbors import KNeighborsClassifier
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# 1. –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –¥–∞–Ω–∏—Ö
X, y = make_classification(
    n_samples=500,
    n_features=2,
    n_informative=2,
    n_redundant=0,
    n_clusters_per_class=1,
    random_state=42
)

# –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# 2. –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è (–ö–†–ò–¢–ò–ß–ù–û –¥–ª—è KNN!)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 3. KNN Classifier
knn = KNeighborsClassifier(
    n_neighbors=5,           # k=5
    weights='uniform',       # –∞–±–æ 'distance' (weighted)
    metric='euclidean',      # –∞–±–æ 'manhattan', 'minkowski'
    algorithm='auto',        # 'ball_tree', 'kd_tree', 'brute'
    n_jobs=-1               # –ü–∞—Ä–∞–ª–µ–ª—ñ–∑–∞—Ü—ñ—è
)

# 4. –ù–∞–≤—á–∞–Ω–Ω—è (–ø—Ä–æ—Å—Ç–æ –∑–±–µ—Ä—ñ–≥–∞—î –¥–∞–Ω—ñ!)
knn.fit(X_train_scaled, y_train)

# 5. –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
y_pred = knn.predict(X_test_scaled)
y_pred_proba = knn.predict_proba(X_test_scaled)

# 6. –û—Ü—ñ–Ω–∫–∞
print("=== KNN Classifier ===")
print(f"Train Accuracy: {knn.score(X_train_scaled, y_train):.4f}")
print(f"Test Accuracy: {accuracy_score(y_test, y_pred):.4f}")

print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
print("\nConfusion Matrix:")
print(cm)

# 7. –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è decision boundary
def plot_decision_boundary(X, y, model, title):
    h = 0.02  # step size
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))
    
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    
    plt.figure(figsize=(10, 6))
    plt.contourf(xx, yy, Z, alpha=0.3, cmap='viridis')
    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', 
                edgecolors='k', s=50)
    plt.xlabel('Feature 1', fontsize=12)
    plt.ylabel('Feature 2', fontsize=12)
    plt.title(title, fontsize=14, fontweight='bold')
    plt.colorbar()
    plt.tight_layout()
    plt.show()

plot_decision_boundary(X_test_scaled, y_test, knn, 
                      'KNN Decision Boundary (k=5)')
```

### –†–µ–≥—Ä–µ—Å—ñ—è

```python
from sklearn.neighbors import KNeighborsRegressor
from sklearn.datasets import make_regression
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

# –î–∞–Ω—ñ
X, y = make_regression(
    n_samples=200,
    n_features=1,
    noise=10,
    random_state=42
)

# –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# KNN Regressor
knn_reg = KNeighborsRegressor(
    n_neighbors=5,
    weights='distance',      # Weighted averaging
    metric='euclidean'
)

# –ù–∞–≤—á–∞–Ω–Ω—è
knn_reg.fit(X_train, y_train)

# –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
y_pred = knn_reg.predict(X_test)

# –ú–µ—Ç—Ä–∏–∫–∏
print("=== KNN Regressor ===")
print(f"R¬≤: {r2_score(y_test, y_pred):.4f}")
print(f"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.2f}")
print(f"MAE: {mean_absolute_error(y_test, y_pred):.2f}")

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
X_plot = np.linspace(X.min(), X.max(), 300).reshape(-1, 1)
y_plot = knn_reg.predict(X_plot)

plt.figure(figsize=(12, 6))
plt.scatter(X_train, y_train, alpha=0.5, s=30, label='Train', color='blue')
plt.scatter(X_test, y_test, alpha=0.5, s=50, label='Test', color='green')
plt.plot(X_plot, y_plot, color='red', linewidth=2, label='KNN Prediction')
plt.xlabel('X', fontsize=12)
plt.ylabel('y', fontsize=12)
plt.title('KNN Regression (k=5)', fontsize=14, fontweight='bold')
plt.legend(fontsize=11)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

---

## –í–∏–±—ñ—Ä –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ k —á–µ—Ä–µ–∑ Cross-Validation

```python
from sklearn.model_selection import cross_val_score

# –¢–µ—Å—Ç—É—î–º–æ —Ä—ñ–∑–Ω—ñ k
k_values = range(1, 51)
train_scores = []
cv_scores = []

for k in k_values:
    knn = KNeighborsClassifier(n_neighbors=k)
    
    # Train score
    knn.fit(X_train_scaled, y_train)
    train_scores.append(knn.score(X_train_scaled, y_train))
    
    # Cross-validation score
    cv_score = cross_val_score(knn, X_train_scaled, y_train, 
                               cv=5, scoring='accuracy').mean()
    cv_scores.append(cv_score)

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
plt.figure(figsize=(12, 6))
plt.plot(k_values, train_scores, 'o-', linewidth=2, label='Train Score')
plt.plot(k_values, cv_scores, 's-', linewidth=2, label='CV Score')
plt.xlabel('k (Number of Neighbors)', fontsize=12)
plt.ylabel('Accuracy', fontsize=12)
plt.title('KNN: Train vs CV Score for Different k', 
          fontsize=14, fontweight='bold')
plt.legend(fontsize=11)
plt.grid(True, alpha=0.3)
plt.axvline(x=k_values[np.argmax(cv_scores)], 
            color='red', linestyle='--', 
            label=f'Optimal k={k_values[np.argmax(cv_scores)]}')
plt.legend(fontsize=11)
plt.tight_layout()
plt.show()

# –û–ø—Ç–∏–º–∞–ª—å–Ω–∏–π k
optimal_k = k_values[np.argmax(cv_scores)]
print(f"Optimal k: {optimal_k}")
print(f"Best CV Score: {max(cv_scores):.4f}")

# –§—ñ–Ω–∞–ª—å–Ω–∞ –º–æ–¥–µ–ª—å
knn_final = KNeighborsClassifier(n_neighbors=optimal_k)
knn_final.fit(X_train_scaled, y_train)
print(f"Test Score: {knn_final.score(X_test_scaled, y_test):.4f}")
```

---

## Weighted vs Uniform KNN

### Uniform (–∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º)

–í—Å—ñ k —Å—É—Å—ñ–¥—ñ–≤ –º–∞—é—Ç—å **–æ–¥–Ω–∞–∫–æ–≤—É –≤–∞–≥—É**:

$$P(\text{class } c) = \frac{\text{count}(c \text{ in k neighbors})}{k}$$

### Distance-weighted

–ë–ª–∏–∂—á—ñ —Å—É—Å—ñ–¥–∏ –º–∞—é—Ç—å **–±—ñ–ª—å—à—É –≤–∞–≥—É**:

$$w_i = \frac{1}{d_i}$$

$$P(\text{class } c) = \frac{\sum_{i \in c} w_i}{\sum_{i=1}^{k} w_i}$$

### –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è

```python
# Uniform
knn_uniform = KNeighborsClassifier(n_neighbors=5, weights='uniform')
knn_uniform.fit(X_train_scaled, y_train)
score_uniform = knn_uniform.score(X_test_scaled, y_test)

# Weighted
knn_weighted = KNeighborsClassifier(n_neighbors=5, weights='distance')
knn_weighted.fit(X_train_scaled, y_train)
score_weighted = knn_weighted.score(X_test_scaled, y_test)

print(f"Uniform weights:   {score_uniform:.4f}")
print(f"Distance weights:  {score_weighted:.4f}")
```

**–ö–æ–ª–∏ weighted –∫—Ä–∞—â–µ:**
- ‚úÖ –ù–µ—Ä—ñ–≤–Ω–æ–º—ñ—Ä–Ω–∏–π —Ä–æ–∑–ø–æ–¥—ñ–ª –¥–∞–Ω–∏—Ö
- ‚úÖ Clusters —Ä—ñ–∑–Ω–∏—Ö —Ä–æ–∑–º—ñ—Ä—ñ–≤
- ‚úÖ k –¥–æ—Å–∏—Ç—å –≤–µ–ª–∏–∫–∏–π

**–ö–æ–ª–∏ uniform –¥–æ—Å—Ç–∞—Ç–Ω—å–æ:**
- ‚úÖ –†—ñ–≤–Ω–æ–º—ñ—Ä–Ω–∏–π —Ä–æ–∑–ø–æ–¥—ñ–ª
- ‚úÖ –ú–∞–ª–∏–π k
- ‚úÖ –ü—Ä–æ—Å—Ç—ñ—à–∞ –º–æ–¥–µ–ª—å

---

## –ê–ª–≥–æ—Ä–∏—Ç–º–∏ –ø–æ—à—É–∫—É —Å—É—Å—ñ–¥—ñ–≤

### 1. Brute Force

**–Ø–∫ –ø—Ä–∞—Ü—é—î:** –û–±—á–∏—Å–ª—é—î –≤—ñ–¥—Å—Ç–∞–Ω—å –¥–æ **–≤—Å—ñ—Ö** —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∏—Ö –∑—Ä–∞–∑–∫—ñ–≤.

**–°–∫–ª–∞–¥–Ω—ñ—Å—Ç—å:**
- Training: O(1) ‚Äî –ø—Ä–æ—Å—Ç–æ –∑–±–µ—Ä—ñ–≥–∞—î –¥–∞–Ω—ñ
- Prediction: O(n¬∑d) ‚Äî n –∑—Ä–∞–∑–∫—ñ–≤, d –æ–∑–Ω–∞–∫

**–ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏:**
- –ú–∞–ª—ñ –¥–∞—Ç–∞—Å–µ—Ç–∏ (n < 1000)
- –ì–∞—Ä–∞–Ω—Ç–æ–≤–∞–Ω–æ —Ç–æ—á–Ω–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç

### 2. KD-Tree

**–Ø–∫ –ø—Ä–∞—Ü—é—î:** –ë—É–¥—É—î –¥–µ—Ä–µ–≤–æ –¥–ª—è —à–≤–∏–¥–∫–æ–≥–æ –ø–æ—à—É–∫—É –≤ –ø—Ä–æ—Å—Ç–æ—Ä—ñ.

**–°–∫–ª–∞–¥–Ω—ñ—Å—Ç—å:**
- Training: O(n¬∑log(n)¬∑d)
- Prediction: O(log(n)¬∑d)

**–ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏:**
- ‚úÖ –°–µ—Ä–µ–¥–Ω—ñ –¥–∞—Ç–∞—Å–µ—Ç–∏ (1k-100k)
- ‚úÖ –ù–∏–∑—å–∫–æ–≤–∏–º—ñ—Ä–Ω—ñ –¥–∞–Ω—ñ (d < 20)
- ‚ùå –ù–ï –ø—Ä–∞—Ü—é—î –¥–æ–±—Ä–µ –ø—Ä–∏ d > 50

### 3. Ball Tree

**–Ø–∫ –ø—Ä–∞—Ü—é—î:** –Ü–Ω—à–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–µ—Ä–µ–≤–∞, –∫—Ä–∞—â–µ –¥–ª—è high-dimensional.

**–°–∫–ª–∞–¥–Ω—ñ—Å—Ç—å:**
- –°—Ö–æ–∂–∞ –Ω–∞ KD-Tree

**–ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏:**
- ‚úÖ High-dimensional data (d > 20)
- ‚úÖ Non-Euclidean metrics

### –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è

```python
import time

algorithms = ['brute', 'kd_tree', 'ball_tree', 'auto']

for algo in algorithms:
    knn = KNeighborsClassifier(n_neighbors=5, algorithm=algo)
    
    # Training time
    start = time.time()
    knn.fit(X_train_scaled, y_train)
    train_time = time.time() - start
    
    # Prediction time
    start = time.time()
    knn.predict(X_test_scaled)
    pred_time = time.time() - start
    
    print(f"{algo:10s}: Train={train_time:.4f}s, Pred={pred_time:.4f}s")
```

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—è:** –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π **'auto'** ‚Äî sklearn —Å–∞–º –æ–±–µ—Ä–µ –Ω–∞–π–∫—Ä–∞—â–∏–π!

---

## Curse of Dimensionality

### –ü—Ä–æ–±–ª–µ–º–∞

**–í high-dimensional –ø—Ä–æ—Å—Ç–æ—Ä—ñ –≤—Å—ñ —Ç–æ—á–∫–∏ —Å—Ç–∞—é—Ç—å "–¥–∞–ª–µ–∫–∏–º–∏" –æ–¥–Ω–∞ –≤—ñ–¥ –æ–¥–Ω–æ—ó.**

### –ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞

–î–ª—è –æ–¥–∏–Ω–∏—á–Ω–æ–≥–æ –∫—É–±—É –≤ $d$ –≤–∏–º—ñ—Ä–∞—Ö, –æ–±'—î–º –∫—É–ª—ñ —Ä–∞–¥—ñ—É—Å–æ–º 0.5:

$$V_d = \frac{\pi^{d/2}}{\Gamma(d/2 + 1)} \cdot 0.5^d$$

**–†–µ–∑—É–ª—å—Ç–∞—Ç:**
- d=2: V ‚âà 0.785 (78.5% –∫—É–±—É)
- d=10: V ‚âà 0.0025 (0.25% –∫—É–±—É!)
- d=100: V ‚âà 0 (–º–∞–π–∂–µ –Ω—ñ—á–æ–≥–æ!)

**–í–∏—Å–Ω–æ–≤–æ–∫:** –í high dimensions –¥–∞–Ω—ñ —Å—Ç–∞—é—Ç—å **sparse** (—Ä–æ–∑—Ä—ñ–¥–∂–µ–Ω–∏–º–∏).

### –ï—Ñ–µ–∫—Ç –Ω–∞ KNN

```
Low dimensions (d=2):        High dimensions (d=100):
    
–¢–æ—á–∫–∏ –∑–≥—Ä—É–ø–æ–≤–∞–Ω—ñ           –í—Å—ñ —Ç–æ—á–∫–∏ –¥–∞–ª–µ–∫–æ –æ–¥–Ω–∞
–ß—ñ—Ç–∫—ñ clusters             –≤—ñ–¥ –æ–¥–Ω–æ—ó
Nearest neighbor           "Nearest" –Ω–µ –º–∞—î —Å–µ–Ω—Å—É
–º–∞—î —Å–µ–Ω—Å ‚úì                 ‚úó
```

### –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è

```python
# –ï–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç: –≤—ñ–¥—Å—Ç–∞–Ω—å –¥–æ –Ω–∞–π–±–ª–∏–∂—á–æ–≥–æ —Å—É—Å—ñ–¥–∞
dimensions = [2, 5, 10, 20, 50, 100]
nearest_distances = []

for d in dimensions:
    X_random = np.random.rand(1000, d)
    knn = KNeighborsClassifier(n_neighbors=1)
    knn.fit(X_random, np.zeros(1000))
    
    # –í—ñ–¥—Å—Ç–∞–Ω—å –¥–æ –Ω–∞–π–±–ª–∏–∂—á–æ–≥–æ —Å—É—Å—ñ–¥–∞
    distances, _ = knn.kneighbors(X_random[:100])
    nearest_distances.append(distances.mean())

plt.figure(figsize=(10, 6))
plt.plot(dimensions, nearest_distances, 'o-', linewidth=2)
plt.xlabel('Number of Dimensions', fontsize=12)
plt.ylabel('Average Distance to Nearest Neighbor', fontsize=12)
plt.title('Curse of Dimensionality Effect on KNN', 
          fontsize=14, fontweight='bold')
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

### –Ø–∫ –±–æ—Ä–æ—Ç–∏—Å—è?

1. **Dimensionality reduction:**
   - PCA
   - Feature selection
   - Feature engineering

2. **Feature scaling:**
   - StandardScaler
   - MinMaxScaler

3. **Distance metric:**
   - –ï–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—É–π –∑ —Ä—ñ–∑–Ω–∏–º–∏ metrics
   - Cosine –¥–ª—è high-dimensional sparse

4. **–í–∏–∫–æ—Ä–∏—Å—Ç–∞–π —ñ–Ω—à—ñ –∞–ª–≥–æ—Ä–∏—Ç–º–∏:**
   - Tree-based models –º–µ–Ω—à —á—É—Ç–ª–∏–≤—ñ

---

## Feature Scaling –¥–ª—è KNN

### –ß–æ–º—É –∫—Ä–∏—Ç–∏—á–Ω–æ –≤–∞–∂–ª–∏–≤–æ?

KNN –±–∞–∑—É—î—Ç—å—Å—è –Ω–∞ **–≤—ñ–¥—Å—Ç–∞–Ω—è—Ö** ‚Üí –æ–∑–Ω–∞–∫–∏ –∑ –≤–µ–ª–∏–∫–∏–º–∏ –∑–Ω–∞—á–µ–Ω–Ω—è–º–∏ –¥–æ–º—ñ–Ω—É—é—Ç—å!

### –ü—Ä–∏–∫–ª–∞–¥ –ø—Ä–æ–±–ª–µ–º–∏

```
–î–∞–Ω—ñ –ë–ï–ó scaling:
  Age: [25, 30, 35] (range: 10)
  Salary: [30000, 50000, 70000] (range: 40000)

–í—ñ–¥—Å—Ç–∞–Ω—å –º—ñ–∂ [25, 30000] —Ç–∞ [30, 50000]:
d = sqrt((30-25)¬≤ + (50000-30000)¬≤)
  = sqrt(25 + 400000000)
  = sqrt(400000025)
  ‚âà 20000

Age –º–∞–π–∂–µ –Ω–µ –≤–ø–ª–∏–≤–∞—î! Salary –¥–æ–º—ñ–Ω—É—î! ‚úó
```

### –†—ñ—à–µ–Ω–Ω—è: Scaling

```python
from sklearn.preprocessing import StandardScaler, MinMaxScaler

# StandardScaler (–Ω–∞–π–ø–æ–ø—É–ª—è—Ä–Ω—ñ—à–∏–π)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# MinMaxScaler (–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞)
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)
```

### –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –∑/–±–µ–∑ scaling

```python
# –ë–ï–ó scaling
knn_no_scale = KNeighborsClassifier(n_neighbors=5)
knn_no_scale.fit(X_train, y_train)
score_no_scale = knn_no_scale.score(X_test, y_test)

# –ó scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

knn_scaled = KNeighborsClassifier(n_neighbors=5)
knn_scaled.fit(X_train_scaled, y_train)
score_scaled = knn_scaled.score(X_test_scaled, y_test)

print(f"Without scaling: {score_no_scale:.4f}")
print(f"With scaling:    {score_scaled:.4f}")
print(f"Improvement:     {(score_scaled - score_no_scale)*100:.2f}%")
```

**–¢–∏–ø–æ–≤–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:**
```
Without scaling: 0.7200
With scaling:    0.8900
Improvement:     17.00%  ‚Üê –í–µ–ª–∏—á–µ–∑–Ω–∞ —Ä—ñ–∑–Ω–∏—Ü—è!
```

---

## –ü–µ—Ä–µ–≤–∞–≥–∏ —Ç–∞ –Ω–µ–¥–æ–ª—ñ–∫–∏

### –ü–µ—Ä–µ–≤–∞–≥–∏ ‚úì

| –ü–µ—Ä–µ–≤–∞–≥–∞ | –ü–æ—è—Å–Ω–µ–Ω–Ω—è |
|----------|-----------|
| **–ü—Ä–æ—Å—Ç–æ—Ç–∞** | –û–¥–∏–Ω –∑ –Ω–∞–π–ø—Ä–æ—Å—Ç—ñ—à–∏—Ö –∞–ª–≥–æ—Ä–∏—Ç–º—ñ–≤ |
| **–Ü–Ω—Ç—É—ó—Ç–∏–≤–Ω—ñ—Å—Ç—å** | –õ–µ–≥–∫–æ –∑—Ä–æ–∑—É–º—ñ—Ç–∏ —Ç–∞ –ø–æ—è—Å–Ω–∏—Ç–∏ |
| **–£–Ω—ñ–≤–µ—Ä—Å–∞–ª—å–Ω—ñ—Å—Ç—å** | –ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è + —Ä–µ–≥—Ä–µ—Å—ñ—è |
| **–ù–µ–ª—ñ–Ω—ñ–π–Ω—ñ boundaries** | –°–∫–ª–∞–¥–Ω—ñ decision boundaries |
| **No training phase** | Instant "–Ω–∞–≤—á–∞–Ω–Ω—è" |
| **Online learning** | –õ–µ–≥–∫–æ –¥–æ–¥–∞–≤–∞—Ç–∏ –Ω–æ–≤—ñ –¥–∞–Ω—ñ |
| **–ù–µ –ø–æ—Ç—Ä–µ–±—É—î assumptions** | –ü—Ä–∞—Ü—é—î –∑ –±—É–¥—å-—è–∫–∏–º–∏ –¥–∞–Ω–∏–º–∏ |

### –ù–µ–¥–æ–ª—ñ–∫–∏ ‚úó

| –ù–µ–¥–æ–ª—ñ–∫ | –ü–æ—è—Å–Ω–µ–Ω–Ω—è |
|---------|-----------|
| **–ü–æ–≤—ñ–ª—å–Ω–∏–π prediction** | –û–±—á–∏—Å–ª—é—î –≤—ñ–¥—Å—Ç–∞–Ω—å –¥–æ –≤—Å—ñ—Ö —Ç–æ—á–æ–∫ |
| **Curse of dimensionality** | –ü–æ–≥–∞–Ω–æ –ø—Ä–∞—Ü—é—î –ø—Ä–∏ d > 20-50 |
| **Memory-intensive** | –ó–±–µ—Ä—ñ–≥–∞—î –≤—Å—ñ —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ |
| **–ß—É—Ç–ª–∏–≤—ñ—Å—Ç—å –¥–æ scaling** | –ö—Ä–∏—Ç–∏—á–Ω–æ –ø–æ—Ç—Ä–µ–±—É—î –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—ó |
| **Irrelevant features** | –í—Å—ñ –æ–∑–Ω–∞–∫–∏ –≤–ø–ª–∏–≤–∞—é—Ç—å –æ–¥–Ω–∞–∫–æ–≤–æ |
| **Imbalanced data** | Bias –¥–æ majority class |
| **–ù–µ –ø—Ä–∞—Ü—é—î –∑ –≤–µ–ª–∏–∫–∏–º–∏ –¥–∞–Ω–∏–º–∏** | n > 100k –¥—É–∂–µ –ø–æ–≤—ñ–ª—å–Ω–æ |
| **–ù–µ —ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ weights** | –ù–µ–º–∞—î feature importance |

---

## –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –∑ —ñ–Ω—à–∏–º–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–∞–º–∏

### KNN vs Decision Tree

| –ö—Ä–∏—Ç–µ—Ä—ñ–π | KNN | Decision Tree |
|----------|-----|---------------|
| **Training** | O(1) | O(n¬∑log(n)¬∑d) |
| **Prediction** | O(n¬∑d) | O(log(n)) |
| **–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å** | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **Feature scaling** | –ö—Ä–∏—Ç–∏—á–Ω–æ | –ù–µ –ø–æ—Ç—Ä—ñ–±–Ω–∞ |
| **Overfitting** | k –∫–æ–Ω—Ç—Ä–æ–ª—é—î | –õ–µ–≥–∫–æ overfits |
| **–í–∏—Å–æ–∫—ñ —Ä–æ–∑–º—ñ—Ä–Ω–æ—Å—Ç—ñ** | ‚ùå –ü–æ–≥–∞–Ω–æ | ‚úÖ –ü—Ä–∞—Ü—é—î |

### KNN vs Logistic Regression

| –ö—Ä–∏—Ç–µ—Ä—ñ–π | KNN | Logistic Regression |
|----------|-----|---------------------|
| **Assumptions** | –ù–µ–º–∞—î | –õ—ñ–Ω—ñ–π–Ω–∞ —Ä–æ–∑–¥—ñ–ª–∏–º—ñ—Å—Ç—å |
| **Boundaries** | –ù–µ–ª—ñ–Ω—ñ–π–Ω—ñ ‚úì | –õ—ñ–Ω—ñ–π–Ω—ñ |
| **Training** | O(1) | O(n¬∑d) |
| **Prediction** | O(n¬∑d) | O(d) |
| **–í–µ–ª–∏–∫—ñ –¥–∞–Ω—ñ** | ‚ùå –ü–æ–≤—ñ–ª—å–Ω–æ | ‚úÖ –®–≤–∏–¥–∫–æ |
| **–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å** | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |

### KNN vs SVM

| –ö—Ä–∏—Ç–µ—Ä—ñ–π | KNN | SVM |
|----------|-----|-----|
| **Training** | O(1) | O(n¬≤) to O(n¬≥) |
| **Prediction** | O(n¬∑d) | O(n_sv¬∑d) |
| **Kernel trick** | ‚ùå –ù–µ–º–∞—î | ‚úÖ –¢–∞–∫ |
| **Margin optimization** | ‚ùå –ù–µ–º–∞—î | ‚úÖ –¢–∞–∫ |
| **–í–∏—Å–æ–∫—ñ —Ä–æ–∑–º—ñ—Ä–Ω–æ—Å—Ç—ñ** | ‚ùå –ü–æ–≥–∞–Ω–æ | ‚úÖ –ü—Ä–∞—Ü—é—î |
| **–ü—Ä–æ—Å—Ç–æ—Ç–∞** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |

---

## –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ KNN

### –Ü–¥–µ–∞–ª—å–Ω–æ –ø—ñ–¥—Ö–æ–¥–∏—Ç—å ‚úì

- **–ú–∞–ª—ñ/—Å–µ—Ä–µ–¥–Ω—ñ –¥–∞—Ç–∞—Å–µ—Ç–∏** (n < 10,000)
- **–ù–∏–∑—å–∫–æ–≤–∏–º—ñ—Ä–Ω—ñ –¥–∞–Ω—ñ** (d < 20)
- –ù–µ–ª—ñ–Ω—ñ–π–Ω—ñ decision boundaries
- –ü–æ—Ç—Ä—ñ–±–µ–Ω **—à–≤–∏–¥–∫–∏–π baseline**
- **–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å** –≤–∞–∂–ª–∏–≤–∞
- –î–∞–Ω—ñ –¥–æ–±—Ä–µ –º–∞—Å—à—Ç–∞–±–æ–≤–∞–Ω—ñ
- Online learning (–¥–æ–¥–∞–≤–∞–Ω–Ω—è –Ω–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö)

### –ö—Ä–∞—â–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ —ñ–Ω—à–µ ‚úó

- **–í–µ–ª–∏–∫—ñ –¥–∞—Ç–∞—Å–µ—Ç–∏** (n > 100k) ‚Üí Random Forest, XGBoost, Linear Models
- **High-dimensional** (d > 50) ‚Üí Tree-based, SVM with kernel
- –ü–æ—Ç—Ä—ñ–±–Ω–∞ **—à–≤–∏–¥–∫—ñ—Å—Ç—å inference** ‚Üí Linear Models, Tree-based
- –ë–∞–≥–∞—Ç–æ irrelevant features ‚Üí Tree-based (–∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∞ feature selection)
- –î–∞–Ω—ñ –Ω–µ –Ω–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω—ñ —ñ –Ω–µ –º–æ–∂–Ω–∞ –Ω–æ—Ä–º–∞–ª—ñ–∑—É–≤–∞—Ç–∏ ‚Üí Tree-based

---

## –ü—Ä–∞–∫—Ç–∏—á–Ω—ñ –ø–æ—Ä–∞–¥–∏ üí°

1. **–ó–ê–í–ñ–î–ò –Ω–æ—Ä–º–∞–ª—ñ–∑—É–π –¥–∞–Ω—ñ** ‚Äî StandardScaler –ø–µ—Ä–µ–¥ KNN
2. **–ü–æ—á–Ω–∏ –∑ k=‚àön** ‚Äî –ø–æ—Ç—ñ–º tuning —á–µ—Ä–µ–∑ CV
3. **–ù–µ–ø–∞—Ä–Ω–µ k** –¥–ª—è –±—ñ–Ω–∞—Ä–Ω–æ—ó –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó
4. **weights='distance'** —á–∞—Å—Ç–æ –∫—Ä–∞—â–µ –∑–∞ 'uniform'
5. **algorithm='auto'** ‚Äî sklearn –æ–±–µ—Ä–µ –Ω–∞–π–∫—Ä–∞—â–∏–π
6. **Cross-validation** –¥–ª—è –≤–∏–±–æ—Ä—É k ‚Äî –Ω–µ –≥–∞–¥–∞–π!
7. **Feature selection** ‚Äî –≤–∏–¥–∞–ª–∏ irrelevant features
8. **–í—ñ–∑—É–∞–ª—ñ–∑—É–π decision boundary** ‚Äî –∑—Ä–æ–∑—É–º—ñ–π –º–æ–¥–µ–ª—å
9. **–û–±–º–µ–∂ d < 20** ‚Äî —ñ–Ω–∞–∫—à–µ curse of dimensionality
10. **–ü–æ—Ä—ñ–≤–Ω—è–π –∑ —ñ–Ω—à–∏–º–∏** ‚Äî KNN —á–∞—Å—Ç–æ baseline

---

## –†–µ–∞–ª—å–Ω–∏–π –ø—Ä–∏–∫–ª–∞–¥: –î—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –¥—ñ–∞–±–µ—Ç—É

```python
import pandas as pd
import numpy as np
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score

# –°–∏–Ω—Ç–µ—Ç–∏—á–Ω—ñ –¥–∞–Ω—ñ
np.random.seed(42)
n_samples = 1000

data = {
    'glucose': np.random.randint(70, 200, n_samples),
    'bmi': np.random.uniform(18, 45, n_samples),
    'age': np.random.randint(21, 81, n_samples),
    'blood_pressure': np.random.randint(60, 120, n_samples),
}

# –°–∏–º—É–ª—é—î–º–æ –¥—ñ–∞–±–µ—Ç
diabetes_prob = (
    (data['glucose'] > 140) * 0.4 +
    (data['bmi'] > 30) * 0.3 +
    (data['age'] > 50) * 0.2 +
    np.random.uniform(0, 0.1, n_samples)
)
data['diabetes'] = (diabetes_prob > 0.5).astype(int)

df = pd.DataFrame(data)

X = df.drop('diabetes', axis=1)
y = df['diabetes']

print("="*70)
print("KNN FOR DIABETES PREDICTION")
print("="*70)
print(f"Dataset: {X.shape[0]} samples, {X.shape[1]} features")
print(f"Diabetes rate: {y.mean():.2%}")

# –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Scaling (–ö–†–ò–¢–ò–ß–ù–û!)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 1. –ë–∞–∑–æ–≤–∞ –º–æ–¥–µ–ª—å (k=5)
print("\n" + "="*70)
print("1. BASELINE KNN (k=5)")
print("="*70)

knn_base = KNeighborsClassifier(n_neighbors=5)
knn_base.fit(X_train_scaled, y_train)

y_pred = knn_base.predict(X_test_scaled)
y_pred_proba = knn_base.predict_proba(X_test_scaled)[:, 1]

print(f"Train Accuracy: {knn_base.score(X_train_scaled, y_train):.4f}")
print(f"Test Accuracy: {knn_base.score(X_test_scaled, y_test):.4f}")
print(f"ROC-AUC: {roc_auc_score(y_test, y_pred_proba):.4f}")

# 2. –ü—ñ–¥–±—ñ—Ä –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ k
print("\n" + "="*70)
print("2. FINDING OPTIMAL k")
print("="*70)

k_values = range(1, 51, 2)  # –ù–µ–ø–∞—Ä–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è
cv_scores = []

for k in k_values:
    knn = KNeighborsClassifier(n_neighbors=k)
    scores = cross_val_score(knn, X_train_scaled, y_train, cv=5)
    cv_scores.append(scores.mean())

optimal_k = k_values[np.argmax(cv_scores)]
print(f"Optimal k: {optimal_k}")
print(f"Best CV Score: {max(cv_scores):.4f}")

# 3. –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è uniform vs weighted
print("\n" + "="*70)
print("3. UNIFORM vs WEIGHTED")
print("="*70)

knn_uniform = KNeighborsClassifier(n_neighbors=optimal_k, weights='uniform')
knn_uniform.fit(X_train_scaled, y_train)
score_uniform = knn_uniform.score(X_test_scaled, y_test)

knn_weighted = KNeighborsClassifier(n_neighbors=optimal_k, weights='distance')
knn_weighted.fit(X_train_scaled, y_train)
score_weighted = knn_weighted.score(X_test_scaled, y_test)

print(f"Uniform weights:   {score_uniform:.4f}")
print(f"Distance weights:  {score_weighted:.4f}")
print(f"Better: {'Weighted' if score_weighted > score_uniform else 'Uniform'}")

# 4. Grid Search –¥–ª—è –≤—Å—ñ—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤
print("\n" + "="*70)
print("4. GRID SEARCH CV")
print("="*70)

param_grid = {
    'n_neighbors': [3, 5, 7, 9, 11, 15, 21],
    'weights': ['uniform', 'distance'],
    'metric': ['euclidean', 'manhattan', 'minkowski']
}

grid_search = GridSearchCV(
    KNeighborsClassifier(),
    param_grid,
    cv=5,
    scoring='roc_auc',
    n_jobs=-1,
    verbose=0
)

grid_search.fit(X_train_scaled, y_train)

print("Best parameters:")
print(grid_search.best_params_)
print(f"Best CV ROC-AUC: {grid_search.best_score_:.4f}")

# 5. –§—ñ–Ω–∞–ª—å–Ω–∞ –º–æ–¥–µ–ª—å
print("\n" + "="*70)
print("5. FINAL MODEL EVALUATION")
print("="*70)

best_knn = grid_search.best_estimator_
y_pred_final = best_knn.predict(X_test_scaled)
y_pred_proba_final = best_knn.predict_proba(X_test_scaled)[:, 1]

print(f"Test Accuracy: {best_knn.score(X_test_scaled, y_test):.4f}")
print(f"Test ROC-AUC: {roc_auc_score(y_test, y_pred_proba_final):.4f}")

print("\nClassification Report:")
print(classification_report(y_test, y_pred_final, 
                          target_names=['No Diabetes', 'Diabetes']))

print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred_final))

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó
fig, axes = plt.subplots(2, 2, figsize=(16, 12))

# 1. k vs CV Score
axes[0, 0].plot(k_values, cv_scores, 'o-', linewidth=2)
axes[0, 0].axvline(x=optimal_k, color='red', linestyle='--', 
                   label=f'Optimal k={optimal_k}')
axes[0, 0].set_xlabel('k (Number of Neighbors)', fontsize=12)
axes[0, 0].set_ylabel('Cross-Validation Score', fontsize=12)
axes[0, 0].set_title('CV Score vs k', fontsize=14, fontweight='bold')
axes[0, 0].legend(fontsize=11)
axes[0, 0].grid(True, alpha=0.3)

# 2. Feature Importance (—á–µ—Ä–µ–∑ perturbation)
from sklearn.inspection import permutation_importance

perm_importance = permutation_importance(
    best_knn, X_test_scaled, y_test, n_repeats=10, random_state=42
)

sorted_idx = perm_importance.importances_mean.argsort()[::-1]
axes[0, 1].barh(range(len(sorted_idx)), 
                perm_importance.importances_mean[sorted_idx])
axes[0, 1].set_yticks(range(len(sorted_idx)))
axes[0, 1].set_yticklabels([X.columns[i] for i in sorted_idx])
axes[0, 1].set_xlabel('Permutation Importance', fontsize=12)
axes[0, 1].set_title('Feature Importance', fontsize=14, fontweight='bold')
axes[0, 1].grid(True, alpha=0.3, axis='x')

# 3. ROC Curve
from sklearn.metrics import roc_curve

fpr, tpr, _ = roc_curve(y_test, y_pred_proba_final)
auc = roc_auc_score(y_test, y_pred_proba_final)

axes[1, 0].plot(fpr, tpr, linewidth=2, label=f'KNN (AUC={auc:.3f})')
axes[1, 0].plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random')
axes[1, 0].set_xlabel('False Positive Rate', fontsize=12)
axes[1, 0].set_ylabel('True Positive Rate', fontsize=12)
axes[1, 0].set_title('ROC Curve', fontsize=14, fontweight='bold')
axes[1, 0].legend(fontsize=11)
axes[1, 0].grid(True, alpha=0.3)

# 4. Prediction Distribution
axes[1, 1].hist(y_pred_proba_final[y_test == 0], bins=20, alpha=0.6, 
                label='No Diabetes', color='blue', edgecolor='black')
axes[1, 1].hist(y_pred_proba_final[y_test == 1], bins=20, alpha=0.6, 
                label='Diabetes', color='red', edgecolor='black')
axes[1, 1].set_xlabel('Predicted Probability', fontsize=12)
axes[1, 1].set_ylabel('Frequency', fontsize=12)
axes[1, 1].set_title('Prediction Distribution', fontsize=14, fontweight='bold')
axes[1, 1].legend(fontsize=11)
axes[1, 1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("\n" + "="*70)
print("ANALYSIS COMPLETE")
print("="*70)
```

---

## –ü–æ—à–∏—Ä–µ–Ω—ñ –ø–æ–º–∏–ª–∫–∏ ‚ùå

### 1. –ó–∞–±—É—Ç–∏ –Ω–æ—Ä–º–∞–ª—ñ–∑—É–≤–∞—Ç–∏ –¥–∞–Ω—ñ

```python
# ‚ùå –ö–†–ò–¢–ò–ß–ù–ê –ü–û–ú–ò–õ–ö–ê
knn = KNeighborsClassifier()
knn.fit(X_train, y_train)  # –ë–ï–ó scaling!

# ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
knn.fit(X_train_scaled, y_train)
```

### 2. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –ø–∞—Ä–Ω–µ k –¥–ª—è –±—ñ–Ω–∞—Ä–Ω–æ—ó –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó

```python
# ‚ùå –ú–û–ñ–ï –ë–£–¢–ò TIE
knn = KNeighborsClassifier(n_neighbors=4)

# ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û (–Ω–µ–ø–∞—Ä–Ω–µ)
knn = KNeighborsClassifier(n_neighbors=5)
```

### 3. –ù–µ –ø—ñ–¥–±–∏—Ä–∞—Ç–∏ k —á–µ—Ä–µ–∑ CV

```python
# ‚ùå –ü–†–û–°–¢–û –í–ó–Ø–¢–ò k=5
knn = KNeighborsClassifier(n_neighbors=5)

# ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û (–ø—ñ–¥—ñ–±—Ä–∞—Ç–∏ —á–µ—Ä–µ–∑ CV)
k_values = range(1, 31, 2)
cv_scores = [cross_val_score(KNeighborsClassifier(n_neighbors=k), 
                              X_train, y_train, cv=5).mean() 
             for k in k_values]
optimal_k = k_values[np.argmax(cv_scores)]
```

### 4. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ KNN –Ω–∞ –≤–µ–ª–∏–∫–∏—Ö –¥–∞–Ω–∏—Ö

```python
# ‚ùå –î–£–ñ–ï –ü–û–í–Ü–õ–¨–ù–û
# X_train –º–∞—î 1,000,000 –∑—Ä–∞–∑–∫—ñ–≤
knn = KNeighborsClassifier()
knn.fit(X_train, y_train)  # Training —à–≤–∏–¥–∫–æ (O(1))
knn.predict(X_test)         # Prediction –î–£–ñ–ï –ü–û–í–Ü–õ–¨–ù–û (O(n¬∑d))

# ‚úÖ –í–ò–ö–û–†–ò–°–¢–ê–ô –Ü–ù–®–ò–ô –ê–õ–ì–û–†–ò–¢–ú
# Random Forest, XGBoost, Logistic Regression
```

### 5. –ù–µ –≤–∏–¥–∞–ª—è—Ç–∏ irrelevant features

```python
# KNN –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î –í–°–Ü –æ–∑–Ω–∞–∫–∏ –æ–¥–Ω–∞–∫–æ–≤–æ
# Irrelevant features –¥–æ–¥–∞—é—Ç—å noise!

# ‚úÖ –ó–†–û–ë–ò FEATURE SELECTION
from sklearn.feature_selection import SelectKBest, f_classif

selector = SelectKBest(f_classif, k=10)
X_selected = selector.fit_transform(X, y)
```

---

## –ü–æ–≤'—è–∑–∞–Ω—ñ —Ç–µ–º–∏

- [[04_SVM]] ‚Äî —ñ–Ω—à–∏–π instance-based –ø—ñ–¥—Ö—ñ–¥
- [[01_Decision_Trees]] ‚Äî –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ –¥–ª—è –Ω–µ–ª—ñ–Ω—ñ–π–Ω–∏—Ö boundaries
- [[Feature_Scaling]] ‚Äî –∫—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è KNN
- [[Cross_Validation]] ‚Äî –≤–∏–±—ñ—Ä k
- [[Distance_Metrics]] ‚Äî —Ä—ñ–∑–Ω—ñ —Å–ø–æ—Å–æ–±–∏ –≤–∏–º—ñ—Ä—é–≤–∞–Ω–Ω—è –≤—ñ–¥—Å—Ç–∞–Ω—ñ

## –†–µ—Å—É—Ä—Å–∏

- [Scikit-learn: KNN](https://scikit-learn.org/stable/modules/neighbors.html)
- [KNN Algorithm Explained](https://towardsdatascience.com/machine-learning-basics-with-the-k-nearest-neighbors-algorithm-6a6e71d01761)
- [StatQuest: KNN](https://www.youtube.com/watch?v=HVXime0nQeI)

---

## –ö–ª—é—á–æ–≤—ñ –≤–∏—Å–Ω–æ–≤–∫–∏

> K-Nearest Neighbors –∫–ª–∞—Å–∏—Ñ—ñ–∫—É—î –æ–±'—î–∫—Ç–∏ –Ω–∞ –æ—Å–Ω–æ–≤—ñ k –Ω–∞–π–±–ª–∏–∂—á–∏—Ö —Å—É—Å—ñ–¥—ñ–≤ —É –ø—Ä–æ—Å—Ç–æ—Ä—ñ –æ–∑–Ω–∞–∫ —á–µ—Ä–µ–∑ –≥–æ–ª–æ—Å—É–≤–∞–Ω–Ω—è –±—ñ–ª—å—à–æ—Å—Ç—ñ.

**–û—Å–Ω–æ–≤–Ω—ñ –ø—Ä–∏–Ω—Ü–∏–ø–∏:**
- **Lazy learning** ‚Äî –Ω–µ —Ç—Ä–µ–Ω—É—î—Ç—å—Å—è, –ø—Ä–æ—Å—Ç–æ –∑–±–µ—Ä—ñ–≥–∞—î –¥–∞–Ω—ñ
- **Instance-based** ‚Äî —Ä—ñ—à–µ–Ω–Ω—è –±–∞–∑—É—î—Ç—å—Å—è –Ω–∞ —Å—Ö–æ–∂–æ—Å—Ç—ñ
- **Distance-based** ‚Äî –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î –º–µ—Ç—Ä–∏–∫–∏ –≤—ñ–¥—Å—Ç–∞–Ω—ñ
- **Non-parametric** ‚Äî –Ω–µ —Ä–æ–±–∏—Ç—å assumptions –ø—Ä–æ —Ä–æ–∑–ø–æ–¥—ñ–ª –¥–∞–Ω–∏—Ö

**–§–æ—Ä–º—É–ª–∞ (–∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è):**
$$\hat{y} = \text{mode}\{y_1, y_2, ..., y_k\} \text{ –¥–ª—è k –Ω–∞–π–±–ª–∏–∂—á–∏—Ö —Å—É—Å—ñ–¥—ñ–≤}$$

**–§–æ—Ä–º—É–ª–∞ (—Ä–µ–≥—Ä–µ—Å—ñ—è):**
$$\hat{y} = \frac{1}{k} \sum_{i=1}^{k} y_i$$

**–ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏:**
- –ú–∞–ª—ñ –¥–∞–Ω—ñ + –Ω–µ–ª—ñ–Ω—ñ–π–Ω—ñ boundaries + —à–≤–∏–¥–∫–∏–π baseline = KNN ‚úì
- –í–µ–ª–∏–∫—ñ –¥–∞–Ω—ñ –∞–±–æ high-dimensional ‚Üí —ñ–Ω—à—ñ –∞–ª–≥–æ—Ä–∏—Ç–º–∏ ‚úì

**–ö–†–ò–¢–ò–ß–ù–û –≤–∞–∂–ª–∏–≤–æ:**
- –ó–∞–≤–∂–¥–∏ –Ω–æ—Ä–º–∞–ª—ñ–∑—É–π –¥–∞–Ω—ñ (StandardScaler)
- –ü—ñ–¥–±–∏—Ä–∞–π k —á–µ—Ä–µ–∑ cross-validation
- –û–±–º–µ–∂ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –æ–∑–Ω–∞–∫ (d < 20 —ñ–¥–µ–∞–ª—å–Ω–æ)
- –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π –Ω–∞ –º–∞–ª–∏—Ö/—Å–µ—Ä–µ–¥–Ω—ñ—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö

**Trade-offs:**
- –ü—Ä–æ—Å—Ç–æ—Ç–∞ vs –®–≤–∏–¥–∫—ñ—Å—Ç—å prediction
- –ì–Ω—É—á–∫—ñ—Å—Ç—å vs Curse of dimensionality
- No training vs –ü–æ–≤—ñ–ª—å–Ω–∏–π inference

---

#ml #supervised-learning #classification #regression #knn #k-nearest-neighbors #instance-based #lazy-learning #distance-metrics
