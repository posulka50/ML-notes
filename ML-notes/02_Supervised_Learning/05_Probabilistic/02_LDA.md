# LDA (Linear Discriminant Analysis)

## –©–æ —Ü–µ?

**Linear Discriminant Analysis (LDA)** ‚Äî —Ü–µ –∞–ª–≥–æ—Ä–∏—Ç–º –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó, —è–∫–∏–π –∑–Ω–∞—Ö–æ–¥–∏—Ç—å **–ª—ñ–Ω—ñ–π–Ω—É –∫–æ–º–±—ñ–Ω–∞—Ü—ñ—é –æ–∑–Ω–∞–∫**, —â–æ –Ω–∞–π–∫—Ä–∞—â–µ —Ä–æ–∑–¥—ñ–ª—è—î –∫–ª–∞—Å–∏, –º–∞–∫—Å–∏–º—ñ–∑—É—é—á–∏ –≤—ñ–¥—Å—Ç–∞–Ω—å –º—ñ–∂ –∫–ª–∞—Å–∞–º–∏ —Ç–∞ –º—ñ–Ω—ñ–º—ñ–∑—É—é—á–∏ variance –≤—Å–µ—Ä–µ–¥–∏–Ω—ñ –∫–ª–∞—Å—ñ–≤.

**–ì–æ–ª–æ–≤–Ω–∞ —ñ–¥–µ—è:** –ø—Ä–æ–µ–∫—Ç—É–≤–∞—Ç–∏ –¥–∞–Ω—ñ –Ω–∞ –ø—Ä–æ—Å—Ç—ñ—Ä –º–µ–Ω—à–æ—ó —Ä–æ–∑–º—ñ—Ä–Ω–æ—Å—Ç—ñ —Ç–∞–∫–∏–º —á–∏–Ω–æ–º, —â–æ–± –∫–ª–∞—Å–∏ –±—É–ª–∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Ä–æ–∑–¥—ñ–ª–µ–Ω—ñ.

## –ù–∞–≤—ñ—â–æ –ø–æ—Ç—Ä—ñ–±–µ–Ω?

- üìä **Dimensionality reduction** + –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è
- üéØ **Probabilistic** ‚Äî –¥–∞—î –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ –∫–ª–∞—Å—ñ–≤
- ‚ö° **–®–≤–∏–¥–∫—ñ—Å—Ç—å** ‚Äî –¥—É–∂–µ —à–≤–∏–¥–∫–∏–π
- üí° **–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å** ‚Äî –ª—ñ–Ω—ñ–π–Ω—ñ decision boundaries
- üîß **Assumes Gaussian** ‚Äî –ø—Ä–∞—Ü—é—î –¥–æ–±—Ä–µ —è–∫—â–æ –ø—Ä–∏–ø—É—â–µ–Ω–Ω—è –≤–∏–∫–æ–Ω—É—é—Ç—å—Å—è
- üìà **Feature extraction** ‚Äî LDA —è–∫ preprocessing

## –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏?

**–ü–æ—Ç—Ä—ñ–±–Ω–æ:**
- –ö–ª–∞—Å–∏ **Gaussian —Ä–æ–∑–ø–æ–¥—ñ–ª–µ–Ω—ñ**
- **Shared covariance** –º—ñ–∂ –∫–ª–∞—Å–∞–º–∏ (–æ–¥–Ω–∞–∫–æ–≤–∞ –¥–ª—è –≤—Å—ñ—Ö –∫–ª–∞—Å—ñ–≤)
- –ü–æ—Ç—Ä—ñ–±–Ω–∞ **dimensionality reduction**
- **Probabilistic predictions**
- –®–≤–∏–¥–∫—ñ—Å—Ç—å –≤–∞–∂–ª–∏–≤–∞
- –ú–∞–ª—ñ/—Å–µ—Ä–µ–¥–Ω—ñ –¥–∞—Ç–∞—Å–µ—Ç–∏

**–ù–µ –ø–æ—Ç—Ä—ñ–±–Ω–æ:**
- **–ù–µ–ª—ñ–Ω—ñ–π–Ω—ñ boundaries** ‚Üí QDA, Kernel methods
- –†—ñ–∑–Ω—ñ covariances –º—ñ–∂ –∫–ª–∞—Å–∞–º–∏ ‚Üí QDA
- –ü–æ—Ç—Ä—ñ–±–Ω–∞ **–º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å** ‚Üí Tree-based, SVM, Neural Networks
- **Multimodal distributions** ‚Üí GMM, Tree-based

---

## –ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞ LDA

### –ü—Ä–∏–ø—É—â–µ–Ω–Ω—è

1. –û–∑–Ω–∞–∫–∏ —Ä–æ–∑–ø–æ–¥—ñ–ª–µ–Ω—ñ **–Ω–æ—Ä–º–∞–ª—å–Ω–æ** –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –∫–ª–∞—Å—É
2. –ö–ª–∞—Å–∏ –º–∞—é—Ç—å **–æ–¥–Ω–∞–∫–æ–≤—É covariance matrix** (shared Œ£)
3. Prior probabilities –≤—ñ–¥–æ–º—ñ –∞–±–æ –æ—Ü—ñ–Ω—é—é—Ç—å—Å—è –∑ –¥–∞–Ω–∏—Ö

### Notation

- $\mu_k$ ‚Äî mean vector –¥–ª—è –∫–ª–∞—Å—É $k$
- $\Sigma$ ‚Äî shared covariance matrix (–æ–¥–Ω–∞–∫–æ–≤–∞ –¥–ª—è –≤—Å—ñ—Ö –∫–ª–∞—Å—ñ–≤)
- $\pi_k$ ‚Äî prior probability –∫–ª–∞—Å—É $k$

### Discriminant Function

**–î–ª—è –∫–ª–∞—Å—É $k$:**

$$\delta_k(x) = x^T \Sigma^{-1} \mu_k - \frac{1}{2} \mu_k^T \Sigma^{-1} \mu_k + \log(\pi_k)$$

**–ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è:**

$$\hat{y} = \arg\max_k \delta_k(x)$$

–û–±–∏—Ä–∞—î–º–æ –∫–ª–∞—Å –∑ –Ω–∞–π–≤–∏—â–∏–º discriminant function value.

### Decision Boundary (2 –∫–ª–∞—Å–∏)

Decision boundary –º—ñ–∂ –∫–ª–∞—Å–∞–º–∏ 1 —Ç–∞ 2:

$$\delta_1(x) = \delta_2(x)$$

–¶–µ –¥–∞—î **–ª—ñ–Ω—ñ–π–Ω—É** decision boundary (–≥—ñ–ø–µ—Ä–ø–ª–æ—â–∏–Ω—É).

---

## –í—ñ–∑—É–∞–ª—å–Ω–∞ —ñ–Ω—Ç—É—ó—Ü—ñ—è

### 2D –ø—Ä–∏–∫–ª–∞–¥

\`\`\`
Original 2D space:

    Feature 2
        |
        |    Class A (‚Ä¢)
        |  ‚Ä¢  ‚Ä¢  ‚Ä¢
        |    ‚Ä¢  ‚Ä¢
        |---------- Feature 1
        |  √ó  √ó
        |√ó  √ó  √ó
        |  Class B (√ó)

LDA –ø—Ä–æ–µ–∫—Ü—ñ—è –Ω–∞ 1D:

    ‚îÄ‚Ä¢‚îÄ‚Ä¢‚îÄ‚Ä¢‚îÄ‚îÄ‚îÄ‚îÄ√ó‚îÄ√ó‚îÄ√ó‚îÄ‚Üí LDA axis
    
–ö–ª–∞—Å–∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Ä–æ–∑–¥—ñ–ª–µ–Ω—ñ!
\`\`\`

### –©–æ —Ä–æ–±–∏—Ç—å LDA?

1. **Maximize between-class variance** ‚Äî –≤—ñ–¥–¥–∞–ª—è—î —Ü–µ–Ω—Ç—Ä–∏ –∫–ª–∞—Å—ñ–≤
2. **Minimize within-class variance** ‚Äî —Ä–æ–±–∏—Ç—å –∫–ª–∞—Å–∏ –∫–æ–º–ø–∞–∫—Ç–Ω–∏–º–∏
3. **–õ—ñ–Ω—ñ–π–Ω–∞ –ø—Ä–æ–µ–∫—Ü—ñ—è** ‚Äî –∑–Ω–∞—Ö–æ–¥–∏—Ç—å –æ–ø—Ç–∏–º–∞–ª—å–Ω—É –≤—ñ—Å—å

---

## LDA —è–∫ Dimensionality Reduction

### –§–æ—Ä–º—É–ª–∞

**–ó–Ω–∞—Ö–æ–¥–∏–º–æ –Ω–∞–ø—Ä—è–º–æ–∫ $w$, —â–æ –º–∞–∫—Å–∏–º—ñ–∑—É—î:**

$$J(w) = \frac{w^T S_B w}{w^T S_W w}$$

–¥–µ:
- $S_B$ ‚Äî between-class scatter matrix
- $S_W$ ‚Äî within-class scatter matrix

### Between-class Scatter

$$S_B = \sum_{k=1}^{K} n_k (\mu_k - \mu)(\mu_k - \mu)^T$$

–¥–µ $\mu$ ‚Äî –∑–∞–≥–∞–ª—å–Ω–∏–π mean.

### Within-class Scatter

$$S_W = \sum_{k=1}^{K} \sum_{x \in C_k} (x - \mu_k)(x - \mu_k)^T$$

### Optimal Direction

**–†–æ–∑–≤'—è–∑—É—î–º–æ eigenvalue problem:**

$$S_W^{-1} S_B w = \lambda w$$

**LDA –ø—Ä–æ–µ–∫—Ü—ñ—è:** top $K-1$ eigenvectors.

---

## –ö–æ–¥ (Python + scikit-learn)

### –ë–∞–∑–æ–≤–∏–π –ø—Ä–∏–∫–ª–∞–¥

\`\`\`python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

# 1. –î–∞–Ω—ñ
iris = load_iris()
X = iris.data
y = iris.target

# –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# 2. LDA
lda = LinearDiscriminantAnalysis()

# 3. –ù–∞–≤—á–∞–Ω–Ω—è
lda.fit(X_train, y_train)

# 4. –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
y_pred = lda.predict(X_test)
y_pred_proba = lda.predict_proba(X_test)

# 5. –û—Ü—ñ–Ω–∫–∞
print("=== Linear Discriminant Analysis ===")
print(f"Train Accuracy: {lda.score(X_train, y_train):.4f}")
print(f"Test Accuracy: {accuracy_score(y_test, y_pred):.4f}")

print("\nClassification Report:")
print(classification_report(y_test, y_pred, target_names=iris.target_names))

# 6. –ü–∞—Ä–∞–º–µ—Ç—Ä–∏
print("\n=== Model Parameters ===")
print(f"Priors: {lda.priors_}")
print(f"Means shape: {lda.means_.shape}")
print(f"Covariance shape: {lda.covariance_.shape}")
\`\`\`

### LDA –¥–ª—è Dimensionality Reduction

\`\`\`python
# LDA –∑ n_components (dimensionality reduction)
lda_2d = LinearDiscriminantAnalysis(n_components=2)
X_lda = lda_2d.fit_transform(X, y)

print(f"\nOriginal shape: {X.shape}")
print(f"LDA shape: {X_lda.shape}")
print(f"Explained variance ratio: {lda_2d.explained_variance_ratio_}")

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
plt.figure(figsize=(10, 6))
colors = ['red', 'green', 'blue']
for i, color in enumerate(colors):
    plt.scatter(X_lda[y == i, 0], X_lda[y == i, 1],
               alpha=0.7, color=color, label=iris.target_names[i])
plt.xlabel(f'LD1 ({lda_2d.explained_variance_ratio_[0]:.2%})', fontsize=12)
plt.ylabel(f'LD2 ({lda_2d.explained_variance_ratio_[1]:.2%})', fontsize=12)
plt.title('LDA Projection to 2D', fontsize=14, fontweight='bold')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
\`\`\`

---

## –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –∑ PCA

### LDA vs PCA

\`\`\`python
from sklearn.decomposition import PCA

# PCA (unsupervised)
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

# LDA (supervised)
lda = LinearDiscriminantAnalysis(n_components=2)
X_lda = lda.fit_transform(X, y)

fig, axes = plt.subplots(1, 2, figsize=(16, 6))

# PCA
for i, color in enumerate(colors):
    axes[0].scatter(X_pca[y == i, 0], X_pca[y == i, 1],
                   alpha=0.7, color=color, label=iris.target_names[i])
axes[0].set_title('PCA (Unsupervised)', fontsize=14, fontweight='bold')
axes[0].set_xlabel('PC1')
axes[0].set_ylabel('PC2')
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# LDA
for i, color in enumerate(colors):
    axes[1].scatter(X_lda[y == i, 0], X_lda[y == i, 1],
                   alpha=0.7, color=color, label=iris.target_names[i])
axes[1].set_title('LDA (Supervised)', fontsize=14, fontweight='bold')
axes[1].set_xlabel('LD1')
axes[1].set_ylabel('LD2')
axes[1].legend()
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
\`\`\`

**–†—ñ–∑–Ω–∏—Ü—è:**
- **PCA:** –º–∞–∫—Å–∏–º—ñ–∑—É—î variance (unsupervised)
- **LDA:** –º–∞–∫—Å–∏–º—ñ–∑—É—î class separation (supervised)

---

## –ü–µ—Ä–µ–≤–∞–≥–∏ —Ç–∞ –Ω–µ–¥–æ–ª—ñ–∫–∏

### –ü–µ—Ä–µ–≤–∞–≥–∏ ‚úì

| –ü–µ—Ä–µ–≤–∞–≥–∞ | –ü–æ—è—Å–Ω–µ–Ω–Ω—è |
|----------|-----------|
| **–®–≤–∏–¥–∫—ñ—Å—Ç—å** | –î—É–∂–µ —à–≤–∏–¥–∫–∏–π |
| **Probabilistic** | –î–∞—î –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ |
| **Dimensionality reduction** | Preprocessing –¥–ª—è —ñ–Ω—à–∏—Ö –º–æ–¥–µ–ª–µ–π |
| **–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å** | –õ—ñ–Ω—ñ–π–Ω—ñ boundaries |
| **–ú–∞–ª—ñ –¥–∞–Ω—ñ** | –ü—Ä–∞—Ü—é—î –¥–æ–±—Ä–µ |

### –ù–µ–¥–æ–ª—ñ–∫–∏ ‚úó

| –ù–µ–¥–æ–ª—ñ–∫ | –ü–æ—è—Å–Ω–µ–Ω–Ω—è |
|---------|-----------|
| **–ü—Ä–∏–ø—É—â–µ–Ω–Ω—è** | Gaussian + shared covariance |
| **–õ—ñ–Ω—ñ–π–Ω—ñ boundaries** | –ù–µ –ø—Ä–∞—Ü—é—î –¥–ª—è –Ω–µ–ª—ñ–Ω—ñ–π–Ω–∏—Ö |
| **–†—ñ–∑–Ω—ñ covariances** | –ü–æ—Ä—É—à–µ–Ω–Ω—è –ø—Ä–∏–ø—É—â–µ–Ω–Ω—è ‚Üí QDA |
| **Multimodal** | –ù–µ –ø—Ä–∞—Ü—é—î –¥–æ–±—Ä–µ |

---

## –ö–ª—é—á–æ–≤—ñ –≤–∏—Å–Ω–æ–≤–∫–∏

> LDA –∑–Ω–∞—Ö–æ–¥–∏—Ç—å –ª—ñ–Ω—ñ–π–Ω—É –ø—Ä–æ–µ–∫—Ü—ñ—é, —â–æ –º–∞–∫—Å–∏–º—ñ–∑—É—î –º—ñ–∂-–∫–ª–∞—Å–æ–≤—É –≤—ñ–¥—Å—Ç–∞–Ω—å —Ç–∞ –º—ñ–Ω—ñ–º—ñ–∑—É—î –≤–Ω—É—Ç—Ä—ñ—à–Ω—å–æ-–∫–ª–∞—Å–æ–≤—É variance.

**–ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏:**
- Gaussian –¥–∞–Ω—ñ + shared covariance ‚Üí LDA ‚úì
- –†—ñ–∑–Ω—ñ covariances ‚Üí QDA ‚úì
- –ù–µ–ª—ñ–Ω—ñ–π–Ω—ñ boundaries ‚Üí Kernel methods ‚úì

---

#ml #lda #discriminant-analysis #dimensionality-reduction #probabilistic
