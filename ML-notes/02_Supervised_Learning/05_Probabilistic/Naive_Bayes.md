# Naive Bayes (–ù–∞—ó–≤–Ω–∏–π –ë–∞–π—î—Å)

## –©–æ —Ü–µ?

**Naive Bayes** ‚Äî —Ü–µ —Å—ñ–º–µ–π—Å—Ç–≤–æ –ø—Ä–æ—Å—Ç–∏—Ö **probabilistic –∞–ª–≥–æ—Ä–∏—Ç–º—ñ–≤** supervised learning, —è–∫—ñ –±–∞–∑—É—é—Ç—å—Å—è –Ω–∞ **—Ç–µ–æ—Ä–µ–º—ñ –ë–∞–π—î—Å–∞** –∑ "–Ω–∞—ó–≤–Ω–∏–º" –ø—Ä–∏–ø—É—â–µ–Ω–Ω—è–º –ø—Ä–æ **–Ω–µ–∑–∞–ª–µ–∂–Ω—ñ—Å—Ç—å –æ–∑–Ω–∞–∫**.

**–ì–æ–ª–æ–≤–Ω–∞ —ñ–¥–µ—è:** –æ–±—á–∏—Å–ª–∏—Ç–∏ –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å –Ω–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ –¥–æ –∫–æ–∂–Ω–æ–≥–æ –∫–ª–∞—Å—É –Ω–∞ –æ—Å–Ω–æ–≤—ñ –æ–∑–Ω–∞–∫, –ø—Ä–∏–ø—É—Å–∫–∞—é—á–∏, —â–æ –≤—Å—ñ –æ–∑–Ω–∞–∫–∏ –Ω–µ–∑–∞–ª–µ–∂–Ω—ñ –º—ñ–∂ —Å–æ–±–æ—é.

## –ù–∞–≤—ñ—â–æ –ø–æ—Ç—Ä—ñ–±–µ–Ω?

- ‚ö° **–®–≤–∏–¥–∫—ñ—Å—Ç—å** ‚Äî –¥—É–∂–µ —à–≤–∏–¥–∫–µ –Ω–∞–≤—á–∞–Ω–Ω—è —Ç–∞ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
- üìä **–ü—Ä–æ—Å—Ç–æ—Ç–∞** ‚Äî –ª–µ–≥–∫–æ –∑—Ä–æ–∑—É–º—ñ—Ç–∏ —Ç–∞ —Ä–µ–∞–ª—ñ–∑—É–≤–∞—Ç–∏
- üéØ **Baseline** ‚Äî —á—É–¥–æ–≤–∏–π –ø–æ—á–∞—Ç–æ–∫ –¥–ª—è –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó
- üìß **Text classification** ‚Äî —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è spam detection, sentiment analysis
- üí° **–ú–∞–ª—ñ –¥–∞–Ω—ñ** ‚Äî –ø—Ä–∞—Ü—é—î –Ω–∞–≤—ñ—Ç—å –∑ –Ω–µ–≤–µ–ª–∏–∫–∏–º–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞–º–∏
- üîß **Probabilistic** ‚Äî –¥–∞—î –∫–∞–ª—ñ–±—Ä–æ–≤–∞–Ω—ñ –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ
- üöÄ **Real-time** ‚Äî —à–≤–∏–¥–∫—ñ online predictions

## –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏?

**–ü–æ—Ç—Ä—ñ–±–Ω–æ:**
- **Text classification** ‚Äî spam detection, sentiment analysis, document categorization
- **–®–≤–∏–¥–∫–∏–π baseline** ‚Äî –ø–µ—Ä—à–∞ –º–æ–¥–µ–ª—å –¥–ª—è —Å–ø—Ä–æ–±–∏
- **–ú–∞–ª—ñ –¥–∞—Ç–∞—Å–µ—Ç–∏** ‚Äî –ø—Ä–∞—Ü—é—î –Ω–∞–≤—ñ—Ç—å –∑ –º–∞–ª–æ—é –∫—ñ–ª—å–∫—ñ—Å—Ç—é –¥–∞–Ω–∏—Ö
- **Real-time predictions** ‚Äî –¥—É–∂–µ —à–≤–∏–¥–∫—ñ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
- **Probabilistic outputs** ‚Äî –ø–æ—Ç—Ä—ñ–±–Ω—ñ –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ –∫–ª–∞—Å—ñ–≤
- **Multi-class classification** ‚Äî –ø—Ä–∏—Ä–æ–¥–Ω–æ –ø—Ä–∞—Ü—é—î –∑ –±–∞–≥–∞—Ç—å–º–∞ –∫–ª–∞—Å–∞–º–∏
- **–í–∏—Å–æ–∫–æ—Ä–æ–∑–º—ñ—Ä–Ω—ñ –¥–∞–Ω—ñ** ‚Äî –Ω–µ —Å—Ç—Ä–∞–∂–¥–∞—î –≤—ñ–¥ curse of dimensionality

**–ù–µ –ø–æ—Ç—Ä—ñ–±–Ω–æ:**
- –û–∑–Ω–∞–∫–∏ **—Å–∏–ª—å–Ω–æ –∫–æ—Ä–µ–ª—é—é—Ç—å** –º—ñ–∂ —Å–æ–±–æ—é ‚Üí –ø–æ—Ä—É—à–µ–Ω–Ω—è –Ω–µ–∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ
- –ü–æ—Ç—Ä—ñ–±–Ω–∞ **–º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å** ‚Üí Tree-based, SVM, Neural Networks
- **–°–∫–ª–∞–¥–Ω—ñ –≤–∑–∞—î–º–æ–¥—ñ—ó** –º—ñ–∂ –æ–∑–Ω–∞–∫–∞–º–∏ ‚Üí Tree-based, Neural Networks
- –ß–∏—Å–ª–æ–≤—ñ –æ–∑–Ω–∞–∫–∏ –∑ –Ω–µ—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∏–º —Ä–æ–∑–ø–æ–¥—ñ–ª–æ–º ‚Üí —ñ–Ω—à—ñ –∞–ª–≥–æ—Ä–∏—Ç–º–∏

---

## –¢–µ–æ—Ä–µ–º–∞ –ë–∞–π—î—Å–∞

### –§–æ—Ä–º—É–ª–∞

$$P(C|X) = \frac{P(X|C) \cdot P(C)}{P(X)}$$

–¥–µ:
- $P(C|X)$ ‚Äî **posterior probability** (–π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å –∫–ª–∞—Å—É $C$ –∑–∞ —É–º–æ–≤–∏ –æ–∑–Ω–∞–∫ $X$)
- $P(X|C)$ ‚Äî **likelihood** (–π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å –æ–∑–Ω–∞–∫ $X$ –∑–∞ —É–º–æ–≤–∏ –∫–ª–∞—Å—É $C$)
- $P(C)$ ‚Äî **prior probability** (–∞–ø—Ä—ñ–æ—Ä–Ω–∞ –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å –∫–ª–∞—Å—É $C$)
- $P(X)$ ‚Äî **evidence** (–π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å –æ–∑–Ω–∞–∫ $X$)

### –Ü–Ω—Ç—É—ó—Ü—ñ—è

**–ü—Ä–∏–∫–ª–∞–¥: –î—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ —Ö–≤–æ—Ä–æ–±–∏**

- $P(\text{–•–≤–æ—Ä–∏–π}|\text{–ü–æ–∑–∏—Ç–∏–≤–Ω–∏–π —Ç–µ—Å—Ç})$ ‚Äî —è–∫–∞ –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å, —â–æ –ª—é–¥–∏–Ω–∞ —Ö–≤–æ—Ä–∞, —è–∫—â–æ —Ç–µ—Å—Ç –ø–æ–∑–∏—Ç–∏–≤–Ω–∏–π?

**–ó–∞ —Ç–µ–æ—Ä–µ–º–æ—é –ë–∞–π—î—Å–∞:**

$$P(\text{–•–≤–æ—Ä–∏–π}|\text{+}) = \frac{P(\text{+}|\text{–•–≤–æ—Ä–∏–π}) \cdot P(\text{–•–≤–æ—Ä–∏–π})}{P(\text{+})}$$

–¥–µ:
- $P(\text{+}|\text{–•–≤–æ—Ä–∏–π})$ ‚Äî —á—É—Ç–ª–∏–≤—ñ—Å—Ç—å —Ç–µ—Å—Ç—É (—è–∫—â–æ —Ö–≤–æ—Ä–∏–π, —è–∫–∞ –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å –ø–æ–∑–∏—Ç–∏–≤–Ω–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É?)
- $P(\text{–•–≤–æ—Ä–∏–π})$ ‚Äî —Å–∫—ñ–ª—å–∫–∏ –ª—é–¥–µ–π —Ö–≤–æ—Ä—ñ –≤ –ø–æ–ø—É–ª—è—Ü—ñ—ó?
- $P(\text{+})$ ‚Äî —Å–∫—ñ–ª—å–∫–∏ —Ç–µ—Å—Ç—ñ–≤ –ø–æ–∑–∏—Ç–∏–≤–Ω—ñ –∑–∞–≥–∞–ª–æ–º?

---

## "Naive" –ø—Ä–∏–ø—É—â–µ–Ω–Ω—è

### –ü—Ä–∏–ø—É—â–µ–Ω–Ω—è –ø—Ä–æ –Ω–µ–∑–∞–ª–µ–∂–Ω—ñ—Å—Ç—å

**Naive Bayes –ø—Ä–∏–ø—É—Å–∫–∞—î:** –≤—Å—ñ –æ–∑–Ω–∞–∫–∏ **–Ω–µ–∑–∞–ª–µ–∂–Ω—ñ** –º—ñ–∂ —Å–æ–±–æ—é –∑–∞ —É–º–æ–≤–∏ –∫–ª–∞—Å—É.

$$P(X|C) = P(x_1, x_2, ..., x_n | C) = \prod_{i=1}^{n} P(x_i | C)$$

**–ß–æ–º—É "naive" (–Ω–∞—ó–≤–Ω–µ)?**

–í —Ä–µ–∞–ª—å–Ω–æ—Å—Ç—ñ –æ–∑–Ω–∞–∫–∏ –º–∞–π–∂–µ –∑–∞–≤–∂–¥–∏ **–∑–∞–ª–µ–∂–Ω—ñ**, –∞–ª–µ –º–∏ –ø—Ä–∏–ø—É—Å–∫–∞—î–º–æ –Ω–µ–∑–∞–ª–µ–∂–Ω—ñ—Å—Ç—å –¥–ª—è —Å–ø—Ä–æ—â–µ–Ω–Ω—è.

### –ü—Ä–∏–∫–ª–∞–¥

**Email spam detection:**

–û–∑–Ω–∞–∫–∏: —Å–ª–æ–≤–∞ "free", "win", "prize"

**Naive –ø—Ä–∏–ø—É—â–µ–Ω–Ω—è:**

$$P(\text{"free", "win", "prize"}|\text{Spam}) = P(\text{"free"}|\text{Spam}) \cdot P(\text{"win"}|\text{Spam}) \cdot P(\text{"prize"}|\text{Spam})$$

**–†–µ–∞–ª—å–Ω—ñ—Å—Ç—å:** –¶—ñ —Å–ª–æ–≤–∞ —á–∞—Å—Ç–æ –π–¥—É—Ç—å —Ä–∞–∑–æ–º —É spam ‚Üí –≤–æ–Ω–∏ **–∑–∞–ª–µ–∂–Ω—ñ**!

**–ß–æ–º—É –≤—Å–µ –æ–¥–Ω–æ –ø—Ä–∞—Ü—é—î?**

–ù–∞–≤—ñ—Ç—å –∑ –ø–æ—Ä—É—à–µ–Ω–Ω—è–º –ø—Ä–∏–ø—É—â–µ–Ω–Ω—è, Naive Bayes —á–∞—Å—Ç–æ –¥–∞—î –ø—Ä–∞–≤–∏–ª—å–Ω–∏–π **–ø–æ—Ä—è–¥–æ–∫** –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç–µ–π ‚Üí –ø—Ä–∞–≤–∏–ª—å–Ω–∞ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è! ‚úì

---

## –ê–ª–≥–æ—Ä–∏—Ç–º Naive Bayes

### Training Phase

1. **–û–±—á–∏—Å–ª–∏—Ç–∏ prior probabilities –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –∫–ª–∞—Å—É:**

$$P(C_k) = \frac{\text{count}(C_k)}{n}$$

2. **–û–±—á–∏—Å–ª–∏—Ç–∏ likelihood –¥–ª—è –∫–æ–∂–Ω–æ—ó –æ–∑–Ω–∞–∫–∏ –∑–∞ —É–º–æ–≤–∏ –∫–ª–∞—Å—É:**

$$P(x_i | C_k)$$

(—Å–ø–æ—Å—ñ–± –æ–±—á–∏—Å–ª–µ–Ω–Ω—è –∑–∞–ª–µ–∂–∏—Ç—å –≤—ñ–¥ —Ç–∏–ø—É –¥–∞–Ω–∏—Ö ‚Äî –¥–∏–≤. –Ω–∏–∂—á–µ)

### Prediction Phase

1. **–î–ª—è –Ω–æ–≤–æ–≥–æ –∑—Ä–∞–∑–∫–∞ $X = [x_1, x_2, ..., x_n]$ –æ–±—á–∏—Å–ª–∏—Ç–∏ posterior –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –∫–ª–∞—Å—É:**

$$P(C_k | X) \propto P(C_k) \cdot \prod_{i=1}^{n} P(x_i | C_k)$$

(–º–æ–∂–Ω–∞ –æ–ø—É—Å—Ç–∏—Ç–∏ $P(X)$, –±–æ –≤—ñ–Ω –æ–¥–Ω–∞–∫–æ–≤–∏–π –¥–ª—è –≤—Å—ñ—Ö –∫–ª–∞—Å—ñ–≤)

2. **–û–±—Ä–∞—Ç–∏ –∫–ª–∞—Å –∑ –Ω–∞–π–≤–∏—â–æ—é posterior probability:**

$$\hat{y} = \arg\max_{C_k} P(C_k | X)$$

### Log-trick –¥–ª—è —Å—Ç–∞–±—ñ–ª—å–Ω–æ—Å—Ç—ñ

**–ü—Ä–æ–±–ª–µ–º–∞:** –ú–Ω–æ–∂–µ–Ω–Ω—è –±–∞–≥–∞—Ç—å–æ—Ö –º–∞–ª–∏—Ö –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç–µ–π ‚Üí **numerical underflow**.

**–†—ñ—à–µ–Ω–Ω—è:** –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ **–ª–æ–≥–∞—Ä–∏—Ñ–º–∏**:

$$\log P(C_k | X) = \log P(C_k) + \sum_{i=1}^{n} \log P(x_i | C_k)$$

–°—É–º–∞ –ª–æ–≥–∞—Ä–∏—Ñ–º—ñ–≤ –∑–∞–º—ñ—Å—Ç—å –¥–æ–±—É—Ç–∫—É –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç–µ–π!

---

## –¢–∏–ø–∏ Naive Bayes

## 1. Gaussian Naive Bayes

### –î–ª—è continuous (—á–∏—Å–ª–æ–≤–∏—Ö) –æ–∑–Ω–∞–∫

**–ü—Ä–∏–ø—É—â–µ–Ω–Ω—è:** –û–∑–Ω–∞–∫–∏ —Ä–æ–∑–ø–æ–¥—ñ–ª–µ–Ω—ñ **–Ω–æ—Ä–º–∞–ª—å–Ω–æ** (Gaussian) –∑–∞ —É–º–æ–≤–∏ –∫–ª–∞—Å—É.

$$P(x_i | C_k) = \frac{1}{\sqrt{2\pi\sigma_{k,i}^2}} \exp\left(-\frac{(x_i - \mu_{k,i})^2}{2\sigma_{k,i}^2}\right)$$

–¥–µ:
- $\mu_{k,i}$ ‚Äî —Å–µ—Ä–µ–¥–Ω—î –æ–∑–Ω–∞–∫–∏ $i$ –¥–ª—è –∫–ª–∞—Å—É $k$
- $\sigma_{k,i}^2$ ‚Äî variance –æ–∑–Ω–∞–∫–∏ $i$ –¥–ª—è –∫–ª–∞—Å—É $k$

### Training

**–î–ª—è –∫–æ–∂–Ω–æ–≥–æ –∫–ª–∞—Å—É $k$ —Ç–∞ –æ–∑–Ω–∞–∫–∏ $i$:**

$$\mu_{k,i} = \frac{1}{n_k} \sum_{x \in C_k} x_i$$

$$\sigma_{k,i}^2 = \frac{1}{n_k} \sum_{x \in C_k} (x_i - \mu_{k,i})^2$$

### –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è

```
Feature distribution by class:

    P(x|C)
      |        Class A         Class B
      |          ‚ï±‚ï≤              ‚ï±‚ï≤
      |         ‚ï±  ‚ï≤            ‚ï±  ‚ï≤
      |        ‚ï±    ‚ï≤          ‚ï±    ‚ï≤
      |       ‚ï±      ‚ï≤        ‚ï±      ‚ï≤
      |______‚ï±________‚ï≤______‚ï±________‚ï≤_____ x
           Œº_A      Œº_B
```

### –ö–æ–¥

```python
from sklearn.naive_bayes import GaussianNB
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

# –î–∞–Ω—ñ
iris = load_iris()
X = iris.data
y = iris.target

# –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Gaussian Naive Bayes
gnb = GaussianNB()

# –ù–∞–≤—á–∞–Ω–Ω—è (–¥—É–∂–µ —à–≤–∏–¥–∫–æ!)
gnb.fit(X_train, y_train)

# –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
y_pred = gnb.predict(X_test)
y_pred_proba = gnb.predict_proba(X_test)

# –û—Ü—ñ–Ω–∫–∞
print("=== Gaussian Naive Bayes ===")
print(f"Train Accuracy: {gnb.score(X_train, y_train):.4f}")
print(f"Test Accuracy: {accuracy_score(y_test, y_pred):.4f}")

print("\nClassification Report:")
print(classification_report(y_test, y_pred, target_names=iris.target_names))

# –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –º–æ–¥–µ–ª—ñ
print("\n=== Model Parameters ===")
print(f"Class priors: {gnb.class_prior_}")
print(f"\nMeans (Œº) for class 0:")
print(gnb.theta_[0])
print(f"\nVariances (œÉ¬≤) for class 0:")
print(gnb.var_[0])
```

### –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ Gaussian NB

‚úÖ Continuous numerical features
‚úÖ –û–∑–Ω–∞–∫–∏ –ø—Ä–∏–±–ª–∏–∑–Ω–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ —Ä–æ–∑–ø–æ–¥—ñ–ª–µ–Ω—ñ
‚úÖ –ú–∞–ª—ñ/—Å–µ—Ä–µ–¥–Ω—ñ –¥–∞—Ç–∞—Å–µ—Ç–∏
‚úÖ –®–≤–∏–¥–∫–∏–π baseline

---

## 2. Multinomial Naive Bayes

### –î–ª—è discrete counts (–ª—ñ—á–∏–ª—å–Ω–∏–∫—ñ–≤)

**–ü—Ä–∏–ø—É—â–µ–Ω–Ω—è:** –û–∑–Ω–∞–∫–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç—å **—á–∞—Å—Ç–æ—Ç–∏** –∞–±–æ **counts** (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥, word counts –≤ –¥–æ–∫—É–º–µ–Ω—Ç—ñ).

$$P(x_i | C_k) = \frac{\text{count}(x_i, C_k) + \alpha}{\sum_{j} \text{count}(x_j, C_k) + \alpha \cdot n_{features}}$$

–¥–µ $\alpha$ ‚Äî **smoothing parameter** (Laplace smoothing).

### Laplace Smoothing

**–ü—Ä–æ–±–ª–µ–º–∞:** –Ø–∫—â–æ —Å–ª–æ–≤–æ –Ω—ñ–∫–æ–ª–∏ –Ω–µ –∑—É—Å—Ç—Ä—ñ—á–∞–ª–æ—Å—å —É –∫–ª–∞—Å—ñ ‚Üí $P(x_i|C_k) = 0$ ‚Üí posterior = 0!

**–†—ñ—à–µ–Ω–Ω—è:** –î–æ–¥–∞—î–º–æ $\alpha$ (–∑–∞–∑–≤–∏—á–∞–π 1) –¥–æ –≤—Å—ñ—Ö counts:

$$P(x_i | C_k) = \frac{\text{count}(x_i, C_k) + \alpha}{N_k + \alpha \cdot V}$$

–¥–µ:
- $N_k$ ‚Äî –∑–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Å–ª—ñ–≤ —É –∫–ª–∞—Å—ñ $k$
- $V$ ‚Äî —Ä–æ–∑–º—ñ—Ä vocabulary

**–ï—Ñ–µ–∫—Ç:**
- $\alpha = 0$ ‚Üí no smoothing (–º–æ–∂–µ –±—É—Ç–∏ 0 –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ)
- $\alpha = 1$ ‚Äî **Laplace smoothing** (—Å—Ç–∞–Ω–¥–∞—Ä—Ç)
- $\alpha > 1$ ‚Äî –±—ñ–ª—å—à–µ –∑–≥–ª–∞–¥–∂—É–≤–∞–Ω–Ω—è

### –ü—Ä–∏–∫–ª–∞–¥: Text Classification

**–î–æ–∫—É–º–µ–Ω—Ç–∏:**
```
Doc 1 (Sport): "football match goal score"
Doc 2 (Sport): "basketball game score"
Doc 3 (Tech):  "python code software"
```

**Vocabulary:** [football, match, goal, score, basketball, game, python, code, software]

**Word counts –¥–ª—è Sport:**
- football: 1
- match: 1
- goal: 1
- score: 2
- basketball: 1
- game: 1
- python: 0 ‚Üê –ù–ï –ó–£–°–¢–†–Ü–ß–ê–Ñ–¢–¨–°–Ø!

**–ó Laplace smoothing (Œ±=1):**

$$P(\text{"python"}|\text{Sport}) = \frac{0 + 1}{7 + 1 \cdot 9} = \frac{1}{16} \neq 0$$ ‚úì

### –ö–æ–¥

```python
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import CountVectorizer

# –î–∞–Ω—ñ (text)
texts = [
    "free money win prize",
    "meeting schedule tomorrow",
    "limited offer click now",
    "project update report",
    "win lottery free cash",
    "team meeting agenda"
]
labels = [1, 0, 1, 0, 1, 0]  # 1=spam, 0=ham

# Vectorization (Bag of Words)
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(texts)

print("=== Bag of Words ===")
print(f"Vocabulary: {vectorizer.get_feature_names_out()}")
print(f"Shape: {X.shape}")
print(f"Example document vector:\n{X[0].toarray()}")

# –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è
X_train, X_test, y_train, y_test = train_test_split(
    X, labels, test_size=0.3, random_state=42
)

# Multinomial Naive Bayes
mnb = MultinomialNB(alpha=1.0)  # Laplace smoothing

# –ù–∞–≤—á–∞–Ω–Ω—è
mnb.fit(X_train, y_train)

# –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
y_pred = mnb.predict(X_test)
y_pred_proba = mnb.predict_proba(X_test)

# –û—Ü—ñ–Ω–∫–∞
print("\n=== Multinomial Naive Bayes ===")
print(f"Train Accuracy: {mnb.score(X_train, y_train):.4f}")
print(f"Test Accuracy: {mnb.score(X_test, y_test):.4f}")

# –ü–∞—Ä–∞–º–µ—Ç—Ä–∏
print(f"\nClass priors: {mnb.class_prior_}")
print(f"Feature log probabilities shape: {mnb.feature_log_prob_.shape}")

# –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –¥–ª—è –Ω–æ–≤–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç—É
new_text = ["free prize money"]
X_new = vectorizer.transform(new_text)
pred = mnb.predict(X_new)
pred_proba = mnb.predict_proba(X_new)

print(f"\nNew document: '{new_text[0]}'")
print(f"Prediction: {'Spam' if pred[0] == 1 else 'Ham'}")
print(f"Probabilities: Ham={pred_proba[0][0]:.4f}, Spam={pred_proba[0][1]:.4f}")
```

### –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ Multinomial NB

‚úÖ **Text classification** (–Ω–∞–π–ø–æ–ø—É–ª—è—Ä–Ω—ñ—à–µ!)
‚úÖ **Document categorization**
‚úÖ **Spam detection**
‚úÖ **Sentiment analysis**
‚úÖ –î–∞–Ω—ñ —É –≤–∏–≥–ª—è–¥—ñ counts/frequencies
‚úÖ Sparse –≤–∏—Å–æ–∫–æ—Ä–æ–∑–º—ñ—Ä–Ω—ñ –¥–∞–Ω—ñ

---

## 3. Bernoulli Naive Bayes

### –î–ª—è binary features (0/1)

**–ü—Ä–∏–ø—É—â–µ–Ω–Ω—è:** –û–∑–Ω–∞–∫–∏ **–±—ñ–Ω–∞—Ä–Ω—ñ** (–ø—Ä–∏—Å—É—Ç–Ω—ñ—Å—Ç—å/–≤—ñ–¥—Å—É—Ç–Ω—ñ—Å—Ç—å).

$$P(x_i | C_k) = P(i | C_k) \cdot x_i + (1 - P(i | C_k)) \cdot (1 - x_i)$$

–¥–µ $P(i | C_k)$ ‚Äî –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å —Ç–æ–≥–æ, —â–æ –æ–∑–Ω–∞–∫–∞ $i$ –ø—Ä–∏—Å—É—Ç–Ω—è –≤ –∫–ª–∞—Å—ñ $k$.

**–†—ñ–∑–Ω–∏—Ü—è –∑ Multinomial:**
- **Multinomial:** –≤—Ä–∞—Ö–æ–≤—É—î **—Å–∫—ñ–ª—å–∫–∏ —Ä–∞–∑—ñ–≤** —Å–ª–æ–≤–æ –∑—É—Å—Ç—Ä—ñ—á–∞—î—Ç—å—Å—è
- **Bernoulli:** –≤—Ä–∞—Ö–æ–≤—É—î —Ç—ñ–ª—å–∫–∏ **—á–∏ –∑—É—Å—Ç—Ä—ñ—á–∞—î—Ç—å—Å—è** (—î/–Ω–µ–º–∞—î)

### –ü—Ä–∏–∫–ª–∞–¥

**–î–æ–∫—É–º–µ–Ω—Ç:** "free free money prize"

**Multinomial representation:**
- free: 2
- money: 1
- prize: 1

**Bernoulli representation:**
- free: 1 (–ø—Ä–∏—Å—É—Ç–Ω—î)
- money: 1 (–ø—Ä–∏—Å—É—Ç–Ω—î)
- prize: 1 (–ø—Ä–∏—Å—É—Ç–Ω—î)

### –ö–æ–¥

```python
from sklearn.naive_bayes import BernoulliNB

# –ë—ñ–Ω–∞—Ä–∏–∑–∞—Ü—ñ—è –¥–∞–Ω–∏—Ö (0/1)
from sklearn.preprocessing import Binarizer

binarizer = Binarizer()
X_binary = binarizer.fit_transform(X.toarray())

# Bernoulli Naive Bayes
bnb = BernoulliNB(alpha=1.0)
bnb.fit(X_binary, labels)

print(f"Bernoulli NB Accuracy: {bnb.score(X_binary, labels):.4f}")
```

### –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ Bernoulli NB

‚úÖ Binary features (presence/absence)
‚úÖ Text classification –∑ binary representation
‚úÖ –ö–æ–ª–∏ –≤–∞–∂–ª–∏–≤—ñ—à–µ **—á–∏ –ø—Ä–∏—Å—É—Ç–Ω—î** —Å–ª–æ–≤–æ, –∞ –Ω–µ **—Å–∫—ñ–ª—å–∫–∏ —Ä–∞–∑—ñ–≤**
‚úÖ –ö–æ—Ä–æ—Ç–∫—ñ –¥–æ–∫—É–º–µ–Ω—Ç–∏

---

## –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è —Ç–∏–ø—ñ–≤ Naive Bayes

| –¢–∏–ø | –î–∞–Ω—ñ | –†–æ–∑–ø–æ–¥—ñ–ª | –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è | –ü—Ä–∏–∫–ª–∞–¥ |
|-----|------|----------|--------------|---------|
| **Gaussian** | Continuous | –ù–æ—Ä–º–∞–ª—å–Ω–∏–π | –ß–∏—Å–ª–æ–≤—ñ –æ–∑–Ω–∞–∫–∏ | Iris classification |
| **Multinomial** | Counts | Multinomial | Text (word counts) | Spam detection |
| **Bernoulli** | Binary | Bernoulli | Text (presence) | Short text classification |

### –í–∏–±—ñ—Ä —Ç–∏–ø—É

```
                –¢–∏–ø –¥–∞–Ω–∏—Ö?
                /         \
        Continuous       Discrete
            |               |
       Gaussian NB      Binary –∞–±–æ Counts?
                        /              \
                   Binary              Counts
                      |                  |
                Bernoulli NB      Multinomial NB
```

---

## TF-IDF –∑ Naive Bayes

### TF-IDF (Term Frequency - Inverse Document Frequency)

**–ö—Ä–∞—â–µ –∑–∞ –ø—Ä–æ—Å—Ç–∏–π Bag of Words** –¥–ª—è —Ç–µ–∫—Å—Ç—ñ–≤:

$$\text{TF-IDF}(t, d) = \text{TF}(t, d) \cdot \text{IDF}(t)$$

–¥–µ:
- $\text{TF}(t, d)$ ‚Äî —á–∞—Å—Ç–æ—Ç–∞ —Ç–µ—Ä–º—ñ–Ω—É $t$ –≤ –¥–æ–∫—É–º–µ–Ω—Ç—ñ $d$
- $\text{IDF}(t) = \log\frac{N}{n_t}$ ‚Äî inverse document frequency

**–Ü–¥–µ—è:** –†—ñ–¥–∫—ñ—Å–Ω—ñ —Å–ª–æ–≤–∞ –≤–∞–∂–ª–∏–≤—ñ—à—ñ –∑–∞ —á–∞—Å—Ç–æ—Ç–Ω—ñ ("the", "a", "is").

### –ö–æ–¥

```python
from sklearn.feature_extraction.text import TfidfVectorizer

# TF-IDF –∑–∞–º—ñ—Å—Ç—å Bag of Words
tfidf_vectorizer = TfidfVectorizer(max_features=1000)
X_tfidf = tfidf_vectorizer.fit_transform(texts)

# Multinomial NB –∑ TF-IDF
mnb_tfidf = MultinomialNB()
mnb_tfidf.fit(X_tfidf, labels)

print(f"MNB with TF-IDF Accuracy: {mnb_tfidf.score(X_tfidf, labels):.4f}")
```

**–ü—Ä–∏–º—ñ—Ç–∫–∞:** TF-IDF –º–æ–∂–µ –¥–∞–≤–∞—Ç–∏ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è –ø—ñ—Å–ª—è —Ü–µ–Ω—Ç—Ä—É–≤–∞–Ω–Ω—è ‚Üí –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π **MultinomialNB**, –∞–ª–µ –±—É–¥—å –æ–±–µ—Ä–µ–∂–Ω–∏–π –∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏.

---

## –ü—Ä–æ—Å—Ç–∏–π –ø—Ä–∏–∫–ª–∞–¥: Email Spam Detection

### –î–∞–Ω—ñ

```
Email 1 (Spam):  "Free money! Win prize now! Click here!"
Email 2 (Ham):   "Meeting tomorrow at 10am. See you there."
Email 3 (Spam):  "Limited offer! Free cash prize!"
Email 4 (Ham):   "Project report attached. Please review."

New email: "Free prize available now!"
```

### –ö—Ä–æ–∫ 1: –û–±—á–∏—Å–ª–∏—Ç–∏ Prior Probabilities

$$P(\text{Spam}) = \frac{2}{4} = 0.5$$
$$P(\text{Ham}) = \frac{2}{4} = 0.5$$

### –ö—Ä–æ–∫ 2: –û–±—á–∏—Å–ª–∏—Ç–∏ Likelihoods

**Vocabulary:** {free, money, win, prize, click, meeting, project, ...}

**Word counts —É Spam:**
- free: 2, money: 1, prize: 2, ...
- Total words in Spam: 14

**Word counts —É Ham:**
- free: 0, money: 0, prize: 0, meeting: 1, project: 1, ...
- Total words in Ham: 12

**–ó Laplace smoothing (Œ±=1):**

$$P(\text{"free"}|\text{Spam}) = \frac{2 + 1}{14 + V} \approx 0.15$$
$$P(\text{"free"}|\text{Ham}) = \frac{0 + 1}{12 + V} \approx 0.05$$

(–¥–µ $V$ ‚Äî —Ä–æ–∑–º—ñ—Ä vocabulary)

### –ö—Ä–æ–∫ 3: –ö–ª–∞—Å–∏—Ñ—ñ–∫—É–≤–∞—Ç–∏ –Ω–æ–≤–∏–π email

**New:** "Free prize available now!"

$$P(\text{Spam}|\text{New}) \propto P(\text{Spam}) \cdot P(\text{"free"}|\text{Spam}) \cdot P(\text{"prize"}|\text{Spam}) \cdot ...$$

$$P(\text{Ham}|\text{New}) \propto P(\text{Ham}) \cdot P(\text{"free"}|\text{Ham}) \cdot P(\text{"prize"}|\text{Ham}) \cdot ...$$

**–†–µ–∑—É–ª—å—Ç–∞—Ç:** $P(\text{Spam}|\text{New}) > P(\text{Ham}|\text{New})$ ‚Üí **Spam!** ‚úì

---

## –°–∫–ª–∞–¥–Ω–∏–π –ø—Ä–∏–∫–ª–∞–¥: Multi-class Document Classification

```python
import pandas as pd
import numpy as np
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt

# –°–∏–Ω—Ç–µ—Ç–∏—á–Ω—ñ –¥–∞–Ω—ñ (document classification)
np.random.seed(42)

# –ö–∞—Ç–µ–≥–æ—Ä—ñ—ó —Ç–∞ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ñ —Å–ª–æ–≤–∞
categories = {
    'Sport': ['football', 'basketball', 'game', 'match', 'score', 'team', 'player', 'win'],
    'Tech': ['python', 'code', 'software', 'computer', 'algorithm', 'data', 'programming'],
    'Politics': ['election', 'president', 'vote', 'government', 'law', 'congress', 'policy']
}

def generate_document(category):
    words = np.random.choice(categories[category], size=np.random.randint(5, 15))
    return ' '.join(words)

# –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –¥–∞—Ç–∞—Å–µ—Ç—É
n_per_class = 200
documents = []
labels = []

for cat_idx, (category, _) in enumerate(categories.items()):
    for _ in range(n_per_class):
        documents.append(generate_document(category))
        labels.append(cat_idx)

print("="*70)
print("MULTINOMIAL NAIVE BAYES FOR DOCUMENT CLASSIFICATION")
print("="*70)
print(f"Dataset: {len(documents)} documents")
print(f"Categories: {list(categories.keys())}")
print(f"Documents per category: {n_per_class}")

# –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è
X_train, X_test, y_train, y_test = train_test_split(
    documents, labels, test_size=0.2, random_state=42, stratify=labels
)

# TF-IDF Vectorization
vectorizer = TfidfVectorizer(
    max_features=500,
    ngram_range=(1, 2),  # Unigrams + Bigrams
    min_df=2
)
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

print(f"\nTF-IDF shape: {X_train_tfidf.shape}")
print(f"Vocabulary size: {len(vectorizer.get_feature_names_out())}")

# 1. –ë–∞–∑–æ–≤–∞ –º–æ–¥–µ–ª—å
print("\n" + "="*70)
print("1. BASELINE MODEL")
print("="*70)

mnb = MultinomialNB(alpha=1.0)
mnb.fit(X_train_tfidf, y_train)

y_pred = mnb.predict(X_test_tfidf)

print(f"Train Accuracy: {mnb.score(X_train_tfidf, y_train):.4f}")
print(f"Test Accuracy: {mnb.score(X_test_tfidf, y_test):.4f}")

# 2. Cross-validation
print("\n" + "="*70)
print("2. CROSS-VALIDATION")
print("="*70)

cv_scores = cross_val_score(mnb, X_train_tfidf, y_train, cv=5)
print(f"CV Scores: {cv_scores}")
print(f"Mean CV Score: {cv_scores.mean():.4f} (+/- {cv_scores.std()*2:.4f})")

# 3. –ü—ñ–¥–±—ñ—Ä alpha (smoothing)
print("\n" + "="*70)
print("3. TUNING ALPHA (SMOOTHING)")
print("="*70)

alphas = [0.01, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0]
train_scores = []
test_scores = []

for alpha in alphas:
    mnb_alpha = MultinomialNB(alpha=alpha)
    mnb_alpha.fit(X_train_tfidf, y_train)
    train_scores.append(mnb_alpha.score(X_train_tfidf, y_train))
    test_scores.append(mnb_alpha.score(X_test_tfidf, y_test))

optimal_alpha = alphas[np.argmax(test_scores)]
print(f"Optimal alpha: {optimal_alpha}")
print(f"Best test score: {max(test_scores):.4f}")

# 4. –§—ñ–Ω–∞–ª—å–Ω–∞ –º–æ–¥–µ–ª—å
print("\n" + "="*70)
print("4. FINAL MODEL EVALUATION")
print("="*70)

mnb_final = MultinomialNB(alpha=optimal_alpha)
mnb_final.fit(X_train_tfidf, y_train)

y_pred_final = mnb_final.predict(X_test_tfidf)
y_pred_proba = mnb_final.predict_proba(X_test_tfidf)

print(f"Test Accuracy: {mnb_final.score(X_test_tfidf, y_test):.4f}")

print("\nClassification Report:")
print(classification_report(y_test, y_pred_final,
                          target_names=list(categories.keys())))

print("\nConfusion Matrix:")
cm = confusion_matrix(y_test, y_pred_final)
print(cm)

# 5. Top words per category
print("\n" + "="*70)
print("5. TOP WORDS PER CATEGORY")
print("="*70)

feature_names = vectorizer.get_feature_names_out()

for cat_idx, category in enumerate(categories.keys()):
    # –û—Ç—Ä–∏–º–∞—Ç–∏ log probabilities
    log_prob = mnb_final.feature_log_prob_[cat_idx]
    
    # –¢–æ–ø 10 —Å–ª—ñ–≤
    top_10_idx = np.argsort(log_prob)[-10:][::-1]
    
    print(f"\n{category}:")
    for idx in top_10_idx:
        print(f"  {feature_names[idx]}: {np.exp(log_prob[idx]):.4f}")

# 6. –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –¥–ª—è –Ω–æ–≤–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤
print("\n" + "="*70)
print("6. PREDICTIONS FOR NEW DOCUMENTS")
print("="*70)

new_docs = [
    "football match score game",
    "python programming code algorithm",
    "election president vote government"
]

X_new = vectorizer.transform(new_docs)
predictions = mnb_final.predict(X_new)
probabilities = mnb_final.predict_proba(X_new)

for doc, pred, proba in zip(new_docs, predictions, probabilities):
    pred_category = list(categories.keys())[pred]
    print(f"\nDocument: '{doc}'")
    print(f"Prediction: {pred_category}")
    print("Probabilities:")
    for cat_idx, category in enumerate(categories.keys()):
        print(f"  {category}: {proba[cat_idx]:.4f}")

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó
fig, axes = plt.subplots(2, 2, figsize=(16, 12))

# 1. Alpha vs Accuracy
axes[0, 0].plot(alphas, train_scores, 'o-', linewidth=2, label='Train')
axes[0, 0].plot(alphas, test_scores, 's-', linewidth=2, label='Test')
axes[0, 0].axvline(x=optimal_alpha, color='red', linestyle='--',
                   label=f'Optimal Œ±={optimal_alpha}')
axes[0, 0].set_xlabel('Alpha (Smoothing)', fontsize=12)
axes[0, 0].set_ylabel('Accuracy', fontsize=12)
axes[0, 0].set_title('Accuracy vs Alpha', fontsize=14, fontweight='bold')
axes[0, 0].set_xscale('log')
axes[0, 0].legend(fontsize=11)
axes[0, 0].grid(True, alpha=0.3)

# 2. Confusion Matrix
import seaborn as sns
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0, 1],
            xticklabels=categories.keys(),
            yticklabels=categories.keys())
axes[0, 1].set_xlabel('Predicted', fontsize=12)
axes[0, 1].set_ylabel('Actual', fontsize=12)
axes[0, 1].set_title('Confusion Matrix', fontsize=14, fontweight='bold')

# 3. Class probabilities distribution
for cat_idx, category in enumerate(categories.keys()):
    probs = y_pred_proba[:, cat_idx]
    axes[1, 0].hist(probs, bins=20, alpha=0.5, label=category)
axes[1, 0].set_xlabel('Predicted Probability', fontsize=12)
axes[1, 0].set_ylabel('Frequency', fontsize=12)
axes[1, 0].set_title('Distribution of Predicted Probabilities',
                    fontsize=14, fontweight='bold')
axes[1, 0].legend(fontsize=11)
axes[1, 0].grid(True, alpha=0.3)

# 4. Cross-validation scores
axes[1, 1].bar(range(len(cv_scores)), cv_scores)
axes[1, 1].axhline(y=cv_scores.mean(), color='red', linestyle='--',
                   label=f'Mean: {cv_scores.mean():.4f}')
axes[1, 1].set_xlabel('Fold', fontsize=12)
axes[1, 1].set_ylabel('Accuracy', fontsize=12)
axes[1, 1].set_title('Cross-Validation Scores', fontsize=14, fontweight='bold')
axes[1, 1].legend(fontsize=11)
axes[1, 1].grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.show()

print("\n" + "="*70)
print("ANALYSIS COMPLETE")
print("="*70)
```

---

## –ü–µ—Ä–µ–≤–∞–≥–∏ —Ç–∞ –Ω–µ–¥–æ–ª—ñ–∫–∏

### –ü–µ—Ä–µ–≤–∞–≥–∏ ‚úì

| –ü–µ—Ä–µ–≤–∞–≥–∞ | –ü–æ—è—Å–Ω–µ–Ω–Ω—è |
|----------|-----------|
| **–®–≤–∏–¥–∫—ñ—Å—Ç—å** | –î—É–∂–µ —à–≤–∏–¥–∫–µ –Ω–∞–≤—á–∞–Ω–Ω—è —Ç–∞ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è |
| **–ü—Ä–æ—Å—Ç–æ—Ç–∞** | –õ–µ–≥–∫–æ –∑—Ä–æ–∑—É–º—ñ—Ç–∏ —Ç–∞ —Ä–µ–∞–ª—ñ–∑—É–≤–∞—Ç–∏ |
| **–ú–∞–ª—ñ –¥–∞–Ω—ñ** | –ü—Ä–∞—Ü—é—î –Ω–∞–≤—ñ—Ç—å –∑ –º–∞–ª–∏–º–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞–º–∏ |
| **Probabilistic** | –î–∞—î –∫–∞–ª—ñ–±—Ä–æ–≤–∞–Ω—ñ –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ |
| **Multi-class** | –ü—Ä–∏—Ä–æ–¥–Ω–æ –ø—Ä–∞—Ü—é—î –∑ –±–∞–≥–∞—Ç—å–º–∞ –∫–ª–∞—Å–∞–º–∏ |
| **Online learning** | –õ–µ–≥–∫–æ –æ–Ω–æ–≤–ª—é–≤–∞—Ç–∏ –Ω–∞ –Ω–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö |
| **High-dimensional** | –ù–µ —Å—Ç—Ä–∞–∂–¥–∞—î –≤—ñ–¥ curse of dimensionality |
| **Baseline** | –ß—É–¥–æ–≤–∞ —Å—Ç–∞—Ä—Ç–æ–≤–∞ –º–æ–¥–µ–ª—å |
| **Text classification** | –°—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è spam, sentiment analysis |
| **–û–±—Ä–æ–±–∫–∞ missing** | Gaussian NB –ø—Ä–∏—Ä–æ–¥–Ω–æ –æ–±—Ä–æ–±–ª—è—î |

### –ù–µ–¥–æ–ª—ñ–∫–∏ ‚úó

| –ù–µ–¥–æ–ª—ñ–∫ | –ü–æ—è—Å–Ω–µ–Ω–Ω—è |
|---------|-----------|
| **Naive –ø—Ä–∏–ø—É—â–µ–Ω–Ω—è** | –û–∑–Ω–∞–∫–∏ –º–∞–π–∂–µ –∑–∞–≤–∂–¥–∏ –∑–∞–ª–µ–∂–Ω—ñ |
| **–¢–æ—á–Ω—ñ—Å—Ç—å** | –ü–æ—Å—Ç—É–ø–∞—î—Ç—å—Å—è —Å–∫–ª–∞–¥–Ω—ñ—à–∏–º –º–æ–¥–µ–ª—è–º |
| **–ù–æ—Ä–º–∞–ª—å–Ω—ñ—Å—Ç—å** | Gaussian NB –≤–∏–º–∞–≥–∞—î –Ω–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ —Ä–æ–∑–ø–æ–¥—ñ–ª—É |
| **–ù–µ optimal** | Decision boundary –Ω–µ –æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∞ |
| **Numerical features** | Gaussian NB –º–æ–∂–µ –ø–æ–≥–∞–Ω–æ –∑ multimodal |
| **–ö–∞—Ç–µ–≥–æ—Ä—ñ–∞–ª—å–Ω—ñ –æ–∑–Ω–∞–∫–∏** | –ü–æ—Ç—Ä–µ–±—É—î encoding |
| **Feature engineering** | –û–±–º–µ–∂–µ–Ω—ñ –º–æ–∂–ª–∏–≤–æ—Å—Ç—ñ |

---

## –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –∑ —ñ–Ω—à–∏–º–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–∞–º–∏

### Naive Bayes vs Logistic Regression

| –ö—Ä–∏—Ç–µ—Ä—ñ–π | Naive Bayes | Logistic Regression |
|----------|-------------|---------------------|
| **Assumptions** | –ù–µ–∑–∞–ª–µ–∂–Ω—ñ—Å—Ç—å –æ–∑–Ω–∞–∫ | –õ—ñ–Ω—ñ–π–Ω–∞ —Ä–æ–∑–¥—ñ–ª–∏–º—ñ—Å—Ç—å |
| **Training** | O(n¬∑d) | O(n¬∑d¬∑k) iterations |
| **Prediction** | O(d) | O(d) |
| **–®–≤–∏–¥–∫—ñ—Å—Ç—å** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **–¢–æ—á–Ω—ñ—Å—Ç—å** | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **–ú–∞–ª—ñ –¥–∞–Ω—ñ** | ‚úÖ –î–æ–±—Ä–µ | ‚ö†Ô∏è –ú–æ–∂–µ overfitting |
| **Text** | ‚úÖ –í—ñ–¥–º—ñ–Ω–Ω–æ | ‚úÖ –î–æ–±—Ä–µ |

### Naive Bayes vs SVM

| –ö—Ä–∏—Ç–µ—Ä—ñ–π | Naive Bayes | SVM |
|----------|-------------|-----|
| **Training** | O(n¬∑d) | O(n¬≤) to O(n¬≥) |
| **Prediction** | O(d) | O(n_sv¬∑d) |
| **–®–≤–∏–¥–∫—ñ—Å—Ç—å** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |
| **–¢–æ—á–Ω—ñ—Å—Ç—å** | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **Text** | ‚úÖ –°—Ç–∞–Ω–¥–∞—Ä—Ç | ‚úÖ –ü—Ä–∞—Ü—é—î |
| **–ü—Ä–æ—Å—Ç–æ—Ç–∞** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |

### Naive Bayes vs Random Forest

| –ö—Ä–∏—Ç–µ—Ä—ñ–π | Naive Bayes | Random Forest |
|----------|-------------|---------------|
| **Training** | O(n¬∑d) | O(n¬∑log(n)¬∑d¬∑T) |
| **Prediction** | O(d) | O(T¬∑log(n)) |
| **–®–≤–∏–¥–∫—ñ—Å—Ç—å** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |
| **–¢–æ—á–Ω—ñ—Å—Ç—å** | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **Interpretability** | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |
| **Feature scaling** | –ù–µ –∫—Ä–∏—Ç–∏—á–Ω–æ | –ù–µ –ø–æ—Ç—Ä—ñ–±–Ω–∞ |

---

## –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ Naive Bayes

### –Ü–¥–µ–∞–ª—å–Ω–æ –ø—ñ–¥—Ö–æ–¥–∏—Ç—å ‚úì

- **Text classification** ‚Äî spam detection, sentiment analysis, document categorization
- **–®–≤–∏–¥–∫–∏–π baseline** ‚Äî –ø–µ—Ä—à–∞ –º–æ–¥–µ–ª—å –¥–ª—è —Å–ø—Ä–æ–±–∏
- **–ú–∞–ª—ñ –¥–∞—Ç–∞—Å–µ—Ç–∏** ‚Äî –ø—Ä–∞—Ü—é—î –Ω–∞–≤—ñ—Ç—å –∑ –º–∞–ª–æ—é –∫—ñ–ª—å–∫—ñ—Å—Ç—é –¥–∞–Ω–∏—Ö
- **Real-time predictions** ‚Äî –¥—É–∂–µ —à–≤–∏–¥–∫—ñ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
- **Multi-class classification** ‚Äî –±–∞–≥–∞—Ç–æ –∫–ª–∞—Å—ñ–≤
- **Probabilistic outputs** ‚Äî –ø–æ—Ç—Ä—ñ–±–Ω—ñ –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ
- **Online learning** ‚Äî –ø–æ—Å—Ç—ñ–π–Ω–µ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ
- **High-dimensional sparse data** ‚Äî text –∑ –≤–µ–ª–∏–∫–∏–º vocabulary

### –ö—Ä–∞—â–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ —ñ–Ω—à–µ ‚úó

- **–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å** ‚Üí Random Forest, XGBoost, SVM
- **–°–∫–ª–∞–¥–Ω—ñ –≤–∑–∞—î–º–æ–¥—ñ—ó** ‚Üí Tree-based, Neural Networks
- **–°–∏–ª—å–Ω–æ –∫–æ—Ä–µ–ª—å–æ–≤–∞–Ω—ñ –æ–∑–Ω–∞–∫–∏** ‚Üí —ñ–Ω—à—ñ –∞–ª–≥–æ—Ä–∏—Ç–º–∏
- **Image/Audio** ‚Üí Deep Learning
- **Structured tabular data** ‚Üí Tree-based —á–∞—Å—Ç–æ –∫—Ä–∞—â–µ

---

## –ü—Ä–∞–∫—Ç–∏—á–Ω—ñ –ø–æ—Ä–∞–¥–∏ üí°

1. **–ü–æ—á–Ω–∏ –∑ Naive Bayes** –¥–ª—è text classification ‚Äî —à–≤–∏–¥–∫–∏–π —Ç–∞ –µ—Ñ–µ–∫—Ç–∏–≤–Ω–∏–π
2. **Multinomial –¥–ª—è text** ‚Äî –∑ TF-IDF –∞–±–æ Bag of Words
3. **Gaussian –¥–ª—è —á–∏—Å–ª–æ–≤–∏—Ö** ‚Äî –ø–µ—Ä–µ–≤—ñ—Ä –Ω–æ—Ä–º–∞–ª—å–Ω—ñ—Å—Ç—å —Ä–æ–∑–ø–æ–¥—ñ–ª—É
4. **Laplace smoothing** ‚Äî –∑–∞–≤–∂–¥–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π (Œ±=1)
5. **TF-IDF –∫—Ä–∞—â–µ –∑–∞ BoW** ‚Äî –¥–ª—è –±—ñ–ª—å—à–æ—Å—Ç—ñ text –∑–∞–¥–∞—á
6. **–ü—ñ–¥–±–∏—Ä–∞–π alpha** ‚Äî —á–µ—Ä–µ–∑ cross-validation
7. **–í—ñ–∑—É–∞–ª—ñ–∑—É–π top words** ‚Äî –∑—Ä–æ–∑—É–º—ñ–π, —â–æ –º–æ–¥–µ–ª—å –Ω–∞–≤—á–∏–ª–∞
8. **–ü–æ—Ä—ñ–≤–Ω—è–π –∑ Logistic Regression** ‚Äî baseline comparison
9. **Online learning** ‚Äî –ª–µ–≥–∫–æ –æ–Ω–æ–≤–ª—é–≤–∞—Ç–∏ `partial_fit()`
10. **–ö–∞–ª—ñ–±—Ä–æ–≤–∞–Ω—ñ –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ** ‚Äî –º–æ–∂–Ω–∞ –¥–æ–≤—ñ—Ä—è—Ç–∏ `predict_proba()`

---

## –ü–æ—à–∏—Ä–µ–Ω—ñ –ø–æ–º–∏–ª–∫–∏ ‚ùå

### 1. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –Ω–∞ —Å–∏–ª—å–Ω–æ –∫–æ—Ä–µ–ª—å–æ–≤–∞–Ω–∏—Ö –æ–∑–Ω–∞–∫–∞—Ö

```python
# –Ø–∫—â–æ –æ–∑–Ω–∞–∫–∏ –¥—É–∂–µ –∫–æ—Ä–µ–ª—å–æ–≤–∞–Ω—ñ ‚Üí –ø–æ—Ä—É—à–µ–Ω–Ω—è –Ω–µ–∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ
# Naive Bayes –º–æ–∂–µ –ø–æ–≥–∞–Ω–æ –ø—Ä–∞—Ü—é–≤–∞—Ç–∏
# ‚úÖ –ö—Ä–∞—â–µ: –≤–∏–¥–∞–ª–∏ –∫–æ—Ä–µ–ª—å–æ–≤–∞–Ω—ñ –æ–∑–Ω–∞–∫–∏ –∞–±–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–π —ñ–Ω—à–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º
```

### 2. –ó–∞–±—É—Ç–∏ –ø—Ä–æ smoothing

```python
# ‚ùå –ë–ï–ó SMOOTHING
mnb = MultinomialNB(alpha=0.0)  # –ú–æ–∂–µ –±—É—Ç–∏ 0 –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ!

# ‚úÖ –ó LAPLACE SMOOTHING
mnb = MultinomialNB(alpha=1.0)
```

### 3. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ Gaussian NB –Ω–∞ non-Gaussian –¥–∞–Ω–∏—Ö

```python
# –Ø–∫—â–æ –¥–∞–Ω—ñ –ù–ï –Ω–æ—Ä–º–∞–ª—å–Ω–æ —Ä–æ–∑–ø–æ–¥—ñ–ª–µ–Ω—ñ
# Gaussian NB –º–æ–∂–µ –ø–æ–≥–∞–Ω–æ –ø—Ä–∞—Ü—é–≤–∞—Ç–∏

# ‚úÖ –ü–µ—Ä–µ–≤—ñ—Ä —Ä–æ–∑–ø–æ–¥—ñ–ª:
import matplotlib.pyplot as plt
plt.hist(X[:, 0], bins=30)
plt.show()

# –Ø–∫—â–æ –Ω–µ –Ω–æ—Ä–º–∞–ª—å–Ω–∏–π ‚Üí —Ä–æ–∑–≥–ª—è–Ω—å —ñ–Ω—à—ñ –∞–ª–≥–æ—Ä–∏—Ç–º–∏
```

### 4. –ù–µ –ø—ñ–¥–±–∏—Ä–∞—Ç–∏ alpha

```python
# ‚ùå –ü–†–û–°–¢–û –í–ó–Ø–¢–ò Œ±=1.0
mnb = MultinomialNB(alpha=1.0)

# ‚úÖ –ü–Ü–î–Ü–ë–†–ê–¢–ò –ß–ï–†–ï–ó CV
from sklearn.model_selection import GridSearchCV
param_grid = {'alpha': [0.01, 0.1, 0.5, 1.0, 2.0, 5.0]}
grid = GridSearchCV(MultinomialNB(), param_grid, cv=5)
grid.fit(X_train, y_train)
```

### 5. –Ü–≥–Ω–æ—Ä—É–≤–∞—Ç–∏ class imbalance

```python
# –Ø–∫—â–æ –∫–ª–∞—Å–∏ –Ω–µ–∑–±–∞–ª–∞–Ω—Å–æ–≤–∞–Ω—ñ, prior probabilities –≤–∞–∂–ª–∏–≤—ñ

# ‚úÖ –ú–æ–∂–Ω–∞ –≤—Å—Ç–∞–Ω–æ–≤–∏—Ç–∏ –≤—Ä—É—á–Ω—É:
from sklearn.naive_bayes import MultinomialNB
mnb = MultinomialNB(class_prior=[0.3, 0.7])  # Custom priors
```

---

## Online Learning –∑ Naive Bayes

### Incremental Learning

Naive Bayes –ø—ñ–¥—Ç—Ä–∏–º—É—î **online learning** —á–µ—Ä–µ–∑ `partial_fit()`:

```python
from sklearn.naive_bayes import MultinomialNB

# –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è
mnb = MultinomialNB()

# –ù–∞–≤—á–∞–Ω–Ω—è –Ω–∞ –ø–µ—Ä—à–æ–º—É batch
X_batch1, y_batch1 = get_first_batch()
mnb.partial_fit(X_batch1, y_batch1, classes=[0, 1, 2])

# –û–Ω–æ–≤–ª–µ–Ω–Ω—è –Ω–∞ –¥—Ä—É–≥–æ–º—É batch
X_batch2, y_batch2 = get_second_batch()
mnb.partial_fit(X_batch2, y_batch2)

# –û–Ω–æ–≤–ª–µ–Ω–Ω—è –Ω–∞ —Ç—Ä–µ—Ç—å–æ–º—É batch
X_batch3, y_batch3 = get_third_batch()
mnb.partial_fit(X_batch3, y_batch3)

# –§—ñ–Ω–∞–ª—å–Ω–∞ –º–æ–¥–µ–ª—å –≤—Ä–∞—Ö–æ–≤—É—î –≤—Å—ñ –±–∞—Ç—á—ñ!
```

**–ü–µ—Ä–µ–≤–∞–≥–∏:**
- ‚úÖ –ù–µ –ø–æ—Ç—Ä—ñ–±–Ω–æ –∑–±–µ—Ä—ñ–≥–∞—Ç–∏ –≤—Å—ñ –¥–∞–Ω—ñ –≤ –ø–∞–º'—è—Ç—ñ
- ‚úÖ –õ–µ–≥–∫–æ –æ–Ω–æ–≤–ª—é–≤–∞—Ç–∏ –º–æ–¥–µ–ª—å
- ‚úÖ Real-time learning

**–í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è:**
- Stream data
- Very large datasets (–Ω–µ –≤–º—ñ—â—É—é—Ç—å—Å—è –≤ –ø–∞–º'—è—Ç—å)
- Continuously updating systems

---

## Probability Calibration

### –ü—Ä–æ–±–ª–µ–º–∞

Naive Bayes —á–∞—Å—Ç–æ –¥–∞—î **–µ–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ñ –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ** (–¥—É–∂–µ –±–ª–∏–∑—å–∫—ñ –¥–æ 0 –∞–±–æ 1).

### –†—ñ—à–µ–Ω–Ω—è: Calibration

```python
from sklearn.calibration import CalibratedClassifierCV

# –ë–∞–∑–æ–≤–∞ –º–æ–¥–µ–ª—å
mnb = MultinomialNB()

# Calibrated classifier
calibrated_mnb = CalibratedClassifierCV(mnb, cv=5, method='sigmoid')
calibrated_mnb.fit(X_train, y_train)

# –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
y_pred_proba_calibrated = calibrated_mnb.predict_proba(X_test)
```

**–ú–µ—Ç–æ–¥–∏ –∫–∞–ª—ñ–±—Ä–∞—Ü—ñ—ó:**
- **Platt scaling** (`method='sigmoid'`)
- **Isotonic regression** (`method='isotonic'`)

---

## –ü–æ–≤'—è–∑–∞–Ω—ñ —Ç–µ–º–∏

- [[02_Logistic_Regression]] ‚Äî –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ –¥–ª—è –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó
- [[04_SVM]] ‚Äî –¥–ª—è high-dimensional text
- [[Text_Classification]] ‚Äî NLP –∑–∞–¥–∞—á—ñ
- [[Feature_Extraction]] ‚Äî TF-IDF, Bag of Words
- [[Probability_Theory]] ‚Äî —Ç–µ–æ—Ä–µ–º–∞ –ë–∞–π—î—Å–∞

## –†–µ—Å—É—Ä—Å–∏

- [Scikit-learn: Naive Bayes](https://scikit-learn.org/stable/modules/naive_bayes.html)
- [Original Paper: "Naive Bayes at Forty"](https://www.cs.unb.ca/~hzhang/publications/FLAIRS04ZhangH.pdf)
- [StatQuest: Naive Bayes](https://www.youtube.com/watch?v=O2L2Uv9pdDA)
- [Text Classification Guide](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html)

---

## –ö–ª—é—á–æ–≤—ñ –≤–∏—Å–Ω–æ–≤–∫–∏

> Naive Bayes ‚Äî —Ü–µ —Å—ñ–º–µ–π—Å—Ç–≤–æ —à–≤–∏–¥–∫–∏—Ö probabilistic –∞–ª–≥–æ—Ä–∏—Ç–º—ñ–≤, —â–æ –±–∞–∑—É—é—Ç—å—Å—è –Ω–∞ —Ç–µ–æ—Ä–µ–º—ñ –ë–∞–π—î—Å–∞ –∑ "–Ω–∞—ó–≤–Ω–∏–º" –ø—Ä–∏–ø—É—â–µ–Ω–Ω—è–º –ø—Ä–æ –Ω–µ–∑–∞–ª–µ–∂–Ω—ñ—Å—Ç—å –æ–∑–Ω–∞–∫.

**–û—Å–Ω–æ–≤–Ω—ñ –ø—Ä–∏–Ω—Ü–∏–ø–∏:**
- **–¢–µ–æ—Ä–µ–º–∞ –ë–∞–π—î—Å–∞:** $P(C|X) = \frac{P(X|C) \cdot P(C)}{P(X)}$
- **Naive –ø—Ä–∏–ø—É—â–µ–Ω–Ω—è:** –æ–∑–Ω–∞–∫–∏ –Ω–µ–∑–∞–ª–µ–∂–Ω—ñ ‚Üí $P(X|C) = \prod P(x_i|C)$
- **–¢—Ä–∏ —Ç–∏–ø–∏:** Gaussian (continuous), Multinomial (counts), Bernoulli (binary)
- **Laplace smoothing:** –∑–∞–ø–æ–±—ñ–≥–∞—î 0 –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—è–º

**–ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è:**
$$\hat{y} = \arg\max_{C_k} P(C_k) \cdot \prod_{i=1}^{n} P(x_i | C_k)$$

**–¢–∏–ø–∏ Naive Bayes:**
- **Gaussian:** –¥–ª—è continuous numerical features
- **Multinomial:** –¥–ª—è text (word counts) ‚Äî –Ω–∞–π–ø–æ–ø—É–ª—è—Ä–Ω—ñ—à–∏–π!
- **Bernoulli:** –¥–ª—è binary features (presence/absence)

**–ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏:**
- Text classification = Multinomial NB ‚úì
- –®–≤–∏–¥–∫–∏–π baseline = Naive Bayes ‚úì
- Real-time predictions = Naive Bayes ‚úì
- –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å ‚Üí —ñ–Ω—à—ñ –∞–ª–≥–æ—Ä–∏—Ç–º–∏

**–ö–†–ò–¢–ò–ß–ù–û –≤–∞–∂–ª–∏–≤–æ:**
- –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π Multinomial –¥–ª—è text (–∑ TF-IDF)
- –ó–∞–≤–∂–¥–∏ –∑–∞—Å—Ç–æ—Å–æ–≤—É–π Laplace smoothing (Œ±=1)
- –ü—ñ–¥–±–∏—Ä–∞–π alpha —á–µ—Ä–µ–∑ cross-validation
- Naive Bayes ‚Äî —á—É–¥–æ–≤–∏–π baseline, –∞–ª–µ –Ω–µ –∑–∞–≤–∂–¥–∏ –Ω–∞–π—Ç–æ—á–Ω—ñ—à–∏–π

**–ü–µ—Ä–µ–≤–∞–≥–∏:**
- –®–≤–∏–¥–∫—ñ—Å—Ç—å ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
- –ü—Ä–æ—Å—Ç–æ—Ç–∞ ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
- Text classification ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê

---

#ml #supervised-learning #classification #naive-bayes #probabilistic #text-classification #bayes-theorem #gaussian #multinomial #bernoulli
