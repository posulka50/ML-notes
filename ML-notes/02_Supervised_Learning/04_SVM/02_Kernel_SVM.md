# Kernel SVM (Kernel Trick)

## –©–æ —Ü–µ?

**Kernel SVM** –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î **kernel trick** –¥–ª—è –æ–±—Ä–æ–±–∫–∏ **–Ω–µ–ª—ñ–Ω—ñ–π–Ω–æ —Ä–æ–∑–¥—ñ–ª–∏–º–∏—Ö –¥–∞–Ω–∏—Ö**, –ø–µ—Ä–µ–Ω–æ—Å—è—á–∏ —ó—Ö —É –≤–∏—â–∏–π –≤–∏–º—ñ—Ä, –¥–µ –≤–æ–Ω–∏ —Å—Ç–∞—é—Ç—å –ª—ñ–Ω—ñ–π–Ω–æ —Ä–æ–∑–¥—ñ–ª–∏–º–∏–º–∏, **–±–µ–∑ —è–≤–Ω–æ–≥–æ –æ–±—á–∏—Å–ª–µ–Ω–Ω—è** —Ü—ñ—î—ó —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ—ó.

**–ì–æ–ª–æ–≤–Ω–∞ —ñ–¥–µ—è:** –ó–∞–º—ñ—Å—Ç—å —è–≤–Ω–æ—ó —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ—ó –≤ –≤–∏—Å–æ–∫–∏–π –≤–∏–º—ñ—Ä, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ kernel function –¥–ª—è –æ–±—á–∏—Å–ª–µ–Ω–Ω—è dot products —É transformed space.

## –ù–∞–≤—ñ—â–æ –ø–æ—Ç—Ä—ñ–±–µ–Ω?

- üîß **–ù–µ–ª—ñ–Ω—ñ–π–Ω—ñ boundaries** ‚Äî –æ–±—Ä–æ–±–∫–∞ —Å–∫–ª–∞–¥–Ω–∏—Ö patterns
- ‚ö° **–ï—Ñ–µ–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å** ‚Äî –±–µ–∑ —è–≤–Ω–æ—ó —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ—ó
- üéØ **–£–Ω—ñ–≤–µ—Ä—Å–∞–ª—å–Ω—ñ—Å—Ç—å** ‚Äî RBF –º–æ–∂–µ –∞–ø—Ä–æ–∫—Å–∏–º—É–≤–∞—Ç–∏ –±—É–¥—å-—è–∫—É —Ñ—É–Ω–∫—Ü—ñ—é
- üìä **–í–∏—Å–æ–∫–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å** ‚Äî –æ–¥–∏–Ω –∑ –Ω–∞–π—Ç–æ—á–Ω—ñ—à–∏—Ö –∞–ª–≥–æ—Ä–∏—Ç–º—ñ–≤
- üí° **Flexibility** ‚Äî —Ä—ñ–∑–Ω—ñ kernels –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö –∑–∞–¥–∞—á

## –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏?

**–ü–æ—Ç—Ä—ñ–±–Ω–æ:**

- **–ù–µ–ª—ñ–Ω—ñ–π–Ω–æ —Ä–æ–∑–¥—ñ–ª–∏–º—ñ** –¥–∞–Ω—ñ
- –°–∫–ª–∞–¥–Ω—ñ **–Ω–µ–ª—ñ–Ω—ñ–π–Ω—ñ patterns**
- –°–µ—Ä–µ–¥–Ω—ñ –¥–∞—Ç–∞—Å–µ—Ç–∏ (n = 1k-100k)
- –ü–æ—Ç—Ä—ñ–±–Ω–∞ **–≤–∏—Å–æ–∫–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å**

**–ù–µ –ø–æ—Ç—Ä—ñ–±–Ω–æ:**

- **–î—É–∂–µ –≤–µ–ª–∏–∫—ñ –¥–∞—Ç–∞—Å–µ—Ç–∏** (n > 100k) ‚Üí –ø–æ–≤—ñ–ª—å–Ω–æ
- –î–∞–Ω—ñ **–ª—ñ–Ω—ñ–π–Ω–æ —Ä–æ–∑–¥—ñ–ª–∏–º—ñ** ‚Üí Linear SVM
- –ü–æ—Ç—Ä—ñ–±–Ω–∞ **—ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å** ‚Üí Tree-based
- **–®–≤–∏–¥–∫—ñ—Å—Ç—å** –∫—Ä–∏—Ç–∏—á–Ω–∞ ‚Üí Linear models

---

## –ü—Ä–æ–±–ª–µ–º–∞ –ª—ñ–Ω—ñ–π–Ω–æ—ó —Ä–æ–∑–¥—ñ–ª–∏–º–æ—Å—Ç—ñ

### –ü—Ä–∏–∫–ª–∞–¥: XOR Problem

```
Original 2D space:

    y
    |  √ó ‚Ä¢ √ó
    |√ó ‚Ä¢ ‚Ä¢ ‚Ä¢ √ó
    | √ó ‚Ä¢ ‚Ä¢ √ó
    |  √ó ‚Ä¢ √ó
    |________ x

–ù–µ–º–æ–∂–ª–∏–≤–æ —Ä–æ–∑–¥—ñ–ª–∏—Ç–∏ –ø—Ä—è–º–æ—é –ª—ñ–Ω—ñ—î—é! ‚úó
```

**Linear SVM –Ω–µ —Å–ø—Ä–∞—Ü—é—î!**

---

## –†—ñ—à–µ–Ω–Ω—è: Kernel Trick

### –Ü–¥–µ—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ—ó

**–ü–µ—Ä–µ–Ω–æ—Å–∏–º–æ –¥–∞–Ω—ñ –≤ –≤–∏—â–∏–π –≤–∏–º—ñ—Ä, –¥–µ –≤–æ–Ω–∏ —Å—Ç–∞—é—Ç—å –ª—ñ–Ω—ñ–π–Ω–æ —Ä–æ–∑–¥—ñ–ª–∏–º—ñ:**

```
After transformation to 3D:
         z
         |
         |    ‚Ä¢ ‚Ä¢ ‚Ä¢
         |  ‚Ä¢  ‚Ä¢  ‚Ä¢
    _____|_________
         |
    √ó √ó √ó|√ó √ó √ó
         |
         
–¢–µ–ø–µ—Ä –º–æ–∂–Ω–∞ —Ä–æ–∑–¥—ñ–ª–∏—Ç–∏ –ø–ª–æ—â–∏–Ω–æ—é! ‚úì
```

### –ü—Ä–æ–±–ª–µ–º–∞ —è–≤–Ω–æ—ó —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ—ó

**–ü—Ä–∏–∫–ª–∞–¥:** Polynomial transformation degree 2 –¥–ª—è 2D ‚Üí 3D:

$$\phi(x) = [x_1, x_2] \rightarrow [x_1^2, \sqrt{2}x_1x_2, x_2^2]$$

**–ü—Ä–æ–±–ª–µ–º–∏:**

- –ü–æ—Ç—Ä—ñ–±–Ω–æ **—è–≤–Ω–æ –æ–±—á–∏—Å–ª—é–≤–∞—Ç–∏** $\phi(x)$ –¥–ª—è –∫–æ–∂–Ω–æ—ó —Ç–æ—á–∫–∏
- **–í–∏—Å–æ–∫–∏–π –≤–∏–º—ñ—Ä** ‚Üí –±–∞–≥–∞—Ç–æ –ø–∞–º'—è—Ç—ñ
- **–ü–æ–≤—ñ–ª—å–Ω–æ** –¥–ª—è –≤–µ–ª–∏–∫–∏—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ–π

### Kernel Trick: –ú–∞–≥—ñ—è!

**–ó–∞–º—ñ—Å—Ç—å –æ–±—á–∏—Å–ª–µ–Ω–Ω—è $\phi(x)$, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ kernel function:**

$$K(x_i, x_j) = \phi(x_i)^T \phi(x_j)$$

**Kernel –æ–±—á–∏—Å–ª—é—î dot product —É transformed space –ë–ï–ó —è–≤–Ω–æ–≥–æ –æ–±—á–∏—Å–ª–µ–Ω–Ω—è $\phi(x)$!**

---

## –ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞ Kernel Trick

### Dual Formulation

**–ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è Linear SVM —á–µ—Ä–µ–∑ dual:**

$$f(x) = \text{sign}\left(\sum_{i=1}^{n} \alpha_i y_i (x_i^T x) + b\right)$$

**–ó kernel trick:**

$$f(x) = \text{sign}\left(\sum_{i=1}^{n} \alpha_i y_i K(x_i, x) + b\right)$$

**–ó–∞–º—ñ–Ω–∏–ª–∏ dot product $x_i^T x$ –Ω–∞ kernel $K(x_i, x)$!**

### Kernel Function

**Kernel function** –æ–±—á–∏—Å–ª—é—î similarity –º—ñ–∂ –¥–≤–æ–º–∞ —Ç–æ—á–∫–∞–º–∏:

$$K(x_i, x_j) = \phi(x_i)^T \phi(x_j)$$

–¥–µ $\phi$ ‚Äî —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ—è –≤ –≤–∏—â–∏–π –≤–∏–º—ñ—Ä.

**–ü—Ä–∏–∫–ª–∞–¥ (Polynomial kernel d=2):**

$$K(x, z) = (x^T z)^2$$

–¶–µ –µ–∫–≤—ñ–≤–∞–ª–µ–Ω—Ç–Ω–æ:

$$\phi(x)^T \phi(z)$$

–¥–µ $\phi([x_1, x_2]) = [x_1^2, \sqrt{2}x_1x_2, x_2^2]$

**–ê–ª–µ –º–∏ –ù–ï –æ–±—á–∏—Å–ª—é—î–º–æ $\phi$ —è–≤–Ω–æ!**

---

## –ü–æ–ø—É–ª—è—Ä–Ω—ñ Kernels

### 1. Linear Kernel

$$K(x_i, x_j) = x_i^T x_j$$

**–ö–æ–ª–∏:**

- ‚úÖ –õ—ñ–Ω—ñ–π–Ω–æ —Ä–æ–∑–¥—ñ–ª–∏–º—ñ –¥–∞–Ω—ñ
- ‚úÖ High-dimensional (text)
- ‚úÖ –®–≤–∏–¥–∫—ñ—Å—Ç—å –≤–∞–∂–ª–∏–≤–∞

**–ï–∫–≤—ñ–≤–∞–ª–µ–Ω—Ç–Ω–æ Linear SVM.**

---

### 2. Polynomial Kernel

$$K(x_i, x_j) = (x_i^T x_j + c)^d$$

–¥–µ:

- $d$ ‚Äî degree (–∑–∞–∑–≤–∏—á–∞–π 2-4)
- $c$ ‚Äî coef0 (–∫–æ–Ω—Å—Ç–∞–Ω—Ç–∞, –∑–∞–∑–≤–∏—á–∞–π 0 –∞–±–æ 1)

#### –ü—Ä–∏–∫–ª–∞–¥: degree=2

–î–ª—è 2D: $x = [x_1, x_2]$

$$K(x, z) = (x^T z + 1)^2$$

–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ—è —É 6D:

$$\phi(x) = [1, \sqrt{2}x_1, \sqrt{2}x_2, x_1^2, x_2^2, \sqrt{2}x_1x_2]$$

#### –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏

- ‚úÖ –ü–æ–ª—ñ–Ω–æ–º—ñ–∞–ª—å–Ω—ñ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ
- ‚úÖ Image processing
- ‚ö†Ô∏è –û–±–º–µ–∂–µ–Ω–∏–π degree (d ‚â§ 4)
- ‚ùå Numerical instability –ø—Ä–∏ –≤–µ–ª–∏–∫–æ–º—É d

#### –ö–æ–¥

```python
from sklearn.svm import SVC

svm_poly = SVC(
    kernel='poly',
    degree=3,           # Polynomial degree
    C=1.0,
    coef0=1,           # –ö–æ–Ω—Å—Ç–∞–Ω—Ç–∞
    gamma='scale',
    random_state=42
)

svm_poly.fit(X_train, y_train)
print(f"Polynomial SVM Accuracy: {svm_poly.score(X_test, y_test):.4f}")
```

---

### 3. RBF Kernel (Gaussian)

$$K(x_i, x_j) = \exp\left(-\gamma ||x_i - x_j||^2\right)$$

–¥–µ $\gamma = \frac{1}{2\sigma^2}$ (–ø–∞—Ä–∞–º–µ—Ç—Ä —à–∏—Ä–∏–Ω–∏).

**–ù–∞–π–ø–æ–ø—É–ª—è—Ä–Ω—ñ—à–∏–π kernel! üåü**

#### –í–ª–∞—Å—Ç–∏–≤–æ—Å—Ç—ñ

- –í—ñ–¥–æ–±—Ä–∞–∂–∞—î –≤ **–±–µ–∑–∫—ñ–Ω–µ—á–Ω–æ–≤–∏–º—ñ—Ä–Ω–∏–π** –ø—Ä–æ—Å—Ç—ñ—Ä
- –°—Ö–æ–∂—ñ—Å—Ç—å –∑–º–µ–Ω—à—É—î—Ç—å—Å—è **–µ–∫—Å–ø–æ–Ω–µ–Ω—Ü—ñ–π–Ω–æ** –∑ –≤—ñ–¥—Å—Ç–∞–Ω–Ω—é
- **–£–Ω—ñ–≤–µ—Ä—Å–∞–ª—å–Ω–∏–π** kernel (–º–æ–∂–µ –∞–ø—Ä–æ–∫—Å–∏–º—É–≤–∞—Ç–∏ –±—É–¥—å-—è–∫—É —Ñ—É–Ω–∫—Ü—ñ—é)
- **–õ–æ–∫–∞–ª—å–Ω–∏–π** kernel ‚Äî —Ç–æ—á–∫–∏ –¥–∞–ª–µ–∫–æ –º–∞–π–∂–µ –Ω–µ –≤–ø–ª–∏–≤–∞—é—Ç—å

#### –Ü–Ω—Ç—É—ó—Ü—ñ—è

RBF kernel –≤–∏–º—ñ—Ä—é—î **—Å—Ö–æ–∂—ñ—Å—Ç—å** –º—ñ–∂ —Ç–æ—á–∫–∞–º–∏:

$$K(x_i, x_j) = \begin{cases} 1 & \text{—è–∫—â–æ } x_i = x_j \ \approx 0 & \text{—è–∫—â–æ } x_i \text{ –¥–∞–ª–µ–∫–æ –≤—ñ–¥ } x_j \end{cases}$$

#### –ü–∞—Ä–∞–º–µ—Ç—Ä Œ≥ (gamma)

```
Œ≥ –º–∞–ª–∏–π (0.01):              Œ≥ –æ–ø—Ç–∏–º–∞–ª—å–Ω–∏–π (1.0):        Œ≥ –≤–µ–ª–∏–∫–∏–π (10):
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê                 ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê                     ‚ïê‚ïê‚ïê
–®–∏—Ä–æ–∫–∏–π Gaussian             –ë–∞–ª–∞–Ω—Å                      –í—É–∑—å–∫–∏–π Gaussian
–ì–ª–∞–¥–∫–∞ boundary              ‚úì –ù–∞–π–∫—Ä–∞—â–µ                  –î—É–∂–µ –ª–æ–∫–∞–ª—å–Ω–∞
High bias                                                High variance
Underfitting                                             Overfitting

  √ó√ó√ó√ó√ó√ó |‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢              √ó√ó√ó√ó |‚Ä¢‚Ä¢‚Ä¢‚Ä¢                 √ó√ó√ó|‚Ä¢‚Ä¢‚Ä¢
  √ó√ó√ó√ó√ó√ó |‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢              √ó√ó√ó√ó |‚Ä¢‚Ä¢‚Ä¢‚Ä¢                 √ó√ó√ó|‚Ä¢‚Ä¢‚Ä¢
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê                 ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê                     ‚ïê‚ïê‚ïê
```

**Trade-off Œ≥:**

- **–ú–∞–ª–∏–π Œ≥:** —Ç–æ—á–∫–∏ –¥–∞–ª–µ–∫–æ –≤–ø–ª–∏–≤–∞—é—Ç—å ‚Üí smooth boundary
- **–í–µ–ª–∏–∫–∏–π Œ≥:** —Ç—ñ–ª—å–∫–∏ –±–ª–∏–∑—å–∫—ñ —Ç–æ—á–∫–∏ ‚Üí wiggly boundary

#### –ö–æ–¥

```python
from sklearn.svm import SVC

svm_rbf = SVC(
    kernel='rbf',
    C=1.0,
    gamma='scale',      # –∞–±–æ 'auto', –∞–±–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–µ –∑–Ω–∞—á–µ–Ω–Ω—è
    random_state=42
)

svm_rbf.fit(X_train, y_train)
print(f"RBF SVM Accuracy: {svm_rbf.score(X_test, y_test):.4f}")
```

#### –ó–Ω–∞—á–µ–Ω–Ω—è gamma

**–ó–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º:**

- `gamma='scale'` (—Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–æ): $\gamma = \frac{1}{n_{features} \cdot \text{Var}(X)}$
- `gamma='auto'`: $\gamma = \frac{1}{n_{features}}$

**–ö–∞—Å—Ç–æ–º–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è:**

- –¢–∏–ø–æ–≤–æ: [0.001, 0.01, 0.1, 1, 10]

---

### 4. Sigmoid Kernel (Tanh)

$$K(x_i, x_j) = \tanh(\gamma x_i^T x_j + c)$$

**–ö–æ–ª–∏:**

- ‚ö†Ô∏è –†—ñ–¥–∫–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è
- –°—Ö–æ–∂–∏–π –Ω–∞ neural networks activation
- –ú–æ–∂–µ –±—É—Ç–∏ –Ω–µ positive semi-definite

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—è:** –ö—Ä–∞—â–µ RBF.

---

## –í–∏–±—ñ—Ä Kernel

### Decision Tree

```
                –î–∞–Ω—ñ –ª—ñ–Ω—ñ–π–Ω–æ —Ä–æ–∑–¥—ñ–ª–∏–º—ñ?
                /                    \
             –¢–∞–∫                      –ù—ñ
              |                        |
       Linear Kernel            –§–æ—Ä–º–∞ boundary?
                                /            \
                           Polynomial      Unknown
                                |              |
                         Polynomial     RBF Kernel ‚úì
                          Kernel        (universal)
```

### –ü—Ä–∞–∫—Ç–∏—á–Ω—ñ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó

1. **–ü–æ—á–Ω–∏ –∑ RBF** ‚Äî –Ω–∞–π–±–µ–∑–ø–µ—á–Ω—ñ—à–∏–π –≤–∏–±—ñ—Ä
2. **Linear –¥–ª—è high-d** ‚Äî text –∑ d > 1000
3. **Polynomial** ‚Äî —è–∫—â–æ domain knowledge –ø—Ä–æ –ø–æ–ª—ñ–Ω–æ–º—ñ–∞–ª—å–Ω—ñ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ
4. **Sigmoid** ‚Äî –º–∞–π–∂–µ –Ω—ñ–∫–æ–ª–∏

---

## –ì—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –¥–ª—è Kernel SVM

### –î–ª—è RBF Kernel (–Ω–∞–π–≤–∞–∂–ª–∏–≤—ñ—à–µ!)

**–î–≤–∞ –≥–æ–ª–æ–≤–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏:**

1. **C** ‚Äî regularization strength
    
    - –ö–æ–Ω—Ç—Ä–æ–ª—é—î trade-off –º—ñ–∂ margin —Ç–∞ –ø–æ–º–∏–ª–∫–∞–º–∏
    - –¢–∏–ø–æ–≤—ñ –∑–Ω–∞—á–µ–Ω–Ω—è: [0.1, 1, 10, 100, 1000]
    
2. **Œ≥ (gamma)** ‚Äî kernel coefficient
    
    - –ö–æ–Ω—Ç—Ä–æ–ª—é—î —à–∏—Ä–∏–Ω—É Gaussian
    - –¢–∏–ø–æ–≤—ñ –∑–Ω–∞—á–µ–Ω–Ω—è: [0.001, 0.01, 0.1, 1, 10]

### Grid Search

```python
from sklearn.model_selection import GridSearchCV

param_grid = {
    'C': [0.1, 1, 10, 100],
    'gamma': [0.001, 0.01, 0.1, 1, 'scale']
}

grid_search = GridSearchCV(
    SVC(kernel='rbf', random_state=42),
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    verbose=1
)

grid_search.fit(X_train, y_train)

print(f"Best params: {grid_search.best_params_}")
print(f"Best CV score: {grid_search.best_score_:.4f}")
```

---

## –í–ø–ª–∏–≤ C —Ç–∞ Œ≥ –Ω–∞ Decision Boundary

```python
from sklearn.datasets import make_moons
import matplotlib.pyplot as plt
import numpy as np

# –ù–µ–ª—ñ–Ω—ñ–π–Ω—ñ –¥–∞–Ω—ñ
X, y = make_moons(n_samples=200, noise=0.15, random_state=42)

params = [
    {'C': 0.1, 'gamma': 0.1, 'title': 'C=0.1, Œ≥=0.1 (Underfitting)'},
    {'C': 1, 'gamma': 1, 'title': 'C=1, Œ≥=1 (Balanced)'},
    {'C': 100, 'gamma': 10, 'title': 'C=100, Œ≥=10 (Overfitting)'},
]

fig, axes = plt.subplots(1, 3, figsize=(18, 5))

for idx, param in enumerate(params):
    svm = SVC(kernel='rbf', C=param['C'], gamma=param['gamma'])
    svm.fit(X, y)
    
    # Decision boundary
    h = 0.02
    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5
    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))
    
    Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    
    axes[idx].contourf(xx, yy, Z, alpha=0.3, cmap='viridis')
    axes[idx].scatter(X[:, 0], X[:, 1], c=y, cmap='viridis',
                     edgecolors='k', s=50)
    axes[idx].scatter(svm.support_vectors_[:, 0],
                     svm.support_vectors_[:, 1],
                     s=200, linewidth=1.5, facecolors='none',
                     edgecolors='red')
    axes[idx].set_title(f"{param['title']}\nSV: {len(svm.support_vectors_)}",
                       fontsize=11, fontweight='bold')

plt.tight_layout()
plt.show()
```

---

## –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è Grid Search

```python
import pandas as pd
import seaborn as sns

# Grid Search results
results = grid_search.cv_results_

# –í–∏—Ç—è–≥—Ç–∏ C —Ç–∞ gamma
C_values = []
gamma_values = []
scores = []

for params, score in zip(results['params'], results['mean_test_score']):
    if isinstance(params['gamma'], float):  # –¢—ñ–ª—å–∫–∏ —á–∏—Å–ª–æ–≤—ñ
        C_values.append(params['C'])
        gamma_values.append(params['gamma'])
        scores.append(score)

# Pivot table –¥–ª—è heatmap
df = pd.DataFrame({
    'C': C_values,
    'gamma': gamma_values,
    'score': scores
})
pivot = df.pivot_table(values='score', index='gamma', columns='C')

# Heatmap
plt.figure(figsize=(10, 6))
sns.heatmap(pivot, annot=True, fmt='.3f', cmap='viridis')
plt.title('Grid Search CV Scores (C vs gamma)', 
          fontsize=14, fontweight='bold')
plt.xlabel('C', fontsize=12)
plt.ylabel('gamma', fontsize=12)
plt.tight_layout()
plt.show()
```

---

## –ü—Ä–∏–∫–ª–∞–¥: –ù–µ–ª—ñ–Ω—ñ–π–Ω–∞ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è

```python
import numpy as np
from sklearn.datasets import make_circles, make_moons
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# –†—ñ–∑–Ω—ñ –Ω–µ–ª—ñ–Ω—ñ–π–Ω—ñ –¥–∞—Ç–∞—Å–µ—Ç–∏
datasets = {
    'Circles': make_circles(n_samples=200, noise=0.1, factor=0.3, random_state=42),
    'Moons': make_moons(n_samples=200, noise=0.15, random_state=42),
}

for name, (X, y) in datasets.items():
    print(f"\n{'='*60}")
    print(f"Dataset: {name}")
    print(f"{'='*60}")
    
    # –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )
    
    # –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # Linear SVM (baseline)
    svm_linear = SVC(kernel='linear', C=1.0)
    svm_linear.fit(X_train_scaled, y_train)
    linear_score = svm_linear.score(X_test_scaled, y_test)
    
    # RBF SVM
    svm_rbf = SVC(kernel='rbf', C=10, gamma=1)
    svm_rbf.fit(X_train_scaled, y_train)
    rbf_score = svm_rbf.score(X_test_scaled, y_test)
    
    print(f"Linear SVM: {linear_score:.4f}")
    print(f"RBF SVM:    {rbf_score:.4f}")
    print(f"Improvement: {(rbf_score - linear_score)*100:.2f}%")
```

---

## –ü–µ—Ä–µ–≤–∞–≥–∏ —Ç–∞ –Ω–µ–¥–æ–ª—ñ–∫–∏ Kernel SVM

### –ü–µ—Ä–µ–≤–∞–≥–∏ ‚úì

|–ü–µ—Ä–µ–≤–∞–≥–∞|–ü–æ—è—Å–Ω–µ–Ω–Ω—è|
|---|---|
|**–ù–µ–ª—ñ–Ω—ñ–π–Ω—ñ boundaries**|–û–±—Ä–æ–±–∫–∞ —Å–∫–ª–∞–¥–Ω–∏—Ö patterns|
|**Kernel trick**|–ï—Ñ–µ–∫—Ç–∏–≤–Ω–æ –±–µ–∑ —è–≤–Ω–æ—ó —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ—ó|
|**–£–Ω—ñ–≤–µ—Ä—Å–∞–ª—å–Ω—ñ—Å—Ç—å**|RBF –º–æ–∂–µ –∞–ø—Ä–æ–∫—Å–∏–º—É–≤–∞—Ç–∏ –≤—Å–µ|
|**–í–∏—Å–æ–∫–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å**|–û–¥–∏–Ω –∑ –Ω–∞–π—Ç–æ—á–Ω—ñ—à–∏—Ö|
|**Flexibility**|–†—ñ–∑–Ω—ñ kernels|

### –ù–µ–¥–æ–ª—ñ–∫–∏ ‚úó

|–ù–µ–¥–æ–ª—ñ–∫|–ü–æ—è—Å–Ω–µ–Ω–Ω—è|
|---|---|
|**–ü–æ–≤—ñ–ª—å–Ω–µ training**|O(n¬≤) –¥–æ O(n¬≥)|
|**–í–∏–±—ñ—Ä kernel**|Domain knowledge|
|**–ì—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∏**|C, Œ≥ –ø–æ—Ç—Ä—ñ–±–Ω–æ –ø—ñ–¥–±–∏—Ä–∞—Ç–∏|
|**–í–µ–ª–∏–∫—ñ –¥–∞–Ω—ñ**|n > 100k –¥—É–∂–µ –ø–æ–≤—ñ–ª—å–Ω–æ|
|**–ß–æ—Ä–Ω–∞ —Å–∫—Ä–∏–Ω—å–∫–∞**|–°–∫–ª–∞–¥–Ω–æ —ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç—É–≤–∞—Ç–∏|

---

## –ü—Ä–∞–∫—Ç–∏—á–Ω—ñ –ø–æ—Ä–∞–¥–∏ üí°

1. **–ü–æ—á–Ω–∏ –∑ RBF kernel** ‚Äî —É–Ω—ñ–≤–µ—Ä—Å–∞–ª—å–Ω–∏–π –≤–∏–±—ñ—Ä
2. **Grid Search –¥–ª—è C —Ç–∞ Œ≥** ‚Äî –æ–±–æ–≤'—è–∑–∫–æ–≤–æ!
3. **–ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è –¥–∞–Ω–∏—Ö** ‚Äî –∫—Ä–∏—Ç–∏—á–Ω–æ –≤–∞–∂–ª–∏–≤–∞
4. **Linear –¥–ª—è high-d** ‚Äî —à–≤–∏–¥—à–µ –¥–ª—è text
5. **–í—ñ–∑—É–∞–ª—ñ–∑—É–π boundaries** ‚Äî —Ä–æ–∑—É–º—ñ–π –ø–æ–≤–µ–¥—ñ–Ω–∫—É
6. **Polynomial –æ–±–µ—Ä–µ–∂–Ω–æ** ‚Äî degree > 4 —Ä—ñ–¥–∫–æ –ø–æ—Ç—Ä—ñ–±–Ω–∏–π
7. **–ü–µ—Ä–µ–≤—ñ—Ä support vectors** ‚Äî —è–∫—â–æ > 50% ‚Üí overfitting
8. **–ü–æ—á–∞—Ç–∫–æ–≤—ñ –∑–Ω–∞—á–µ–Ω–Ω—è** ‚Äî C=1, gamma='scale'
9. **Cross-validation** ‚Äî –∑–∞–≤–∂–¥–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π CV
10. **–ü–æ—Ä—ñ–≤–Ω—è–π kernels** ‚Äî —Å–ø—Ä–æ–±—É–π –∫—ñ–ª—å–∫–∞ –≤–∞—Ä—ñ–∞–Ω—Ç—ñ–≤

---

## –ö–ª—é—á–æ–≤—ñ –≤–∏—Å–Ω–æ–≤–∫–∏

> Kernel SVM –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î kernel trick –¥–ª—è –Ω–µ–ª—ñ–Ω—ñ–π–Ω–∏—Ö boundaries –±–µ–∑ —è–≤–Ω–æ—ó —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ—ó –≤ –≤–∏—â–∏–π –≤–∏–º—ñ—Ä.

**Kernel Trick:**

$$K(x_i, x_j) = \phi(x_i)^T \phi(x_j)$$

**–ü–æ–ø—É–ª—è—Ä–Ω—ñ kernels:**

- **Linear:** $K(x, z) = x^T z$
- **Polynomial:** $K(x, z) = (x^T z + c)^d$
- **RBF:** $K(x, z) = \exp(-\gamma ||x - z||^2)$ ‚≠ê

**–ì—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∏ RBF:**

- **C:** –º–∞–ª–∏–π ‚Üí wide margin, –≤–µ–ª–∏–∫–∏–π ‚Üí narrow margin
- **Œ≥:** –º–∞–ª–∏–π ‚Üí smooth, –≤–µ–ª–∏–∫–∏–π ‚Üí wiggly

**–ö–†–ò–¢–ò–ß–ù–û:**

- –ü–æ—á–Ω–∏ –∑ RBF kernel
- Grid Search –¥–ª—è C —Ç–∞ Œ≥
- –ó–∞–≤–∂–¥–∏ –Ω–æ—Ä–º–∞–ª—ñ–∑—É–π –¥–∞–Ω—ñ

---

#ml #svm #kernel-svm #rbf #kernel-trick #nonlinear