# SVR (Support Vector Regression)

## –©–æ —Ü–µ?

**Support Vector Regression (SVR)** ‚Äî —Ü–µ –∞–¥–∞–ø—Ç–∞—Ü—ñ—è SVM –¥–ª—è **—Ä–µ–≥—Ä–µ—Å—ñ—ó**, —è–∫–∞ –∑–∞–º—ñ—Å—Ç—å –º–∞–∫—Å–∏–º—ñ–∑–∞—Ü—ñ—ó margin –¥–ª—è –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó, **–º—ñ–Ω—ñ–º—ñ–∑—É—î –ø–æ–º–∏–ª–∫—É –≤ –º–µ–∂–∞—Ö Œµ-tube** (epsilon-insensitive loss).

**–ì–æ–ª–æ–≤–Ω–∞ —ñ–¥–µ—è:** –∑–Ω–∞–π—Ç–∏ —Ñ—É–Ω–∫—Ü—ñ—é, —è–∫–∞ –º–∞—î –º–∞–∫—Å–∏–º—É–º Œµ –≤—ñ–¥—Ö–∏–ª–µ–Ω–Ω—è –≤—ñ–¥ —Ñ–∞–∫—Ç–∏—á–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω—å, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—á–∏ —è–∫–æ–º–æ–≥–∞ –º–µ–Ω—à–µ —Ç–æ—á–æ–∫ (support vectors).

## –ù–∞–≤—ñ—â–æ –ø–æ—Ç—Ä—ñ–±–µ–Ω?

- üìà **Robust regression** ‚Äî —Å—Ç—ñ–π–∫–∏–π –¥–æ outliers
- üéØ **Œµ-insensitive** ‚Äî —ñ–≥–Ω–æ—Ä—É—î –º–∞–ª—ñ –ø–æ–º–∏–ª–∫–∏
- üîß **Kernel trick** ‚Äî –Ω–µ–ª—ñ–Ω—ñ–π–Ω—ñ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ
- üí° **High-dimensional** ‚Äî –ø—Ä–∞—Ü—é—î –ø—Ä–∏ d > n
- ‚ö° **Support vectors** ‚Äî –∫–æ–º–ø–∞–∫—Ç–Ω–∞ –º–æ–¥–µ–ª—å

## –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏?

**–ü–æ—Ç—Ä—ñ–±–Ω–æ:**
- **Robust regression** –ø–æ—Ç—Ä—ñ–±–Ω–∞
- **Outliers** –≤ –¥–∞–Ω–∏—Ö
- **–ù–µ–ª—ñ–Ω—ñ–π–Ω—ñ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ** (–∑ kernels)
- –°–µ—Ä–µ–¥–Ω—ñ –¥–∞—Ç–∞—Å–µ—Ç–∏ (n = 1k-50k)
- **High-dimensional** –¥–∞–Ω—ñ

**–ù–µ –ø–æ—Ç—Ä—ñ–±–Ω–æ:**
- **–î—É–∂–µ –≤–µ–ª–∏–∫—ñ –¥–∞—Ç–∞—Å–µ—Ç–∏** (n > 50k) ‚Üí –ø–æ–≤—ñ–ª—å–Ω–æ
- –ü—Ä–æ—Å—Ça **–ª—ñ–Ω—ñ–π–Ω–∞ —Ä–µ–≥—Ä–µ—Å—ñ—è** –¥–æ—Å—Ç–∞—Ç–Ω—è ‚Üí Linear Regression
- –ü–æ—Ç—Ä—ñ–±–Ω–∞ **—ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å** ‚Üí Linear/Polynomial Regression
- **–®–≤–∏–¥–∫—ñ—Å—Ç—å** –∫—Ä–∏—Ç–∏—á–Ω–∞ ‚Üí Linear models

---

## –ö–æ–Ω—Ü–µ–ø—Ü—ñ—è: Œµ-insensitive Loss

### –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è Œµ-tube

\`\`\`
        y
        |     Œµ-tube
        |  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    √ó   | ‚Ä¢  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  ‚Üê Predicted function
  √ó   √ó |‚Ä¢  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
√ó   ‚Ä¢   ‚Ä¢
      √ó |
        |____________ x

–¢–æ—á–∫–∏ –≤—Å–µ—Ä–µ–¥–∏–Ω—ñ Œµ-tube: penalty = 0
–¢–æ—á–∫–∏ –∑–∞ –º–µ–∂–∞–º–∏ Œµ-tube: penalty –ø—Ä–æ–ø–æ—Ä—Ü—ñ–π–Ω–æ –≤—ñ–¥—Å—Ç–∞–Ω—ñ
\`\`\`

### –ß–æ–º—É Œµ-tube?

**–ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –∑ —ñ–Ω—à–∏–º–∏ loss functions:**

\`\`\`
Squared Error (MSE):        Œµ-insensitive:
     Loss                        Loss
      |                            |
      |    /                       |    ___/
      |   /                        |___/    
      |  /                         |
      |_/______ Error              |_______ Error
      0                           -Œµ  0  Œµ

MSE: –≤—Å—ñ –ø–æ–º–∏–ª–∫–∏ –º–∞—é—Ç—å penalty    SVR: –º–∞–ª—ñ –ø–æ–º–∏–ª–∫–∏ —ñ–≥–Ω–æ—Ä—É—é—Ç—å—Å—è
\`\`\`

**–ü–µ—Ä–µ–≤–∞–≥–∏ Œµ-insensitive:**
- ‚úÖ **Sparse solution** ‚Äî –±–∞–≥–∞—Ç–æ —Ç–æ—á–æ–∫ –≤—Å–µ—Ä–µ–¥–∏–Ω—ñ tube ‚Üí –Ω–µ —î SV
- ‚úÖ **Robust –¥–æ noise** ‚Äî –º–∞–ª—ñ –ø–æ–º–∏–ª–∫–∏ –Ω–µ —à—Ç—Ä–∞—Ñ—É—é—Ç—å—Å—è
- ‚úÖ **Outlier resistance** ‚Äî –≤–µ–ª–∏–∫—ñ –ø–æ–º–∏–ª–∫–∏ —à—Ç—Ä–∞—Ñ—É—é—Ç—å—Å—è –ª—ñ–Ω—ñ–π–Ω–æ (–Ω–µ –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–æ)

---

## –ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞ SVR

### –û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ–π–Ω–∞ –∑–∞–¥–∞—á–∞

**–ú–µ—Ç–∞:** –ó–Ω–∞–π—Ç–∏ —Ñ—É–Ω–∫—Ü—ñ—é $f(x) = w^T x + b$ —è–∫–∞ –∞–ø—Ä–æ–∫—Å–∏–º—É—î –¥–∞–Ω—ñ –∑ –º–∞–∫—Å–∏–º—É–º Œµ –≤—ñ–¥—Ö–∏–ª–µ–Ω–Ω—è–º.

$$\min_{w, b, \xi, \xi^*} \frac{1}{2}||w||^2 + C \sum_{i=1}^{n} (\xi_i + \xi_i^*)$$

subject to:

$$y_i - (w^T x_i + b) \leq \varepsilon + \xi_i$$
$$(w^T x_i + b) - y_i \leq \varepsilon + \xi_i^*$$
$$\xi_i, \xi_i^* \geq 0$$

–¥–µ:
- $\varepsilon$ ‚Äî —à–∏—Ä–∏–Ω–∞ tube (tolerance)
- $\xi_i$ ‚Äî slack variable –¥–ª—è –≤–µ—Ä—Ö–Ω—å–æ–≥–æ –ø–æ—Ä—É—à–µ–Ω–Ω—è
- $\xi_i^*$ ‚Äî slack variable –¥–ª—è –Ω–∏–∂–Ω—å–æ–≥–æ –ø–æ—Ä—É—à–µ–Ω–Ω—è
- $C$ ‚Äî regularization parameter

### –Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü—ñ—è

**–ö–æ–º–ø–æ–Ω–µ–Ω—Ç–∏:**
1. $\frac{1}{2}||w||^2$ ‚Äî regularization (smooth function)
2. $C \sum (\xi_i + \xi_i^*)$ ‚Äî penalty –∑–∞ –ø–æ–º–∏–ª–∫–∏ > Œµ

**Slack variables:**
- $\xi_i = 0$ —Ç–∞ $\xi_i^* = 0$ ‚Üí —Ç–æ—á–∫–∞ –≤—Å–µ—Ä–µ–¥–∏–Ω—ñ Œµ-tube ‚úì
- $\xi_i > 0$ ‚Üí —Ç–æ—á–∫–∞ –≤–∏—â–µ –≤–µ—Ä—Ö–Ω—å–æ—ó –º–µ–∂—ñ tube
- $\xi_i^* > 0$ ‚Üí —Ç–æ—á–∫–∞ –Ω–∏–∂—á–µ –Ω–∏–∂–Ω—å–æ—ó –º–µ–∂—ñ tube

---

## Kernel SVR

### –ù–µ–ª—ñ–Ω—ñ–π–Ω–∞ —Ä–µ–≥—Ä–µ—Å—ñ—è —á–µ—Ä–µ–∑ kernels

**–¢–∞–∫ —Å–∞–º–æ —è–∫ Kernel SVM, SVR –ø—ñ–¥—Ç—Ä–∏–º—É—î kernel trick:**

$$f(x) = \sum_{i \in SV} (\alpha_i - \alpha_i^*) K(x_i, x) + b$$

–¥–µ $K(x_i, x)$ ‚Äî kernel function.

### –ü–æ–ø—É–ª—è—Ä–Ω—ñ kernels –¥–ª—è SVR

1. **Linear:** $K(x, z) = x^T z$
   - –õ—ñ–Ω—ñ–π–Ω–∞ –∑–∞–ª–µ–∂–Ω—ñ—Å—Ç—å
   
2. **Polynomial:** $K(x, z) = (x^T z + c)^d$
   - –ü–æ–ª—ñ–Ω–æ–º—ñ–∞–ª—å–Ω–∞ –∑–∞–ª–µ–∂–Ω—ñ—Å—Ç—å
   
3. **RBF (Gaussian):** $K(x, z) = \exp(-\gamma ||x - z||^2)$
   - –ù–µ–ª—ñ–Ω—ñ–π–Ω–∞ –∑–∞–ª–µ–∂–Ω—ñ—Å—Ç—å (—É–Ω—ñ–≤–µ—Ä—Å–∞–ª—å–Ω–∏–π)

---

## –ö–æ–¥ (Python + scikit-learn)

### –ë–∞–∑–æ–≤–∏–π –ø—Ä–∏–∫–ª–∞–¥: Linear SVR

\`\`\`python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.svm import SVR
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score

# 1. –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –¥–∞–Ω–∏—Ö
X, y = make_regression(
    n_samples=200,
    n_features=1,
    noise=10,
    random_state=42
)

# –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# 2. –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è (–†–ï–ö–û–ú–ï–ù–î–û–í–ê–ù–û)
scaler_X = StandardScaler()
scaler_y = StandardScaler()

X_train_scaled = scaler_X.fit_transform(X_train)
X_test_scaled = scaler_X.transform(X_test)
y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).ravel()
y_test_scaled = scaler_y.transform(y_test.reshape(-1, 1)).ravel()

# 3. Linear SVR
svr_linear = SVR(
    kernel='linear',
    C=1.0,              # Regularization
    epsilon=0.1         # Œµ-tube width
)

# 4. –ù–∞–≤—á–∞–Ω–Ω—è
svr_linear.fit(X_train_scaled, y_train_scaled)

# 5. –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
y_pred_scaled = svr_linear.predict(X_test_scaled)
y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).ravel()

# 6. –ú–µ—Ç—Ä–∏–∫–∏
print("=== Linear SVR ===")
print(f"R¬≤: {r2_score(y_test, y_pred):.4f}")
print(f"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.2f}")
print(f"Support vectors: {len(svr_linear.support_)}")
print(f"SV percentage: {len(svr_linear.support_)/len(X_train)*100:.1f}%")
\`\`\`

### RBF SVR –¥–ª—è –Ω–µ–ª—ñ–Ω—ñ–π–Ω–∏—Ö –¥–∞–Ω–∏—Ö

\`\`\`python
# –ù–µ–ª—ñ–Ω—ñ–π–Ω—ñ –¥–∞–Ω—ñ
X = np.sort(5 * np.random.rand(200, 1), axis=0)
y = np.sin(X).ravel() + np.random.randn(200) * 0.1

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# RBF SVR
svr_rbf = SVR(
    kernel='rbf',
    C=100,
    gamma=0.1,
    epsilon=0.1
)

svr_rbf.fit(X_train, y_train)

# –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
X_plot = np.linspace(0, 5, 300).reshape(-1, 1)
y_plot = svr_rbf.predict(X_plot)

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
plt.figure(figsize=(12, 6))
plt.scatter(X_train, y_train, alpha=0.5, s=30, label='Train', color='blue')
plt.scatter(X_test, y_test, alpha=0.5, s=50, label='Test', color='green')
plt.plot(X_plot, y_plot, color='red', linewidth=2, label='SVR Prediction')

# Œµ-tube
epsilon = 0.1
plt.fill_between(X_plot.ravel(),
                 y_plot - epsilon,
                 y_plot + epsilon,
                 alpha=0.2, color='red', label=f'Œµ-tube (Œµ={epsilon})')

# Support vectors
plt.scatter(X_train[svr_rbf.support_], y_train[svr_rbf.support_],
           s=200, facecolors='none', edgecolors='red',
           linewidth=2, label='Support Vectors')

plt.xlabel('X', fontsize=12)
plt.ylabel('y', fontsize=12)
plt.title('Support Vector Regression (RBF)', fontsize=14, fontweight='bold')
plt.legend(fontsize=11)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

print(f"\nR¬≤: {r2_score(y_test, svr_rbf.predict(X_test)):.4f}")
print(f"Support vectors: {len(svr_rbf.support_)} / {len(X_train)}")
\`\`\`

---

## –ì—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∏ SVR

### –¢—Ä–∏ –≥–æ–ª–æ–≤–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏

1. **C** ‚Äî regularization strength
   - –ú–∞–ª–∏–π C ‚Üí –≥–ª–∞–¥–∫–∞ —Ñ—É–Ω–∫—Ü—ñ—è, –±—ñ–ª—å—à–µ –ø–æ–º–∏–ª–æ–∫
   - –í–µ–ª–∏–∫–∏–π C ‚Üí —Ç–æ—á–Ω—ñ—à–∞ –∞–ø—Ä–æ–∫—Å–∏–º–∞—Ü—ñ—è, —Ä–∏–∑–∏–∫ overfitting

2. **Œµ (epsilon)** ‚Äî tube width
   - –ú–∞–ª–∏–π Œµ ‚Üí –≤—É–∂—á–∏–π tube, –±—ñ–ª—å—à–µ SV
   - –í–µ–ª–∏–∫–∏–π Œµ ‚Üí —à–∏—Ä—à–∏–π tube, –º–µ–Ω—à–µ SV

3. **Œ≥ (gamma)** ‚Äî –¥–ª—è RBF kernel
   - –ú–∞–ª–∏–π Œ≥ ‚Üí smooth prediction
   - –í–µ–ª–∏–∫–∏–π Œ≥ ‚Üí wiggly prediction

### –í–ø–ª–∏–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤

\`\`\`
Œµ –º–∞–ª–∏–π (0.01):              Œµ –æ–ø—Ç–∏–º–∞–ª—å–Ω–∏–π (0.1):        Œµ –≤–µ–ª–∏–∫–∏–π (1.0):
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê                  ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê                 ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
–í—É–∑—å–∫–∏–π tube                 –ë–∞–ª–∞–Ω—Å                      –®–∏—Ä–æ–∫–∏–π tube
–ë–∞–≥–∞—Ç–æ SV                    ‚úì –ù–∞–π–∫—Ä–∞—â–µ                  –ú–∞–ª–æ SV
–¢–æ—á–Ω–∞ –∞–ø—Ä–æ–∫—Å–∏–º–∞—Ü—ñ—è           Robust                      –ì—Ä—É–±–∞ –∞–ø—Ä–æ–∫—Å–∏–º–∞—Ü—ñ—è

    ‚Ä¢  ‚Ä¢  ‚Ä¢                      ‚Ä¢  ‚Ä¢  ‚Ä¢                     ‚Ä¢  ‚Ä¢  ‚Ä¢
   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                     ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                   ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    ‚Ä¢  ‚Ä¢  ‚Ä¢                      ‚Ä¢  ‚Ä¢  ‚Ä¢                     ‚Ä¢  ‚Ä¢  ‚Ä¢
\`\`\`

### Grid Search –¥–ª—è SVR

\`\`\`python
from sklearn.model_selection import GridSearchCV

param_grid = {
    'C': [0.1, 1, 10, 100],
    'epsilon': [0.01, 0.1, 0.5, 1.0],
    'gamma': [0.001, 0.01, 0.1, 1, 'scale']
}

grid_search = GridSearchCV(
    SVR(kernel='rbf'),
    param_grid,
    cv=5,
    scoring='r2',
    n_jobs=-1,
    verbose=1
)

grid_search.fit(X_train, y_train)

print(f"Best params: {grid_search.best_params_}")
print(f"Best CV R¬≤: {grid_search.best_score_:.4f}")

# –¢–µ—Å—Ç
best_svr = grid_search.best_estimator_
test_r2 = best_svr.score(X_test, y_test)
print(f"Test R¬≤: {test_r2:.4f}")
\`\`\`

---

## –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è SVR –∑ —ñ–Ω—à–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏

\`\`\`python
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor

# –ù–µ–ª—ñ–Ω—ñ–π–Ω—ñ –¥–∞–Ω—ñ
X = np.sort(5 * np.random.rand(100, 1), axis=0)
y = np.sin(X).ravel() + np.random.randn(100) * 0.1

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# –ú–æ–¥–µ–ª—ñ
models = {
    'Linear Regression': LinearRegression(),
    'Ridge': Ridge(alpha=1.0),
    'Decision Tree': DecisionTreeRegressor(max_depth=5, random_state=42),
    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),
    'SVR (Linear)': SVR(kernel='linear', C=1.0),
    'SVR (RBF)': SVR(kernel='rbf', C=100, gamma=0.1, epsilon=0.1)
}

print("="*60)
print("MODEL COMPARISON")
print("="*60)

for name, model in models.items():
    model.fit(X_train, y_train)
    train_r2 = model.score(X_train, y_train)
    test_r2 = model.score(X_test, y_test)
    
    print(f"{name:20} Train R¬≤: {train_r2:.4f}  Test R¬≤: {test_r2:.4f}")
\`\`\`

---

## –ü–µ—Ä–µ–≤–∞–≥–∏ —Ç–∞ –Ω–µ–¥–æ–ª—ñ–∫–∏ SVR

### –ü–µ—Ä–µ–≤–∞–≥–∏ ‚úì

| –ü–µ—Ä–µ–≤–∞–≥–∞ | –ü–æ—è—Å–Ω–µ–Ω–Ω—è |
|----------|-----------|
| **Robust –¥–æ outliers** | Œµ-insensitive loss |
| **Kernel trick** | –ù–µ–ª—ñ–Ω—ñ–π–Ω—ñ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ |
| **Sparse solution** | –¢—ñ–ª—å–∫–∏ support vectors |
| **High-dimensional** | –ü—Ä–∞—Ü—é—î –ø—Ä–∏ d > n |
| **Regularization** | –ü–∞—Ä–∞–º–µ—Ç—Ä C |

### –ù–µ–¥–æ–ª—ñ–∫–∏ ‚úó

| –ù–µ–¥–æ–ª—ñ–∫ | –ü–æ—è—Å–Ω–µ–Ω–Ω—è |
|---------|-----------|
| **–ü–æ–≤—ñ–ª—å–Ω–µ training** | O(n¬≤) –¥–æ O(n¬≥) |
| **–ì—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∏** | C, Œµ, Œ≥ –ø–æ—Ç—Ä—ñ–±–Ω–æ –ø—ñ–¥–±–∏—Ä–∞—Ç–∏ |
| **–í–µ–ª–∏–∫—ñ –¥–∞—Ç–∞—Å–µ—Ç–∏** | n > 50k –¥—É–∂–µ –ø–æ–≤—ñ–ª—å–Ω–æ |
| **–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å** | –ß–æ—Ä–Ω–∞ —Å–∫—Ä–∏–Ω—å–∫–∞ |
| **Sensitivity –¥–æ scaling** | –ü–æ—Ç—Ä–µ–±—É—î –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—ó |

---

## –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è SVR –∑ —ñ–Ω—à–∏–º–∏

### SVR vs Linear Regression

| –ö—Ä–∏—Ç–µ—Ä—ñ–π | SVR | Linear Regression |
|----------|-----|-------------------|
| **Outliers** | Robust | –ß—É—Ç–ª–∏–≤–∏–π |
| **Loss function** | Œµ-insensitive | MSE |
| **–®–≤–∏–¥–∫—ñ—Å—Ç—å** | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **–ù–µ–ª—ñ–Ω—ñ–π–Ω—ñ** | ‚úÖ –ó kernels | ‚ùå –¢—ñ–ª—å–∫–∏ linear |
| **–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å** | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |

### SVR vs Random Forest

| –ö—Ä–∏—Ç–µ—Ä—ñ–π | SVR | Random Forest |
|----------|-----|---------------|
| **–¢–æ—á–Ω—ñ—Å—Ç—å** | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **Training** | –ü–æ–≤—ñ–ª—å–Ω–∏–π | –®–≤–∏–¥–∫–∏–π |
| **Tuning** | –°–∫–ª–∞–¥–Ω–∏–π | –ü—Ä–æ—Å—Ç–∏–π |
| **High-dimensional** | ‚úÖ –ü—Ä–∞—Ü—é—î | ‚ö†Ô∏è –ú–æ–∂–µ –ø–æ–≥–∞–Ω–æ |

---

## –ü—Ä–∞–∫—Ç–∏—á–Ω—ñ –ø–æ—Ä–∞–¥–∏ üí°

1. **–ù–æ—Ä–º–∞–ª—ñ–∑—É–π X —Ç–∞ y** ‚Äî –∫—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è SVR
2. **–ü–æ—á–Ω–∏ –∑ RBF kernel** ‚Äî —É–Ω—ñ–≤–µ—Ä—Å–∞–ª—å–Ω–∏–π
3. **Grid Search** ‚Äî –ø—ñ–¥–±–∏—Ä–∞–π C, Œµ, Œ≥
4. **Œµ –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º** ‚Äî –ø–æ—á–Ω–∏ –∑ 0.1
5. **–ú–∞–ª—ñ –¥–∞—Ç–∞—Å–µ—Ç–∏** ‚Äî SVR –ø—Ä–∞—Ü—é—î –¥–æ–±—Ä–µ
6. **–í—ñ–∑—É–∞–ª—ñ–∑—É–π SV** ‚Äî —è–∫—â–æ > 50% ‚Üí –∑–º–µ–Ω—à Œµ –∞–±–æ –∑–±—ñ–ª—å—à C
7. **–ü–æ—Ä—ñ–≤–Ω—è–π –∑ baseline** ‚Äî Linear Regression, Random Forest
8. **Linear –¥–ª—è interpretability** ‚Äî —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–∞ –ø—Ä–æ—Å—Ç–æ—Ç–∞
9. **Outliers** ‚Äî SVR robust, –∞–ª–µ –ø–µ—Ä–µ–≤—ñ—Ä –≤—ñ–∑—É–∞–ª—å–Ω–æ
10. **Cross-validation** ‚Äî –∑–∞–≤–∂–¥–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π CV

---

## –ü—Ä–∏–∫–ª–∞–¥: Real Estate Price Prediction

\`\`\`python
import pandas as pd
from sklearn.svm import SVR
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler

# –°–∏–Ω—Ç–µ—Ç–∏—á–Ω—ñ –¥–∞–Ω—ñ
np.random.seed(42)
n = 200

data = pd.DataFrame({
    'size_sqm': np.random.randint(50, 200, n),
    'rooms': np.random.randint(1, 6, n),
    'age_years': np.random.randint(0, 50, n),
    'distance_center_km': np.random.uniform(1, 20, n)
})

# –¶—ñ–Ω–∞ –∑ –Ω–µ–ª—ñ–Ω—ñ–π–Ω–æ—é –∑–∞–ª–µ–∂–Ω—ñ—Å—Ç—é
data['price'] = (
    5000 * data['size_sqm'] +
    20000 * data['rooms'] -
    1000 * data['age_years'] -
    5000 * np.log(data['distance_center_km']) +
    np.random.normal(0, 50000, n)
)

X = data.drop('price', axis=1)
y = data['price']

# –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è
scaler_X = StandardScaler()
scaler_y = StandardScaler()

X_train_scaled = scaler_X.fit_transform(X_train)
X_test_scaled = scaler_X.transform(X_test)
y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1)).ravel()

# SVR
svr = SVR(kernel='rbf', C=100, gamma='scale', epsilon=0.1)
svr.fit(X_train_scaled, y_train_scaled)

# –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
y_pred_scaled = svr.predict(X_test_scaled)
y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).ravel()

# –û—Ü—ñ–Ω–∫–∞
print("=== SVR for Real Estate ===")
print(f"R¬≤: {r2_score(y_test, y_pred):.4f}")
print(f"RMSE: ${np.sqrt(mean_squared_error(y_test, y_pred)):,.0f}")
print(f"Support vectors: {len(svr.support_)} / {len(X_train)}")

# CV score
cv_scores = cross_val_score(svr, X_train_scaled, y_train_scaled, cv=5, scoring='r2')
print(f"\nCV R¬≤ mean: {cv_scores.mean():.4f} (+/- {cv_scores.std()*2:.4f})")
\`\`\`

---

## –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ SVR

### –Ü–¥–µ–∞–ª—å–Ω–æ –ø—ñ–¥—Ö–æ–¥–∏—Ç—å ‚úì

- **Robust regression** –ø–æ—Ç—Ä—ñ–±–Ω–∞
- **Outliers** –≤ –¥–∞–Ω–∏—Ö
- **–ù–µ–ª—ñ–Ω—ñ–π–Ω—ñ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ**
- **High-dimensional** –¥–∞–Ω—ñ
- –°–µ—Ä–µ–¥–Ω—ñ –¥–∞—Ç–∞—Å–µ—Ç–∏ (n = 1k-50k)
- **Regularization** –≤–∞–∂–ª–∏–≤–∞

### –ö—Ä–∞—â–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ —ñ–Ω—à–µ ‚úó

- **–î—É–∂–µ –≤–µ–ª–∏–∫—ñ –¥–∞–Ω—ñ** (n > 50k) ‚Üí Ridge, Lasso, SGDRegressor
- **–õ—ñ–Ω—ñ–π–Ω–∞ —Ä–µ–≥—Ä–µ—Å—ñ—è –¥–æ—Å—Ç–∞—Ç–Ω—è** ‚Üí Linear Regression
- **–ü–æ—Ç—Ä—ñ–±–Ω–∞ —à–≤–∏–¥–∫—ñ—Å—Ç—å** ‚Üí Linear models
- **Interpretability** –∫—Ä–∏—Ç–∏—á–Ω–∞ ‚Üí Linear/Polynomial Regression
- **Structured tabular** ‚Üí Tree-based —á–∞—Å—Ç–æ –∫—Ä–∞—â–µ

---

## –ö–ª—é—á–æ–≤—ñ –≤–∏—Å–Ω–æ–≤–∫–∏

> SVR –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î Œµ-insensitive loss –¥–ª—è robust regression –∑ kernel trick –¥–ª—è –Ω–µ–ª—ñ–Ω—ñ–π–Ω–∏—Ö –∑–∞–ª–µ–∂–Ω–æ—Å—Ç–µ–π.

**Œµ-insensitive loss:**
- –¢–æ—á–∫–∏ –≤—Å–µ—Ä–µ–¥–∏–Ω—ñ Œµ-tube: penalty = 0
- –¢–æ—á–∫–∏ –∑–∞ –º–µ–∂–∞–º–∏: penalty –ø—Ä–æ–ø–æ—Ä—Ü—ñ–π–Ω–æ –≤—ñ–¥—Å—Ç–∞–Ω—ñ

**–ì—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∏:**
- **C:** regularization (–º–∞–ª–∏–π ‚Üí smooth, –≤–µ–ª–∏–∫–∏–π ‚Üí accurate)
- **Œµ:** tube width (–º–∞–ª–∏–π ‚Üí –≤—É–∑—å–∫–∏–π, –≤–µ–ª–∏–∫–∏–π ‚Üí —à–∏—Ä–æ–∫–∏–π)
- **Œ≥:** –¥–ª—è RBF (–º–∞–ª–∏–π ‚Üí smooth, –≤–µ–ª–∏–∫–∏–π ‚Üí wiggly)

**–ö–†–ò–¢–ò–ß–ù–û:**
- –ù–æ—Ä–º–∞–ª—ñ–∑—É–π X —Ç–∞ y
- Grid Search –¥–ª—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤
- RBF kernel –¥–ª—è –Ω–µ–ª—ñ–Ω—ñ–π–Ω–∏—Ö
- –í—ñ–∑—É–∞–ª—ñ–∑—É–π support vectors

---

#ml #svr #regression #support-vector-regression #epsilon-insensitive #robust-regression
