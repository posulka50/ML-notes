# Ensemble Methods (–ê–Ω—Å–∞–º–±–ª–µ–≤—ñ –º–µ—Ç–æ–¥–∏)

## –©–æ —Ü–µ?

**Ensemble Methods** ‚Äî —Ü–µ –ø—ñ–¥—Ö—ñ–¥ —É –º–∞—à–∏–Ω–Ω–æ–º—É –Ω–∞–≤—á–∞–Ω–Ω—ñ, —è–∫–∏–π –∫–æ–º–±—ñ–Ω—É—î **–º–Ω–æ–∂–∏–Ω—É –º–æ–¥–µ–ª–µ–π** –¥–ª—è –æ—Ç—Ä–∏–º–∞–Ω–Ω—è –∫—Ä–∞—â–∏—Ö –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω—å, –Ω—ñ–∂ –±—É–¥—å-—è–∫–∞ –æ–∫—Ä–µ–º–∞ –º–æ–¥–µ–ª—å.

**–ì–æ–ª–æ–≤–Ω–∞ —ñ–¥–µ—è:** "–º—É–¥—Ä—ñ—Å—Ç—å –Ω–∞—Ç–æ–≤–ø—É" ‚Äî –±–∞–≥–∞—Ç–æ —Ä—ñ–∑–Ω–∏—Ö, –Ω–∞–≤—ñ—Ç—å —Å–ª–∞–±–∫–∏—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞–∑–æ–º –¥–∞—é—Ç—å —Å–∏–ª—å–Ω—ñ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è.

## –ù–∞–≤—ñ—â–æ –ø–æ—Ç—Ä—ñ–±–Ω—ñ?

- üéØ **–í–∏—â–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å** ‚Äî –∑–º–µ–Ω—à–µ–Ω–Ω—è –ø–æ–º–∏–ª–æ–∫
- üõ°Ô∏è **–†–æ–±–∞—Å—Ç–Ω—ñ—Å—Ç—å** ‚Äî –º–µ–Ω—à–∞ —á—É—Ç–ª–∏–≤—ñ—Å—Ç—å –¥–æ —à—É–º—É
- üìä **–ó–º–µ–Ω—à–µ–Ω–Ω—è variance** ‚Äî —Å—Ç–∞–±—ñ–ª—å–Ω—ñ—à—ñ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
- üîß **–ó–º–µ–Ω—à–µ–Ω–Ω—è bias** ‚Äî –∫—Ä–∞—â–µ —É–∑–∞–≥–∞–ª—å–Ω–µ–Ω–Ω—è
- üí° **–£–Ω—ñ–≤–µ—Ä—Å–∞–ª—å–Ω—ñ—Å—Ç—å** ‚Äî –ø—Ä–∞—Ü—é—î –∑ —Ä—ñ–∑–Ω–∏–º–∏ base learners

## –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏?

**–ü–æ—Ç—Ä—ñ–±–Ω–æ:**
- –ü–æ—Ç—Ä—ñ–±–Ω–∞ **–≤–∏—Å–æ–∫–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å**
- –û–¥–Ω–∞ –º–æ–¥–µ–ª—å –Ω–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ —Ç–æ—á–Ω–∞
- **Kaggle competitions** ‚Äî –º–∞–π–∂–µ –∑–∞–≤–∂–¥–∏
- –†—ñ–∑–Ω—ñ –º–æ–¥–µ–ª—ñ –¥–æ—Å—Ç—É–ø–Ω—ñ –¥–ª—è –∫–æ–º–±—ñ–Ω—É–≤–∞–Ω–Ω—è
- –ú–æ–∂–Ω–∞ —Å–æ–±—ñ –¥–æ–∑–≤–æ–ª–∏—Ç–∏ **–æ–±—á–∏—Å–ª—é–≤–∞–ª—å–Ω—ñ —Ä–µ—Å—É—Ä—Å–∏**

**–ù–µ –ø–æ—Ç—Ä—ñ–±–Ω–æ:**
- **–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å –∫—Ä–∏—Ç–∏—á–Ω–∞** ‚Üí –æ–¥–Ω–∞ –ø—Ä–æ—Å—Ç–∞ –º–æ–¥–µ–ª—å
- –î—É–∂–µ –æ–±–º–µ–∂–µ–Ω—ñ —Ä–µ—Å—É—Ä—Å–∏
- –†–µ–∞–ª-—Ç–∞–π–º inference –∑ –∂–æ—Ä—Å—Ç–∫–∏–º–∏ –≤–∏–º–æ–≥–∞–º–∏
- –û–¥–Ω–∞ –º–æ–¥–µ–ª—å –≤–∂–µ –¥–æ—Å–∏—Ç—å —Ç–æ—á–Ω–∞

---

## –§—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–∞ —ñ–¥–µ—è

### –ê–Ω–∞–ª–æ–≥—ñ—è: –ö–æ–ª–µ–∫—Ç–∏–≤–Ω–µ —Ä—ñ—à–µ–Ω–Ω—è

**–°—Ü–µ–Ω–∞—Ä—ñ–π:** –ì—Ä—É–ø–∞ –µ–∫—Å–ø–µ—Ä—Ç—ñ–≤ –ø—Ä–∏–π–º–∞—î —Ä—ñ—à–µ–Ω–Ω—è

**–í–∞—Ä—ñ–∞–Ω—Ç 1:** –û–¥–∏–Ω –µ–∫—Å–ø–µ—Ä—Ç
- –ú–æ–∂–µ –ø–æ–º–∏–ª—è—Ç–∏—Å—è —á–µ—Ä–µ–∑ —É–ø–µ—Ä–µ–¥–∂–µ–Ω–Ω—è –∞–±–æ –Ω–µ–∑–Ω–∞–Ω–Ω—è
- –í–∏—Å–æ–∫–∞ variance (—Ä—ñ–∑–Ω—ñ –µ–∫—Å–ø–µ—Ä—Ç–∏ ‚Üí —Ä—ñ–∑–Ω—ñ —Ä—ñ—à–µ–Ω–Ω—è)

**–í–∞—Ä—ñ–∞–Ω—Ç 2:** –ö–æ–º—ñ—Ç–µ—Ç –µ–∫—Å–ø–µ—Ä—Ç—ñ–≤
- –ö–æ–∂–µ–Ω –º–∞—î —Å–≤–æ—é –¥—É–º–∫—É
- –£—Å–µ—Ä–µ–¥–Ω–µ–Ω–Ω—è –¥—É–º–æ–∫ ‚Üí –ø–æ–º–∏–ª–∫–∏ –∫–æ–º–ø–µ–Ω—Å—É—é—Ç—å—Å—è
- –ë—ñ–ª—å—à –Ω–∞–¥—ñ–π–Ω–µ —Ç–∞ —Å—Ç–∞–±—ñ–ª—å–Ω–µ —Ä—ñ—à–µ–Ω–Ω—è ‚úì

**–£–º–æ–≤–∏ —É—Å–ø—ñ—Ö—É:**
1. **–†—ñ–∑–Ω–æ–º–∞–Ω—ñ—Ç–Ω—ñ—Å—Ç—å** ‚Äî –µ–∫—Å–ø–µ—Ä—Ç–∏ –Ω–µ –ø–æ–≤–∏–Ω–Ω—ñ –¥—É–º–∞—Ç–∏ –æ–¥–Ω–∞–∫–æ–≤–æ
2. **–ù–µ–∑–∞–ª–µ–∂–Ω—ñ—Å—Ç—å** ‚Äî –∫–æ–∂–µ–Ω –ø—Ä–∏–π–º–∞—î —Ä—ñ—à–µ–Ω–Ω—è —Å–∞–º–æ—Å—Ç—ñ–π–Ω–æ
3. **–ö–æ–º–ø–µ—Ç–µ–Ω—Ç–Ω—ñ—Å—Ç—å** ‚Äî –∫–æ–∂–µ–Ω –∫—Ä–∞—â–µ –∑–∞ –≤–∏–ø–∞–¥–∫–æ–≤–µ –≥–∞–¥–∞–Ω–Ω—è

---

## –ú–∞—Ç–µ–º–∞—Ç–∏—á–Ω–∞ —ñ–Ω—Ç—É—ó—Ü—ñ—è

### –ó–º–µ–Ω—à–µ–Ω–Ω—è Variance —á–µ—Ä–µ–∑ —É—Å–µ—Ä–µ–¥–Ω–µ–Ω–Ω—è

–ü—Ä–∏–ø—É—Å—Ç–∏–º–æ, –º–∞—î–º–æ $M$ –º–æ–¥–µ–ª–µ–π –∑ variance $\sigma^2$ –∫–æ–∂–Ω–∞.

**–Ø–∫—â–æ –º–æ–¥–µ–ª—ñ –ù–ï–ó–ê–õ–ï–ñ–ù–Ü:**

$$\text{Var}(\text{average}) = \frac{\sigma^2}{M}$$

**–í–∏—Å–Ω–æ–≤–æ–∫:** –£—Å–µ—Ä–µ–¥–Ω–µ–Ω–Ω—è $M$ –Ω–µ–∑–∞–ª–µ–∂–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π –∑–º–µ–Ω—à—É—î variance –≤ $M$ —Ä–∞–∑—ñ–≤! ‚úì

**–Ø–∫—â–æ –º–æ–¥–µ–ª—ñ –ö–û–†–ï–õ–¨–û–í–ê–ù–Ü** (–∫–æ—Ä–µ–ª—è—Ü—ñ—è $\rho$):

$$\text{Var}(\text{average}) = \rho\sigma^2 + \frac{1-\rho}{M}\sigma^2$$

**–°–ø–æ—Å—Ç–µ—Ä–µ–∂–µ–Ω–Ω—è:**
- –ü—Ä–∏ $\rho = 0$ (–Ω–µ–∑–∞–ª–µ–∂–Ω—ñ) ‚Üí $\text{Var} = \frac{\sigma^2}{M}$
- –ü—Ä–∏ $\rho = 1$ (—ñ–¥–µ–Ω—Ç–∏—á–Ω—ñ) ‚Üí $\text{Var} = \sigma^2$ (–Ω–µ–º–∞—î –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è!)

**–í–∏—Å–Ω–æ–≤–æ–∫:** –ü–æ—Ç—Ä—ñ–±–Ω–∞ **—Ä—ñ–∑–Ω–æ–º–∞–Ω—ñ—Ç–Ω—ñ—Å—Ç—å** –º–æ–¥–µ–ª–µ–π! üåü

---

## –¢–∏–ø–∏ Ensemble Methods

```
Ensemble Methods
       |
       |--- Bagging (Bootstrap Aggregating)
       |     |
       |     |--- Random Forest
       |     |--- Bagged Trees
       |     |--- Extra Trees
       |
       |--- Boosting (Sequential Ensembling)
       |     |
       |     |--- AdaBoost
       |     |--- Gradient Boosting
       |     |--- XGBoost, LightGBM, CatBoost
       |
       |--- Stacking (Meta-Learning)
       |     |
       |     |--- Stacked Generalization
       |     |--- Blending
       |
       |--- Voting
             |
             |--- Hard Voting
             |--- Soft Voting
```

---

## 1. Bagging (Bootstrap Aggregating)

### –ö–æ–Ω—Ü–µ–ø—Ü—ñ—è

**–ü–∞—Ä–∞–ª–µ–ª—å–Ω–æ** –Ω–∞–≤—á–∞—î–º–æ –º–Ω–æ–∂–∏–Ω—É –º–æ–¥–µ–ª–µ–π –Ω–∞ —Ä—ñ–∑–Ω–∏—Ö bootstrap samples —Ç–∞ **—É—Å–µ—Ä–µ–¥–Ω—é—î–º–æ** —ó—Ö –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è.

### –ê–ª–≥–æ—Ä–∏—Ç–º

1. –°—Ç–≤–æ—Ä–∏—Ç–∏ $M$ bootstrap samples (–≤–∏–±—ñ—Ä–∫–∞ –∑ –ø–æ–≤–µ—Ä–Ω–µ–Ω–Ω—è–º)
2. –ù–∞–≤—á–∏—Ç–∏ –º–æ–¥–µ–ª—å –Ω–∞ –∫–æ–∂–Ω–æ–º—É sample
3. –ö–æ–º–±—ñ–Ω—É–≤–∞—Ç–∏ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è:
   - **–†–µ–≥—Ä–µ—Å—ñ—è:** —Å–µ—Ä–µ–¥–Ω—î –∞—Ä–∏—Ñ–º–µ—Ç–∏—á–Ω–µ
   - **–ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è:** –≥–æ–ª–æ—Å—É–≤–∞–Ω–Ω—è –±—ñ–ª—å—à–æ—Å—Ç—ñ

### –§–æ—Ä–º—É–ª–∞

**–†–µ–≥—Ä–µ—Å—ñ—è:**
$$\hat{y} = \frac{1}{M} \sum_{m=1}^{M} f_m(x)$$

**–ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è:**
$$\hat{y} = \text{mode}\{f_1(x), f_2(x), ..., f_M(x)\}$$

### –ü–µ—Ä–µ–≤–∞–≥–∏ Bagging

- ‚úÖ –ó–º–µ–Ω—à—É—î **variance**
- ‚úÖ –ü–∞—Ä–∞–ª–µ–ª—ñ–∑—É—î—Ç—å—Å—è
- ‚úÖ –†–æ–±–∞—Å—Ç–Ω–∏–π –¥–æ **overfitting**
- ‚úÖ –ü—Ä–∞—Ü—é—î –∑ high-variance –º–æ–¥–µ–ª—è–º–∏ (deep trees)

### –ü—Ä–∏–∫–ª–∞–¥: Random Forest

```python
from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(
    n_estimators=100,     # M –º–æ–¥–µ–ª–µ–π
    max_features='sqrt',  # –î–æ–¥–∞—Ç–∫–æ–≤–∞ —Ä—ñ–∑–Ω–æ–º–∞–Ω—ñ—Ç–Ω—ñ—Å—Ç—å
    bootstrap=True,       # Bootstrap sampling
    n_jobs=-1             # –ü–∞—Ä–∞–ª–µ–ª—ñ–∑–∞—Ü—ñ—è
)

rf.fit(X_train, y_train)
y_pred = rf.predict(X_test)
```

---

## 2. Boosting

### –ö–æ–Ω—Ü–µ–ø—Ü—ñ—è

**–ü–æ—Å–ª—ñ–¥–æ–≤–Ω–æ** –Ω–∞–≤—á–∞—î–º–æ –º–æ–¥–µ–ª—ñ, –¥–µ –∫–æ–∂–Ω–∞ –Ω–∞—Å—Ç—É–ø–Ω–∞ **–≤–∏–ø—Ä–∞–≤–ª—è—î –ø–æ–º–∏–ª–∫–∏** –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ—Ö.

### –ü—ñ–¥—Ö–æ–¥–∏

#### AdaBoost
- –ó–º—ñ–Ω—é—î **–≤–∞–≥–∏ –∑—Ä–∞–∑–∫—ñ–≤**
- –§–æ–∫—É—Å –Ω–∞ –≤–∞–∂–∫–æ –∫–ª–∞—Å–∏—Ñ—ñ–∫–æ–≤–∞–Ω–∏—Ö –ø—Ä–∏–∫–ª–∞–¥–∞—Ö

#### Gradient Boosting
- –§—ñ—Ç—É—î –Ω–∞ **residuals** (–≥—Ä–∞–¥—ñ—î–Ω—Ç loss)
- –ë—ñ–ª—å—à–∞ –≥–Ω—É—á–∫—ñ—Å—Ç—å (—Ä—ñ–∑–Ω—ñ loss functions)

### –ê–ª–≥–æ—Ä–∏—Ç–º (–∑–∞–≥–∞–ª—å–Ω–∏–π)

1. –ü–æ—á–∞—Ç–∏ –∑ –ø—Ä–æ—Å—Ç–æ–≥–æ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è (–∫–æ–Ω—Å—Ç–∞–Ω—Ç–∞)
2. –î–ª—è $m = 1$ –¥–æ $M$:
   - –û–±—á–∏—Å–ª–∏—Ç–∏ –ø–æ–º–∏–ª–∫–∏ –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ—ó –º–æ–¥–µ–ª—ñ
   - –ù–∞–≤—á–∏—Ç–∏ –Ω–æ–≤—É –º–æ–¥–µ–ª—å –Ω–∞ –ø–æ–º–∏–ª–∫–∞—Ö
   - –î–æ–¥–∞—Ç–∏ –Ω–æ–≤—É –º–æ–¥–µ–ª—å –¥–æ –∞–Ω—Å–∞–º–±–ª—é (–∑ –≤–∞–≥–æ—é)
3. –§—ñ–Ω–∞–ª—å–Ω–µ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è = —Å—É–º–∞ –≤—Å—ñ—Ö –º–æ–¥–µ–ª–µ–π

### –§–æ—Ä–º—É–ª–∞

$$F_M(x) = F_0(x) + \sum_{m=1}^{M} \nu \cdot h_m(x)$$

–¥–µ:
- $F_0(x)$ ‚Äî –ø–æ—á–∞—Ç–∫–æ–≤–µ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
- $h_m(x)$ ‚Äî $m$-—Ç–∞ –º–æ–¥–µ–ª—å
- $\nu$ ‚Äî learning rate

### –ü–µ—Ä–µ–≤–∞–≥–∏ Boosting

- ‚úÖ –ó–º–µ–Ω—à—É—î **bias**
- ‚úÖ –í–∏—Å–æ–∫–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å
- ‚úÖ –ü—Ä–∞—Ü—é—î –∑ weak learners
- ‚úÖ SOTA –Ω–∞ —Ç–∞–±–ª–∏—á–Ω–∏—Ö –¥–∞–Ω–∏—Ö

### –ù–µ–¥–æ–ª—ñ–∫–∏ Boosting

- ‚ùå –ü–æ—Å–ª—ñ–¥–æ–≤–Ω–µ (–Ω–µ –ø–∞—Ä–∞–ª–µ–ª–∏—Ç—å—Å—è)
- ‚ùå –†–∏–∑–∏–∫ overfitting
- ‚ùå –ß—É—Ç–ª–∏–≤–µ –¥–æ —à—É–º—É

### –ü—Ä–∏–∫–ª–∞–¥: Gradient Boosting

```python
from sklearn.ensemble import GradientBoostingClassifier

gb = GradientBoostingClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=3,          # –ú—ñ–ª–∫—ñ –¥–µ—Ä–µ–≤–∞!
    subsample=0.8
)

gb.fit(X_train, y_train)
y_pred = gb.predict(X_test)
```

---

## 3. Stacking (Stacked Generalization)

### –ö–æ–Ω—Ü–µ–ø—Ü—ñ—è

**–ë–∞–≥–∞—Ç–æ—Ä—ñ–≤–Ω–µ–≤–∏–π –ø—ñ–¥—Ö—ñ–¥:**
- **Level 0:** –†—ñ–∑–Ω—ñ base learners –ø–µ—Ä–µ–¥–±–∞—á–∞—é—Ç—å –Ω–∞ –¥–∞–Ω–∏—Ö
- **Level 1:** Meta-learner –Ω–∞–≤—á–∞—î—Ç—å—Å—è –Ω–∞ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è—Ö base learners

### –ê–ª–≥–æ—Ä–∏—Ç–º

1. **–†–æ–∑–¥—ñ–ª–∏—Ç–∏ –¥–∞–Ω—ñ:**
   - Train set ‚Üí –¥–ª—è base learners
   - Hold-out set ‚Üí –¥–ª—è meta-learner

2. **–ù–∞–≤—á–∏—Ç–∏ base learners:**
   - –ö—ñ–ª—å–∫–∞ —Ä—ñ–∑–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ train set
   - –û—Ç—Ä–∏–º–∞—Ç–∏ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –Ω–∞ hold-out set

3. **–ù–∞–≤—á–∏—Ç–∏ meta-learner:**
   - –í—Ö—ñ–¥ = –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è base learners
   - –í–∏—Ö—ñ–¥ = —Å–ø—Ä–∞–≤–∂–Ω—ñ labels

4. **–§—ñ–Ω–∞–ª—å–Ω–µ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è:**
   - Base learners –ø–µ—Ä–µ–¥–±–∞—á–∞—é—Ç—å –Ω–∞ test
   - Meta-learner –∫–æ–º–±—ñ–Ω—É—î —ó—Ö –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è

### –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è

```
Train Data
    |
    |--- Model 1 (RF) ----‚Üí Predictions 1 ‚îê
    |--- Model 2 (XGB) ---‚Üí Predictions 2 ‚îú‚Üí Meta-learner (LR) ‚Üí Final
    |--- Model 3 (SVM) ---‚Üí Predictions 3 ‚îò
    |
    ‚Üì
Test Data (same flow)
```

### –ü–µ—Ä–µ–≤–∞–≥–∏ Stacking

- ‚úÖ –ö–æ–º–±—ñ–Ω—É—î –ø–µ—Ä–µ–≤–∞–≥–∏ —Ä—ñ–∑–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π
- ‚úÖ –ß–∞—Å—Ç–æ –Ω–∞–π–∫—Ä–∞—â–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å
- ‚úÖ –ì–Ω—É—á–∫—ñ—Å—Ç—å —É –≤–∏–±–æ—Ä—ñ –º–æ–¥–µ–ª–µ–π

### –ù–µ–¥–æ–ª—ñ–∫–∏ Stacking

- ‚ùå –°–∫–ª–∞–¥–Ω–∞ —Ä–µ–∞–ª—ñ–∑–∞—Ü—ñ—è
- ‚ùå –†–∏–∑–∏–∫ overfitting
- ‚ùå –û–±—á–∏—Å–ª—é–≤–∞–ª—å–Ω–æ –¥–æ—Ä–æ–≥–æ
- ‚ùå –í–∞–∂–∫–æ —ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç—É–≤–∞—Ç–∏

### –ö–æ–¥ (scikit-learn)

```python
from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC

# Base learners
estimators = [
    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),
    ('gb', GradientBoostingClassifier(n_estimators=100, random_state=42)),
    ('svm', SVC(probability=True, random_state=42))
]

# Stacking with meta-learner
stacking = StackingClassifier(
    estimators=estimators,
    final_estimator=LogisticRegression(),  # Meta-learner
    cv=5  # Cross-validation –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω—å
)

stacking.fit(X_train, y_train)
y_pred = stacking.predict(X_test)

print(f"Stacking Accuracy: {stacking.score(X_test, y_test):.4f}")
```

---

## 4. Voting

### Hard Voting (Majority Vote)

**–ö–æ–∂–Ω–∞ –º–æ–¥–µ–ª—å –≥–æ–ª–æ—Å—É—î –∑–∞ –∫–ª–∞—Å, –æ–±–∏—Ä–∞—î—Ç—å—Å—è –Ω–∞–π–ø–æ–ø—É–ª—è—Ä–Ω—ñ—à–∏–π.**

$$\hat{y} = \text{mode}\{f_1(x), f_2(x), ..., f_M(x)\}$$

**–ü—Ä–∏–∫–ª–∞–¥:**
```
Model 1: Class A
Model 2: Class A
Model 3: Class B
Model 4: Class A

Final: Class A (3 –≥–æ–ª–æ—Å–∏)
```

### Soft Voting (Weighted Average of Probabilities)

**–£—Å–µ—Ä–µ–¥–Ω—é—î–º–æ –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ –∫–ª–∞—Å—ñ–≤.**

$$\hat{p}_k = \frac{1}{M} \sum_{m=1}^{M} p_{m,k}(x)$$

$$\hat{y} = \arg\max_k \hat{p}_k$$

**–ü—Ä–∏–∫–ª–∞–¥:**
```
Model 1: [0.7, 0.3] (Class A prob, Class B prob)
Model 2: [0.6, 0.4]
Model 3: [0.4, 0.6]

Average: [0.57, 0.43]
Final: Class A (0.57 > 0.43)
```

### –ö–æ–¥

```python
from sklearn.ensemble import VotingClassifier

# –ú–æ–¥–µ–ª—ñ
clf1 = LogisticRegression(random_state=42)
clf2 = RandomForestClassifier(random_state=42)
clf3 = GradientBoostingClassifier(random_state=42)

# Hard Voting
voting_hard = VotingClassifier(
    estimators=[('lr', clf1), ('rf', clf2), ('gb', clf3)],
    voting='hard'
)

# Soft Voting (–∫—Ä–∞—â–µ, —è–∫—â–æ —î predict_proba)
voting_soft = VotingClassifier(
    estimators=[('lr', clf1), ('rf', clf2), ('gb', clf3)],
    voting='soft'
)

voting_soft.fit(X_train, y_train)
y_pred = voting_soft.predict(X_test)

print(f"Voting Accuracy: {voting_soft.score(X_test, y_test):.4f}")
```

---

## –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –º–µ—Ç–æ–¥—ñ–≤

| –ú–µ—Ç–æ–¥ | –¢–∏–ø | –†—ñ–∑–Ω–æ–º–∞–Ω—ñ—Ç–Ω—ñ—Å—Ç—å | –°–∫–ª–∞–¥–Ω—ñ—Å—Ç—å | –¢–æ—á–Ω—ñ—Å—Ç—å | –ü–∞—Ä–∞–ª–µ–ª—ñ–∑–∞—Ü—ñ—è |
|-------|-----|-----------------|------------|----------|---------------|
| **Bagging** | –ü–∞—Ä–∞–ª–µ–ª—å–Ω–∏–π | Bootstrap | ‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ |
| **Random Forest** | –ü–∞—Ä–∞–ª–µ–ª—å–Ω–∏–π | Bootstrap + Features | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ |
| **Boosting** | –ü–æ—Å–ª—ñ–¥–æ–≤–Ω–∏–π | Error-based | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚ùå |
| **Stacking** | –ë–∞–≥–∞—Ç–æ—Ä—ñ–≤–Ω–µ–≤–∏–π | –†—ñ–∑–Ω—ñ –º–æ–¥–µ–ª—ñ | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö†Ô∏è |
| **Voting** | –ü–∞—Ä–∞–ª–µ–ª—å–Ω–∏–π | –†—ñ–∑–Ω—ñ –º–æ–¥–µ–ª—ñ | ‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚úÖ |

---

## Diversity (–†—ñ–∑–Ω–æ–º–∞–Ω—ñ—Ç–Ω—ñ—Å—Ç—å)

### –ß–æ–º—É –≤–∞–∂–ª–∏–≤–∞?

**–¢–µ–æ—Ä–µ–º–∞:** –ê–Ω—Å–∞–º–±–ª—å –ø—Ä–∞—Ü—é—î –¥–æ–±—Ä–µ, –∫–æ–ª–∏ –º–æ–¥–µ–ª—ñ:
1. **–¢–æ—á–Ω—ñ** (–∫—Ä–∞—â–µ –∑–∞ –≤–∏–ø–∞–¥–∫–æ–≤–µ –≥–∞–¥–∞–Ω–Ω—è)
2. **–†—ñ–∑–Ω–æ–º–∞–Ω—ñ—Ç–Ω—ñ** (—Ä–æ–±–ª—è—Ç—å —Ä—ñ–∑–Ω—ñ –ø–æ–º–∏–ª–∫–∏)

### –°–ø–æ—Å–æ–±–∏ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ä—ñ–∑–Ω–æ–º–∞–Ω—ñ—Ç–Ω–æ—Å—Ç—ñ

#### 1. Data Diversity (Bagging)
- Bootstrap sampling
- Different subsets

#### 2. Feature Diversity (Random Forest)
- Random feature subsets
- Feature bagging

#### 3. Algorithm Diversity (Stacking, Voting)
- –†—ñ–∑–Ω—ñ —Ç–∏–ø–∏ –º–æ–¥–µ–ª–µ–π (Tree, Linear, SVM)
- –†—ñ–∑–Ω—ñ –≥—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∏

#### 4. Parameter Diversity
- –†—ñ–∑–Ω–∞ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è
- –†—ñ–∑–Ω—ñ random seeds

### –í–∏–º—ñ—Ä—é–≤–∞–Ω–Ω—è —Ä—ñ–∑–Ω–æ–º–∞–Ω—ñ—Ç–Ω–æ—Å—Ç—ñ

**Q-statistic –º—ñ–∂ –¥–≤–æ–º–∞ –º–æ–¥–µ–ª—è–º–∏:**

$$Q_{ij} = \frac{N^{11}N^{00} - N^{01}N^{10}}{N^{11}N^{00} + N^{01}N^{10}}$$

–¥–µ:
- $N^{11}$ ‚Äî –æ–±–∏–¥–≤—ñ –ø—Ä–∞–≤–∏–ª—å–Ω—ñ
- $N^{00}$ ‚Äî –æ–±–∏–¥–≤—ñ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ñ
- $N^{01}$ ‚Äî $i$ –ø—Ä–∞–≤–∏–ª—å–Ω–∞, $j$ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞
- $N^{10}$ ‚Äî $i$ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞, $j$ –ø—Ä–∞–≤–∏–ª—å–Ω–∞

**–î—ñ–∞–ø–∞–∑–æ–Ω:** [-1, 1]
- $Q = 1$ ‚Üí –∑–∞–≤–∂–¥–∏ –æ–¥–Ω–∞–∫–æ–≤—ñ (–Ω–µ–º–∞—î diversity)
- $Q = 0$ ‚Üí –Ω–µ–∑–∞–ª–µ–∂–Ω—ñ
- $Q = -1$ ‚Üí –∑–∞–≤–∂–¥–∏ —Ä—ñ–∑–Ω—ñ

---

## Bias-Variance Decomposition –¥–ª—è –ê–Ω—Å–∞–º–±–ª—ñ–≤

### –ó–∞–≥–∞–ª—å–Ω–∞ –ø–æ–º–∏–ª–∫–∞

$$\text{Error} = \text{Bias}^2 + \text{Variance} + \text{Irreducible Error}$$

### –ï—Ñ–µ–∫—Ç –∞–Ω—Å–∞–º–±–ª—ñ–≤

**Bagging:**
- ‚úÖ –ó–º–µ–Ω—à—É—î **Variance** (—á–µ—Ä–µ–∑ —É—Å–µ—Ä–µ–¥–Ω–µ–Ω–Ω—è)
- ‚ùå –ú–∞–π–∂–µ –Ω–µ –≤–ø–ª–∏–≤–∞—î –Ω–∞ **Bias**
- **–í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è:** High-variance –º–æ–¥–µ–ª—ñ (deep trees)

**Boosting:**
- ‚úÖ –ó–º–µ–Ω—à—É—î **Bias** (–ø–æ—Å–ª—ñ–¥–æ–≤–Ω–µ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è)
- ‚úÖ –ú–æ–∂–µ –∑–º–µ–Ω—à–∏—Ç–∏ **Variance** (–∑ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—î—é)
- **–í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è:** High-bias –º–æ–¥–µ–ª—ñ (shallow trees)

### –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è

```
               High Variance
                    |
       Random       |    Bagging ‚úì
       Forest ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îº‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚Üí
                    |
Low Bias ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îº‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ High Bias
                    |
       Boosting ‚úì   |
            ‚Üì       |
       (–∑–º–µ–Ω—à—É—î     |
        —ñ bias,     |
        —ñ variance) |
                    |
```

---

## –ü—Ä–∞–∫—Ç–∏—á–Ω—ñ –ø–æ—Ä–∞–¥–∏ üí°

### –í–∏–±—ñ—Ä –º–µ—Ç–æ–¥—É

**1. Bagging / Random Forest:**
```python
# –ö–æ–ª–∏:
# - –í–∏—Å–æ–∫–∞ variance (overfitting)
# - –ü–æ—Ç—Ä—ñ–±–Ω–∞ –ø–∞—Ä–∞–ª–µ–ª—ñ–∑–∞—Ü—ñ—è
# - –®–≤–∏–¥–∫–∏–π baseline

rf = RandomForestClassifier(n_estimators=100)
```

**2. Boosting:**
```python
# –ö–æ–ª–∏:
# - –ü–æ—Ç—Ä—ñ–±–Ω–∞ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å
# - High bias (underfitting)
# - –Ñ —á–∞—Å –Ω–∞ tuning

import xgboost as xgb
xgb_clf = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1)
```

**3. Stacking:**
```python
# –ö–æ–ª–∏:
# - Kaggle competition
# - –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å –≤–∞–∂–ª–∏–≤—ñ—à–∞ –∑–∞ —Å–∫–ª–∞–¥–Ω—ñ—Å—Ç—å
# - –Ñ —Ä—ñ–∑–Ω—ñ –º–æ–¥–µ–ª—ñ

stacking = StackingClassifier(estimators=[...], final_estimator=...)
```

**4. Voting:**
```python
# –ö–æ–ª–∏:
# - –ü—Ä–æ—Å—Ç–∏–π –ø—ñ–¥—Ö—ñ–¥ –¥–æ –∫–æ–º–±—ñ–Ω—É–≤–∞–Ω–Ω—è
# - –í–∂–µ —î –Ω–∞–≤—á–µ–Ω—ñ –º–æ–¥–µ–ª—ñ
# - –ü–æ—Ç—Ä—ñ–±–µ–Ω —à–≤–∏–¥–∫–∏–π boost —Ç–æ—á–Ω–æ—Å—Ç—ñ

voting = VotingClassifier(estimators=[...], voting='soft')
```

---

## –ü–æ–≤–Ω–∏–π –ø—Ä–∏–∫–ª–∞–¥: –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –≤—Å—ñ—Ö –º–µ—Ç–æ–¥—ñ–≤

```python
import numpy as np
import pandas as pd
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import accuracy_score, classification_report

# –†—ñ–∑–Ω—ñ –º–æ–¥–µ–ª—ñ
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import (
    RandomForestClassifier,
    GradientBoostingClassifier,
    AdaBoostClassifier,
    VotingClassifier,
    StackingClassifier,
    BaggingClassifier
)

# –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –¥–∞–Ω–∏—Ö
X, y = make_classification(
    n_samples=2000,
    n_features=20,
    n_informative=15,
    n_redundant=5,
    random_state=42
)

# –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print("="*70)
print("COMPARING ENSEMBLE METHODS")
print("="*70)

# 1. Single Decision Tree (baseline)
dt = DecisionTreeClassifier(max_depth=5, random_state=42)
dt.fit(X_train, y_train)
dt_score = dt.score(X_test, y_test)
print(f"\n1. Single Decision Tree:        {dt_score:.4f}")

# 2. Bagging
bagging = BaggingClassifier(
    estimator=DecisionTreeClassifier(max_depth=5),
    n_estimators=50,
    random_state=42
)
bagging.fit(X_train, y_train)
bagging_score = bagging.score(X_test, y_test)
print(f"2. Bagging (50 trees):          {bagging_score:.4f} (+{bagging_score-dt_score:.4f})")

# 3. Random Forest
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)
rf_score = rf.score(X_test, y_test)
print(f"3. Random Forest:               {rf_score:.4f} (+{rf_score-dt_score:.4f})")

# 4. AdaBoost
ada = AdaBoostClassifier(n_estimators=50, random_state=42)
ada.fit(X_train, y_train)
ada_score = ada.score(X_test, y_test)
print(f"4. AdaBoost:                    {ada_score:.4f} (+{ada_score-dt_score:.4f})")

# 5. Gradient Boosting
gb = GradientBoostingClassifier(n_estimators=100, random_state=42)
gb.fit(X_train, y_train)
gb_score = gb.score(X_test, y_test)
print(f"5. Gradient Boosting:           {gb_score:.4f} (+{gb_score-dt_score:.4f})")

# 6. Voting (Soft)
voting = VotingClassifier(
    estimators=[
        ('rf', RandomForestClassifier(n_estimators=50, random_state=42)),
        ('gb', GradientBoostingClassifier(n_estimators=50, random_state=42)),
        ('lr', LogisticRegression(random_state=42, max_iter=1000))
    ],
    voting='soft'
)
voting.fit(X_train, y_train)
voting_score = voting.score(X_test, y_test)
print(f"6. Voting (RF+GB+LR):           {voting_score:.4f} (+{voting_score-dt_score:.4f})")

# 7. Stacking
stacking = StackingClassifier(
    estimators=[
        ('rf', RandomForestClassifier(n_estimators=50, random_state=42)),
        ('gb', GradientBoostingClassifier(n_estimators=50, random_state=42)),
        ('ada', AdaBoostClassifier(n_estimators=50, random_state=42))
    ],
    final_estimator=LogisticRegression(),
    cv=5
)
stacking.fit(X_train, y_train)
stacking_score = stacking.score(X_test, y_test)
print(f"7. Stacking (RF+GB+Ada ‚Üí LR):   {stacking_score:.4f} (+{stacking_score-dt_score:.4f})")

print("\n" + "="*70)
print(f"BEST METHOD: ", end="")
scores = {
    'Bagging': bagging_score,
    'Random Forest': rf_score,
    'AdaBoost': ada_score,
    'Gradient Boosting': gb_score,
    'Voting': voting_score,
    'Stacking': stacking_score
}
best_method = max(scores, key=scores.get)
print(f"{best_method} ({scores[best_method]:.4f})")
print("="*70)

# –î–µ—Ç–∞–ª—å–Ω–∏–π –∑–≤—ñ—Ç –¥–ª—è –Ω–∞–π–∫—Ä–∞—â–æ–≥–æ –º–µ—Ç–æ–¥—É
print(f"\n=== {best_method} - Detailed Report ===")
best_model = eval(best_method.lower().replace(' ', '_'))
y_pred = best_model.predict(X_test)
print(classification_report(y_test, y_pred))
```

---

## –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ —è–∫–∏–π –º–µ—Ç–æ–¥?

### Decision Tree

**–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π –∫–æ–ª–∏:**
- ‚úÖ –Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å –∫—Ä–∏—Ç–∏—á–Ω–∞
- ‚úÖ –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è —Ä—ñ—à–µ–Ω—å –ø–æ—Ç—Ä—ñ–±–Ω–∞
- ‚úÖ –®–≤–∏–¥–∫–∏–π –ø—Ä–æ—Ç–æ—Ç–∏–ø

### Bagging / Random Forest

**–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π –∫–æ–ª–∏:**
- ‚úÖ –û–≤erfitting –∑ –æ–¥–Ω–∏–º –¥–µ—Ä–µ–≤–æ–º
- ‚úÖ –ü–æ—Ç—Ä—ñ–±–Ω–∞ –ø–∞—Ä–∞–ª–µ–ª—ñ–∑–∞—Ü—ñ—è
- ‚úÖ –®–≤–∏–¥–∫–∏–π accurate baseline
- ‚úÖ –†–æ–±–∞—Å—Ç–Ω—ñ—Å—Ç—å –¥–æ —à—É–º—É

### Boosting (GB, XGBoost)

**–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π –∫–æ–ª–∏:**
- ‚úÖ –ü–æ—Ç—Ä—ñ–±–Ω–∞ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å
- ‚úÖ Kaggle / production ML
- ‚úÖ –Ñ —á–∞—Å –Ω–∞ tuning
- ‚úÖ –¢–∞–±–ª–∏—á–Ω—ñ –¥–∞–Ω—ñ

### Stacking

**–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π –∫–æ–ª–∏:**
- ‚úÖ Kaggle competition (top positions)
- ‚úÖ –ê–±—Å–æ–ª—é—Ç–Ω–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å –≤–∞–∂–ª–∏–≤—ñ—à–∞ –∑–∞ –≤—Å–µ
- ‚úÖ –†—ñ–∑–Ω—ñ –º–æ–¥–µ–ª—ñ –≤–∂–µ –Ω–∞–≤—á–µ–Ω—ñ
- ‚úÖ –Ñ –æ–±—á–∏—Å–ª—é–≤–∞–ª—å–Ω—ñ —Ä–µ—Å—É—Ä—Å–∏

### Voting

**–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π –∫–æ–ª–∏:**
- ‚úÖ –ü—Ä–æ—Å—Ç–∏–π —Å–ø–æ—Å—ñ–± –∫–æ–º–±—ñ–Ω—É–≤–∞—Ç–∏ –º–æ–¥–µ–ª—ñ
- ‚úÖ –í–∂–µ —î –∫—ñ–ª—å–∫–∞ –Ω–∞–≤—á–µ–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π
- ‚úÖ –ü–æ—Ç—Ä—ñ–±–µ–Ω —à–≤–∏–¥–∫–∏–π boost

---

## –ü–æ—à–∏—Ä–µ–Ω—ñ –ø–æ–º–∏–ª–∫–∏ ‚ùå

### 1. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ —ñ–¥–µ–Ω—Ç–∏—á–Ω—ñ –º–æ–¥–µ–ª—ñ

```python
# ‚ùå –ù–ï–ü–†–ê–í–ò–õ–¨–ù–û (–Ω–µ–º–∞—î —Ä—ñ–∑–Ω–æ–º–∞–Ω—ñ—Ç–Ω–æ—Å—Ç—ñ)
voting = VotingClassifier([
    ('rf1', RandomForestClassifier(random_state=42)),
    ('rf2', RandomForestClassifier(random_state=42)),
    ('rf3', RandomForestClassifier(random_state=42))
])

# ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û (—Ä—ñ–∑–Ω—ñ –º–æ–¥–µ–ª—ñ)
voting = VotingClassifier([
    ('rf', RandomForestClassifier()),
    ('gb', GradientBoostingClassifier()),
    ('lr', LogisticRegression())
])
```

### 2. Overfitting —É stacking

```python
# ‚ùå –†–ò–ó–ò–ö OVERFITTING
stacking = StackingClassifier(
    estimators=[...],
    cv=None  # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î train predictions
)

# ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û
stacking = StackingClassifier(
    estimators=[...],
    cv=5  # Cross-validation
)
```

### 3. –Ü–≥–Ω–æ—Ä—É–≤–∞—Ç–∏ —Ä—ñ–∑–Ω–æ–º–∞–Ω—ñ—Ç–Ω—ñ—Å—Ç—å

```python
# –ü–µ—Ä–µ–≤—ñ—Ä diversity –ø–µ—Ä–µ–¥ –∫–æ–º–±—ñ–Ω—É–≤–∞–Ω–Ω—è–º
from sklearn.metrics import confusion_matrix

predictions = [model.predict(X_test) for model in models]

# –Ø–∫—â–æ –≤—Å—ñ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –æ–¥–Ω–∞–∫–æ–≤—ñ ‚Üí –Ω–µ–º–∞—î —Å–µ–Ω—Å—É –≤ –∞–Ω—Å–∞–º–±–ª—ñ!
```

### 4. –ù–∞–¥—Ç–æ —Å–∫–ª–∞–¥–Ω–∏–π –∞–Ω—Å–∞–º–±–ª—å

```python
# ‚ùå OVERKILL
# Stacking of stacking of voting of bagging...

# ‚úÖ –ü—Ä–æ—Å—Ç—ñ –º–µ—Ç–æ–¥–∏ —á–∞—Å—Ç–æ –¥–æ—Å—Ç–∞—Ç–Ω—å–æ
# Random Forest –∞–±–æ XGBoost —á–∞—Å—Ç–æ –≤—Å–µ —â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ
```

---

## –ü–æ–≤'—è–∑–∞–Ω—ñ —Ç–µ–º–∏

- [[01_Decision_Trees]] ‚Äî –±–∞–∑–æ–≤—ñ learners
- [[02_Random_Forest]] ‚Äî bagging –∞–Ω—Å–∞–º–±–ª—å
- [[03_Gradient_Boosting]] ‚Äî boosting –∞–Ω—Å–∞–º–±–ª—å
- [[04_AdaBoost]] ‚Äî –∫–ª–∞—Å–∏—á–Ω–∏–π boosting
- [[Cross_Validation]] ‚Äî –æ—Ü—ñ–Ω–∫–∞ –∞–Ω—Å–∞–º–±–ª—ñ–≤

## –†–µ—Å—É—Ä—Å–∏

- [Scikit-learn: Ensemble Methods](https://scikit-learn.org/stable/modules/ensemble.html)
- [Zhou: Ensemble Methods - Foundations and Algorithms](https://www.routledge.com/Ensemble-Methods-Foundations-and-Algorithms/Zhou/p/book/9781439830031)
- [Kaggle: Ensemble Guide](https://www.kaggle.com/arthurtok/introduction-to-ensembling-stacking-in-python)

---

## –ö–ª—é—á–æ–≤—ñ –≤–∏—Å–Ω–æ–≤–∫–∏

> Ensemble Methods –∫–æ–º–±—ñ–Ω—É—é—Ç—å –º–Ω–æ–∂–∏–Ω—É –º–æ–¥–µ–ª–µ–π –¥–ª—è –æ—Ç—Ä–∏–º–∞–Ω–Ω—è –∫—Ä–∞—â–∏—Ö –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω—å —á–µ—Ä–µ–∑ –∑–º–µ–Ω—à–µ–Ω–Ω—è bias —Ç–∞/–∞–±–æ variance.

**–û—Å–Ω–æ–≤–Ω—ñ —Ç–∏–ø–∏:**
- **Bagging** ‚Äî –ø–∞—Ä–∞–ª–µ–ª—å–Ω–æ, —Ä—ñ–∑–Ω—ñ –¥–∞–Ω—ñ, –∑–º–µ–Ω—à—É—î variance
- **Boosting** ‚Äî –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ, —Ñ–æ–∫—É—Å –Ω–∞ –ø–æ–º–∏–ª–∫–∞—Ö, –∑–º–µ–Ω—à—É—î bias
- **Stacking** ‚Äî –±–∞–≥–∞—Ç–æ—Ä—ñ–≤–Ω–µ–≤–∏–π, —Ä—ñ–∑–Ω—ñ –º–æ–¥–µ–ª—ñ, –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å
- **Voting** ‚Äî –ø—Ä–æ—Å—Ç–∏–π, —É—Å–µ—Ä–µ–¥–Ω–µ–Ω–Ω—è/–≥–æ–ª–æ—Å—É–≤–∞–Ω–Ω—è

**–ö–ª—é—á–æ–≤—ñ –ø—Ä–∏–Ω—Ü–∏–ø–∏:**
- **–†—ñ–∑–Ω–æ–º–∞–Ω—ñ—Ç–Ω—ñ—Å—Ç—å** –º–æ–¥–µ–ª–µ–π –∫—Ä–∏—Ç–∏—á–Ω–∞
- –ú–æ–¥–µ–ª—ñ –ø–æ–≤–∏–Ω–Ω—ñ –±—É—Ç–∏ **—Ç–æ—á–Ω–∏–º–∏** —Ç–∞ **—Ä—ñ–∑–Ω–∏–º–∏**
- –ë–∞–ª–∞–Ω—Å –º—ñ–∂ **—Å–∫–ª–∞–¥–Ω—ñ—Å—Ç—é** —Ç–∞ **–ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è–º**

**–§–æ—Ä–º—É–ª–∞ —É—Å–µ—Ä–µ–¥–Ω–µ–Ω–Ω—è:**
$$\text{Var}(\text{average}) = \rho\sigma^2 + \frac{1-\rho}{M}\sigma^2$$

**–ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏:**
- –®–≤–∏–¥–∫–∏–π baseline = Random Forest ‚úì
- –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å = Gradient Boosting (XGBoost) ‚úì
- –ê–±—Å–æ–ª—é—Ç–Ω–∏–π –º–∞–∫—Å–∏–º—É–º = Stacking ‚úì
- –ü—Ä–æ—Å—Ç–æ—Ç–∞ –∫–æ–º–±—ñ–Ω—É–≤–∞–Ω–Ω—è = Voting ‚úì

**–ù–∞–π–≤–∞–∂–ª–∏–≤—ñ—à–µ:**
- –†—ñ–∑–Ω–æ–º–∞–Ω—ñ—Ç–Ω—ñ—Å—Ç—å > –ö—ñ–ª—å–∫—ñ—Å—Ç—å –º–æ–¥–µ–ª–µ–π
- Random Forest ‚Äî –Ω–∞–π–∫—Ä–∞—â–∏–π —É–Ω—ñ–≤–µ—Ä—Å–∞–ª—å–Ω–∏–π –≤–∏–±—ñ—Ä
- XGBoost/LightGBM ‚Äî –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ—ó —Ç–æ—á–Ω–æ—Å—Ç—ñ
- Stacking ‚Äî –¥–ª—è competitions, –∞–ª–µ —Å–∫–ª–∞–¥–Ω–æ

---

#ml #supervised-learning #ensemble #bagging #boosting #stacking #voting #random-forest #gradient-boosting #tree-based
