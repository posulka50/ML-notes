# AdaBoost (Adaptive Boosting)

## –©–æ —Ü–µ?

**AdaBoost (Adaptive Boosting)** ‚Äî —Ü–µ –∫–ª–∞—Å–∏—á–Ω–∏–π –∞–Ω—Å–∞–º–±–ª–µ–≤–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º boosting, —è–∫–∏–π **–∞–¥–∞–ø—Ç–∏–≤–Ω–æ** –∑–º—ñ–Ω—é—î –≤–∞–≥–∏ —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∏—Ö –∑—Ä–∞–∑–∫—ñ–≤, —Ñ–æ–∫—É—Å—É—é—á–∏—Å—å –Ω–∞ –≤–∞–∂–∫–∏—Ö –¥–ª—è –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó –ø—Ä–∏–∫–ª–∞–¥–∞—Ö.

**–ì–æ–ª–æ–≤–Ω–∞ —ñ–¥–µ—è:** –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ –Ω–∞–≤—á–∞—Ç–∏ —Å–ª–∞–±–∫—ñ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä–∏ (weak learners), –¥–∞—é—á–∏ –±—ñ–ª—å—à—É –≤–∞–≥—É –∑—Ä–∞–∑–∫–∞–º, —è–∫—ñ –±—É–ª–∏ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ –∫–ª–∞—Å–∏—Ñ—ñ–∫–æ–≤–∞–Ω—ñ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–º–∏ –º–æ–¥–µ–ª—è–º–∏.

## –ù–∞–≤—ñ—â–æ –ø–æ—Ç—Ä—ñ–±–µ–Ω?

- üéØ **–ü—Ä–æ—Å—Ç–æ—Ç–∞** ‚Äî –ª–µ–≥–∫–æ –∑—Ä–æ–∑—É–º—ñ—Ç–∏ —Ç–∞ —Ä–µ–∞–ª—ñ–∑—É–≤–∞—Ç–∏
- üìö **–Ü—Å—Ç–æ—Ä–∏—á–Ω–∞ –≤–∞–∂–ª–∏–≤—ñ—Å—Ç—å** ‚Äî –ø–µ—Ä—à–∏–π –ø—Ä–∞–∫—Ç–∏—á–Ω–∏–π boosting –∞–ª–≥–æ—Ä–∏—Ç–º (1996)
- üîß **–£–Ω—ñ–≤–µ—Ä—Å–∞–ª—å–Ω—ñ—Å—Ç—å** ‚Äî –ø—Ä–∞—Ü—é—î –∑ –±—É–¥—å-—è–∫–∏–º–∏ —Å–ª–∞–±–∫–∏–º–∏ learners
- üí° **–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å** ‚Äî –∑—Ä–æ–∑—É–º—ñ–ª–∞ –ª–æ–≥—ñ–∫–∞ –≤–∞–≥—É–≤–∞–Ω–Ω—è
- ‚ö° **–ï—Ñ–µ–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å** ‚Äî –º–æ–∂–µ –∑–Ω–∞—á–Ω–æ –ø–æ–∫—Ä–∞—â–∏—Ç–∏ weak learners

## –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏?

**–ü–æ—Ç—Ä—ñ–±–Ω–æ:**

- **–ë—ñ–Ω–∞—Ä–Ω–∞ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è** ‚Äî –æ—Å–Ω–æ–≤–Ω–∞ –∑–∞–¥–∞—á–∞ AdaBoost
- –ü—Ä–æ—Å—Ç–∏–π baseline –¥–ª—è boosting
- **–ù–∞–≤—á–∞–ª—å–Ω—ñ —Ü—ñ–ª—ñ** ‚Äî —Ä–æ–∑—É–º—ñ–Ω–Ω—è –ø—Ä–∏–Ω—Ü–∏–ø—ñ–≤ boosting
- Weak learners (decision stumps) –¥–æ—Å—Ç—É–ø–Ω—ñ
- –î–∞–Ω—ñ –Ω–µ –¥—É–∂–µ –∑–∞—à—É–º–ª–µ–Ω—ñ

**–ù–µ –ø–æ—Ç—Ä—ñ–±–Ω–æ:**
- **–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å** ‚Üí Gradient Boosting (XGBoost, LightGBM)
- –†–µ–≥—Ä–µ—Å—ñ—è ‚Üí Gradient Boosting Regressor
- **–ó–∞—à—É–º–ª–µ–Ω—ñ –¥–∞–Ω—ñ** ‚Üí Random Forest (—Ä–æ–±–∞—Å—Ç–Ω—ñ—à–∏–π)
- –ë–∞–≥–∞—Ç–æ–∫–ª–∞—Å–æ–≤–∞ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è ‚Üí Gradient Boosting, XGBoost

---

## –í—ñ–¥–º—ñ–Ω–Ω—ñ—Å—Ç—å –≤—ñ–¥ —ñ–Ω—à–∏—Ö –º–µ—Ç–æ–¥—ñ–≤

### AdaBoost vs Gradient Boosting

| –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞ | AdaBoost | Gradient Boosting |
|----------------|----------|-------------------|
| **–ü—ñ–¥—Ö—ñ–¥** | –ó–º—ñ–Ω—é—î –≤–∞–≥–∏ –∑—Ä–∞–∑–∫—ñ–≤ | –§—ñ—Ç—É—î –Ω–∞ residuals |
| **–§—É–Ω–∫—Ü—ñ—è –≤—Ç—Ä–∞—Ç** | Exponential loss | –†—ñ–∑–Ω—ñ (MSE, Log-loss) |
| **Weak learners** | –ë—É–¥—å-—è–∫—ñ | –ó–∞–∑–≤–∏—á–∞–π –¥–µ—Ä–µ–≤–∞ |
| **–°–∫–ª–∞–¥–Ω—ñ—Å—Ç—å** | –ü—Ä–æ—Å—Ç—ñ—à–∏–π | –°–∫–ª–∞–¥–Ω—ñ—à–∏–π, –≥–Ω—É—á–∫—ñ—à–∏–π |
| **–†–µ–≥—Ä–µ—Å—ñ—è** | ‚ùå (—Å–∫–ª–∞–¥–Ω–æ) | ‚úÖ –¢–∞–∫ |
| **–ü–æ–ø—É–ª—è—Ä–Ω—ñ—Å—Ç—å** | –Ü—Å—Ç–æ—Ä–∏—á–Ω–∞ | ‚úÖ SOTA |

### AdaBoost vs Random Forest

| –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞ | AdaBoost | Random Forest |
|----------------|----------|---------------|
| **–¢–∏–ø –∞–Ω—Å–∞–º–±–ª—é** | Boosting (–ø–æ—Å–ª—ñ–¥–æ–≤–Ω–∏–π) | Bagging (–ø–∞—Ä–∞–ª–µ–ª—å–Ω–∏–π) |
| **–§–æ–∫—É—Å** | –í–∞–∂–∫—ñ –∑—Ä–∞–∑–∫–∏ | –†—ñ–∑–Ω–æ–º–∞–Ω—ñ—Ç–Ω—ñ—Å—Ç—å |
| **Overfitting** | –ú–æ–∂–µ –ø–µ—Ä–µ–æ–±—É—á–∏—Ç–∏—Å—è | –†–æ–±–∞—Å—Ç–Ω–∏–π |
| **–®–≤–∏–¥–∫—ñ—Å—Ç—å** | –ü–æ—Å–ª—ñ–¥–æ–≤–Ω–µ | –ü–∞—Ä–∞–ª–µ–ª—å–Ω–µ |

---

## –Ø–∫ –ø—Ä–∞—Ü—é—î AdaBoost?

### –Ü–Ω—Ç—É—ó—Ü—ñ—è

**–ê–Ω–∞–ª–æ–≥—ñ—è: –ù–∞–≤—á–∞–Ω–Ω—è —Å—Ç—É–¥–µ–Ω—Ç–∞**

1. **–ü–µ—Ä—à–∏–π —Ç–µ—Å—Ç (–º–æ–¥–µ–ª—å 1):**
   - –°—Ç—É–¥–µ–Ω—Ç –≤–∏—Ä—ñ—à—É—î –∑–∞–¥–∞—á—ñ
   - –î–µ—è–∫—ñ –ø—Ä–∞–≤–∏–ª—å–Ω–æ ‚úì, –¥–µ—è–∫—ñ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ ‚úó

2. **–î—Ä—É–≥–∏–π —Ç–µ—Å—Ç (–º–æ–¥–µ–ª—å 2):**
   - –í–∏–∫–ª–∞–¥–∞—á **–±—ñ–ª—å—à–µ —É–≤–∞–≥–∏** –ø—Ä–∏–¥—ñ–ª—è—î —Å–∫–ª–∞–¥–Ω–∏–º –∑–∞–¥–∞—á–∞–º (–∑ –ø–æ–º–∏–ª–∫–∞–º–∏)
   - –°—Ç—É–¥–µ–Ω—Ç —Ñ–æ–∫—É—Å—É—î—Ç—å—Å—è –Ω–∞ –≤–∞–∂–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö

3. **–¢—Ä–µ—Ç—ñ–π —Ç–µ—Å—Ç (–º–æ–¥–µ–ª—å 3):**
   - –ó–Ω–æ–≤—É —Ñ–æ–∫—É—Å –Ω–∞ –∑–∞–¥–∞—á–∞—Ö, –¥–µ –±—É–ª–∏ –ø–æ–º–∏–ª–∫–∏
   - –ü–æ—Å—Ç—É–ø–æ–≤–æ –≤—Å—ñ –∑–∞–¥–∞—á—ñ –≤–∏—Ä—ñ—à–µ–Ω—ñ

**–§—ñ–Ω–∞–ª—å–Ω–∏–π —ñ—Å–ø–∏—Ç:**
- –ö–æ–∂–µ–Ω —Ç–µ—Å—Ç –º–∞—î **–≤–∞–≥—É** (–≤–∞–∂–ª–∏–≤—ñ—Å—Ç—å)
- –¢–µ—Å—Ç–∏, –¥–µ —Å—Ç—É–¥–µ–Ω—Ç –∫—Ä–∞—â–µ —Å–ø—Ä–∞–≤–∏–≤—Å—è ‚Üí –±—ñ–ª—å—à–∞ –≤–∞–≥–∞
- –§—ñ–Ω–∞–ª—å–Ω–∞ –æ—Ü—ñ–Ω–∫–∞ = –∑–≤–∞–∂–µ–Ω–∞ –∫–æ–º–±—ñ–Ω–∞—Ü—ñ—è –≤—Å—ñ—Ö —Ç–µ—Å—Ç—ñ–≤

### –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è

```
–î–∞–Ω—ñ: O O O X X X (O = –∫–ª–∞—Å 0, X = –∫–ª–∞—Å 1)

–ú–æ–¥–µ–ª—å 1 (decision stump):
    –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è: O O X X X X
    –ü–æ–º–∏–ª–∫–∏:         ‚úó       ‚Üê –ø–æ–º–∏–ª–∫–∞
    –î–∞—Ç–∏ –±—ñ–ª—å—à—É –≤–∞–≥—É —Ü—å–æ–º—É –∑—Ä–∞–∑–∫—É!

–ú–æ–¥–µ–ª—å 2 (—Ñ–æ–∫—É—Å –Ω–∞ –≤–∞–∂–∫–∏—Ö –∑—Ä–∞–∑–∫–∞—Ö):
    –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è: O O O X X X
    –í—Å—ñ –ø—Ä–∞–≤–∏–ª—å–Ω–æ! ‚úì
    
–§—ñ–Ω–∞–ª—å–Ω–µ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è:
    Œ±‚ÇÅ * –ú–æ–¥–µ–ª—å‚ÇÅ + Œ±‚ÇÇ * –ú–æ–¥–µ–ª—å‚ÇÇ
    –¥–µ Œ±‚ÇÅ, Œ±‚ÇÇ ‚Äî –≤–∞–≥–∏ –º–æ–¥–µ–ª–µ–π
```

---

## –ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞

### –ê–ª–≥–æ—Ä–∏—Ç–º AdaBoost (SAMME)

**–í—Ö—ñ–¥:**
- –¢—Ä–µ–Ω—É–≤–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ: $(x_1, y_1), ..., (x_n, y_n)$ –¥–µ $y_i \in \{-1, +1\}$
- –ö—ñ–ª—å–∫—ñ—Å—Ç—å —ñ—Ç–µ—Ä–∞—Ü—ñ–π: $T$
- –ë–∞–∑–æ–≤–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º (weak learner)

**1. –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –≤–∞–≥:**
$$w_i^{(1)} = \frac{1}{n} \quad \text{–¥–ª—è –≤—Å—ñ—Ö } i = 1, ..., n$$

–í—Å—ñ –∑—Ä–∞–∑–∫–∏ —Å–ø–æ—á–∞—Ç–∫—É –º–∞—é—Ç—å —Ä—ñ–≤–Ω—ñ –≤–∞–≥–∏.

**2. –î–ª—è $t = 1$ –¥–æ $T$:**

   **a) –ù–∞–≤—á–∏—Ç–∏ weak learner $h_t$ –Ω–∞ –¥–∞–Ω–∏—Ö –∑ –≤–∞–≥–∞–º–∏ $w^{(t)}$**
   
   **b) –û–±—á–∏—Å–ª–∏—Ç–∏ –∑–≤–∞–∂–µ–Ω—É –ø–æ–º–∏–ª–∫—É:**
   $$\epsilon_t = \sum_{i=1}^{n} w_i^{(t)} \cdot \mathbb{1}[h_t(x_i) \neq y_i]$$
   
   –¥–µ $\mathbb{1}[\cdot]$ ‚Äî —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è (1 —è–∫—â–æ –ø–æ–º–∏–ª–∫–∞, 0 —ñ–Ω–∞–∫—à–µ)

   **c) –û–±—á–∏—Å–ª–∏—Ç–∏ –≤–∞–≥—É –º–æ–¥–µ–ª—ñ:**
   $$\alpha_t = \frac{1}{2} \ln\left(\frac{1 - \epsilon_t}{\epsilon_t}\right)$$
   
   - –Ø–∫—â–æ $\epsilon_t$ –º–∞–ª–∞ (–º–æ–¥–µ–ª—å —Ç–æ—á–Ω–∞) ‚Üí $\alpha_t$ –≤–µ–ª–∏–∫–∞ ‚úì
   - –Ø–∫—â–æ $\epsilon_t$ –≤–µ–ª–∏–∫–∞ (–º–æ–¥–µ–ª—å –ø–æ–≥–∞–Ω–∞) ‚Üí $\alpha_t$ –º–∞–ª–∞ ‚úó

   **d) –û–Ω–æ–≤–∏—Ç–∏ –≤–∞–≥–∏ –∑—Ä–∞–∑–∫—ñ–≤:**
   $$w_i^{(t+1)} = w_i^{(t)} \cdot \exp(-\alpha_t \cdot y_i \cdot h_t(x_i))$$
   
   –∞–±–æ –µ–∫–≤—ñ–≤–∞–ª–µ–Ω—Ç–Ω–æ:
   $$w_i^{(t+1)} = w_i^{(t)} \cdot \begin{cases}
   e^{-\alpha_t} & \text{—è–∫—â–æ –ø—Ä–∞–≤–∏–ª—å–Ω–æ –∫–ª–∞—Å–∏—Ñ—ñ–∫–æ–≤–∞–Ω–æ} \\
   e^{\alpha_t} & \text{—è–∫—â–æ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ –∫–ª–∞—Å–∏—Ñ—ñ–∫–æ–≤–∞–Ω–æ}
   \end{cases}$$
   
   **e) –ù–æ—Ä–º–∞–ª—ñ–∑—É–≤–∞—Ç–∏ –≤–∞–≥–∏:**
   $$w_i^{(t+1)} = \frac{w_i^{(t+1)}}{\sum_{j=1}^{n} w_j^{(t+1)}}$$

**3. –§—ñ–Ω–∞–ª—å–Ω–∞ –º–æ–¥–µ–ª—å (–∑–≤–∞–∂–µ–Ω–µ –≥–æ–ª–æ—Å—É–≤–∞–Ω–Ω—è):**
$$H(x) = \text{sign}\left(\sum_{t=1}^{T} \alpha_t \cdot h_t(x)\right)$$

---

## –î–µ—Ç–∞–ª—å–Ω–∏–π –ø—Ä–∏–∫–ª–∞–¥

### –î–∞–Ω—ñ

| ‚Ññ | x‚ÇÅ | x‚ÇÇ | y (–∫–ª–∞—Å) |
|---|----|----|----------|
| 1 | 1  | 2  | -1 |
| 2 | 2  | 3  | -1 |
| 3 | 3  | 3  | +1 |
| 4 | 4  | 5  | +1 |
| 5 | 5  | 4  | +1 |

### Iteration 1

**a) –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –≤–∞–≥:**
$$w^{(1)} = [0.2, 0.2, 0.2, 0.2, 0.2]$$

**b) –ù–∞–≤—á–∏—Ç–∏ weak learner (decision stump):**

–ü—Ä–∏–ø—É—Å—Ç–∏–º–æ, –Ω–∞–π–∫—Ä–∞—â–µ –ø—Ä–∞–≤–∏–ª–æ: "–Ø–∫—â–æ x‚ÇÅ ‚â§ 2.5, —Ç–æ –∫–ª–∞—Å = -1, —ñ–Ω–∞–∫—à–µ –∫–ª–∞—Å = +1"

**–ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è:**
| ‚Ññ | –ü—Ä–∞–≤–∏–ª—å–Ω–∏–π y | –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è h‚ÇÅ | –ü—Ä–∞–≤–∏–ª—å–Ω–æ? |
|---|--------------|-----------------|------------|
| 1 | -1 | -1 | ‚úì |
| 2 | -1 | -1 | ‚úì |
| 3 | +1 | +1 | ‚úì |
| 4 | +1 | +1 | ‚úì |
| 5 | +1 | +1 | ‚úì |

**c) –û–±—á–∏—Å–ª–∏—Ç–∏ –ø–æ–º–∏–ª–∫—É:**
$$\epsilon_1 = 0.2 \times 0 + 0.2 \times 0 + 0.2 \times 0 + 0.2 \times 0 + 0.2 \times 0 = 0$$

‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ = 0 –Ω–µ –¥–æ–∑–≤–æ–ª–µ–Ω–∞! –£ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç—ñ –±—É–¥–µ ~0.001

–ü—Ä–∏–ø—É—Å—Ç–∏–º–æ, –æ–¥–Ω–∞ –ø–æ–º–∏–ª–∫–∞ (–∑—Ä–∞–∑–æ–∫ 3):
$$\epsilon_1 = 0.2$$

**d) –í–∞–≥–∞ –º–æ–¥–µ–ª—ñ:**
$$\alpha_1 = \frac{1}{2} \ln\left(\frac{1 - 0.2}{0.2}\right) = \frac{1}{2} \ln(4) \approx 0.693$$

**e) –û–Ω–æ–≤–ª–µ–Ω–Ω—è –≤–∞–≥ –∑—Ä–∞–∑–∫—ñ–≤:**

–î–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–∏—Ö (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥, –∑—Ä–∞–∑–æ–∫ 1):
$$w_1^{(2)} = 0.2 \times e^{-0.693} = 0.2 \times 0.5 = 0.1$$

–î–ª—è –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∏—Ö (–∑—Ä–∞–∑–æ–∫ 3):
$$w_3^{(2)} = 0.2 \times e^{0.693} = 0.2 \times 2 = 0.4$$

**f) –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è:**
$$\text{–°—É–º–∞} = 0.1 + 0.1 + 0.4 + 0.1 + 0.1 = 0.8$$

$$w^{(2)} = \left[\frac{0.1}{0.8}, \frac{0.1}{0.8}, \frac{0.4}{0.8}, \frac{0.1}{0.8}, \frac{0.1}{0.8}\right]$$
$$= [0.125, 0.125, 0.5, 0.125, 0.125]$$

**–°–ø–æ—Å—Ç–µ—Ä–µ–∂–µ–Ω–Ω—è:** –ó—Ä–∞–∑–æ–∫ 3 —Ç–µ–ø–µ—Ä –º–∞—î –≤–∞–≥—É 0.5 (–∑–∞–º—ñ—Å—Ç—å 0.2) ‚Äî –±—ñ–ª—å—à–∏–π —Ñ–æ–∫—É—Å!

### Iteration 2

**a) –ù–∞–≤—á–∏—Ç–∏ –Ω–æ–≤–∏–π weak learner –∑ –æ–Ω–æ–≤–ª–µ–Ω–∏–º–∏ –≤–∞–≥–∞–º–∏ $w^{(2)}$**

–ú–æ–¥–µ–ª—å —Ñ–æ–∫—É—Å—É—î—Ç—å—Å—è –Ω–∞ –∑—Ä–∞–∑–∫—É 3 (–Ω–∞–π–±—ñ–ª—å—à–∞ –≤–∞–≥–∞).

**b-f) –ü–æ–≤—Ç–æ—Ä–∏—Ç–∏ –∫—Ä–æ–∫–∏...**

### –§—ñ–Ω–∞–ª—å–Ω–µ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è

–ü—ñ—Å–ª—è $T$ —ñ—Ç–µ—Ä–∞—Ü—ñ–π:

$$H(x) = \text{sign}(\alpha_1 h_1(x) + \alpha_2 h_2(x) + ... + \alpha_T h_T(x))$$

**–ü—Ä–∏–∫–ª–∞–¥:**
- $h_1(x) = +1$, $\alpha_1 = 0.693$
- $h_2(x) = -1$, $\alpha_2 = 0.405$
- $h_3(x) = +1$, $\alpha_3 = 0.916$

$$H(x) = \text{sign}(0.693 \times 1 - 0.405 \times 1 + 0.916 \times 1)$$
$$= \text{sign}(1.204) = +1$$

---

## –í–∞–≥–∞ –º–æ–¥–µ–ª—ñ (Œ±)

### –§–æ—Ä–º—É–ª–∞

$$\alpha_t = \frac{1}{2} \ln\left(\frac{1 - \epsilon_t}{\epsilon_t}\right)$$

### –ì—Ä–∞—Ñ—ñ–∫ Œ± vs Œµ

```
Œ± (–≤–∞–≥–∞ –º–æ–¥–µ–ª—ñ)
    |
  3 |                   ‚Ä¢
    |                 ‚ï±
  2 |               ‚ï±
    |             ‚ï±
  1 |          ‚Ä¢‚ï±
    |        ‚ï±
  0 |______‚Ä¢_______________ Œµ (–ø–æ–º–∏–ª–∫–∞)
    0    0.2  0.5  0.8   1.0

–ü—Ä–∏ Œµ = 0.5 (–≤–∏–ø–∞–¥–∫–æ–≤–µ –≥–∞–¥–∞–Ω–Ω—è) ‚Üí Œ± = 0 (–º–æ–¥–µ–ª—å –Ω–µ–ø–æ—Ç—Ä—ñ–±–Ω–∞)
–ü—Ä–∏ Œµ ‚Üí 0 (—ñ–¥–µ–∞–ª—å–Ω–∞ –º–æ–¥–µ–ª—å) ‚Üí Œ± ‚Üí ‚àû (–¥—É–∂–µ –≤–∞–∂–ª–∏–≤–∞)
–ü—Ä–∏ Œµ ‚Üí 1 (–∑–∞–≤–∂–¥–∏ –ø–æ–º–∏–ª—è—î—Ç—å—Å—è) ‚Üí Œ± ‚Üí -‚àû (–Ω–µ–≥–∞—Ç–∏–≤–Ω–∞ –≤–∞–≥–∞)
```

**–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü—ñ—è:**
- **Œµ < 0.5:** –º–æ–¥–µ–ª—å –∫—Ä–∞—â–∞ –∑–∞ –≤–∏–ø–∞–¥–∫–æ–≤–µ –≥–∞–¥–∞–Ω–Ω—è ‚Üí Œ± > 0 ‚úì
- **Œµ = 0.5:** –º–æ–¥–µ–ª—å = –≤–∏–ø–∞–¥–∫–æ–≤–µ –≥–∞–¥–∞–Ω–Ω—è ‚Üí Œ± = 0
- **Œµ > 0.5:** –º–æ–¥–µ–ª—å –≥—ñ—Ä—à–∞ –∑–∞ –≤–∏–ø–∞–¥–∫–æ–≤–µ –≥–∞–¥–∞–Ω–Ω—è ‚Üí Œ± < 0 ‚úó

---

## –ö–æ–¥ (scikit-learn)

### –ë–∞–∑–æ–≤–∏–π –ø—Ä–∏–∫–ª–∞–¥

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

# –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –¥–∞–Ω–∏—Ö
X, y = make_classification(
    n_samples=1000,
    n_features=20,
    n_informative=15,
    n_redundant=5,
    random_state=42
)

# –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# AdaBoost –∑ decision stumps (max_depth=1)
ada_clf = AdaBoostClassifier(
    estimator=DecisionTreeClassifier(max_depth=1),  # Decision stump
    n_estimators=50,          # –ö—ñ–ª—å–∫—ñ—Å—Ç—å weak learners
    learning_rate=1.0,        # –®–≤–∏–¥–∫—ñ—Å—Ç—å –Ω–∞–≤—á–∞–Ω–Ω—è
    algorithm='SAMME',        # –∞–±–æ 'SAMME.R' (–≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ)
    random_state=42
)

# –ù–∞–≤—á–∞–Ω–Ω—è
ada_clf.fit(X_train, y_train)

# –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
y_pred = ada_clf.predict(X_test)

# –ú–µ—Ç—Ä–∏–∫–∏
print("=== AdaBoost Classifier ===")
print(f"Train Accuracy: {ada_clf.score(X_train, y_train):.4f}")
print(f"Test Accuracy: {accuracy_score(y_test, y_pred):.4f}")

print("\n" + classification_report(y_test, y_pred))

# –í–∞–≥–∏ –º–æ–¥–µ–ª–µ–π (estimator_weights_)
print("\n=== Model Weights (first 10) ===")
print(ada_clf.estimator_weights_[:10])

# –ü–æ–º–∏–ª–∫–∏ –º–æ–¥–µ–ª–µ–π (estimator_errors_)
print("\n=== Model Errors (first 10) ===")
print(ada_clf.estimator_errors_[:10])
```

### –ó —Ä—ñ–∑–Ω–∏–º–∏ weak learners

```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression

# 1. Decision Stumps (–Ω–∞–π–ø–æ–ø—É–ª—è—Ä–Ω—ñ—à–µ)
ada_stumps = AdaBoostClassifier(
    estimator=DecisionTreeClassifier(max_depth=1),
    n_estimators=50,
    random_state=42
)

# 2. –î–µ—Ä–µ–≤–∞ –≥–ª–∏–±–∏–Ω–æ—é 3
ada_trees = AdaBoostClassifier(
    estimator=DecisionTreeClassifier(max_depth=3),
    n_estimators=50,
    random_state=42
)

# 3. Logistic Regression
ada_lr = AdaBoostClassifier(
    estimator=LogisticRegression(max_iter=1000),
    n_estimators=50,
    random_state=42
)

# –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è
for name, model in [('Stumps', ada_stumps), 
                     ('Trees', ada_trees), 
                     ('LR', ada_lr)]:
    model.fit(X_train, y_train)
    score = model.score(X_test, y_test)
    print(f"{name}: Test Accuracy = {score:.4f}")
```

---

## Staged Predictions

### –ú–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥ –Ω–∞–≤—á–∞–Ω–Ω—è

```python
# –ù–∞–≤—á–∞–Ω–Ω—è
ada = AdaBoostClassifier(n_estimators=100, random_state=42)
ada.fit(X_train, y_train)

# –ü–æ–µ—Ç–∞–ø–Ω–∞ accuracy
train_scores = []
test_scores = []

for train_pred, test_pred in zip(ada.staged_predict(X_train),
                                   ada.staged_predict(X_test)):
    train_scores.append(accuracy_score(y_train, train_pred))
    test_scores.append(accuracy_score(y_test, test_pred))

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
plt.figure(figsize=(12, 6))
plt.plot(range(1, len(train_scores) + 1), train_scores, 
         label='Train', linewidth=2)
plt.plot(range(1, len(test_scores) + 1), test_scores, 
         label='Test', linewidth=2)
plt.xlabel('Number of Estimators', fontsize=12)
plt.ylabel('Accuracy', fontsize=12)
plt.title('AdaBoost: Accuracy vs Number of Estimators', 
          fontsize=14, fontweight='bold')
plt.legend(fontsize=11)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# –û–ø—Ç–∏–º–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å estimators
optimal_n = np.argmax(test_scores) + 1
print(f"Optimal number of estimators: {optimal_n}")
print(f"Best Test Accuracy: {test_scores[optimal_n - 1]:.4f}")
```

---

## –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è Decision Boundaries

```python
from sklearn.datasets import make_moons
import numpy as np

# –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –¥–∞–Ω–∏—Ö
X, y = make_moons(n_samples=500, noise=0.3, random_state=42)

# –†—ñ–∑–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å estimators
n_estimators_list = [1, 5, 10, 50]
fig, axes = plt.subplots(2, 2, figsize=(14, 12))
axes = axes.ravel()

for idx, n_est in enumerate(n_estimators_list):
    # –ú–æ–¥–µ–ª—å
    ada = AdaBoostClassifier(
        estimator=DecisionTreeClassifier(max_depth=1),
        n_estimators=n_est,
        random_state=42
    )
    ada.fit(X, y)
    
    # –°—ñ—Ç–∫–∞ –¥–ª—è –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó
    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5
    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5
    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),
                         np.linspace(y_min, y_max, 200))
    
    # –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
    Z = ada.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    
    # –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
    axes[idx].contourf(xx, yy, Z, alpha=0.3, cmap='viridis')
    axes[idx].scatter(X[:, 0], X[:, 1], c=y, cmap='viridis',
                     edgecolors='k', s=50)
    axes[idx].set_title(f'n_estimators={n_est}\nAccuracy={ada.score(X, y):.3f}',
                       fontsize=12, fontweight='bold')
    axes[idx].set_xlabel('Feature 1')
    axes[idx].set_ylabel('Feature 2')

plt.tight_layout()
plt.show()
```

---

## SAMME vs SAMME.R

### SAMME (Discrete AdaBoost)

**–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î –¥–∏—Å–∫—Ä–µ—Ç–Ω—ñ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –∫–ª–∞—Å—ñ–≤:**

$$\alpha_t = \frac{1}{2} \ln\left(\frac{1 - \epsilon_t}{\epsilon_t}\right) + \ln(K - 1)$$

–¥–µ $K$ ‚Äî –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∫–ª–∞—Å—ñ–≤.

```python
ada_samme = AdaBoostClassifier(
    algorithm='SAMME',
    n_estimators=50
)
```

### SAMME.R (Real AdaBoost)

**–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ –∫–ª–∞—Å—ñ–≤:**

–ë—ñ–ª—å—à –≥–Ω—É—á–∫–∏–π —Ç–∞ –∑–∞–∑–≤–∏—á–∞–π **—Ç–æ—á–Ω—ñ—à–∏–π**.

```python
ada_sammer = AdaBoostClassifier(
    algorithm='SAMME.R',  # –ó–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º
    n_estimators=50
)
```

### –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è

```python
# SAMME
ada_samme = AdaBoostClassifier(algorithm='SAMME', n_estimators=100)
ada_samme.fit(X_train, y_train)
score_samme = ada_samme.score(X_test, y_test)

# SAMME.R
ada_sammer = AdaBoostClassifier(algorithm='SAMME.R', n_estimators=100)
ada_sammer.fit(X_train, y_train)
score_sammer = ada_sammer.score(X_test, y_test)

print(f"SAMME Accuracy: {score_samme:.4f}")
print(f"SAMME.R Accuracy: {score_sammer:.4f}")
```

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—è:** –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π **SAMME.R** (–∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º) –¥–ª—è –∫—Ä–∞—â–æ—ó —Ç–æ—á–Ω–æ—Å—Ç—ñ.

---

## Learning Rate

### –í–ø–ª–∏–≤ –Ω–∞ –Ω–∞–≤—á–∞–Ω–Ω—è

$$F(x) = \text{sign}\left(\sum_{t=1}^{T} \nu \cdot \alpha_t \cdot h_t(x)\right)$$

–¥–µ $\nu$ ‚Äî learning rate.

```python
# –ï–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç –∑ learning rate
learning_rates = [0.1, 0.5, 1.0, 2.0]

for lr in learning_rates:
    ada = AdaBoostClassifier(
        n_estimators=100,
        learning_rate=lr,
        random_state=42
    )
    ada.fit(X_train, y_train)
    score = ada.score(X_test, y_test)
    print(f"Learning Rate {lr}: Test Accuracy = {score:.4f}")
```

**–¢–∏–ø–æ–≤–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:**
```
Learning Rate 0.1: Test Accuracy = 0.8450
Learning Rate 0.5: Test Accuracy = 0.8700
Learning Rate 1.0: Test Accuracy = 0.8750  ‚Üê –ù–∞–π–∫—Ä–∞—â–µ
Learning Rate 2.0: Test Accuracy = 0.8600  ‚Üê Overfitting
```

**Trade-off:**
- –ú–∞–ª–∏–π LR ‚Üí –ø–æ—Ç—Ä—ñ–±–Ω–æ –±—ñ–ª—å—à–µ estimators
- –í–µ–ª–∏–∫–∏–π LR ‚Üí —Ä–∏–∑–∏–∫ overfitting

---

## –ü–µ—Ä–µ–≤–∞–≥–∏ —Ç–∞ –Ω–µ–¥–æ–ª—ñ–∫–∏

### –ü–µ—Ä–µ–≤–∞–≥–∏ ‚úì

| –ü–µ—Ä–µ–≤–∞–≥–∞ | –ü–æ—è—Å–Ω–µ–Ω–Ω—è |
|----------|-----------|
| **–ü—Ä–æ—Å—Ç–æ—Ç–∞** | –õ–µ–≥–∫–æ –∑—Ä–æ–∑—É–º—ñ—Ç–∏ –∫–æ–Ω—Ü–µ–ø—Ü—ñ—é |
| **–£–Ω—ñ–≤–µ—Ä—Å–∞–ª—å–Ω—ñ—Å—Ç—å** | –ü—Ä–∞—Ü—é—î –∑ —Ä—ñ–∑–Ω–∏–º–∏ weak learners |
| **–ù–µ –ø–æ—Ç—Ä–µ–±—É—î tuning** | –ü—Ä–∞—Ü—é—î –¥–æ–±—Ä–µ "out-of-the-box" |
| **–Ü—Å—Ç–æ—Ä–∏—á–Ω–∞ –≤–∞–∂–ª–∏–≤—ñ—Å—Ç—å** | –ö–ª–∞—Å–∏—á–Ω–∏–π boosting –∞–ª–≥–æ—Ä–∏—Ç–º |
| **–ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∏–π feature selection** | –§–æ–∫—É—Å –Ω–∞ –≤–∞–∂–ª–∏–≤–∏—Ö –æ–∑–Ω–∞–∫–∞—Ö |
| **–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å** | –ó—Ä–æ–∑—É–º—ñ–ª–∞ –ª–æ–≥—ñ–∫–∞ –≤–∞–≥—É–≤–∞–Ω–Ω—è |

### –ù–µ–¥–æ–ª—ñ–∫–∏ ‚úó

| –ù–µ–¥–æ–ª—ñ–∫ | –ü–æ—è—Å–Ω–µ–Ω–Ω—è |
|---------|-----------|
| **–ß—É—Ç–ª–∏–≤—ñ—Å—Ç—å –¥–æ —à—É–º—É** | –ü–µ—Ä–µ–æ–±—É—á—É—î—Ç—å—Å—è –Ω–∞ noise —Ç–∞ outliers |
| **–ß—É—Ç–ª–∏–≤—ñ—Å—Ç—å –¥–æ –≤–∏–∫–∏–¥—ñ–≤** | –î–∞—î —ó–º –≤–µ–ª–∏–∫—É –≤–∞–≥—É |
| **–ë—ñ–Ω–∞—Ä–Ω–∞ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è** | –û—Å–Ω–æ–≤–Ω–∞ –∑–∞–¥–∞—á–∞ (–±–∞–≥–∞—Ç–æ–∫–ª–∞—Å–æ–≤–∞ —Å–∫–ª–∞–¥–Ω—ñ—à–∞) |
| **–ü–æ–≤—ñ–ª—å–Ω—ñ—à–µ –∑–∞ RF** | –ü–æ—Å–ª—ñ–¥–æ–≤–Ω–∞ –ø—Ä–∏—Ä–æ–¥–∞ |
| **–ó–∞—Å—Ç–∞—Ä—ñ–ª—ñ—Å—Ç—å** | Gradient Boosting –∑–∞–∑–≤–∏—á–∞–π –∫—Ä–∞—â–∏–π |
| **–†–µ–≥—Ä–µ—Å—ñ—è** | –°–∫–ª–∞–¥–Ω–æ –∞–¥–∞–ø—Ç—É–≤–∞—Ç–∏ |

---

## –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –∑ —ñ–Ω—à–∏–º–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–∞–º–∏

### AdaBoost vs Gradient Boosting

**–ö–æ–ª–∏ AdaBoost:**
‚úÖ –ù–∞–≤—á–∞–ª—å–Ω–∞ –∑–∞–¥–∞—á–∞ (—Ä–æ–∑—É–º—ñ–Ω–Ω—è boosting)
‚úÖ –ü—Ä–æ—Å—Ç–∏–π baseline
‚úÖ Weak learners –≤–∂–µ —î

**–ö–æ–ª–∏ Gradient Boosting:**
‚úÖ **–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å**
‚úÖ –†–µ–≥—Ä–µ—Å—ñ—è
‚úÖ Production ML
‚úÖ –ë—ñ–ª—å—à–∞ –≥–Ω—É—á–∫—ñ—Å—Ç—å (—Ä—ñ–∑–Ω—ñ loss functions)

### AdaBoost vs Random Forest

**–ö–æ–ª–∏ AdaBoost:**
‚úÖ –ü–æ—Å–ª—ñ–¥–æ–≤–Ω–∞ –ø–æ–±—É–¥–æ–≤–∞ –º–æ–¥–µ–ª–µ–π –º–∞—î —Å–µ–Ω—Å
‚úÖ Weak learners –ø—Ä–∏—Ä–æ–¥–Ω–æ –¥–æ—Å—Ç—É–ø–Ω—ñ

**–ö–æ–ª–∏ Random Forest:**
‚úÖ **–†–æ–±–∞—Å—Ç–Ω—ñ—Å—Ç—å –¥–æ —à—É–º—É**
‚úÖ –ü–∞—Ä–∞–ª–µ–ª—ñ–∑–∞—Ü—ñ—è –≤–∞–∂–ª–∏–≤–∞
‚úÖ –ú–µ–Ω—à–µ —Ä–∏–∑–∏–∫—É overfitting
‚úÖ **–†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–æ –¥–ª—è production**

---

## –ü—Ä–∞–∫—Ç–∏—á–Ω—ñ –ø–æ—Ä–∞–¥–∏ üí°

1. **Decision stumps** ‚Äî –Ω–∞–π–∫—Ä–∞—â–µ —è–∫ weak learners
2. **n_estimators=50-200** ‚Äî —Ç–∏–ø–æ–≤—ñ –∑–Ω–∞—á–µ–Ω–Ω—è
3. **SAMME.R** ‚Äî –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º
4. **Staged predictions** ‚Äî –º–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥ overfitting
5. **–û–±–µ—Ä–µ–∂–Ω–æ –∑ outliers** ‚Äî AdaBoost –¥—É–∂–µ —á—É—Ç–ª–∏–≤–∏–π
6. **–û—á–∏—Å—Ç–∏ –¥–∞–Ω—ñ** ‚Äî –≤–∏–¥–∞–ª–∏ —à—É–º –ø–µ—Ä–µ–¥ –Ω–∞–≤—á–∞–Ω–Ω—è–º
7. **–ü–æ—Ä—ñ–≤–Ω—è–π –∑ Gradient Boosting** ‚Äî –∑–∞–∑–≤–∏—á–∞–π —Ç–æ—á–Ω—ñ—à–µ
8. **Learning rate=1.0** ‚Äî —Ö–æ—Ä–æ—à–∏–π —Å—Ç–∞—Ä—Ç
9. **–í—ñ–∑—É–∞–ª—ñ–∑—É–π –≤–∞–≥–∏** ‚Äî —Ä–æ–∑—É–º—ñ–π, –Ω–∞ —á–æ–º—É —Ñ–æ–∫—É—Å—É—î—Ç—å—Å—è –º–æ–¥–µ–ª—å
10. **–ù–µ –¥–ª—è production** ‚Äî –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π XGBoost/LightGBM

---

## –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ AdaBoost

### –Ü–¥–µ–∞–ª—å–Ω–æ –ø—ñ–¥—Ö–æ–¥–∏—Ç—å ‚úì

- **–ù–∞–≤—á–∞–ª—å–Ω—ñ —Ü—ñ–ª—ñ** ‚Äî —Ä–æ–∑—É–º—ñ–Ω–Ω—è –∫–æ–Ω—Ü–µ–ø—Ü—ñ—ó boosting
- –ü—Ä–æ—Å—Ç–∏–π baseline –¥–ª—è –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó
- Weak learners –≤–∂–µ –¥–æ—Å—Ç—É–ø–Ω—ñ
- **–Ü—Å—Ç–æ—Ä–∏—á–Ω–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç** ‚Äî –∫–ª–∞—Å–∏—á–Ω–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º
- –ù–µ–≤–µ–ª–∏–∫—ñ –¥–∞—Ç–∞—Å–µ—Ç–∏ –±–µ–∑ —à—É–º—É

### –ö—Ä–∞—â–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ —ñ–Ω—à–µ ‚úó

- **Production ML** ‚Üí Gradient Boosting (XGBoost, LightGBM)
- –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å ‚Üí Gradient Boosting
- **–ó–∞—à—É–º–ª–µ–Ω—ñ –¥–∞–Ω—ñ** ‚Üí Random Forest
- –†–µ–≥—Ä–µ—Å—ñ—è ‚Üí Gradient Boosting Regressor
- –í–µ–ª–∏–∫—ñ –¥–∞–Ω—ñ ‚Üí LightGBM

---

## –†–µ–∞–ª—å–Ω–∏–π –ø—Ä–∏–∫–ª–∞–¥: –î–µ—Ç–µ–∫—Ü—ñ—è —Å–ø–∞–º—É

```python
import pandas as pd
import numpy as np
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score

# –°–∏–Ω—Ç–µ—Ç–∏—á–Ω—ñ –¥–∞–Ω—ñ (spam detection)
np.random.seed(42)
n_samples = 2000

# –û–∑–Ω–∞–∫–∏ email
data = {
    'num_capital_letters': np.random.randint(0, 200, n_samples),
    'num_exclamation': np.random.randint(0, 20, n_samples),
    'num_links': np.random.randint(0, 15, n_samples),
    'word_count': np.random.randint(10, 500, n_samples),
    'num_suspicious_words': np.random.randint(0, 30, n_samples),
    'has_attachment': np.random.randint(0, 2, n_samples),
}

# –°–∏–º—É–ª—é—î–º–æ spam
spam_prob = (
    (data['num_exclamation'] > 10) * 0.3 +
    (data['num_suspicious_words'] > 15) * 0.4 +
    (data['num_capital_letters'] > 100) * 0.2 +
    np.random.uniform(0, 0.1, n_samples)
)
data['is_spam'] = (spam_prob > 0.5).astype(int)

df = pd.DataFrame(data)

X = df.drop('is_spam', axis=1)
y = df['is_spam']

print(f"Dataset shape: {X.shape}")
print(f"Spam rate: {y.mean():.2%}")

# –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# AdaBoost –∑ decision stumps
ada = AdaBoostClassifier(
    estimator=DecisionTreeClassifier(max_depth=1),
    n_estimators=100,
    learning_rate=1.0,
    algorithm='SAMME.R',
    random_state=42
)

# –ù–∞–≤—á–∞–Ω–Ω—è
print("\nTraining AdaBoost...")
ada.fit(X_train, y_train)

# Cross-validation
cv_scores = cross_val_score(ada, X_train, y_train, cv=5)
print(f"CV Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std()*2:.4f})")

# –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
y_pred = ada.predict(X_test)
y_pred_proba = ada.predict_proba(X_test)[:, 1]

# –ú–µ—Ç—Ä–∏–∫–∏
print("\n" + "="*60)
print("=== Model Performance ===")
print("="*60)
print(f"Train Accuracy: {ada.score(X_train, y_train):.4f}")
print(f"Test Accuracy: {ada.score(X_test, y_test):.4f}")
print(f"ROC-AUC: {roc_auc_score(y_test, y_pred_proba):.4f}")

print("\n" + classification_report(y_test, y_pred, 
                                   target_names=['Not Spam', 'Spam']))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
print("\nConfusion Matrix:")
print(cm)

# Feature Importance (—á–µ—Ä–µ–∑ —á–∞—Å—Ç–æ—Ç—É –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è)
feature_importance = pd.DataFrame({
    'feature': X.columns,
    'importance': ada.feature_importances_
}).sort_values('importance', ascending=False)

print("\n" + "="*60)
print("=== Top Features ===")
print("="*60)
for idx, row in feature_importance.iterrows():
    print(f"{row['feature']}: {row['importance']:.4f}")

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó
fig, axes = plt.subplots(2, 2, figsize=(16, 12))

# 1. Staged Accuracy
train_staged = []
test_staged = []

for train_pred, test_pred in zip(ada.staged_predict(X_train),
                                   ada.staged_predict(X_test)):
    train_staged.append(accuracy_score(y_train, train_pred))
    test_staged.append(accuracy_score(y_test, test_pred))

axes[0, 0].plot(range(1, len(train_staged) + 1), train_staged, 
                label='Train', linewidth=2)
axes[0, 0].plot(range(1, len(test_staged) + 1), test_staged, 
                label='Test', linewidth=2)
axes[0, 0].set_xlabel('Number of Estimators', fontsize=12)
axes[0, 0].set_ylabel('Accuracy', fontsize=12)
axes[0, 0].set_title('Learning Curve', fontsize=14, fontweight='bold')
axes[0, 0].legend(fontsize=11)
axes[0, 0].grid(True, alpha=0.3)

# 2. Feature Importance
axes[0, 1].barh(feature_importance['feature'], 
                feature_importance['importance'])
axes[0, 1].set_xlabel('Importance', fontsize=12)
axes[0, 1].set_title('Feature Importances', fontsize=14, fontweight='bold')
axes[0, 1].grid(True, alpha=0.3, axis='x')

# 3. Model Weights Distribution
axes[1, 0].hist(ada.estimator_weights_, bins=20, edgecolor='black')
axes[1, 0].set_xlabel('Model Weight (Œ±)', fontsize=12)
axes[1, 0].set_ylabel('Frequency', fontsize=12)
axes[1, 0].set_title('Distribution of Model Weights', 
                     fontsize=14, fontweight='bold')
axes[1, 0].grid(True, alpha=0.3)

# 4. Error Distribution
axes[1, 1].scatter(range(len(ada.estimator_errors_)), 
                   ada.estimator_errors_, alpha=0.6)
axes[1, 1].axhline(y=0.5, color='r', linestyle='--', 
                   label='Random Guess (Œµ=0.5)')
axes[1, 1].set_xlabel('Estimator Index', fontsize=12)
axes[1, 1].set_ylabel('Error (Œµ)', fontsize=12)
axes[1, 1].set_title('Model Errors Over Iterations', 
                     fontsize=14, fontweight='bold')
axes[1, 1].legend(fontsize=11)
axes[1, 1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

---

## –ü–æ—à–∏—Ä–µ–Ω—ñ –ø–æ–º–∏–ª–∫–∏ ‚ùå

### 1. –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –≥–ª–∏–±–æ–∫–∏—Ö –¥–µ—Ä–µ–≤

```python
# ‚ùå –ù–ï–ü–†–ê–í–ò–õ–¨–ù–û
ada = AdaBoostClassifier(
    estimator=DecisionTreeClassifier(max_depth=10)
)

# ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û (decision stumps!)
ada = AdaBoostClassifier(
    estimator=DecisionTreeClassifier(max_depth=1)
)
```

### 2. –ù–µ –æ—á–∏—â–∞—Ç–∏ –¥–∞–Ω—ñ –≤—ñ–¥ outliers

```python
# AdaBoost –¥—É–∂–µ —á—É—Ç–ª–∏–≤–∏–π –¥–æ outliers!
# ‚úÖ –ó–∞–≤–∂–¥–∏ –æ—á–∏—â—É–π –¥–∞–Ω—ñ —Å–ø–æ—á–∞—Ç–∫—É
from sklearn.preprocessing import RobustScaler

scaler = RobustScaler()
X_train_scaled = scaler.fit_transform(X_train)
```

### 3. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ SAMME –∑–∞–º—ñ—Å—Ç—å SAMME.R

```python
# ‚ùå –ú–µ–Ω—à —Ç–æ—á–Ω–æ
ada = AdaBoostClassifier(algorithm='SAMME')

# ‚úÖ –¢–æ—á–Ω—ñ—à–µ (–∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º)
ada = AdaBoostClassifier(algorithm='SAMME.R')
```

### 4. –ù–µ –º–æ–Ω—ñ—Ç–æ—Ä–∏—Ç–∏ overfitting

```python
# ‚úÖ –ó–∞–≤–∂–¥–∏ –ø–µ—Ä–µ–≤—ñ—Ä—è–π staged predictions
for i, pred in enumerate(ada.staged_predict(X_test)):
    if i % 10 == 0:
        print(f"After {i+1} estimators: {accuracy_score(y_test, pred):.4f}")
```

---

## –ü–æ–≤'—è–∑–∞–Ω—ñ —Ç–µ–º–∏

- [[01_Decision_Trees]] ‚Äî weak learners
- [[03_Gradient_Boosting]] ‚Äî —Å—É—á–∞—Å–Ω—ñ—à–∞ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞
- [[05_Ensemble_Methods]] ‚Äî —Ç–µ–æ—Ä—ñ—è –∞–Ω—Å–∞–º–±–ª—ñ–≤
- [[02_Random_Forest]] ‚Äî bagging vs boosting

## –†–µ—Å—É—Ä—Å–∏

- [Scikit-learn: AdaBoost](https://scikit-learn.org/stable/modules/ensemble.html#adaboost)
- [Original Paper: Freund & Schapire (1997)](https://www.sciencedirect.com/science/article/pii/S002200009791504X)
- [StatQuest: AdaBoost](https://www.youtube.com/watch?v=LsK-xG1cLYA)

---

## –ö–ª—é—á–æ–≤—ñ –≤–∏—Å–Ω–æ–≤–∫–∏

> AdaBoost –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ –Ω–∞–≤—á–∞—î weak learners, –∞–¥–∞–ø—Ç–∏–≤–Ω–æ –∑–±—ñ–ª—å—à—É—é—á–∏ –≤–∞–≥–∏ –≤–∞–∂–∫–∏—Ö –¥–ª—è –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó –∑—Ä–∞–∑–∫—ñ–≤ —Ç–∞ –∫–æ–º–±—ñ–Ω—É—é—á–∏ –º–æ–¥–µ–ª—ñ —á–µ—Ä–µ–∑ –∑–≤–∞–∂–µ–Ω–µ –≥–æ–ª–æ—Å—É–≤–∞–Ω–Ω—è.

**–û—Å–Ω–æ–≤–Ω—ñ –ø—Ä–∏–Ω—Ü–∏–ø–∏:**
- **Adaptive re-weighting** ‚Äî —Ñ–æ–∫—É—Å –Ω–∞ –≤–∞–∂–∫–∏—Ö –∑—Ä–∞–∑–∫–∞—Ö
- **Weak learners** ‚Äî –∑–∞–∑–≤–∏—á–∞–π decision stumps (max_depth=1)
- **–ó–≤–∞–∂–µ–Ω–µ –≥–æ–ª–æ—Å—É–≤–∞–Ω–Ω—è** ‚Äî –∫—Ä–∞—â—ñ –º–æ–¥–µ–ª—ñ –º–∞—é—Ç—å –±—ñ–ª—å—à—É –≤–∞–≥—É
- **Exponential loss** ‚Äî —Ñ—É–Ω–∫—Ü—ñ—è –≤—Ç—Ä–∞—Ç

**–§–æ—Ä–º—É–ª–∞ –≤–∞–≥–∏ –º–æ–¥–µ–ª—ñ:**
$$\alpha_t = \frac{1}{2} \ln\left(\frac{1 - \epsilon_t}{\epsilon_t}\right)$$

**–ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏:**
- –ù–∞–≤—á–∞–Ω–Ω—è –∫–æ–Ω—Ü–µ–ø—Ü—ñ—ó boosting = AdaBoost ‚úì
- Production ML = Gradient Boosting (XGBoost) ‚úì
- –†–æ–±–∞—Å—Ç–Ω—ñ—Å—Ç—å –¥–æ —à—É–º—É = Random Forest ‚úì

**–ù–∞–π–≤–∞–∂–ª–∏–≤—ñ—à–µ:**
- Decision stumps —è–∫ weak learners
- SAMME.R –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º
- –û–±–µ—Ä–µ–∂–Ω–æ –∑ outliers —Ç–∞ —à—É–º–æ–º
- –î–ª—è production –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π XGBoost/LightGBM

---

#ml #supervised-learning #ensemble #adaboost #boosting #classification #weak-learners #tree-based
