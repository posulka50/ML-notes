# Feature Importance (–í–∞–∂–ª–∏–≤—ñ—Å—Ç—å –æ–∑–Ω–∞–∫)

## –©–æ —Ü–µ?

**Feature Importance** ‚Äî —Ü–µ –º–µ—Ç—Ä–∏–∫–∞, —è–∫–∞ –ø–æ–∫–∞–∑—É—î, –Ω–∞—Å–∫—ñ–ª—å–∫–∏ –∫–æ–∂–Ω–∞ –æ–∑–Ω–∞–∫–∞ –∫–æ—Ä–∏—Å–Ω–∞ –¥–ª—è –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω—å –º–æ–¥–µ–ª—ñ. –î–æ–ø–æ–º–∞–≥–∞—î –∑—Ä–æ–∑—É–º—ñ—Ç–∏, —è–∫—ñ –æ–∑–Ω–∞–∫–∏ –Ω–∞–π–±—ñ–ª—å—à–µ –≤–ø–ª–∏–≤–∞—é—Ç—å –Ω–∞ —Ä—ñ—à–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ.

**–ì–æ–ª–æ–≤–Ω–∞ —ñ–¥–µ—è:** –≤–∏–∑–Ω–∞—á–∏—Ç–∏, —è–∫—ñ –æ–∑–Ω–∞–∫–∏ –Ω–∞–π–±—ñ–ª—å—à–µ –≤–ø–ª–∏–≤–∞—é—Ç—å –Ω–∞ —è–∫—ñ—Å—Ç—å –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω—å, —â–æ–± –∫—Ä–∞—â–µ –∑—Ä–æ–∑—É–º—ñ—Ç–∏ –¥–∞–Ω—ñ —Ç–∞ –º–æ–¥–µ–ª—å.

## –ù–∞–≤—ñ—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ?

- üîç **–†–æ–∑—É–º—ñ–Ω–Ω—è –¥–∞–Ω–∏—Ö** ‚Äî —è–∫—ñ —Ñ–∞–∫—Ç–æ—Ä–∏ –Ω–∞–π–≤–∞–∂–ª–∏–≤—ñ—à—ñ?
- üéØ **Feature selection** ‚Äî –≤–∏–¥–∞–ª–∏—Ç–∏ –Ω–µ–ø–æ—Ç—Ä—ñ–±–Ω—ñ –æ–∑–Ω–∞–∫–∏
- üí° **–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü—ñ—è –º–æ–¥–µ–ª—ñ** ‚Äî –ø–æ—è—Å–Ω–∏—Ç–∏ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
- üìä **–ü–æ–∫—Ä–∞—â–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ** ‚Äî —Ñ–æ–∫—É—Å –Ω–∞ –≤–∞–∂–ª–∏–≤–∏—Ö –æ–∑–Ω–∞–∫–∞—Ö
- üîß **Debugging** ‚Äî –∑–Ω–∞–π—Ç–∏ –ø—Ä–æ–±–ª–µ–º–∏ –∑ –¥–∞–Ω–∏–º–∏
- üíº **Business insights** ‚Äî –ø—Ä–∞–∫—Ç–∏—á–Ω—ñ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó

## –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏?

**–ü–æ—Ç—Ä—ñ–±–Ω–æ:**
- –†–æ–∑—É–º—ñ—Ç–∏, **—á–æ–º—É** –º–æ–¥–µ–ª—å —Ä–æ–±–∏—Ç—å –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
- **Feature selection** –ø–µ—Ä–µ–¥ –Ω–∞–≤—á–∞–Ω–Ω—è–º
- –ë—ñ–∑–Ω–µ—Å –ø–æ—Ç—Ä–µ–±—É—î **—ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü—ñ—ó**
- –ë–∞–≥–∞—Ç–æ –æ–∑–Ω–∞–∫ (feature engineering)
- **Debugging** –º–æ–¥–µ–ª—ñ

**–ù–µ –ø–æ—Ç—Ä—ñ–±–Ω–æ:**
- –ú–æ–¥–µ–ª—å –≤–∂–µ —ñ–¥–µ–∞–ª—å–Ω–æ –ø—Ä–∞—Ü—é—î —ñ –Ω–µ –ø–æ—Ç—Ä–µ–±—É—î –ø–æ—è—Å–Ω–µ–Ω—å
- Deep Learning (—Å–∫–ª–∞–¥–Ω–æ —ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç—É–≤–∞—Ç–∏)
- –î—É–∂–µ –º–∞–ª–æ –æ–∑–Ω–∞–∫ (–≤—Å—ñ –æ—á–µ–≤–∏–¥–Ω—ñ)

---

## –¢–∏–ø–∏ Feature Importance

```
Feature Importance Methods
       |
       |--- Model-Specific
       |     |
       |     |--- Mean Decrease Impurity (MDI)
       |     |     ‚îî‚îÄ‚îÄ Decision Trees, Random Forest, Gradient Boosting
       |     |
       |     |--- Coefficients
       |     |     ‚îî‚îÄ‚îÄ Linear/Logistic Regression
       |     |
       |     |--- SHAP Values
       |           ‚îî‚îÄ‚îÄ Any model
       |
       |--- Model-Agnostic
             |
             |--- Permutation Importance
             |--- Drop-Column Importance
             |--- LIME
             |--- Partial Dependence Plots
```

---

## 1. Mean Decrease Impurity (MDI)

### –©–æ —Ü–µ?

**–î–ª—è tree-based –º–æ–¥–µ–ª–µ–π:** —Å—É–º–∞ –∑–º–µ–Ω—à–µ–Ω—å impurity (Gini/Entropy) –ø–æ –≤—Å—ñ—Ö –¥–µ—Ä–µ–≤–∞—Ö, –∑–≤–∞–∂–µ–Ω–∞ –Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∑—Ä–∞–∑–∫—ñ–≤.

### –§–æ—Ä–º—É–ª–∞

$$\text{Importance}(f) = \frac{\sum_{t \in \text{trees}} \sum_{s \in \text{splits using } f} n_s \Delta I_s}{\sum_{t \in \text{trees}} \sum_{s \in \text{all splits}} n_s \Delta I_s}$$

–¥–µ:
- $n_s$ ‚Äî –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∑—Ä–∞–∑–∫—ñ–≤ —É –≤—É–∑–ª—ñ $s$
- $\Delta I_s$ ‚Äî –∑–º–µ–Ω—à–µ–Ω–Ω—è impurity –ø—ñ—Å–ª—è —Ä–æ–∑–±–∏—Ç—Ç—è

### –í–ª–∞—Å—Ç–∏–≤–æ—Å—Ç—ñ

- ‚úÖ –®–≤–∏–¥–∫–æ –æ–±—á–∏—Å–ª—é—î—Ç—å—Å—è
- ‚úÖ –í–∂–µ —î –≤ –º–æ–¥–µ–ª—ñ (`.feature_importances_`)
- ‚ùå Bias –≤ —Å—Ç–æ—Ä–æ–Ω—É —á–∏—Å–ª–æ–≤–∏—Ö –æ–∑–Ω–∞–∫
- ‚ùå Bias –≤ —Å—Ç–æ—Ä–æ–Ω—É high-cardinality –∫–∞—Ç–µ–≥–æ—Ä—ñ–∞–ª—å–Ω–∏—Ö –æ–∑–Ω–∞–∫
- ‚ùå –ú–æ–∂–µ –ø–æ–∫–∞–∑—É–≤–∞—Ç–∏ importance –Ω–∞–≤—ñ—Ç—å –¥–ª—è irrelevant features

### –ö–æ–¥ (scikit-learn)

```python
from sklearn.ensemble import RandomForestClassifier
import numpy as np
import matplotlib.pyplot as plt

# –ú–æ–¥–µ–ª—å
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

# Feature importance (MDI)
importances = rf.feature_importances_
indices = np.argsort(importances)[::-1]

# –í–∏–≤–µ–¥–µ–Ω–Ω—è
print("=== Feature Importance (MDI) ===")
for i in range(len(importances)):
    print(f"{i+1}. {feature_names[indices[i]]}: {importances[indices[i]]:.4f}")

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
plt.figure(figsize=(10, 6))
plt.bar(range(len(importances)), importances[indices])
plt.xticks(range(len(importances)), 
           [feature_names[i] for i in indices], 
           rotation=45, ha='right')
plt.xlabel('Feature', fontsize=12)
plt.ylabel('Importance', fontsize=12)
plt.title('Feature Importances (MDI)', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.show()
```

### –ü—Ä–∏–∫–ª–∞–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É

```
=== Feature Importance (MDI) ===
1. age: 0.2845
2. income: 0.2134
3. credit_score: 0.1876
4. employment_length: 0.1245
5. debt_to_income: 0.0987
6. num_accounts: 0.0456
7. region: 0.0321
8. education: 0.0136
```

---

## 2. Permutation Importance

### –©–æ —Ü–µ?

**Model-agnostic –º–µ—Ç–æ–¥:** –ø–µ—Ä–µ–º—ñ—à—É—î–º–æ –æ–¥–Ω—É –æ–∑–Ω–∞–∫—É —ñ –¥–∏–≤–∏–º–æ—Å—å, –Ω–∞—Å–∫—ñ–ª—å–∫–∏ –ø–æ–≥—ñ—Ä—à—É—î—Ç—å—Å—è –º–æ–¥–µ–ª—å.

### –ê–ª–≥–æ—Ä–∏—Ç–º

1. –û–±—á–∏—Å–ª–∏—Ç–∏ baseline –º–µ—Ç—Ä–∏–∫—É (accuracy, R¬≤) –Ω–∞ –≤–∞–ª—ñ–¥–∞—Ü—ñ–π–Ω–∏—Ö –¥–∞–Ω–∏—Ö
2. –î–ª—è –∫–æ–∂–Ω–æ—ó –æ–∑–Ω–∞–∫–∏:
   - –ü–µ—Ä–µ–º—ñ—à–∞—Ç–∏ (shuffle) –∑–Ω–∞—á–µ–Ω–Ω—è –æ–∑–Ω–∞–∫–∏
   - –û–±—á–∏—Å–ª–∏—Ç–∏ –º–µ—Ç—Ä–∏–∫—É –Ω–∞ –ø–µ—Ä–µ–º—ñ—à–∞–Ω–∏—Ö –¥–∞–Ω–∏—Ö
   - Importance = baseline - permuted metric
3. –ü–æ–≤—Ç–æ—Ä–∏—Ç–∏ –∫—ñ–ª—å–∫–∞ —Ä–∞–∑—ñ–≤ –¥–ª—è —Å—Ç–∞–±—ñ–ª—å–Ω–æ—Å—Ç—ñ

### –§–æ—Ä–º—É–ª–∞

$$\text{Importance}(f) = \text{Score}_{\text{original}} - \text{Score}_{\text{permuted } f}$$

### –í–ª–∞—Å—Ç–∏–≤–æ—Å—Ç—ñ

- ‚úÖ Model-agnostic (–ø—Ä–∞—Ü—é—î –∑ –±—É–¥—å-—è–∫–æ—é –º–æ–¥–µ–ª–ª—é)
- ‚úÖ –ù–µ –º–∞—î bias –¥–æ —Ç–∏–ø—ñ–≤ –æ–∑–Ω–∞–∫
- ‚úÖ –í—ñ–¥–æ–±—Ä–∞–∂–∞—î —Ä–µ–∞–ª—å–Ω–∏–π –≤–ø–ª–∏–≤ –Ω–∞ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
- ‚ùå –ü–æ–≤—ñ–ª—å–Ω—ñ—à–µ –æ–±—á–∏—Å–ª—é—î—Ç—å—Å—è
- ‚ùå –ü–æ—Ç—Ä–µ–±—É—î –≤–∞–ª—ñ–¥–∞—Ü—ñ–π–Ω–∏–π set

### –ö–æ–¥

```python
from sklearn.inspection import permutation_importance

# –û–±—á–∏—Å–ª–∏—Ç–∏ permutation importance
perm_importance = permutation_importance(
    rf,              # –ú–æ–¥–µ–ª—å
    X_test,          # –í–∞–ª—ñ–¥–∞—Ü—ñ–π–Ω—ñ –¥–∞–Ω—ñ
    y_test,          # Labels
    n_repeats=10,    # –ü–æ–≤—Ç–æ—Ä–µ–Ω–Ω—è –¥–ª—è —Å—Ç–∞–±—ñ–ª—å–Ω–æ—Å—Ç—ñ
    random_state=42,
    n_jobs=-1
)

# –†–µ–∑—É–ª—å—Ç–∞—Ç–∏
print("\n=== Permutation Importance ===")
for i in perm_importance.importances_mean.argsort()[::-1]:
    print(f"{feature_names[i]}: "
          f"{perm_importance.importances_mean[i]:.4f} "
          f"+/- {perm_importance.importances_std[i]:.4f}")

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –∑ error bars
sorted_idx = perm_importance.importances_mean.argsort()[::-1]

fig, ax = plt.subplots(figsize=(10, 6))
ax.barh(range(len(sorted_idx)), 
        perm_importance.importances_mean[sorted_idx])
ax.errorbar(perm_importance.importances_mean[sorted_idx],
            range(len(sorted_idx)),
            xerr=perm_importance.importances_std[sorted_idx],
            fmt='none', ecolor='black', capsize=3)
ax.set_yticks(range(len(sorted_idx)))
ax.set_yticklabels([feature_names[i] for i in sorted_idx])
ax.set_xlabel('Permutation Importance', fontsize=12)
ax.set_title('Permutation Importance with Error Bars', 
             fontsize=14, fontweight='bold')
ax.grid(True, alpha=0.3, axis='x')
plt.tight_layout()
plt.show()
```

---

## 3. Drop-Column Importance

### –©–æ —Ü–µ?

**–ù–∞–π—ñ–Ω—Ç—É—ó—Ç–∏–≤–Ω—ñ—à–∏–π –º–µ—Ç–æ–¥:** –≤–∏–¥–∞–ª—è—î–º–æ –æ–∑–Ω–∞–∫—É —ñ –¥–∏–≤–∏–º–æ—Å—å, –Ω–∞—Å–∫—ñ–ª—å–∫–∏ –ø–æ–≥—ñ—Ä—à—É—î—Ç—å—Å—è –º–æ–¥–µ–ª—å.

### –ê–ª–≥–æ—Ä–∏—Ç–º

1. –ù–∞–≤—á–∏—Ç–∏ –º–æ–¥–µ–ª—å –Ω–∞ –≤—Å—ñ—Ö –æ–∑–Ω–∞–∫–∞—Ö ‚Üí baseline score
2. –î–ª—è –∫–æ–∂–Ω–æ—ó –æ–∑–Ω–∞–∫–∏:
   - –í–∏–¥–∞–ª–∏—Ç–∏ –æ–∑–Ω–∞–∫—É –∑ –¥–∞–Ω–∏—Ö
   - –ù–∞–≤—á–∏—Ç–∏ –º–æ–¥–µ–ª—å –±–µ–∑ —Ü—ñ—î—ó –æ–∑–Ω–∞–∫–∏
   - –û–±—á–∏—Å–ª–∏—Ç–∏ score
   - Importance = baseline score - score without feature
3. –ß–∏–º –±—ñ–ª—å—à–µ –ø–∞–¥—ñ–Ω–Ω—è ‚Üí –≤–∞–∂–ª–∏–≤—ñ—à–∞ –æ–∑–Ω–∞–∫–∞

### –í–ª–∞—Å—Ç–∏–≤–æ—Å—Ç—ñ

- ‚úÖ –î—É–∂–µ —ñ–Ω—Ç—É—ó—Ç–∏–≤–Ω–æ
- ‚úÖ –†–µ–∞–ª—å–Ω–æ –≤—ñ–¥–æ–±—Ä–∞–∂–∞—î –≤–ø–ª–∏–≤ –≤–∏–¥–∞–ª–µ–Ω–Ω—è –æ–∑–Ω–∞–∫–∏
- ‚ùå –î—É–∂–µ –ø–æ–≤—ñ–ª—å–Ω–æ (–ø–æ—Ç—Ä—ñ–±–Ω–æ $p$ —Ä–∞–∑—ñ–≤ –Ω–∞–≤—á–∞—Ç–∏ –º–æ–¥–µ–ª—å)
- ‚ùå –ù–µ –≤—Ä–∞—Ö–æ–≤—É—î –≤–∑–∞—î–º–æ–¥—ñ—ó –º—ñ–∂ –æ–∑–Ω–∞–∫–∞–º–∏

### –ö–æ–¥

```python
from sklearn.base import clone
from sklearn.metrics import accuracy_score

# Baseline score (–≤—Å—ñ –æ–∑–Ω–∞–∫–∏)
baseline_score = rf.score(X_test, y_test)

# Drop-column importance
drop_importances = {}

for i, feature in enumerate(feature_names):
    # –í–∏–¥–∞–ª–∏—Ç–∏ –æ–∑–Ω–∞–∫—É
    X_train_dropped = np.delete(X_train, i, axis=1)
    X_test_dropped = np.delete(X_test, i, axis=1)
    
    # –ù–∞–≤—á–∏—Ç–∏ –º–æ–¥–µ–ª—å –±–µ–∑ –æ–∑–Ω–∞–∫–∏
    model_dropped = clone(rf)
    model_dropped.fit(X_train_dropped, y_train)
    
    # Score –±–µ–∑ –æ–∑–Ω–∞–∫–∏
    score_dropped = model_dropped.score(X_test_dropped, y_test)
    
    # Importance = –ø–∞–¥—ñ–Ω–Ω—è score
    importance = baseline_score - score_dropped
    drop_importances[feature] = importance
    
    print(f"{feature}: {importance:.4f}")

# –°–æ—Ä—Ç—É–≤–∞–Ω–Ω—è
sorted_features = sorted(drop_importances.items(), 
                        key=lambda x: x[1], 
                        reverse=True)

print("\n=== Top 5 Most Important Features (Drop-Column) ===")
for feature, importance in sorted_features[:5]:
    print(f"{feature}: {importance:.4f}")
```

---

## 4. Coefficients (Linear Models)

### –©–æ —Ü–µ?

**–î–ª—è –ª—ñ–Ω—ñ–π–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π:** –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç–∏ –ø–æ–∫–∞–∑—É—é—Ç—å –≤–ø–ª–∏–≤ –∫–æ–∂–Ω–æ—ó –æ–∑–Ω–∞–∫–∏.

### –§–æ—Ä–º—É–ª–∞

–î–ª—è Linear Regression:
$$y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_p x_p$$

**–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü—ñ—è $\beta_j$:**
- –ó–±—ñ–ª—å—à–µ–Ω–Ω—è $x_j$ –Ω–∞ 1 –æ–¥–∏–Ω–∏—Ü—é ‚Üí –∑–º—ñ–Ω–∞ $y$ –Ω–∞ $\beta_j$

### –í–∞–∂–ª–∏–≤–æ: Normalization!

**–ü—Ä–æ–±–ª–µ–º–∞:** –Ø–∫—â–æ –æ–∑–Ω–∞–∫–∏ –≤ —Ä—ñ–∑–Ω–∏—Ö –º–∞—Å—à—Ç–∞–±–∞—Ö, –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç–∏ –Ω–µ –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—ñ.

**–†—ñ—à–µ–Ω–Ω—è:** –ù–æ—Ä–º–∞–ª—ñ–∑—É–≤–∞—Ç–∏ –¥–∞–Ω—ñ –ø–µ—Ä–µ–¥ –Ω–∞–≤—á–∞–Ω–Ω—è–º.

### –ö–æ–¥

```python
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler

# –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# –õ–æ–≥—ñ—Å—Ç–∏—á–Ω–∞ —Ä–µ–≥—Ä–µ—Å—ñ—è
lr = LogisticRegression(random_state=42, max_iter=1000)
lr.fit(X_train_scaled, y_train)

# Coefficients
coefficients = lr.coef_[0]
abs_coefficients = np.abs(coefficients)

# –°–æ—Ä—Ç—É–≤–∞–Ω–Ω—è –∑–∞ –∞–±—Å–æ–ª—é—Ç–Ω–∏–º –∑–Ω–∞—á–µ–Ω–Ω—è–º
indices = np.argsort(abs_coefficients)[::-1]

print("=== Feature Importance (Coefficients) ===")
for i in indices:
    print(f"{feature_names[i]}: {coefficients[i]:.4f}")

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
fig, ax = plt.subplots(figsize=(10, 6))
colors = ['red' if c < 0 else 'blue' for c in coefficients[indices]]
ax.barh(range(len(coefficients)), coefficients[indices], color=colors)
ax.set_yticks(range(len(coefficients)))
ax.set_yticklabels([feature_names[i] for i in indices])
ax.set_xlabel('Coefficient Value', fontsize=12)
ax.set_title('Feature Coefficients (Logistic Regression)', 
             fontsize=14, fontweight='bold')
ax.axvline(x=0, color='black', linestyle='--', linewidth=1)
ax.grid(True, alpha=0.3, axis='x')
plt.tight_layout()
plt.show()
```

---

## 5. SHAP Values

### –©–æ —Ü–µ?

**SHapley Additive exPlanations** ‚Äî –º–µ—Ç–æ–¥ –∑ —Ç–µ–æ—Ä—ñ—ó —ñ–≥–æ—Ä –¥–ª—è —Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ–≥–æ —Ä–æ–∑–ø–æ–¥—ñ–ª—É "–∑–∞—Å–ª—É–≥" –º—ñ–∂ –æ–∑–Ω–∞–∫–∞–º–∏.

### –ö–æ–Ω—Ü–µ–ø—Ü—ñ—è

–í—ñ–¥–ø–æ–≤—ñ–¥—å –Ω–∞ –ø–∏—Ç–∞–Ω–Ω—è: **"–Ø–∫–∏–π –≤–∫–ª–∞–¥ –∫–æ–∂–Ω–æ—ó –æ–∑–Ω–∞–∫–∏ —É –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∑—Ä–∞–∑–∫–∞?"**

### –í–ª–∞—Å—Ç–∏–≤–æ—Å—Ç—ñ Shapley Values

1. **Local Accuracy:** —Å—É–º–∞ SHAP values = –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è - baseline
2. **Missingness:** —è–∫—â–æ –æ–∑–Ω–∞–∫–∞ –≤—ñ–¥—Å—É—Ç–Ω—è, —ó—ó SHAP = 0
3. **Consistency:** —è–∫—â–æ –æ–∑–Ω–∞–∫–∞ —Å—Ç–∞—î –∫–æ—Ä–∏—Å–Ω—ñ—à–æ—é, SHAP –Ω–µ –∑–º–µ–Ω—à—É—î—Ç—å—Å—è

### –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó SHAP

#### Summary Plot

–ü–æ–∫–∞–∑—É—î –≤–∞–∂–ª–∏–≤—ñ—Å—Ç—å —Ç–∞ –Ω–∞–ø—Ä—è–º–æ–∫ –≤–ø–ª–∏–≤—É –∫–æ–∂–Ω–æ—ó –æ–∑–Ω–∞–∫–∏.

#### Force Plot

–ü–æ–∫–∞–∑—É—î, —è–∫ –∫–æ–∂–Ω–∞ –æ–∑–Ω–∞–∫–∞ –≤–ø–ª–∏–≤–∞—î –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–µ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è.

#### Dependence Plot

–ü–æ–∫–∞–∑—É—î –∑–∞–ª–µ–∂–Ω—ñ—Å—Ç—å –º—ñ–∂ –∑–Ω–∞—á–µ–Ω–Ω—è–º –æ–∑–Ω–∞–∫–∏ —Ç–∞ —ó—ó SHAP value.

### –ö–æ–¥

```python
import shap

# –°—Ç–≤–æ—Ä–∏—Ç–∏ SHAP explainer
explainer = shap.TreeExplainer(rf)

# –û–±—á–∏—Å–ª–∏—Ç–∏ SHAP values
shap_values = explainer.shap_values(X_test)

# –î–ª—è –±—ñ–Ω–∞—Ä–Ω–æ—ó –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó –±–µ—Ä–µ–º–æ –∫–ª–∞—Å 1
if len(shap_values) == 2:
    shap_values = shap_values[1]

# 1. Summary Plot (Feature Importance)
shap.summary_plot(shap_values, X_test, 
                  feature_names=feature_names,
                  plot_type="bar")

# 2. Summary Plot (Detailed)
shap.summary_plot(shap_values, X_test, 
                  feature_names=feature_names)

# 3. Force Plot (–¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∑—Ä–∞–∑–∫–∞)
shap.initjs()
shap.force_plot(explainer.expected_value[1], 
                shap_values[0], 
                X_test[0],
                feature_names=feature_names)

# 4. Dependence Plot (–¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ—ó –æ–∑–Ω–∞–∫–∏)
shap.dependence_plot("age", shap_values, X_test,
                     feature_names=feature_names)
```

**–ü–µ—Ä–µ–≤–∞–≥–∏ SHAP:**
- ‚úÖ –¢–µ–æ—Ä–µ—Ç–∏—á–Ω–æ –æ–±“ë—Ä—É–Ω—Ç–æ–≤–∞–Ω–æ
- ‚úÖ –ü—Ä–∞—Ü—é—î –∑ –±—É–¥—å-—è–∫–∏–º –º–æ–¥–µ–ª–ª—é
- ‚úÖ Local explanations (–¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –∑—Ä–∞–∑–∫–∞)
- ‚úÖ –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó –¥—É–∂–µ —ñ–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ñ

**–ù–µ–¥–æ–ª—ñ–∫–∏ SHAP:**
- ‚ùå –û–±—á–∏—Å–ª—é–≤–∞–ª—å–Ω–æ –¥–æ—Ä–æ–≥–æ (–æ—Å–æ–±–ª–∏–≤–æ –¥–ª—è –≤–µ–ª–∏–∫–∏—Ö –¥–∞–Ω–∏—Ö)
- ‚ùå –°–∫–ª–∞–¥–Ω—ñ—à–µ —ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç—É–≤–∞—Ç–∏

---

## 6. LIME (Local Interpretable Model-agnostic Explanations)

### –©–æ —Ü–µ?

**LIME** –ø–æ—è—Å–Ω—é—î –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ –¥–ª—è **–∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∑—Ä–∞–∑–∫–∞**, –∞–ø—Ä–æ–∫—Å–∏–º—É—é—á–∏ —Å–∫–ª–∞–¥–Ω—É –º–æ–¥–µ–ª—å –ø—Ä–æ—Å—Ç–æ—é (linear) –ª–æ–∫–∞–ª—å–Ω–æ.

### –ê–ª–≥–æ—Ä–∏—Ç–º

1. –í–∑—è—Ç–∏ –∑—Ä–∞–∑–æ–∫ –¥–ª—è –ø–æ—è—Å–Ω–µ–Ω–Ω—è
2. –ó–≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ "—Å—É—Å—ñ–¥—ñ–≤" (perturbations)
3. –û—Ç—Ä–∏–º–∞—Ç–∏ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –¥–ª—è —Å—É—Å—ñ–¥—ñ–≤
4. –ù–∞–≤—á–∏—Ç–∏ –ø—Ä–æ—Å—Ç—É –º–æ–¥–µ–ª—å (linear) –Ω–∞ —Å—É—Å—ñ–¥–∞—Ö
5. –ö–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç–∏ –ø—Ä–æ—Å—Ç–æ—ó –º–æ–¥–µ–ª—ñ = –ø–æ—è—Å–Ω–µ–Ω–Ω—è

### –ö–æ–¥

```python
import lime
import lime.lime_tabular

# –°—Ç–≤–æ—Ä–∏—Ç–∏ LIME explainer
explainer = lime.lime_tabular.LimeTabularExplainer(
    X_train,
    feature_names=feature_names,
    class_names=['Class 0', 'Class 1'],
    mode='classification'
)

# –ü–æ—è—Å–Ω–∏—Ç–∏ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –¥–ª—è –∑—Ä–∞–∑–∫–∞
i = 0  # –Ü–Ω–¥–µ–∫—Å –∑—Ä–∞–∑–∫–∞
exp = explainer.explain_instance(
    X_test[i], 
    rf.predict_proba,
    num_features=10
)

# –ü–æ–∫–∞–∑–∞—Ç–∏ –ø–æ—è—Å–Ω–µ–Ω–Ω—è
exp.show_in_notebook(show_table=True)

# –ê–±–æ —è–∫ —Ç–µ–∫—Å—Ç
print(exp.as_list())

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
fig = exp.as_pyplot_figure()
plt.tight_layout()
plt.show()
```

---

## –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –º–µ—Ç–æ–¥—ñ–≤

| –ú–µ—Ç–æ–¥ | –®–≤–∏–¥–∫—ñ—Å—Ç—å | Model-Agnostic | Global/Local | Bias | –Ü–Ω—Ç—É—ó—Ç–∏–≤–Ω—ñ—Å—Ç—å |
|-------|-----------|----------------|--------------|------|---------------|
| **MDI** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚ùå (Tree-based) | Global | ‚ö†Ô∏è –Ñ | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **Permutation** | ‚≠ê‚≠ê‚≠ê | ‚úÖ | Global | ‚ùå –ù–µ–º–∞—î | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **Drop-Column** | ‚≠ê | ‚úÖ | Global | ‚ùå –ù–µ–º–∞—î | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **Coefficients** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚ùå (Linear) | Global | ‚ùå –ù–µ–º–∞—î | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **SHAP** | ‚≠ê‚≠ê | ‚úÖ | Both | ‚ùå –ù–µ–º–∞—î | ‚≠ê‚≠ê‚≠ê |
| **LIME** | ‚≠ê‚≠ê | ‚úÖ | Local | ‚ùå –ù–µ–º–∞—î | ‚≠ê‚≠ê‚≠ê‚≠ê |

---

## Feature Selection –Ω–∞ –æ—Å–Ω–æ–≤—ñ Importance

### 1. Threshold-based Selection

```python
from sklearn.feature_selection import SelectFromModel

# –í–∏–±—Ä–∞—Ç–∏ –æ–∑–Ω–∞–∫–∏ –∑ importance > threshold
selector = SelectFromModel(rf, threshold='median')  # –∞–±–æ 'mean', 0.1, etc.
selector.fit(X_train, y_train)

# –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º—É–≤–∞—Ç–∏ –¥–∞–Ω—ñ
X_train_selected = selector.transform(X_train)
X_test_selected = selector.transform(X_test)

# –Ø–∫—ñ –æ–∑–Ω–∞–∫–∏ –≤–∏–±—Ä–∞–Ω—ñ?
selected_features = [feature_names[i] for i in selector.get_support(indices=True)]
print(f"Selected {len(selected_features)} features:")
print(selected_features)

# –ù–∞–≤—á–∏—Ç–∏ –º–æ–¥–µ–ª—å –Ω–∞ –≤–∏–±—Ä–∞–Ω–∏—Ö –æ–∑–Ω–∞–∫–∞—Ö
rf_selected = RandomForestClassifier(n_estimators=100, random_state=42)
rf_selected.fit(X_train_selected, y_train)

print(f"\nOriginal features: {rf.score(X_test, y_test):.4f}")
print(f"Selected features: {rf_selected.score(X_test_selected, y_test):.4f}")
```

### 2. Iterative Feature Selection

```python
# –ü–æ—Å–ª—ñ–¥–æ–≤–Ω–æ –≤–∏–¥–∞–ª—è—Ç–∏ –Ω–∞–π–º–µ–Ω—à –≤–∞–∂–ª–∏–≤—ñ –æ–∑–Ω–∞–∫–∏
importances = rf.feature_importances_
threshold_values = np.linspace(0, importances.max(), 20)

scores = []
n_features_list = []

for threshold in threshold_values:
    selector = SelectFromModel(rf, threshold=threshold, prefit=True)
    X_train_sel = selector.transform(X_train)
    X_test_sel = selector.transform(X_test)
    
    if X_train_sel.shape[1] == 0:
        continue
    
    rf_temp = RandomForestClassifier(n_estimators=50, random_state=42)
    rf_temp.fit(X_train_sel, y_train)
    score = rf_temp.score(X_test_sel, y_test)
    
    scores.append(score)
    n_features_list.append(X_train_sel.shape[1])

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
plt.figure(figsize=(10, 6))
plt.plot(n_features_list, scores, 'o-', linewidth=2)
plt.xlabel('Number of Features', fontsize=12)
plt.ylabel('Test Accuracy', fontsize=12)
plt.title('Accuracy vs Number of Features', fontsize=14, fontweight='bold')
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# –û–ø—Ç–∏–º–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –æ–∑–Ω–∞–∫
optimal_idx = np.argmax(scores)
print(f"Optimal number of features: {n_features_list[optimal_idx]}")
print(f"Best accuracy: {scores[optimal_idx]:.4f}")
```

---

## –ü—Ä–∞–∫—Ç–∏—á–Ω—ñ –ø–æ—Ä–∞–¥–∏ üí°

### 1. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π –∫—ñ–ª—å–∫–∞ –º–µ—Ç–æ–¥—ñ–≤

```python
# –ü–æ—Ä—ñ–≤–Ω—è–π —Ä—ñ–∑–Ω—ñ –º–µ—Ç–æ–¥–∏
print("=== MDI ===")
print_top_features(rf.feature_importances_, feature_names)

print("\n=== Permutation ===")
perm_imp = permutation_importance(rf, X_test, y_test, n_repeats=10)
print_top_features(perm_imp.importances_mean, feature_names)
```

### 2. –ù–æ—Ä–º–∞–ª—ñ–∑—É–π –¥–ª—è –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç—ñ–≤

```python
# –ó–ê–í–ñ–î–ò –Ω–æ—Ä–º–∞–ª—ñ–∑—É–π –ø–µ—Ä–µ–¥ –ª—ñ–Ω—ñ–π–Ω–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
```

### 3. –í—ñ–∑—É–∞–ª—ñ–∑—É–π –∑ error bars

```python
# –î–ª—è permutation importance
plt.errorbar(x, importances_mean, yerr=importances_std)
```

### 4. –ü–µ—Ä–µ–≤—ñ—Ä—è–π stability

```python
# –ù–∞–≤—á–∏ –º–æ–¥–µ–ª—å –∫—ñ–ª—å–∫–∞ —Ä–∞–∑—ñ–≤
importances_list = []
for i in range(10):
    rf = RandomForestClassifier(random_state=i)
    rf.fit(X_train, y_train)
    importances_list.append(rf.feature_importances_)

# –°–µ—Ä–µ–¥–Ω—î —Ç–∞ std
mean_importance = np.mean(importances_list, axis=0)
std_importance = np.std(importances_list, axis=0)
```

### 5. Domain knowledge

**–ù–µ –ø–æ–∫–ª–∞–¥–∞–π—Å—è —Ç—ñ–ª—å–∫–∏ –Ω–∞ —á–∏—Å–ª–∞!**
- –ü–µ—Ä–µ–≤—ñ—Ä, —á–∏ –º–∞—é—Ç—å —Å–µ–Ω—Å –≤–∞–∂–ª–∏–≤—ñ –æ–∑–Ω–∞–∫–∏
- –ö–æ–Ω—Å—É–ª—å—Ç—É–π—Å—è –∑ –µ–∫—Å–ø–µ—Ä—Ç–∞–º–∏
- –ü–æ–¥—É–º–∞–π –ø—Ä–æ –ø—Ä–∏—á–∏–Ω–Ω–æ-–Ω–∞—Å–ª—ñ–¥–∫–æ–≤—ñ –∑–≤'—è–∑–∫–∏

---

## –ü–æ–≤–Ω–∏–π –ø—Ä–∏–∫–ª–∞–¥: –ê–Ω–∞–ª—ñ–∑ –≤–∞–∂–ª–∏–≤–æ—Å—Ç—ñ –æ–∑–Ω–∞–∫

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.inspection import permutation_importance
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression

# –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö
data = load_breast_cancer()
X = data.data
y = data.target
feature_names = data.feature_names

# –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print("="*70)
print("FEATURE IMPORTANCE ANALYSIS")
print("="*70)
print(f"Dataset: {X.shape[0]} samples, {X.shape[1]} features")
print(f"Classes: {np.unique(y)}")

# 1. Random Forest —Å MDI
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

mdi_importances = rf.feature_importances_
mdi_indices = np.argsort(mdi_importances)[::-1]

print("\n" + "="*70)
print("1. MEAN DECREASE IMPURITY (MDI)")
print("="*70)
for i in range(5):
    idx = mdi_indices[i]
    print(f"{i+1}. {feature_names[idx]}: {mdi_importances[idx]:.4f}")

# 2. Permutation Importance
perm_importance = permutation_importance(
    rf, X_test, y_test, n_repeats=10, random_state=42
)

perm_indices = perm_importance.importances_mean.argsort()[::-1]

print("\n" + "="*70)
print("2. PERMUTATION IMPORTANCE")
print("="*70)
for i in range(5):
    idx = perm_indices[i]
    print(f"{i+1}. {feature_names[idx]}: "
          f"{perm_importance.importances_mean[idx]:.4f} "
          f"+/- {perm_importance.importances_std[idx]:.4f}")

# 3. Linear Model Coefficients
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

lr = LogisticRegression(max_iter=10000, random_state=42)
lr.fit(X_train_scaled, y_train)

coefficients = np.abs(lr.coef_[0])
coef_indices = np.argsort(coefficients)[::-1]

print("\n" + "="*70)
print("3. LOGISTIC REGRESSION COEFFICIENTS")
print("="*70)
for i in range(5):
    idx = coef_indices[i]
    print(f"{i+1}. {feature_names[idx]}: {coefficients[idx]:.4f}")

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó
fig, axes = plt.subplots(2, 2, figsize=(16, 12))

# 1. MDI
top_n = 15
axes[0, 0].barh(range(top_n), mdi_importances[mdi_indices[:top_n]][::-1])
axes[0, 0].set_yticks(range(top_n))
axes[0, 0].set_yticklabels([feature_names[i] for i in mdi_indices[:top_n]][::-1])
axes[0, 0].set_xlabel('Importance', fontsize=11)
axes[0, 0].set_title('Mean Decrease Impurity (MDI)', 
                     fontsize=13, fontweight='bold')
axes[0, 0].grid(True, alpha=0.3, axis='x')

# 2. Permutation with error bars
axes[0, 1].barh(range(top_n), 
                perm_importance.importances_mean[perm_indices[:top_n]][::-1])
axes[0, 1].errorbar(
    perm_importance.importances_mean[perm_indices[:top_n]][::-1],
    range(top_n),
    xerr=perm_importance.importances_std[perm_indices[:top_n]][::-1],
    fmt='none', ecolor='black', capsize=3
)
axes[0, 1].set_yticks(range(top_n))
axes[0, 1].set_yticklabels([feature_names[i] for i in perm_indices[:top_n]][::-1])
axes[0, 1].set_xlabel('Importance', fontsize=11)
axes[0, 1].set_title('Permutation Importance', 
                     fontsize=13, fontweight='bold')
axes[0, 1].grid(True, alpha=0.3, axis='x')

# 3. Coefficients
axes[1, 0].barh(range(top_n), coefficients[coef_indices[:top_n]][::-1])
axes[1, 0].set_yticks(range(top_n))
axes[1, 0].set_yticklabels([feature_names[i] for i in coef_indices[:top_n]][::-1])
axes[1, 0].set_xlabel('Absolute Coefficient', fontsize=11)
axes[1, 0].set_title('Logistic Regression Coefficients', 
                     fontsize=13, fontweight='bold')
axes[1, 0].grid(True, alpha=0.3, axis='x')

# 4. Comparison of methods
comparison_df = pd.DataFrame({
    'Feature': feature_names,
    'MDI': mdi_importances / mdi_importances.max(),  # Normalize
    'Permutation': perm_importance.importances_mean / 
                   perm_importance.importances_mean.max(),
    'Coefficients': coefficients / coefficients.max()
})

top_features_union = list(set(
    list(mdi_indices[:10]) + 
    list(perm_indices[:10]) + 
    list(coef_indices[:10])
))

comparison_subset = comparison_df.iloc[top_features_union]
comparison_subset = comparison_subset.set_index('Feature')

comparison_subset.plot(kind='barh', ax=axes[1, 1], width=0.8)
axes[1, 1].set_xlabel('Normalized Importance', fontsize=11)
axes[1, 1].set_title('Comparison of Methods (Top Features)', 
                     fontsize=13, fontweight='bold')
axes[1, 1].legend(fontsize=10, loc='lower right')
axes[1, 1].grid(True, alpha=0.3, axis='x')

plt.tight_layout()
plt.show()

# –ö–æ–Ω—Å–µ–Ω—Å—É—Å: —è–∫—ñ –æ–∑–Ω–∞–∫–∏ –≤–∞–∂–ª–∏–≤—ñ –∑–∞ –≤—Å—ñ–º–∞ –º–µ—Ç–æ–¥–∞–º–∏?
print("\n" + "="*70)
print("CONSENSUS: TOP FEATURES BY ALL METHODS")
print("="*70)

top_k = 10
consensus = set(mdi_indices[:top_k]) & \
            set(perm_indices[:top_k]) & \
            set(coef_indices[:top_k])

print(f"Features in top-{top_k} of all three methods:")
for idx in consensus:
    print(f"- {feature_names[idx]}")

if len(consensus) == 0:
    print("No features in top-10 of all methods!")
    print("\nFeatures in top-10 of at least 2 methods:")
    for idx in top_features_union:
        count = 0
        if idx in mdi_indices[:top_k]: count += 1
        if idx in perm_indices[:top_k]: count += 1
        if idx in coef_indices[:top_k]: count += 1
        if count >= 2:
            print(f"- {feature_names[idx]} (in {count} methods)")
```

---

## –ü–æ—à–∏—Ä–µ–Ω—ñ –ø–æ–º–∏–ª–∫–∏ ‚ùå

### 1. –ü–æ—Ä—ñ–≤–Ω—é–≤–∞—Ç–∏ –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç–∏ –±–µ–∑ –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—ó

```python
# ‚ùå –ù–ï–ü–†–ê–í–ò–õ–¨–ù–û
lr.fit(X_train, y_train)  # –ë–µ–∑ scaling
coefficients = lr.coef_

# ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_train)
lr.fit(X_scaled, y_train)
coefficients = lr.coef_
```

### 2. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ MDI –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä—ñ–∞–ª—å–Ω–∏—Ö high-cardinality

```python
# MDI –º–∞—î bias –¥–ª—è high-cardinality –æ–∑–Ω–∞–∫!
# ‚úÖ –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π Permutation Importance –∑–∞–º—ñ—Å—Ç—å
```

### 3. –î–æ–≤—ñ—Ä—è—Ç–∏ —Ç—ñ–ª—å–∫–∏ –æ–¥–Ω–æ–º—É –º–µ—Ç–æ–¥—É

```python
# ‚ùå –ù–ï –ø–æ–∫–ª–∞–¥–∞–π—Å—è —Ç—ñ–ª—å–∫–∏ –Ω–∞ MDI
# ‚úÖ –ü–æ—Ä—ñ–≤–Ω—è–π MDI, Permutation, —Ç–∞ —ñ–Ω—à—ñ –º–µ—Ç–æ–¥–∏
```

### 4. –Ü–≥–Ω–æ—Ä—É–≤–∞—Ç–∏ domain knowledge

```python
# –Ø–∫—â–æ "–≤—ñ–∫ –ø–∞—Ü—ñ—î–Ω—Ç–∞" –Ω–∞–π–≤–∞–∂–ª–∏–≤—ñ—à–∞ –æ–∑–Ω–∞–∫–∞ –¥–ª—è –¥—ñ–∞–≥–Ω–æ–∑—É —Ä–∞–∫—É,
# –∞–ª–µ domain expert –∫–∞–∂–µ, —â–æ —Ü–µ –¥–∏–≤–Ω–æ ‚Üí –ø–µ—Ä–µ–≤—ñ—Ä –¥–∞–Ω—ñ!
```

---

## –ü–æ–≤'—è–∑–∞–Ω—ñ —Ç–µ–º–∏

- [[01_Decision_Trees]] ‚Äî MDI importance
- [[02_Random_Forest]] ‚Äî feature importance
- [[03_Gradient_Boosting]] ‚Äî feature importance
- [[Feature_Selection]] ‚Äî –≤–∏–±—ñ—Ä –æ–∑–Ω–∞–∫
- [[Feature_Engineering]] ‚Äî —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –æ–∑–Ω–∞–∫

## –†–µ—Å—É—Ä—Å–∏

- [Scikit-learn: Permutation Importance](https://scikit-learn.org/stable/modules/permutation_importance.html)
- [SHAP Documentation](https://shap.readthedocs.io/)
- [LIME Documentation](https://github.com/marcotcr/lime)
- [Interpretable ML Book](https://christophm.github.io/interpretable-ml-book/)

---

## –ö–ª—é—á–æ–≤—ñ –≤–∏—Å–Ω–æ–≤–∫–∏

> Feature Importance –≤–∏–∑–Ω–∞—á–∞—î, —è–∫—ñ –æ–∑–Ω–∞–∫–∏ –Ω–∞–π–±—ñ–ª—å—à–µ –≤–ø–ª–∏–≤–∞—é—Ç—å –Ω–∞ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ, –¥–æ–ø–æ–º–∞–≥–∞—é—á–∏ –∑—Ä–æ–∑—É–º—ñ—Ç–∏ –¥–∞–Ω—ñ —Ç–∞ –ø–æ–∫—Ä–∞—â–∏—Ç–∏ –º–æ–¥–µ–ª—å.

**–û—Å–Ω–æ–≤–Ω—ñ –º–µ—Ç–æ–¥–∏:**
- **MDI** ‚Äî —à–≤–∏–¥–∫–æ, tree-based, –º–∞—î bias
- **Permutation** ‚Äî model-agnostic, –±–µ–∑ bias, –ø–æ–≤—ñ–ª—å–Ω—ñ—à–µ
- **Drop-Column** ‚Äî —ñ–Ω—Ç—É—ó—Ç–∏–≤–Ω–æ, –¥—É–∂–µ –ø–æ–≤—ñ–ª—å–Ω–æ
- **Coefficients** ‚Äî –¥–ª—è linear models, –ø–æ—Ç—Ä–µ–±—É—î scaling
- **SHAP** ‚Äî —Ç–µ–æ—Ä–µ—Ç–∏—á–Ω–æ –æ–±“ë—Ä—É–Ω—Ç–æ–≤–∞–Ω–æ, local + global
- **LIME** ‚Äî local explanations, model-agnostic

**–ü—Ä–∞–∫—Ç–∏—á–Ω—ñ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó:**
- –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π **–∫—ñ–ª—å–∫–∞ –º–µ—Ç–æ–¥—ñ–≤** –¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏
- **–ù–æ—Ä–º–∞–ª—ñ–∑—É–π** –¥–∞–Ω—ñ –¥–ª—è linear models
- **Permutation Importance** ‚Äî –Ω–∞–π–Ω–∞–¥—ñ–π–Ω—ñ—à–∏–π model-agnostic –º–µ—Ç–æ–¥
- **SHAP** ‚Äî –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª—ñ–∑—É
- **Domain knowledge** –≤–∞–∂–ª–∏–≤—ñ—à–∞ –∑–∞ —á–∏—Å–ª–∞

**–ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏:**
- Feature selection = Permutation Importance ‚úì
- Quick check = MDI (tree-based) ‚úì
- Deep analysis = SHAP ‚úì
- Local explanations = LIME ‚úì

**–ù–∞–π–≤–∞–∂–ª–∏–≤—ñ—à–µ:**
- –ù–µ –¥–æ–≤—ñ—Ä—è–π –æ–¥–Ω–æ–º—É –º–µ—Ç–æ–¥—É
- –í—ñ–∑—É–∞–ª—ñ–∑—É–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
- –ü–µ—Ä–µ–≤—ñ—Ä—è–π –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é domain knowledge
- –°—Ç–∞–±—ñ–ª—å–Ω—ñ—Å—Ç—å > –∞–±—Å–æ–ª—é—Ç–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è

---

#ml #feature-importance #interpretability #feature-selection #explainability #shap #lime #permutation #tree-based
