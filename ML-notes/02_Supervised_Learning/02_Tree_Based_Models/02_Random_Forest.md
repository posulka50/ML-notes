# Random Forest (–í–∏–ø–∞–¥–∫–æ–≤–∏–π –ª—ñ—Å)

## –©–æ —Ü–µ?

**Random Forest** ‚Äî —Ü–µ **–∞–Ω—Å–∞–º–±–ª–µ–≤–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º**, —è–∫–∏–π –±—É–¥—É—î –º–Ω–æ–∂–∏–Ω—É decision trees –Ω–∞ —Ä—ñ–∑–Ω–∏—Ö –ø—ñ–¥–º–Ω–æ–∂–∏–Ω–∞—Ö –¥–∞–Ω–∏—Ö —Ç–∞ –æ–±'—î–¥–Ω—É—î —ó—Ö –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –¥–ª—è –æ—Ç—Ä–∏–º–∞–Ω–Ω—è –±—ñ–ª—å—à —Ç–æ—á–Ω–∏—Ö —ñ —Å—Ç–∞–±—ñ–ª—å–Ω–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤.

**–ì–æ–ª–æ–≤–Ω–∞ —ñ–¥–µ—è:** "–º—É–¥—Ä—ñ—Å—Ç—å –Ω–∞—Ç–æ–≤–ø—É" ‚Äî –±–∞–≥–∞—Ç–æ –ø—Ä–æ—Å—Ç–∏—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞–∑–æ–º –ø—Ä–∞—Ü—é—é—Ç—å –∫—Ä–∞—â–µ –∑–∞ –æ–¥–Ω—É —Å–∫–ª–∞–¥–Ω—É –º–æ–¥–µ–ª—å.

## –ù–∞–≤—ñ—â–æ –ø–æ—Ç—Ä—ñ–±–µ–Ω?

- üéØ **–í–∏—Å–æ–∫–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å** ‚Äî –æ–¥–∏–Ω –∑ –Ω–∞–π–∫—Ä–∞—â–∏—Ö "out-of-the-box" –∞–ª–≥–æ—Ä–∏—Ç–º—ñ–≤
- üõ°Ô∏è **–†–æ–±–∞—Å—Ç–Ω—ñ—Å—Ç—å** ‚Äî –º–µ–Ω—à —Å—Ö–∏–ª—å–Ω–∏–π –¥–æ overfitting –Ω—ñ–∂ –æ–¥–Ω–µ –¥–µ—Ä–µ–≤–æ
- üìä **–£–Ω—ñ–≤–µ—Ä—Å–∞–ª—å–Ω—ñ—Å—Ç—å** ‚Äî —Ä–µ–≥—Ä–µ—Å—ñ—è —Ç–∞ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è
- üöÄ **–ü—Ä–æ—Å—Ç–æ—Ç–∞** ‚Äî –º–∞–ª–æ –≥—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ –¥–ª—è –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è
- üí° **Feature importance** ‚Äî –ø–æ–∫–∞–∑—É—î –≤–∞–∂–ª–∏–≤—ñ—Å—Ç—å –æ–∑–Ω–∞–∫
- ‚ö° **–ü–∞—Ä–∞–ª–µ–ª—ñ–∑–∞—Ü—ñ—è** ‚Äî –¥–µ—Ä–µ–≤–∞ –Ω–∞–≤—á–∞—é—Ç—å—Å—è –Ω–µ–∑–∞–ª–µ–∂–Ω–æ

## –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏?

**–ü–æ—Ç—Ä—ñ–±–Ω–æ:**
- –ü–æ—Ç—Ä—ñ–±–Ω–∞ **–≤–∏—Å–æ–∫–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å** –±–µ–∑ —Å–∫–ª–∞–¥–Ω–æ–≥–æ tuning
- **–¢–∞–±–ª–∏—á–Ω—ñ –¥–∞–Ω—ñ** (structured data)
- –ö–∞—Ç–µ–≥–æ—Ä—ñ–∞–ª—å–Ω—ñ + —á–∏—Å–ª–æ–≤—ñ –æ–∑–Ω–∞–∫–∏
- **Baseline** –ø–µ—Ä–µ–¥ —Å–∫–ª–∞–¥–Ω—ñ—à–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏
- **Feature selection** ‚Äî –∞–Ω–∞–ª—ñ–∑ –≤–∞–∂–ª–∏–≤–æ—Å—Ç—ñ –æ–∑–Ω–∞–∫
- –†–æ–±–∞—Å—Ç–Ω—ñ—Å—Ç—å –¥–æ —à—É–º—É —Ç–∞ –≤–∏–∫–∏–¥—ñ–≤

**–ù–µ –ø–æ—Ç—Ä—ñ–±–Ω–æ:**
- **–ü–æ—Ç—Ä—ñ–±–Ω–∞ —ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å** ‚Üí Decision Tree (–æ–¥–Ω–µ –¥–µ—Ä–µ–≤–æ)
- –î—É–∂–µ –≤–µ–ª–∏–∫—ñ –¥–∞–Ω—ñ (>1M —Ä—è–¥–∫—ñ–≤) ‚Üí Gradient Boosting (LightGBM, XGBoost)
- –ó–æ–±—Ä–∞–∂–µ–Ω–Ω—è, —Ç–µ–∫—Å—Ç ‚Üí Deep Learning
- –ü–æ—Ç—Ä—ñ–±–Ω–∞ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å –Ω–∞ —Ç–∞–±–ª–∏—á–Ω–∏—Ö –¥–∞–Ω–∏—Ö ‚Üí **Gradient Boosting**

---

## –Ø–∫ –ø—Ä–∞—Ü—é—î Random Forest?

### –û—Å–Ω–æ–≤–Ω–∞ —ñ–¥–µ—è: Bootstrap + Feature Randomness

**–î–≤–∞ —Ä—ñ–≤–Ω—ñ —Ä–∞–Ω–¥–æ–º—ñ–∑–∞—Ü—ñ—ó:**

1. **Bootstrap Aggregating (Bagging)**
   - –î–ª—è –∫–æ–∂–Ω–æ–≥–æ –¥–µ—Ä–µ–≤–∞ —Å—Ç–≤–æ—Ä—é—î–º–æ –≤–∏–ø–∞–¥–∫–æ–≤—É –ø—ñ–¥–º–Ω–æ–∂–∏–Ω—É –¥–∞–Ω–∏—Ö (–∑ –ø–æ–≤–µ—Ä–Ω–µ–Ω–Ω—è–º)
   - –ö–æ–∂–Ω–µ –¥–µ—Ä–µ–≤–æ –±–∞—á–∏—Ç—å ~63% —É–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö –∑—Ä–∞–∑–∫—ñ–≤

2. **Feature Randomness**
   - –ü—Ä–∏ –∫–æ–∂–Ω–æ–º—É —Ä–æ–∑–±–∏—Ç—Ç—ñ —Ä–æ–∑–≥–ª—è–¥–∞—î–º–æ –≤–∏–ø–∞–¥–∫–æ–≤—É –ø—ñ–¥–º–Ω–æ–∂–∏–Ω—É –æ–∑–Ω–∞–∫
   - –ó–∞–∑–≤–∏—á–∞–π $\sqrt{p}$ –¥–ª—è –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó, $p/3$ –¥–ª—è —Ä–µ–≥—Ä–µ—Å—ñ—ó

### –ê–ª–≥–æ—Ä–∏—Ç–º

```
–î–ª—è i = 1 –¥–æ n_estimators:
    1. –°—Ç–≤–æ—Ä–∏—Ç–∏ bootstrap sample (–≤–∏–ø–∞–¥–∫–æ–≤–∞ –≤–∏–±—ñ—Ä–∫–∞ –∑ –ø–æ–≤–µ—Ä–Ω–µ–Ω–Ω—è–º)
    2. –ü–æ–±—É–¥—É–≤–∞—Ç–∏ –¥–µ—Ä–µ–≤–æ:
        - –ù–∞ –∫–æ–∂–Ω–æ–º—É –≤—É–∑–ª—ñ:
            a. –í–∏–±—Ä–∞—Ç–∏ –≤–∏–ø–∞–¥–∫–æ–≤—É –ø—ñ–¥–º–Ω–æ–∂–∏–Ω—É –æ–∑–Ω–∞–∫
            b. –ó–Ω–∞–π—Ç–∏ –Ω–∞–π–∫—Ä–∞—â–µ —Ä–æ–∑–±–∏—Ç—Ç—è —Å–µ—Ä–µ–¥ —Ü–∏—Ö –æ–∑–Ω–∞–∫
            c. –†–æ–∑–¥—ñ–ª–∏—Ç–∏ –≤—É–∑–æ–ª
    3. –ó–±–µ—Ä–µ–≥—Ç–∏ –¥–µ—Ä–µ–≤–æ

–ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è:
    - –ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è: –≥–æ–ª–æ—Å—É–≤–∞–Ω–Ω—è –±—ñ–ª—å—à–æ—Å—Ç—ñ (majority vote)
    - –†–µ–≥—Ä–µ—Å—ñ—è: —É—Å–µ—Ä–µ–¥–Ω–µ–Ω–Ω—è –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω—å
```

### –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è

```
Dataset (1000 samples)
        |
        |-- Bootstrap Sample 1 (1000 samples with replacement)
        |   ‚Üí Train Tree 1 (using random features at each split)
        |
        |-- Bootstrap Sample 2 (1000 samples with replacement)
        |   ‚Üí Train Tree 2 (using random features at each split)
        |
        |-- ...
        |
        |-- Bootstrap Sample 100
            ‚Üí Train Tree 100 (using random features at each split)

Prediction for new sample X:
    Tree 1: Class A    Tree 2: Class B    ...    Tree 100: Class A
    
    Majority Vote: Class A (60 votes) > Class B (40 votes)
    ‚Üí Final Prediction: Class A
```

---

## Bootstrap Aggregating (Bagging)

### –©–æ —Ç–∞–∫–µ Bootstrap?

**–í–∏–±—ñ—Ä–∫–∞ –∑ –ø–æ–≤–µ—Ä–Ω–µ–Ω–Ω—è–º** ‚Äî –≤–∏–ø–∞–¥–∫–æ–≤–æ –æ–±–∏—Ä–∞—î–º–æ $n$ –∑—Ä–∞–∑–∫—ñ–≤ –∑ $n$ –¥–æ—Å—Ç—É–ø–Ω–∏—Ö, –∞–ª–µ **–¥–æ–∑–≤–æ–ª—è—î–º–æ –ø–æ–≤—Ç–æ—Ä–µ–Ω–Ω—è**.

**–ü—Ä–∏–∫–ª–∞–¥:**
```
Original dataset: [A, B, C, D, E]

Bootstrap sample 1: [A, B, A, C, E]  ‚Üê A –¥–≤—ñ—á—ñ, D –≤—ñ–¥—Å—É—Ç–Ω—ñ–π
Bootstrap sample 2: [D, D, B, C, A]  ‚Üê D –¥–≤—ñ—á—ñ, E –≤—ñ–¥—Å—É—Ç–Ω—ñ–π
Bootstrap sample 3: [B, E, C, A, B]  ‚Üê B –¥–≤—ñ—á—ñ, D –≤—ñ–¥—Å—É—Ç–Ω—ñ–π
```

### –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ Bootstrap

**–ô–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å, —â–æ –∑—Ä–∞–∑–æ–∫ –ù–ï –±—É–¥–µ –æ–±—Ä–∞–Ω–∏–π:**
$$P(\text{not selected}) = \left(1 - \frac{1}{n}\right)^n \approx e^{-1} \approx 0.368$$

**–†–µ–∑—É–ª—å—Ç–∞—Ç:**
- ~**63.2%** —É–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö –∑—Ä–∞–∑–∫—ñ–≤ —É bootstrap sample
- ~**36.8%** –∑—Ä–∞–∑–∫—ñ–≤ –Ω–µ —É–≤—ñ–π—à–ª–∏ (Out-Of-Bag samples)

### Out-Of-Bag (OOB) Error

**OOB –∑—Ä–∞–∑–∫–∏** ‚Äî –∑—Ä–∞–∑–∫–∏, —è–∫—ñ –ù–ï –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞–ª–∏—Å—è –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –¥–µ—Ä–µ–≤–∞.

**–í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è:**
- –ë–µ–∑–∫–æ—à—Ç–æ–≤–Ω–∞ –≤–∞–ª—ñ–¥–∞—Ü—ñ—è –±–µ–∑ –æ–∫—Ä–µ–º–æ–≥–æ test set!
- –î–ª—è –∫–æ–∂–Ω–æ–≥–æ –∑—Ä–∞–∑–∫–∞ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –¥–µ—Ä–µ–≤–∞, —è–∫—ñ –π–æ–≥–æ –Ω–µ –±–∞—á–∏–ª–∏
- OOB error ‚âà —Ç–µ—Å—Ç–æ–≤–∞ –ø–æ–º–∏–ª–∫–∞

```python
rf = RandomForestClassifier(n_estimators=100, oob_score=True)
rf.fit(X_train, y_train)

print(f"OOB Score: {rf.oob_score_:.4f}")  # –û—Ü—ñ–Ω–∫–∞ –Ω–∞ OOB –∑—Ä–∞–∑–∫–∞—Ö
```

---

## Feature Randomness

### –ù–∞–≤—ñ—â–æ?

**–ü—Ä–æ–±–ª–µ–º–∞ –∫–æ—Ä–µ–ª—å–æ–≤–∞–Ω–∏—Ö –¥–µ—Ä–µ–≤:**
- –Ø–∫—â–æ –æ–¥–Ω–∞ –æ–∑–Ω–∞–∫–∞ –¥—É–∂–µ —Å–∏–ª—å–Ω–∞ ‚Üí –≤—Å—ñ –¥–µ—Ä–µ–≤–∞ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞—é—Ç—å —ó—ó —Å–ø–æ—á–∞—Ç–∫—É
- –î–µ—Ä–µ–≤–∞ —Å—Ç–∞–Ω—É—Ç—å —Å—Ö–æ–∂–∏–º–∏ ‚Üí –º–∞–ª–æ —Ä—ñ–∑–Ω–æ–º–∞–Ω—ñ—Ç–Ω–æ—Å—Ç—ñ
- –ê–Ω—Å–∞–º–±–ª—å –ø—Ä–∞—Ü—é—î –≥—ñ—Ä—à–µ

**–†—ñ—à–µ–Ω–Ω—è: –≤–∏–ø–∞–¥–∫–æ–≤—ñ –ø—ñ–¥–º–Ω–æ–∂–∏–Ω–∏ –æ–∑–Ω–∞–∫**

### –°–∫—ñ–ª—å–∫–∏ –æ–∑–Ω–∞–∫ –≤–∏–±–∏—Ä–∞—Ç–∏?

| –ó–∞–¥–∞—á–∞ | max_features | –ü–æ—è—Å–Ω–µ–Ω–Ω—è |
|--------|--------------|-----------|
| **–ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è** | $\sqrt{p}$ | –ó–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º |
| **–†–µ–≥—Ä–µ—Å—ñ—è** | $p/3$ –∞–±–æ $p$ | –ó–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º |

–¥–µ $p$ ‚Äî –∑–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –æ–∑–Ω–∞–∫.

**–ü—Ä–∏–∫–ª–∞–¥:**
- 100 –æ–∑–Ω–∞–∫, –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è ‚Üí $\sqrt{100} = 10$ –æ–∑–Ω–∞–∫ –Ω–∞ —Ä–æ–∑–±–∏—Ç—Ç—è
- 100 –æ–∑–Ω–∞–∫, —Ä–µ–≥—Ä–µ—Å—ñ—è ‚Üí $100/3 \approx 33$ –æ–∑–Ω–∞–∫–∏ –Ω–∞ —Ä–æ–∑–±–∏—Ç—Ç—è

### –ï—Ñ–µ–∫—Ç max_features

```
max_features = 1:      –î—É–∂–µ —Ä—ñ–∑–Ω—ñ –¥–µ—Ä–µ–≤–∞, –≤–∏—Å–æ–∫–∞ variance
max_features = sqrt(p): –ë–∞–ª–∞–Ω—Å (–∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º –¥–ª—è –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó)
max_features = p:       –°—Ö–æ–∂—ñ –¥–µ—Ä–µ–≤–∞, –º–µ–Ω—à–µ —Ä—ñ–∑–Ω–æ–º–∞–Ω—ñ—Ç–Ω–æ—Å—Ç—ñ
```

---

## –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è (Aggregation)

### –ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è: Majority Voting

**–ö–æ–∂–Ω–µ –¥–µ—Ä–µ–≤–æ –≥–æ–ª–æ—Å—É—î –∑–∞ –∫–ª–∞—Å:**

```
100 –¥–µ—Ä–µ–≤ –ø–µ—Ä–µ–¥–±–∞—á–∞—é—Ç—å –¥–ª—è –∑—Ä–∞–∑–∫–∞ X:

Tree 1: Class A
Tree 2: Class A
Tree 3: Class B
...
Tree 100: Class A

Votes:
    Class A: 65 votes
    Class B: 30 votes
    Class C: 5 votes

Final Prediction: Class A (majority)
```

**–§–æ—Ä–º—É–ª–∞:**
$$\hat{y} = \text{mode}\{h_1(x), h_2(x), ..., h_T(x)\}$$

### –†–µ–≥—Ä–µ—Å—ñ—è: Averaging

**–£—Å–µ—Ä–µ–¥–Ω–µ–Ω–Ω—è –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω—å:**

```
100 –¥–µ—Ä–µ–≤ –ø–µ—Ä–µ–¥–±–∞—á—É—é—Ç—å –¥–ª—è –∑—Ä–∞–∑–∫–∞ X:

Tree 1: 50.2
Tree 2: 51.8
Tree 3: 49.5
...
Tree 100: 50.7

Average: (50.2 + 51.8 + ... + 50.7) / 100 = 50.4

Final Prediction: 50.4
```

**–§–æ—Ä–º—É–ª–∞:**
$$\hat{y} = \frac{1}{T} \sum_{t=1}^{T} h_t(x)$$

### –ô–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ (–¥–ª—è –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó)

```python
# Predict probabilities
probas = rf.predict_proba(X_test)

# –ö–æ–∂–Ω–µ –¥–µ—Ä–µ–≤–æ –¥–∞—î soft vote (–π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å)
# –§—ñ–Ω–∞–ª—å–Ω–∞ –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å = —Å–µ—Ä–µ–¥–Ω—î –ø–æ –¥–µ—Ä–µ–≤–∞—Ö
```

---

## –ö–æ–¥ (Python + scikit-learn)

### –ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import load_wine
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# 1. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö
wine = load_wine()
X = wine.data
y = wine.target

# –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# 2. –ú–æ–¥–µ–ª—å Random Forest
rf_clf = RandomForestClassifier(
    n_estimators=100,        # –ö—ñ–ª—å–∫—ñ—Å—Ç—å –¥–µ—Ä–µ–≤
    max_depth=None,          # –ù–µ–æ–±–º–µ–∂–µ–Ω–∞ –≥–ª–∏–±–∏–Ω–∞
    min_samples_split=2,     # –ú—ñ–Ω. –∑—Ä–∞–∑–∫—ñ–≤ –¥–ª—è —Ä–æ–∑–±–∏—Ç—Ç—è
    min_samples_leaf=1,      # –ú—ñ–Ω. –∑—Ä–∞–∑–∫—ñ–≤ —É –ª–∏—Å—Ç–∫—É
    max_features='sqrt',     # sqrt(n_features) –Ω–∞ —Ä–æ–∑–±–∏—Ç—Ç—è
    bootstrap=True,          # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ bootstrap
    oob_score=True,          # –û–±—á–∏—Å–ª—é–≤–∞—Ç–∏ OOB score
    n_jobs=-1,               # –ü–∞—Ä–∞–ª–µ–ª—ñ–∑–∞—Ü—ñ—è (–≤—Å—ñ —è–¥—Ä–∞)
    random_state=42
)

# 3. –ù–∞–≤—á–∞–Ω–Ω—è
rf_clf.fit(X_train, y_train)

# 4. –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
y_pred = rf_clf.predict(X_test)
y_pred_proba = rf_clf.predict_proba(X_test)

# 5. –û—Ü—ñ–Ω–∫–∞
print("=== Metrics ===")
print(f"Train Accuracy: {rf_clf.score(X_train, y_train):.4f}")
print(f"Test Accuracy: {accuracy_score(y_test, y_pred):.4f}")
print(f"OOB Score: {rf_clf.oob_score_:.4f}")

print("\nClassification Report:")
print(classification_report(y_test, y_pred, target_names=wine.target_names))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
print("\nConfusion Matrix:")
print(cm)

# 6. Feature Importance
print("\n=== Feature Importance ===")
importances = rf_clf.feature_importances_
indices = np.argsort(importances)[::-1]

for i in range(X.shape[1]):
    print(f"{i+1}. {wine.feature_names[indices[i]]}: {importances[indices[i]]:.4f}")

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è Feature Importance
plt.figure(figsize=(10, 6))
plt.bar(range(X.shape[1]), importances[indices])
plt.xticks(range(X.shape[1]), 
           [wine.feature_names[i] for i in indices], 
           rotation=45, ha='right')
plt.xlabel('Feature', fontsize=12)
plt.ylabel('Importance', fontsize=12)
plt.title('Feature Importances - Random Forest', 
          fontsize=14, fontweight='bold')
plt.tight_layout()
plt.show()
```

### –†–µ–≥—Ä–µ—Å—ñ—è

```python
from sklearn.ensemble import RandomForestRegressor
from sklearn.datasets import make_regression
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

# –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –¥–∞–Ω–∏—Ö
X, y = make_regression(
    n_samples=500,
    n_features=10,
    n_informative=7,
    noise=10,
    random_state=42
)

# –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Random Forest Regressor
rf_reg = RandomForestRegressor(
    n_estimators=100,
    max_depth=None,
    min_samples_split=2,
    min_samples_leaf=1,
    max_features=1.0,       # –í—Å—ñ –æ–∑–Ω–∞–∫–∏ (–∞–±–æ 'sqrt', 0.5)
    bootstrap=True,
    oob_score=True,
    n_jobs=-1,
    random_state=42
)

# –ù–∞–≤—á–∞–Ω–Ω—è
rf_reg.fit(X_train, y_train)

# –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
y_pred_train = rf_reg.predict(X_train)
y_pred_test = rf_reg.predict(X_test)

# –ú–µ—Ç—Ä–∏–∫–∏
print("=== Regression Metrics ===")
print(f"Train R¬≤: {r2_score(y_train, y_pred_train):.4f}")
print(f"Test R¬≤: {r2_score(y_test, y_pred_test):.4f}")
print(f"OOB Score: {rf_reg.oob_score_:.4f}")
print(f"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_test)):.2f}")
print(f"MAE: {mean_absolute_error(y_test, y_pred_test):.2f}")

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred_test, alpha=0.5, s=50)
plt.plot([y_test.min(), y_test.max()], 
         [y_test.min(), y_test.max()], 
         'r--', lw=2, label='Perfect Prediction')
plt.xlabel('True Values', fontsize=12)
plt.ylabel('Predictions', fontsize=12)
plt.title('Random Forest Regression: Predictions vs True Values', 
          fontsize=14, fontweight='bold')
plt.legend(fontsize=11)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

---

## –ì—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∏

### –û—Å–Ω–æ–≤–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏

| –ü–∞—Ä–∞–º–µ—Ç—Ä | –û–ø–∏—Å | –¢–∏–ø–æ–≤—ñ –∑–Ω–∞—á–µ–Ω–Ω—è | –ï—Ñ–µ–∫—Ç |
|----------|------|-----------------|-------|
| **n_estimators** | –ö—ñ–ª—å–∫—ñ—Å—Ç—å –¥–µ—Ä–µ–≤ | 100-500 | –ë—ñ–ª—å—à–µ ‚Üí —Ç–æ—á–Ω—ñ—à–µ, –∞–ª–µ –ø–æ–≤—ñ–ª—å–Ω—ñ—à–µ |
| **max_depth** | –ú–∞–∫—Å. –≥–ª–∏–±–∏–Ω–∞ –¥–µ—Ä–µ–≤–∞ | None (–Ω–µ–æ–±–º–µ–∂–µ–Ω–∞) | –û–±–º–µ–∂—É—î —Å–∫–ª–∞–¥–Ω—ñ—Å—Ç—å |
| **min_samples_split** | –ú—ñ–Ω. –∑—Ä–∞–∑–∫—ñ–≤ –¥–ª—è —Ä–æ–∑–±–∏—Ç—Ç—è | 2-20 | –ö–æ–Ω—Ç—Ä–æ–ª—å overfitting |
| **min_samples_leaf** | –ú—ñ–Ω. –∑—Ä–∞–∑–∫—ñ–≤ —É –ª–∏—Å—Ç–∫—É | 1-10 | –ó–≥–ª–∞–¥–∂—É–≤–∞–Ω–Ω—è |
| **max_features** | –ú–∞–∫—Å. –æ–∑–Ω–∞–∫ –Ω–∞ —Ä–æ–∑–±–∏—Ç—Ç—è | 'sqrt', 'log2', 0.5 | –†—ñ–∑–Ω–æ–º–∞–Ω—ñ—Ç–Ω—ñ—Å—Ç—å –¥–µ—Ä–µ–≤ |
| **bootstrap** | –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ bootstrap | True | Bagging |
| **oob_score** | –û–±—á–∏—Å–ª—é–≤–∞—Ç–∏ OOB error | False | –í–∞–ª—ñ–¥–∞—Ü—ñ—è |
| **n_jobs** | –ü–∞—Ä–∞–ª–µ–ª—ñ–∑–∞—Ü—ñ—è | -1 (–≤—Å—ñ —è–¥—Ä–∞) | –®–≤–∏–¥–∫—ñ—Å—Ç—å |

### –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –¥–µ—Ä–µ–≤

**Random Forest —É—Å–ø–∞–¥–∫–æ–≤—É—î –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ Decision Tree:**
- `max_leaf_nodes`
- `min_impurity_decrease`
- `criterion` ('gini', 'entropy', 'squared_error')

---

## –ü—ñ–¥–±—ñ—Ä –≥—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤

### Grid Search

```python
from sklearn.model_selection import GridSearchCV

# –°—ñ—Ç–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['sqrt', 'log2', 0.5]
}

# Grid Search
grid_search = GridSearchCV(
    RandomForestClassifier(random_state=42),
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    verbose=1
)

grid_search.fit(X_train, y_train)

# –ö—Ä–∞—â—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏
print("Best parameters:")
print(grid_search.best_params_)
print(f"\nBest CV score: {grid_search.best_score_:.4f}")

# –ö—Ä–∞—âa –º–æ–¥–µ–ª—å
best_rf = grid_search.best_estimator_
test_score = best_rf.score(X_test, y_test)
print(f"Test score: {test_score:.4f}")
```

### Randomized Search (—à–≤–∏–¥—à–µ)

```python
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint, uniform

# –†–æ–∑–ø–æ–¥—ñ–ª–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤
param_distributions = {
    'n_estimators': randint(50, 500),
    'max_depth': [None] + list(randint(5, 50).rvs(10)),
    'min_samples_split': randint(2, 20),
    'min_samples_leaf': randint(1, 10),
    'max_features': ['sqrt', 'log2'] + list(uniform(0.1, 0.9).rvs(5))
}

# Randomized Search
random_search = RandomizedSearchCV(
    RandomForestClassifier(random_state=42),
    param_distributions,
    n_iter=100,
    cv=5,
    scoring='accuracy',
    random_state=42,
    n_jobs=-1,
    verbose=1
)

random_search.fit(X_train, y_train)
print("Best parameters:", random_search.best_params_)
```

---

## Bias-Variance Tradeoff

### –Ø–∫ Random Forest –∑–º–µ–Ω—à—É—î Variance

**–û–¥–Ω–µ –¥–µ—Ä–µ–≤–æ:**
- **–í–∏—Å–æ–∫–∏–π variance** ‚Äî –º–∞–ª—ñ –∑–º—ñ–Ω–∏ –¥–∞–Ω–∏—Ö ‚Üí —Ä—ñ–∑–Ω—ñ –¥–µ—Ä–µ–≤–∞
- –°—Ö–∏–ª—å–Ω–µ –¥–æ overfitting

**Random Forest:**
- **–ó–º–µ–Ω—à—É—î variance** —á–µ—Ä–µ–∑ —É—Å–µ—Ä–µ–¥–Ω–µ–Ω–Ω—è
- –ö–æ–∂–Ω–µ –¥–µ—Ä–µ–≤–æ –º–∞—î –≤–∏—Å–æ–∫–∏–π variance, –∞–ª–µ –Ω–µ–∫–æ—Ä–µ–ª—å–æ–≤–∞–Ω—ñ –ø–æ–º–∏–ª–∫–∏
- **–£—Å–µ—Ä–µ–¥–Ω–µ–Ω–Ω—è –Ω–µ–∫–æ—Ä–µ–ª—å–æ–≤–∞–Ω–∏—Ö –ø–æ–º–∏–ª–æ–∫** ‚Üí –∑–º–µ–Ω—à–µ–Ω–Ω—è variance

### –ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞

–î–ª—è –Ω–µ–∫–æ—Ä–µ–ª—å–æ–≤–∞–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π –∑ variance $\sigma^2$:

**Variance –æ–¥–Ω–æ–≥–æ –¥–µ—Ä–µ–≤–∞:** $\sigma^2$

**Variance —É—Å–µ—Ä–µ–¥–Ω–µ–Ω–Ω—è T –¥–µ—Ä–µ–≤:**
$$\text{Var}(\text{average}) = \frac{\sigma^2}{T}$$

**–ü—Ä–∏ –∫–æ—Ä–µ–ª—å–æ–≤–∞–Ω–∏—Ö –º–æ–¥–µ–ª—è—Ö** (–∫–æ—Ä–µ–ª—è—Ü—ñ—è $\rho$):
$$\text{Var}(\text{average}) = \rho \sigma^2 + \frac{1 - \rho}{T} \sigma^2$$

**–í–∏—Å–Ω–æ–≤–æ–∫:**
- –ë—ñ–ª—å—à–µ –¥–µ—Ä–µ–≤ (T ‚Üë) ‚Üí –º–µ–Ω—à–µ variance ‚úì
- –ú–µ–Ω—à–∞ –∫–æ—Ä–µ–ª—è—Ü—ñ—è ($\rho$ ‚Üì) ‚Üí –º–µ–Ω—à–µ variance ‚úì
- **Feature randomness –∑–Ω–∏–∂—É—î $\rho$!**

---

## –í–ø–ª–∏–≤ n_estimators

### –ï–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç

```python
from sklearn.model_selection import cross_val_score

# –¢–µ—Å—Ç—É—î–º–æ —Ä—ñ–∑–Ω—É –∫—ñ–ª—å–∫—ñ—Å—Ç—å –¥–µ—Ä–µ–≤
n_estimators_range = [1, 5, 10, 20, 50, 100, 200, 500]
train_scores = []
test_scores = []
oob_scores = []

for n_est in n_estimators_range:
    rf = RandomForestClassifier(
        n_estimators=n_est,
        oob_score=True,
        random_state=42,
        n_jobs=-1
    )
    
    # Train
    rf.fit(X_train, y_train)
    train_scores.append(rf.score(X_train, y_train))
    test_scores.append(rf.score(X_test, y_test))
    oob_scores.append(rf.oob_score_)

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
plt.figure(figsize=(12, 6))
plt.plot(n_estimators_range, train_scores, 'o-', 
         linewidth=2, label='Train Score')
plt.plot(n_estimators_range, test_scores, 's-', 
         linewidth=2, label='Test Score')
plt.plot(n_estimators_range, oob_scores, '^-', 
         linewidth=2, label='OOB Score')
plt.xlabel('Number of Trees (n_estimators)', fontsize=12)
plt.ylabel('Accuracy', fontsize=12)
plt.title('Random Forest: Performance vs Number of Trees', 
          fontsize=14, fontweight='bold')
plt.legend(fontsize=11)
plt.grid(True, alpha=0.3)
plt.xscale('log')
plt.tight_layout()
plt.show()
```

**–°–ø–æ—Å—Ç–µ—Ä–µ–∂–µ–Ω–Ω—è:**
- **1 –¥–µ—Ä–µ–≤–æ:** –≤–∏—Å–æ–∫–∞ variance, –Ω–∏–∑—å–∫–∞ accuracy
- **10-50 –¥–µ—Ä–µ–≤:** —à–≤–∏–¥–∫–µ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è
- **100+ –¥–µ—Ä–µ–≤:** —Å—Ç–∞–±—ñ–ª—ñ–∑–∞—Ü—ñ—è (diminishing returns)
- **500+ –¥–µ—Ä–µ–≤:** –º–∞–π–∂–µ –Ω–µ–º–∞—î –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è, –∞–ª–µ –ø–æ–≤—ñ–ª—å–Ω—ñ—à–µ

**–í–∏—Å–Ω–æ–≤–æ–∫:** n_estimators=100-200 –∑–∞–∑–≤–∏—á–∞–π –¥–æ—Å—Ç–∞—Ç–Ω—å–æ.

---

## Feature Importance

### –î–≤–∞ —Ç–∏–ø–∏ –≤–∞–∂–ª–∏–≤–æ—Å—Ç—ñ

### 1. Mean Decrease Impurity (MDI)

**–Ø–∫ –æ–±—á–∏—Å–ª—é—î—Ç—å—Å—è:**
- –°—É–º–∞ –∑–º–µ–Ω—à–µ–Ω—å impurity (Gini/Entropy) –ø–æ –≤—Å—ñ—Ö –¥–µ—Ä–µ–≤–∞—Ö
- –ó–≤–∞–∂–µ–Ω–∞ –Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∑—Ä–∞–∑–∫—ñ–≤

**–§–æ—Ä–º—É–ª–∞:**
$$\text{Importance}(f) = \frac{1}{T} \sum_{t=1}^{T} \sum_{n \in \text{splits using } f} \frac{n_{\text{samples}}}{n_{\text{total}}} \Delta I_n$$

**–£ scikit-learn:**
```python
importances = rf.feature_importances_  # MDI –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º
```

**–ü–µ—Ä–µ–≤–∞–≥–∏:**
- ‚úÖ –®–≤–∏–¥–∫–æ –æ–±—á–∏—Å–ª—é—î—Ç—å—Å—è
- ‚úÖ –í–∂–µ —î –≤ –º–æ–¥–µ–ª—ñ

**–ù–µ–¥–æ–ª—ñ–∫–∏:**
- ‚ùå Bias –≤ —Å—Ç–æ—Ä–æ–Ω—É —á–∏—Å–ª–æ–≤–∏—Ö –æ–∑–Ω–∞–∫
- ‚ùå Bias –≤ —Å—Ç–æ—Ä–æ–Ω—É –æ–∑–Ω–∞–∫ –∑ –±–∞–≥–∞—Ç—å–º–∞ –∫–∞—Ç–µ–≥–æ—Ä—ñ—è–º–∏

### 2. Permutation Importance

**–Ø–∫ –æ–±—á–∏—Å–ª—é—î—Ç—å—Å—è:**
- –ü–µ—Ä–µ–º—ñ—à–∞—Ç–∏ –æ–¥–Ω—É –æ–∑–Ω–∞–∫—É ‚Üí –≤–∏–º—ñ—Ä—è—Ç–∏ –ø–∞–¥—ñ–Ω–Ω—è accuracy
- –ë—ñ–ª—å—à–µ –ø–∞–¥—ñ–Ω–Ω—è ‚Üí –æ–∑–Ω–∞–∫–∞ –≤–∞–∂–ª–∏–≤—ñ—à–∞

**–ö–æ–¥:**
```python
from sklearn.inspection import permutation_importance

# –û–±—á–∏—Å–ª–∏—Ç–∏ permutation importance
perm_importance = permutation_importance(
    rf_clf,
    X_test,
    y_test,
    n_repeats=10,
    random_state=42,
    n_jobs=-1
)

# –†–µ–∑—É–ª—å—Ç–∞—Ç–∏
for i in perm_importance.importances_mean.argsort()[::-1]:
    print(f"{wine.feature_names[i]}: "
          f"{perm_importance.importances_mean[i]:.4f} "
          f"+/- {perm_importance.importances_std[i]:.4f}")
```

**–ü–µ—Ä–µ–≤–∞–≥–∏:**
- ‚úÖ –ù–µ –º–∞—î bias –≤ —Å—Ç–æ—Ä–æ–Ω—É –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∏—Ö —Ç–∏–ø—ñ–≤ –æ–∑–Ω–∞–∫
- ‚úÖ –ü—Ä–∞—Ü—é—î –∑ –±—É–¥—å-—è–∫–æ—é –º–æ–¥–µ–ª–ª—é

**–ù–µ–¥–æ–ª—ñ–∫–∏:**
- ‚ùå –ü–æ–≤—ñ–ª—å–Ω—ñ—à–µ –æ–±—á–∏—Å–ª—é—î—Ç—å—Å—è

### –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è

```python
fig, axes = plt.subplots(1, 2, figsize=(16, 6))

# MDI
indices_mdi = np.argsort(rf_clf.feature_importances_)[::-1]
axes[0].bar(range(X.shape[1]), rf_clf.feature_importances_[indices_mdi])
axes[0].set_xticks(range(X.shape[1]))
axes[0].set_xticklabels([wine.feature_names[i] for i in indices_mdi], 
                        rotation=45, ha='right')
axes[0].set_ylabel('Importance', fontsize=12)
axes[0].set_title('Mean Decrease Impurity (MDI)', 
                  fontsize=14, fontweight='bold')
axes[0].grid(True, alpha=0.3)

# Permutation
indices_perm = perm_importance.importances_mean.argsort()[::-1]
axes[1].bar(range(X.shape[1]), 
            perm_importance.importances_mean[indices_perm])
axes[1].errorbar(range(X.shape[1]), 
                perm_importance.importances_mean[indices_perm],
                yerr=perm_importance.importances_std[indices_perm],
                fmt='none', ecolor='black', capsize=3)
axes[1].set_xticks(range(X.shape[1]))
axes[1].set_xticklabels([wine.feature_names[i] for i in indices_perm], 
                        rotation=45, ha='right')
axes[1].set_ylabel('Importance', fontsize=12)
axes[1].set_title('Permutation Importance', 
                  fontsize=14, fontweight='bold')
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

---

## –ü–µ—Ä–µ–≤–∞–≥–∏ —Ç–∞ –Ω–µ–¥–æ–ª—ñ–∫–∏

### –ü–µ—Ä–µ–≤–∞–≥–∏ ‚úì

| –ü–µ—Ä–µ–≤–∞–≥–∞ | –ü–æ—è—Å–Ω–µ–Ω–Ω—è |
|----------|-----------|
| **–í–∏—Å–æ–∫–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å** | –û–¥–∏–Ω –∑ –Ω–∞–π–∫—Ä–∞—â–∏—Ö "out-of-the-box" |
| **–†–æ–±–∞—Å—Ç–Ω—ñ—Å—Ç—å –¥–æ overfitting** | –£—Å–µ—Ä–µ–¥–Ω–µ–Ω–Ω—è –∑–º–µ–Ω—à—É—î variance |
| **–£–Ω—ñ–≤–µ—Ä—Å–∞–ª—å–Ω—ñ—Å—Ç—å** | –†–µ–≥—Ä–µ—Å—ñ—è + –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è |
| **–ú–∞–ª–æ tuning** | –ü—Ä–∞—Ü—é—î –¥–æ–±—Ä–µ –∑ defaults |
| **–ù–µ –ø–æ—Ç—Ä—ñ–±–Ω–∞ –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è** | –ü—Ä–∞—Ü—é—î –∑ —Ä—ñ–∑–Ω–∏–º–∏ –º–∞—Å—à—Ç–∞–±–∞–º–∏ |
| **–ö–∞—Ç–µ–≥–æ—Ä—ñ–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ** | –û–±—Ä–æ–±–ª—è—î –±–µ–∑ One-Hot |
| **–ù–µ–ª—ñ–Ω—ñ–π–Ω—ñ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ** | –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –≤–∏—è–≤–ª—è—î |
| **Feature importance** | –ê–Ω–∞–ª—ñ–∑ –≤–∞–∂–ª–∏–≤–æ—Å—Ç—ñ –æ–∑–Ω–∞–∫ |
| **OOB error** | –ë–µ–∑–∫–æ—à—Ç–æ–≤–Ω–∞ –≤–∞–ª—ñ–¥–∞—Ü—ñ—è |
| **–ü–∞—Ä–∞–ª–µ–ª—ñ–∑–∞—Ü—ñ—è** | –®–≤–∏–¥–∫–µ –Ω–∞–≤—á–∞–Ω–Ω—è (n_jobs=-1) |
| **–†–æ–±–∞—Å—Ç–Ω—ñ—Å—Ç—å –¥–æ –≤–∏–∫–∏–¥—ñ–≤** | –ú–µ–Ω—à —á—É—Ç–ª–∏–≤—ñ |
| **Missing values** | –ú–æ–∂–µ –ø—Ä–∞—Ü—é–≤–∞—Ç–∏ (–∑ –æ–±–º–µ–∂–µ–Ω–Ω—è–º–∏) |

### –ù–µ–¥–æ–ª—ñ–∫–∏ ‚úó

| –ù–µ–¥–æ–ª—ñ–∫ | –ü–æ—è—Å–Ω–µ–Ω–Ω—è |
|---------|-----------|
| **–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å** | –°–∫–ª–∞–¥–Ω–æ –ø–æ—è—Å–Ω–∏—Ç–∏ (—á–æ—Ä–Ω–∞ —Å–∫—Ä–∏–Ω—å–∫–∞) |
| **–†–æ–∑–º—ñ—Ä –º–æ–¥–µ–ª—ñ** | –ë–∞–≥–∞—Ç–æ –ø–∞–º'—è—Ç—ñ (–∑–±–µ—Ä—ñ–≥–∞—î –≤—Å—ñ –¥–µ—Ä–µ–≤–∞) |
| **–ü–æ–≤—ñ–ª—å–Ω—ñ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è** | –ü–æ—Ç—Ä—ñ–±–Ω–æ –ø—Ä–æ–π—Ç–∏ –≤—Å—ñ –¥–µ—Ä–µ–≤–∞ |
| **–ù–µ –µ–∫—Å—Ç—Ä–∞–ø–æ–ª—é—î** | –ü–æ–≥–∞–Ω–æ –∑–∞ –º–µ–∂–∞–º–∏ train –¥–∞–Ω–∏—Ö |
| **–¢–∞–±–ª–∏—á–Ω—ñ –¥–∞–Ω—ñ** | –î–ª—è –∑–æ–±—Ä–∞–∂–µ–Ω—å/—Ç–µ–∫—Å—Ç—É ‚Üí CNN/RNN |
| **–¢–æ—á–Ω—ñ—Å—Ç—å** | Gradient Boosting —á–∞—Å—Ç–æ —Ç–æ—á–Ω—ñ—à–µ |
| **–†–µ–∞–ª-—Ç–∞–π–º** | –ü–æ–≤—ñ–ª—å–Ω—ñ—à–µ –∑–∞ linear models |

---

## –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –∑ —ñ–Ω—à–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏

### Random Forest vs Decision Tree

| –ö—Ä–∏—Ç–µ—Ä—ñ–π | Decision Tree | Random Forest |
|----------|---------------|---------------|
| **–¢–æ—á–Ω—ñ—Å—Ç—å** | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **Overfitting** | –í–∏—Å–æ–∫–∏–π —Ä–∏–∑–∏–∫ | –ù–∏–∑—å–∫–∏–π —Ä–∏–∑–∏–∫ |
| **–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê |
| **–®–≤–∏–¥–∫—ñ—Å—Ç—å –Ω–∞–≤—á–∞–Ω–Ω—è** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |
| **–®–≤–∏–¥–∫—ñ—Å—Ç—å –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |
| **–°—Ç–∞–±—ñ–ª—å–Ω—ñ—Å—Ç—å** | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |

### Random Forest vs Gradient Boosting

| –ö—Ä–∏—Ç–µ—Ä—ñ–π | Random Forest | Gradient Boosting |
|----------|---------------|-------------------|
| **–¢–æ—á–Ω—ñ—Å—Ç—å** | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **Tuning** | –õ–µ–≥–∫–∏–π | –°–∫–ª–∞–¥–Ω—ñ—à–∏–π |
| **Overfitting** | –†–æ–±–∞—Å—Ç–Ω–∏–π | –ú–æ–∂–µ overfitting |
| **–®–≤–∏–¥–∫—ñ—Å—Ç—å –Ω–∞–≤—á–∞–Ω–Ω—è** | ‚≠ê‚≠ê‚≠ê‚≠ê (–ø–∞—Ä–∞–ª–µ–ª—å–Ω–æ) | ‚≠ê‚≠ê (–ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ) |
| **–ü–∞—Ä–∞–ª–µ–ª—ñ–∑–∞—Ü—ñ—è** | ‚úÖ –¢–∞–∫ | ‚ùå –°–∫–ª–∞–¥–Ω–æ |
| **–í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è** | Baseline, features | Production, Kaggle |

---

## –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ Random Forest

### –Ü–¥–µ–∞–ª—å–Ω–æ –ø—ñ–¥—Ö–æ–¥–∏—Ç—å ‚úì

- –ü–æ—Ç—Ä—ñ–±–µ–Ω **—à–≤–∏–¥–∫–∏–π baseline** –∑ —Ö–æ—Ä–æ—à–æ—é —Ç–æ—á–Ω—ñ—Å—Ç—é
- **–¢–∞–±–ª–∏—á–Ω—ñ –¥–∞–Ω—ñ** (structured data)
- –ö–∞—Ç–µ–≥–æ—Ä—ñ–∞–ª—å–Ω—ñ + —á–∏—Å–ª–æ–≤—ñ –æ–∑–Ω–∞–∫–∏
- **Feature importance** –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É
- –ù–µ–≤–µ–ª–∏–∫–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –≥—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ –¥–ª—è tuning
- **–†–æ–±–∞—Å—Ç–Ω—ñ—Å—Ç—å –¥–æ —à—É–º—É** –≤–∞–∂–ª–∏–≤–∞
- –î–æ—Å—Ç–∞—Ç–Ω—å–æ –ø–∞–º'—è—Ç—ñ —Ç–∞ –æ–±—á–∏—Å–ª—é–≤–∞–ª—å–Ω–∏—Ö —Ä–µ—Å—É—Ä—Å—ñ–≤

### –ö—Ä–∞—â–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ —ñ–Ω—à–µ ‚úó

- **–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å** ‚Üí Gradient Boosting (XGBoost, LightGBM, CatBoost)
- **–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å** ‚Üí Decision Tree (–æ–¥–Ω–µ), Logistic Regression
- **–î—É–∂–µ –≤–µ–ª–∏–∫—ñ –¥–∞–Ω—ñ** (>10M) ‚Üí Linear models, LightGBM
- **–ó–æ–±—Ä–∞–∂–µ–Ω–Ω—è/–¢–µ–∫—Å—Ç** ‚Üí Deep Learning (CNN, RNN)
- **–†–µ–∞–ª-—Ç–∞–π–º inference** ‚Üí Linear models (—à–≤–∏–¥—à—ñ)
- **–ú–∞–ª–æ –ø–∞–º'—è—Ç—ñ** ‚Üí Linear models, –æ–¥–Ω–µ –¥–µ—Ä–µ–≤–æ

---

## –ü—Ä–∞–∫—Ç–∏—á–Ω—ñ –ø–æ—Ä–∞–¥–∏ üí°

1. **–ü–æ—á–Ω–∏ –∑ defaults** ‚Äî n_estimators=100 –∑–∞–∑–≤–∏—á–∞–π –¥–æ–±—Ä–µ
2. **–ó–±—ñ–ª—å—à n_estimators** ‚Äî –¥–æ 200-500 –¥–ª—è –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è
3. **OOB score** ‚Äî –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π –¥–ª—è —à–≤–∏–¥–∫–æ—ó –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó
4. **n_jobs=-1** ‚Äî –∑–∞–≤–∂–¥–∏ –ø–∞—Ä–∞–ª–µ–ª—ñ–∑—É–π!
5. **max_features='sqrt'** ‚Äî –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è (–∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º)
6. **–ù–µ –æ–±–º–µ–∂—É–π –≥–ª–∏–±–∏–Ω—É** ‚Äî RF —Ä–æ–±–∞—Å—Ç–Ω–∏–π –¥–æ overfitting
7. **Feature importance** ‚Äî –≤–∏–¥–∞–ª—è–π –Ω–µ–ø–æ—Ç—Ä—ñ–±–Ω—ñ –æ–∑–Ω–∞–∫–∏
8. **–ü–æ—Ä—ñ–≤–Ω—è–π –∑ Gradient Boosting** ‚Äî –º–æ–∂–ª–∏–≤–æ —Ç–æ—á–Ω—ñ—à–µ
9. **class_weight='balanced'** –¥–ª—è –Ω–µ–∑–±–∞–ª–∞–Ω—Å–æ–≤–∞–Ω–∏—Ö –∫–ª–∞—Å—ñ–≤
10. **–ó–±–µ—Ä–µ–∂–∏ –º–æ–¥–µ–ª—å** ‚Äî `joblib.dump(rf, 'model.pkl')`

---

## –†–µ–∞–ª—å–Ω–∏–π –ø—Ä–∏–∫–ª–∞–¥: –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è —Ö–≤–æ—Ä–æ–±–∏ —Å–µ—Ä—Ü—è

```python
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import classification_report, roc_auc_score

# –°–∏–Ω—Ç–µ—Ç–∏—á–Ω—ñ –¥–∞–Ω—ñ (–≤ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç—ñ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–π UCI Heart Disease Dataset)
np.random.seed(42)
n_samples = 1000

data = {
    'age': np.random.randint(30, 80, n_samples),
    'sex': np.random.randint(0, 2, n_samples),
    'cp': np.random.randint(0, 4, n_samples),  # chest pain type
    'trestbps': np.random.randint(90, 200, n_samples),  # blood pressure
    'chol': np.random.randint(120, 400, n_samples),  # cholesterol
    'fbs': np.random.randint(0, 2, n_samples),  # fasting blood sugar
    'restecg': np.random.randint(0, 3, n_samples),
    'thalach': np.random.randint(70, 200, n_samples),  # max heart rate
    'exang': np.random.randint(0, 2, n_samples),
    'oldpeak': np.random.uniform(0, 6, n_samples),
}

# Target (—Å–∏–º—É–ª—é—î–º–æ –∑–∞–ª–µ–∂–Ω—ñ—Å—Ç—å)
data['target'] = (
    (data['age'] > 55).astype(int) +
    (data['chol'] > 240).astype(int) +
    (data['thalach'] < 120).astype(int) +
    np.random.randint(0, 2, n_samples)
) > 1

df = pd.DataFrame(data)

X = df.drop('target', axis=1)
y = df['target']

# –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Random Forest
rf = RandomForestClassifier(
    n_estimators=200,
    max_features='sqrt',
    oob_score=True,
    class_weight='balanced',  # –ù–µ–∑–±–∞–ª–∞–Ω—Å–æ–≤–∞–Ω—ñ –∫–ª–∞—Å–∏
    n_jobs=-1,
    random_state=42
)

# –ù–∞–≤—á–∞–Ω–Ω—è
rf.fit(X_train, y_train)

# Cross-validation
cv_scores = cross_val_score(rf, X_train, y_train, cv=5)
print(f"Cross-validation scores: {cv_scores}")
print(f"Mean CV score: {cv_scores.mean():.4f} (+/- {cv_scores.std()*2:.4f})")

# –û—Ü—ñ–Ω–∫–∞
y_pred = rf.predict(X_test)
y_pred_proba = rf.predict_proba(X_test)[:, 1]

print("\n" + "="*60)
print("=== Model Performance ===")
print("="*60)
print(f"Train Accuracy: {rf.score(X_train, y_train):.4f}")
print(f"Test Accuracy: {rf.score(X_test, y_test):.4f}")
print(f"OOB Score: {rf.oob_score_:.4f}")
print(f"ROC-AUC: {roc_auc_score(y_test, y_pred_proba):.4f}")

print("\n" + classification_report(y_test, y_pred, 
                                   target_names=['No Disease', 'Disease']))

# Feature Importance
print("\n" + "="*60)
print("=== Top 5 Most Important Features ===")
print("="*60)
importances = pd.DataFrame({
    'feature': X.columns,
    'importance': rf.feature_importances_
}).sort_values('importance', ascending=False)

for idx, row in importances.head(5).iterrows():
    print(f"{row['feature']}: {row['importance']:.4f}")

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
fig, axes = plt.subplots(1, 2, figsize=(16, 6))

# Feature Importance
axes[0].barh(importances['feature'][:10], importances['importance'][:10])
axes[0].set_xlabel('Importance', fontsize=12)
axes[0].set_title('Top 10 Feature Importances', 
                  fontsize=14, fontweight='bold')
axes[0].grid(True, alpha=0.3, axis='x')

# Number of Trees vs Performance
n_trees = list(range(10, 201, 10))
train_scores_prog = []
test_scores_prog = []

for n in n_trees:
    rf_temp = RandomForestClassifier(n_estimators=n, random_state=42, n_jobs=-1)
    rf_temp.fit(X_train, y_train)
    train_scores_prog.append(rf_temp.score(X_train, y_train))
    test_scores_prog.append(rf_temp.score(X_test, y_test))

axes[1].plot(n_trees, train_scores_prog, 'o-', label='Train', linewidth=2)
axes[1].plot(n_trees, test_scores_prog, 's-', label='Test', linewidth=2)
axes[1].set_xlabel('Number of Trees', fontsize=12)
axes[1].set_ylabel('Accuracy', fontsize=12)
axes[1].set_title('Performance vs Number of Trees', 
                  fontsize=14, fontweight='bold')
axes[1].legend(fontsize=11)
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

---

## –ü–æ—à–∏—Ä–µ–Ω—ñ –ø–æ–º–∏–ª–∫–∏ ‚ùå

### 1. –ó–∞–Ω–∞–¥—Ç–æ –º–∞–ª–æ –¥–µ—Ä–µ–≤

```python
# ‚ùå –ù–ï–ü–†–ê–í–ò–õ–¨–ù–û
rf = RandomForestClassifier(n_estimators=10)  # –ó–∞–Ω–∞–¥—Ç–æ –º–∞–ª–æ

# ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û
rf = RandomForestClassifier(n_estimators=100)  # –ú—ñ–Ω—ñ–º—É–º 100
```

### 2. –ù–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –ø–∞—Ä–∞–ª–µ–ª—ñ–∑–∞—Ü—ñ—é

```python
# ‚ùå –ü–û–í–Ü–õ–¨–ù–û
rf = RandomForestClassifier(n_jobs=1)

# ‚úÖ –®–í–ò–î–ö–û
rf = RandomForestClassifier(n_jobs=-1)  # –í—Å—ñ —è–¥—Ä–∞
```

### 3. –û–±–º–µ–∂—É–≤–∞—Ç–∏ –≥–ª–∏–±–∏–Ω—É –±–µ–∑ –ø—Ä–∏—á–∏–Ω–∏

```python
# ‚ùå –ë–ï–ó –ü–û–¢–†–ï–ë–ò
rf = RandomForestClassifier(max_depth=5)  # RF —Ä–æ–±–∞—Å—Ç–Ω–∏–π –¥–æ overfitting

# ‚úÖ –ö–†–ê–©–ï
rf = RandomForestClassifier(max_depth=None)  # –ù–µ–æ–±–º–µ–∂–µ–Ω–∞ –≥–ª–∏–±–∏–Ω–∞
```

### 4. –Ü–≥–Ω–æ—Ä—É–≤–∞—Ç–∏ OOB score

```python
# ‚ùå –í–¢–†–ê–ß–ï–ù–ê –ú–û–ñ–õ–ò–í–Ü–°–¢–¨
rf = RandomForestClassifier(oob_score=False)

# ‚úÖ –í–ò–ö–û–†–ò–°–¢–û–í–£–ô
rf = RandomForestClassifier(oob_score=True)
print(f"OOB Score: {rf.oob_score_}")  # –ë–µ–∑–∫–æ—à—Ç–æ–≤–Ω–∞ –≤–∞–ª—ñ–¥–∞—Ü—ñ—è!
```

---

## –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ç–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ

```python
import joblib

# –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è
joblib.dump(rf, 'random_forest_model.pkl')
print("Model saved!")

# –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è
rf_loaded = joblib.load('random_forest_model.pkl')
print("Model loaded!")

# –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è
predictions = rf_loaded.predict(X_new)
```

---

## –ü–æ–≤'—è–∑–∞–Ω—ñ —Ç–µ–º–∏

- [[01_Decision_Trees]] ‚Äî –±–∞–∑–æ–≤–∏–π –±–ª–æ–∫ RF
- [[03_Gradient_Boosting]] ‚Äî –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∏–π –∞–Ω—Å–∞–º–±–ª—å
- [[05_Ensemble_Methods]] ‚Äî —Ç–µ–æ—Ä—ñ—è –∞–Ω—Å–∞–º–±–ª—ñ–≤
- [[06_Feature_Importance]] ‚Äî –∞–Ω–∞–ª—ñ–∑ –æ–∑–Ω–∞–∫
- [[Cross_Validation]] ‚Äî –æ—Ü—ñ–Ω–∫–∞ –º–æ–¥–µ–ª–µ–π

## –†–µ—Å—É—Ä—Å–∏

- [Scikit-learn: Random Forest](https://scikit-learn.org/stable/modules/ensemble.html#forest)
- [Original Paper: Breiman (2001)](https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf)
- [StatQuest: Random Forest](https://www.youtube.com/watch?v=J4Wdy0Wc_xQ)

---

## –ö–ª—é—á–æ–≤—ñ –≤–∏—Å–Ω–æ–≤–∫–∏

> Random Forest ‚Äî —Ü–µ –∞–Ω—Å–∞–º–±–ª—å Decision Trees, –Ω–∞–≤—á–µ–Ω–∏—Ö –Ω–∞ —Ä—ñ–∑–Ω–∏—Ö bootstrap samples –∑ –≤–∏–ø–∞–¥–∫–æ–≤–∏–º –≤–∏–±–æ—Ä–æ–º –æ–∑–Ω–∞–∫, —â–æ –æ–±'—î–¥–Ω—É—î —ó—Ö –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è —á–µ—Ä–µ–∑ voting/averaging.

**–û—Å–Ω–æ–≤–Ω—ñ –ø—Ä–∏–Ω—Ü–∏–ø–∏:**
- **Bootstrap Aggregating (Bagging)** ‚Äî —Ä—ñ–∑–Ω—ñ –ø—ñ–¥–º–Ω–æ–∂–∏–Ω–∏ –¥–∞–Ω–∏—Ö
- **Feature Randomness** ‚Äî —Ä—ñ–∑–Ω—ñ –ø—ñ–¥–º–Ω–æ–∂–∏–Ω–∏ –æ–∑–Ω–∞–∫
- **Majority Voting** (–∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è) –∞–±–æ **Averaging** (—Ä–µ–≥—Ä–µ—Å—ñ—è)
- **OOB Error** ‚Äî –±–µ–∑–∫–æ—à—Ç–æ–≤–Ω–∞ –≤–∞–ª—ñ–¥–∞—Ü—ñ—è

**–§–æ—Ä–º—É–ª–∞ (–∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è):**
$$\hat{y} = \text{mode}\{h_1(x), h_2(x), ..., h_T(x)\}$$

**–ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏:**
- –¢–∞–±–ª–∏—á–Ω—ñ –¥–∞–Ω—ñ + —à–≤–∏–¥–∫–∏–π baseline + —Ä–æ–±–∞—Å—Ç–Ω—ñ—Å—Ç—å = Random Forest ‚úì
- –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å –Ω–∞ —Ç–∞–±–ª–∏—á–Ω–∏—Ö ‚Üí Gradient Boosting ‚úì

**–ù–∞–π–≤–∞–∂–ª–∏–≤—ñ—à–µ:**
- n_estimators=100-200, n_jobs=-1, oob_score=True
- –î–æ–±—Ä–µ –ø—Ä–∞—Ü—é—î "out-of-the-box"
- –î–ª—è production —Ä–æ–∑–≥–ª—è–Ω—å—Ç–µ Gradient Boosting

---

#ml #supervised-learning #ensemble #random-forest #bagging #classification #regression #tree-based
