# Random Forest (–í–∏–ø–∞–¥–∫–æ–≤–∏–π –ª—ñ—Å)

## –©–æ —Ü–µ?

**Random Forest** ‚Äî —Ü–µ **–∞–Ω—Å–∞–º–±–ª—å** –±–∞–≥–∞—Ç—å–æ—Ö Decision Trees, —è–∫—ñ –Ω–∞–≤—á–∞—é—Ç—å—Å—è –Ω–µ–∑–∞–ª–µ–∂–Ω–æ –Ω–∞ —Ä—ñ–∑–Ω–∏—Ö –ø—ñ–¥–º–Ω–æ–∂–∏–Ω–∞—Ö –¥–∞–Ω–∏—Ö —Ç–∞ –æ–∑–Ω–∞–∫, –∞ —ó—Ö –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –æ–±'—î–¥–Ω—É—é—Ç—å—Å—è —á–µ—Ä–µ–∑ –≥–æ–ª–æ—Å—É–≤–∞–Ω–Ω—è (–∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è) –∞–±–æ —É—Å–µ—Ä–µ–¥–Ω–µ–Ω–Ω—è (—Ä–µ–≥—Ä–µ—Å—ñ—è).

**–ì–æ–ª–æ–≤–Ω–∞ —ñ–¥–µ—è:** "–º—É–¥—Ä—ñ—Å—Ç—å –Ω–∞—Ç–æ–≤–ø—É" ‚Äî –±–∞–≥–∞—Ç–æ –Ω–µ–∑–∞–ª–µ–∂–Ω–∏—Ö –ø—Ä–æ—Å—Ç–∏—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞–∑–æ–º –¥–∞—é—Ç—å –∫—Ä–∞—â–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç, –Ω—ñ–∂ –æ–¥–Ω–∞ —Å–∫–ª–∞–¥–Ω–∞ –º–æ–¥–µ–ª—å.

## –ù–∞–≤—ñ—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–∏–π?

- üéØ **–í–∏—Å–æ–∫–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å** ‚Äî –æ–¥–∏–Ω –∑ –Ω–∞–π–∫—Ä–∞—â–∏—Ö out-of-the-box –∞–ª–≥–æ—Ä–∏—Ç–º—ñ–≤
- üõ°Ô∏è **–†–æ–±–∞—Å—Ç–Ω—ñ—Å—Ç—å** ‚Äî —Å—Ç—ñ–π–∫–∏–π –¥–æ overfitting
- ‚ö° **–£–Ω—ñ–≤–µ—Ä—Å–∞–ª—å–Ω—ñ—Å—Ç—å** ‚Äî –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è —Ç–∞ —Ä–µ–≥—Ä–µ—Å—ñ—è
- üìä **Feature importance** ‚Äî –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∏–π –≤—ñ–¥–±—ñ—Ä –≤–∞–∂–ª–∏–≤–∏—Ö –æ–∑–Ω–∞–∫
- üö´ **–ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∞ –ø—ñ–¥–≥–æ—Ç–æ–≤–∫–∞** ‚Äî –Ω–µ –ø–æ—Ç—Ä–µ–±—É—î –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—ó
- üîß **–ú–∞–ª–æ –≥—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤** ‚Äî –ª–µ–≥–∫–æ –Ω–∞–ª–∞—à—Ç—É–≤–∞—Ç–∏
- üí™ **–ü–∞—Ä–∞–ª–µ–ª—ñ–∑–∞—Ü—ñ—è** ‚Äî —à–≤–∏–¥–∫–µ –Ω–∞–≤—á–∞–Ω–Ω—è –Ω–∞ –±–∞–≥–∞—Ç—å–æ—Ö —è–¥—Ä–∞—Ö

## –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏?

**–ü–æ—Ç—Ä—ñ–±–Ω–æ:**
- –ü–æ—Ç—Ä—ñ–±–Ω–∞ **–≤–∏—Å–æ–∫–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å** –Ω–∞ —Ç–∞–±–ª–∏—á–Ω–∏—Ö –¥–∞–Ω–∏—Ö
- **Robust baseline** ‚Äî —à–≤–∏–¥–∫–∏–π —Å—Ç–∞—Ä—Ç
- –ú–∞–ª–æ —á–∞—Å—É –Ω–∞ feature engineering
- –ù–µ–ª—ñ–Ω—ñ–π–Ω—ñ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ
- –í–∞–∂–ª–∏–≤—ñ feature importance
- –°—Ç–∞–±—ñ–ª—å–Ω—ñ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è

**–ù–µ –ø–æ—Ç—Ä—ñ–±–Ω–æ:**
- **–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å –∫—Ä–∏—Ç–∏—á–Ω–∞** ‚Üí Decision Tree
- –î—É–∂–µ –≤–µ–ª–∏–∫—ñ –¥–∞–Ω—ñ ‚Üí XGBoost, LightGBM (—à–≤–∏–¥—à—ñ)
- –õ—ñ–Ω—ñ–π–Ω—ñ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ ‚Üí Linear models
- –ó–æ–±—Ä–∞–∂–µ–Ω–Ω—è, —Ç–µ–∫—Å—Ç ‚Üí Neural Networks
- –ü–æ—Ç—Ä—ñ–±–Ω–∞ –∫–∞–ª—ñ–±—Ä–æ–≤–∞–Ω–∞ –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å ‚Üí Logistic Regression

---

## –Ø–∫ –ø—Ä–∞—Ü—é—î Random Forest?

### –°—Ö–µ–º–∞ –∞–Ω—Å–∞–º–±–ª—é

```
                    [–¢—Ä–µ–Ω—É–≤–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ]
                            |
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         |                  |                   |
    Bootstrap 1        Bootstrap 2         Bootstrap N
         |                  |                   |
   [Decision Tree 1]  [Decision Tree 2]  [Decision Tree N]
         |                  |                   |
    Prediction 1       Prediction 2        Prediction N
         |                  |                   |
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚Üì
                    [Voting/Averaging]
                            ‚Üì
                  [Final Prediction]
```

### –î–≤–∞ –∫–ª—é—á–æ–≤—ñ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∏

#### 1. Bagging (Bootstrap Aggregating)

**Bootstrap sampling:** –∑ —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö (n –ø—Ä–∏–∫–ª–∞–¥—ñ–≤) —Å—Ç–≤–æ—Ä—é—î–º–æ k –ø—ñ–¥–≤–∏–±—ñ—Ä–æ–∫ —Ä–æ–∑–º—ñ—Ä–æ–º n **–∑ –ø–æ–≤–µ—Ä–Ω–µ–Ω–Ω—è–º**.

```
–û—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ (100 –ø—Ä–∏–∫–ª–∞–¥—ñ–≤):
[1, 2, 3, 4, ..., 100]

Bootstrap 1: [1, 1, 5, 7, 10, ..., 99]  ‚Üê –º–æ–∂–µ –ø–æ–≤—Ç–æ—Ä—é–≤–∞—Ç–∏
Bootstrap 2: [2, 3, 3, 15, 20, ..., 100]
Bootstrap 3: [1, 4, 8, 8, 11, ..., 98]
...

–ö–æ–∂–Ω–∞ –ø—ñ–¥–≤–∏–±—ñ—Ä–∫–∞: ~63% —É–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö –ø—Ä–∏–∫–ª–∞–¥—ñ–≤
Out-of-Bag (OOB): ~37% –Ω–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω—ñ
```

#### 2. Random Feature Selection

**–ü—Ä–∏ –∫–æ–∂–Ω–æ–º—É —Ä–æ–∑–±–∏—Ç—Ç—ñ –≤—É–∑–ª–∞:**
- –†–æ–∑–≥–ª—è–¥–∞—î–º–æ —Ç—ñ–ª—å–∫–∏ **–≤–∏–ø–∞–¥–∫–æ–≤—É –ø—ñ–¥–º–Ω–æ–∂–∏–Ω—É** –∑ $m$ –æ–∑–Ω–∞–∫
- –ó–∞–∑–≤–∏—á–∞–π: $m = \sqrt{p}$ –¥–ª—è –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó, $m = p/3$ –¥–ª—è —Ä–µ–≥—Ä–µ—Å—ñ—ó
- –¶–µ **–¥–µ–∫–æ—Ä–µ–ª—é—î** –¥–µ—Ä–µ–≤–∞ ‚Üí –±—ñ–ª—å—à–µ —Ä—ñ–∑–Ω–æ–º–∞–Ω—ñ—Ç–Ω–æ—Å—Ç—ñ

```
–£—Å—å–æ–≥–æ –æ–∑–Ω–∞–∫: 10
–ü—Ä–∏ —Ä–æ–∑–±–∏—Ç—Ç—ñ —Ä–æ–∑–≥–ª—è–¥–∞—î–º–æ: sqrt(10) ‚âà 3 –≤–∏–ø–∞–¥–∫–æ–≤—ñ –æ–∑–Ω–∞–∫–∏

–î–µ—Ä–µ–≤–æ 1, –≤—É–∑–æ–ª 1: —Ä–æ–∑–≥–ª—è–¥–∞—î –æ–∑–Ω–∞–∫–∏ [2, 5, 8]
–î–µ—Ä–µ–≤–æ 1, –≤—É–∑–æ–ª 2: —Ä–æ–∑–≥–ª—è–¥–∞—î –æ–∑–Ω–∞–∫–∏ [1, 3, 9]
–î–µ—Ä–µ–≤–æ 2, –≤—É–∑–æ–ª 1: —Ä–æ–∑–≥–ª—è–¥–∞—î –æ–∑–Ω–∞–∫–∏ [4, 6, 7]
...

–†–µ–∑—É–ª—å—Ç–∞—Ç: —Ä—ñ–∑–Ω—ñ –¥–µ—Ä–µ–≤–∞ ‚Üí —Ä—ñ–∑–Ω—ñ –ø–æ–º–∏–ª–∫–∏ ‚Üí —É—Å–µ—Ä–µ–¥–Ω–µ–Ω–Ω—è –ø–æ–∫—Ä–∞—â—É—î
```

### –û–±'—î–¥–Ω–∞–Ω–Ω—è –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω—å

**–ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è (Voting):**
```
–ü—Ä–∏–∫–ª–∞–¥: 100 –¥–µ—Ä–µ–≤ –ø–µ—Ä–µ–¥–±–∞—á–∞—é—Ç—å –∫–ª–∞—Å –¥–ª—è –Ω–æ–≤–æ–≥–æ –æ–±'—î–∫—Ç–∞

–î–µ—Ä–µ–≤–æ 1 ‚Üí –ö–ª–∞—Å A
–î–µ—Ä–µ–≤–æ 2 ‚Üí –ö–ª–∞—Å B
–î–µ—Ä–µ–≤–æ 3 ‚Üí –ö–ª–∞—Å A
...
–î–µ—Ä–µ–≤–æ 100 ‚Üí –ö–ª–∞—Å A

–†–µ–∑—É–ª—å—Ç–∞—Ç:
–ö–ª–∞—Å A: 65 –≥–æ–ª–æ—Å—ñ–≤ ‚Üí –ü–ï–†–ï–ú–û–ñ–ï–¶–¨ ‚úì
–ö–ª–∞—Å B: 35 –≥–æ–ª–æ—Å—ñ–≤
```

**–†–µ–≥—Ä–µ—Å—ñ—è (Averaging):**
```
–î–µ—Ä–µ–≤–æ 1 ‚Üí 150.2
–î–µ—Ä–µ–≤–æ 2 ‚Üí 148.5
–î–µ—Ä–µ–≤–æ 3 ‚Üí 152.1
...
–î–µ—Ä–µ–≤–æ 100 ‚Üí 149.8

–§—ñ–Ω–∞–ª—å–Ω–µ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è = mean([150.2, 148.5, ..., 149.8]) = 150.1
```

---

## –ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞

### Variance Reduction

**–ß–æ–º—É –∞–Ω—Å–∞–º–±–ª—å –∫—Ä–∞—â–µ?**

–î–ª—è N –Ω–µ–∑–∞–ª–µ–∂–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π –∑ variance œÉ¬≤:

$$\text{Var}(\text{average}) = \frac{\sigma^2}{N}$$

**–ü—Ä–∏–∫–ª–∞–¥:**
- –û–¥–Ω–µ –¥–µ—Ä–µ–≤–æ: variance = 100
- 100 –Ω–µ–∑–∞–ª–µ–∂–Ω–∏—Ö –¥–µ—Ä–µ–≤: variance = 100/100 = 1 ‚úì

**–ü—Ä–æ–±–ª–µ–º–∞:** –¥–µ—Ä–µ–≤–∞ –Ω–µ –ø–æ–≤–Ω—ñ—Å—Ç—é –Ω–µ–∑–∞–ª–µ–∂–Ω—ñ (–∫–æ—Ä–µ–ª—é—é—Ç—å).

–î–ª—è –∫–æ—Ä–µ–ª—å–æ–≤–∞–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π –∑ –∫–æ—Ä–µ–ª—è—Ü—ñ—î—é œÅ:

$$\text{Var}(\text{average}) = \rho \sigma^2 + \frac{1-\rho}{N}\sigma^2$$

**–†—ñ—à–µ–Ω–Ω—è Random Forest:**
- Random feature selection ‚Üí –∑–º–µ–Ω—à—É—î –∫–æ—Ä–µ–ª—è—Ü—ñ—é œÅ
- Bagging ‚Üí –∑–±—ñ–ª—å—à—É—î —Ä—ñ–∑–Ω–æ–º–∞–Ω—ñ—Ç–Ω—ñ—Å—Ç—å

### Out-of-Bag (OOB) Error

**OOB –¥–∞–Ω—ñ:** –ø—Ä–∏–∫–ª–∞–¥–∏, —è–∫—ñ –Ω–µ –ø–æ—Ç—Ä–∞–ø–∏–ª–∏ —É bootstrap –≤–∏–±—ñ—Ä–∫—É (~37%).

$$\text{OOB Error} = \frac{1}{n} \sum_{i=1}^{n} L(y_i, \hat{y}_i^{\text{OOB}})$$

–¥–µ $\hat{y}_i^{\text{OOB}}$ ‚Äî –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è —Ç—ñ–ª—å–∫–∏ —Ç–∏—Ö –¥–µ—Ä–µ–≤, —è–∫—ñ –Ω–µ –±–∞—á–∏–ª–∏ –ø—Ä–∏–∫–ª–∞–¥ $i$.

**–ü–µ—Ä–µ–≤–∞–≥–∞:** –±–µ–∑–∫–æ—à—Ç–æ–≤–Ω–∞ –≤–∞–ª—ñ–¥–∞—Ü—ñ—è –±–µ–∑ –æ–∫—Ä–µ–º–æ–≥–æ test set!

---

## –ü—Ä–æ—Å—Ç–∏–π –ø—Ä–∏–∫–ª–∞–¥: –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –∑–∞—Ö–≤–æ—Ä—é–≤–∞–Ω–Ω—è

### –î–∞–Ω—ñ

50 –ø–∞—Ü—ñ—î–Ω—Ç—ñ–≤:

| –í—ñ–∫ | –¢–∏—Å–∫ | –ì–ª—é–∫–æ–∑–∞ | –ö—É—Ä—ñ–Ω–Ω—è | –•–≤–æ—Ä–∏–π |
|-----|------|---------|---------|--------|
| 45  | 120  | 100     | –¢–∞–∫     | –¢–∞–∫    |
| 30  | 110  | 85      | –ù—ñ      | –ù—ñ     |
| 60  | 150  | 130     | –¢–∞–∫     | –¢–∞–∫    |
| ...

### Random Forest –∑ 3 –¥–µ—Ä–µ–≤

**–î–µ—Ä–µ–≤–æ 1** (–Ω–∞–≤—á–∞—î—Ç—å—Å—è –Ω–∞ bootstrap 1, —Ä–æ–∑–≥–ª—è–¥–∞—î –æ–∑–Ω–∞–∫–∏: –í—ñ–∫, –ì–ª—é–∫–æ–∑–∞)
```
      –í—ñ–∫ >= 50?
      /        \
    –ù—ñ          –¢–∞–∫
    /            \
–ó–¥–æ—Ä–æ–≤–∏–π      –•–≤–æ—Ä–∏–π
```

**–î–µ—Ä–µ–≤–æ 2** (–Ω–∞–≤—á–∞—î—Ç—å—Å—è –Ω–∞ bootstrap 2, —Ä–æ–∑–≥–ª—è–¥–∞—î –æ–∑–Ω–∞–∫–∏: –¢–∏—Å–∫, –ö—É—Ä—ñ–Ω–Ω—è)
```
     –¢–∏—Å–∫ >= 140?
      /         \
    –ù—ñ           –¢–∞–∫
    /              \
–ö—É—Ä—ñ–Ω–Ω—è?         –•–≤–æ—Ä–∏–π
/      \
–¢–∞–∫    –ù—ñ
/        \
–•–≤–æ—Ä–∏–π –ó–¥–æ—Ä–æ–≤–∏–π
```

**–î–µ—Ä–µ–≤–æ 3** (–Ω–∞–≤—á–∞—î—Ç—å—Å—è –Ω–∞ bootstrap 3, —Ä–æ–∑–≥–ª—è–¥–∞—î –æ–∑–Ω–∞–∫–∏: –í—ñ–∫, –¢–∏—Å–∫, –ö—É—Ä—ñ–Ω–Ω—è)
```
       –ö—É—Ä—ñ–Ω–Ω—è?
       /      \
     –¢–∞–∫      –ù—ñ
     /          \
–•–≤–æ—Ä–∏–π      –í—ñ–∫ >= 55?
            /        \
          –¢–∞–∫        –ù—ñ
          /            \
      –•–≤–æ—Ä–∏–π        –ó–¥–æ—Ä–æ–≤–∏–π
```

### –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –¥–ª—è –Ω–æ–≤–æ–≥–æ –ø–∞—Ü—ñ—î–Ω—Ç–∞

**–ü–∞—Ü—ñ—î–Ω—Ç:** –í—ñ–∫=55, –¢–∏—Å–∫=145, –ì–ª—é–∫–æ–∑–∞=115, –ö—É—Ä—ñ–Ω–Ω—è=–¢–∞–∫

- **–î–µ—Ä–µ–≤–æ 1:** –í—ñ–∫=55 >= 50 ‚Üí **–•–≤–æ—Ä–∏–π**
- **–î–µ—Ä–µ–≤–æ 2:** –¢–∏—Å–∫=145 >= 140 ‚Üí **–•–≤–æ—Ä–∏–π**
- **–î–µ—Ä–µ–≤–æ 3:** –ö—É—Ä—ñ–Ω–Ω—è=–¢–∞–∫ ‚Üí **–•–≤–æ—Ä–∏–π**

**–ì–æ–ª–æ—Å—É–≤–∞–Ω–Ω—è:** 3/3 –∑–∞ "–•–≤–æ—Ä–∏–π" ‚Üí **–§—ñ–Ω–∞–ª—å–Ω–µ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è: –•–≤–æ—Ä–∏–π** ‚ö†Ô∏è

---

## –ö–æ–¥ (Python + scikit-learn)

### –ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è

```python
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.datasets import load_breast_cancer

# 1. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö
data = load_breast_cancer()
X = data.data
y = data.target

# –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# 2. –ú–æ–¥–µ–ª—å
rf = RandomForestClassifier(
    n_estimators=100,        # –∫—ñ–ª—å–∫—ñ—Å—Ç—å –¥–µ—Ä–µ–≤
    max_depth=10,            # –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –≥–ª–∏–±–∏–Ω–∞ –∫–æ–∂–Ω–æ–≥–æ –¥–µ—Ä–µ–≤–∞
    min_samples_split=5,     # –º—ñ–Ω. –ø—Ä–∏–∫–ª–∞–¥—ñ–≤ –¥–ª—è —Ä–æ–∑–±–∏—Ç—Ç—è
    min_samples_leaf=2,      # –º—ñ–Ω. –ø—Ä–∏–∫–ª–∞–¥—ñ–≤ —É –ª–∏—Å—Ç—ñ
    max_features='sqrt',     # sqrt(p) –æ–∑–Ω–∞–∫ –ø—Ä–∏ —Ä–æ–∑–±–∏—Ç—Ç—ñ
    bootstrap=True,          # –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ bootstrap
    oob_score=True,          # –æ–±—á–∏—Å–ª—é–≤–∞—Ç–∏ OOB error
    n_jobs=-1,               # –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –≤—Å—ñ —è–¥—Ä–∞
    random_state=42
)

# 3. –ù–∞–≤—á–∞–Ω–Ω—è
rf.fit(X_train, y_train)

# 4. –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
y_pred = rf.predict(X_test)
y_pred_proba = rf.predict_proba(X_test)

# 5. –û—Ü—ñ–Ω–∫–∞
print("=== Classification Report ===")
print(classification_report(y_test, y_pred, target_names=data.target_names))

print(f"\nAccuracy: {accuracy_score(y_test, y_pred):.4f}")
print(f"OOB Score: {rf.oob_score_:.4f}")

print("\n=== Confusion Matrix ===")
print(confusion_matrix(y_test, y_pred))

# 6. Feature Importance
feature_importance = rf.feature_importances_
indices = np.argsort(feature_importance)[::-1]

print("\n=== Top 10 Important Features ===")
for i in range(10):
    print(f"{i+1}. {data.feature_names[indices[i]]}: {feature_importance[indices[i]]:.4f}")
```

### –†–µ–≥—Ä–µ—Å—ñ—è

```python
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

# –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –¥–∞–Ω–∏—Ö
from sklearn.datasets import make_regression
X, y = make_regression(
    n_samples=1000,
    n_features=20,
    n_informative=15,
    noise=10,
    random_state=42
)

# –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# –ú–æ–¥–µ–ª—å
rf_reg = RandomForestRegressor(
    n_estimators=100,
    max_depth=15,
    min_samples_split=5,
    min_samples_leaf=2,
    max_features='sqrt',
    bootstrap=True,
    oob_score=True,
    n_jobs=-1,
    random_state=42
)

# –ù–∞–≤—á–∞–Ω–Ω—è
rf_reg.fit(X_train, y_train)

# –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
y_pred = rf_reg.predict(X_test)

# –û—Ü—ñ–Ω–∫–∞
print(f"R¬≤ Score: {r2_score(y_test, y_pred):.4f}")
print(f"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.2f}")
print(f"MAE: {mean_absolute_error(y_test, y_pred):.2f}")
print(f"OOB Score: {rf_reg.oob_score_:.4f}")
```

---

## –ì—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∏

### –û—Å–Ω–æ–≤–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏

| –ü–∞—Ä–∞–º–µ—Ç—Ä | –ó–Ω–∞—á–µ–Ω–Ω—è –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º | –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó |
|----------|---------------------------|--------------|
| **n_estimators** | 100 | –ë—ñ–ª—å—à–µ = –∫—Ä–∞—â–µ (–∞–ª–µ –ø–æ–≤—ñ–ª—å–Ω—ñ—à–µ). –°–ø—Ä–æ–±—É–π 100-500 |
| **max_depth** | None | –û–±–º–µ–∂ (10-30) –¥–ª—è –∑–∞–ø–æ–±—ñ–≥–∞–Ω–Ω—è overfitting |
| **min_samples_split** | 2 | –ó–±—ñ–ª—å—à (5-10) –¥–ª—è –ø—Ä–æ—Å—Ç—ñ—à–∏—Ö –¥–µ—Ä–µ–≤ |
| **min_samples_leaf** | 1 | –ó–±—ñ–ª—å—à (2-5) –¥–ª—è –∑–≥–ª–∞–¥–∂—É–≤–∞–Ω–Ω—è |
| **max_features** | 'sqrt' (clf), 'sqrt' (reg) | 'sqrt' –¥–ª—è –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó, 'log2' –∞–±–æ p/3 –¥–ª—è —Ä–µ–≥—Ä–µ—Å—ñ—ó |
| **bootstrap** | True | –ó–∞–≤–∂–¥–∏ True –¥–ª—è Random Forest |
| **oob_score** | False | True –¥–ª—è –±–µ–∑–∫–æ—à—Ç–æ–≤–Ω–æ—ó –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó |
| **n_jobs** | None | -1 –¥–ª—è –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –≤—Å—ñ—Ö —è–¥–µ—Ä |

### –ü—ñ–¥–±—ñ—Ä –≥—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤

```python
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint, uniform

# –ü—Ä–æ—Å—Ç—ñ—Ä –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤
param_distributions = {
    'n_estimators': randint(100, 500),
    'max_depth': [10, 20, 30, 40, None],
    'min_samples_split': randint(2, 20),
    'min_samples_leaf': randint(1, 10),
    'max_features': ['sqrt', 'log2', None]
}

# Randomized Search (—à–≤–∏–¥—à–µ –∑–∞ Grid Search)
random_search = RandomizedSearchCV(
    RandomForestClassifier(random_state=42),
    param_distributions=param_distributions,
    n_iter=50,              # –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∫–æ–º–±—ñ–Ω–∞—Ü—ñ–π
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    random_state=42,
    verbose=1
)

random_search.fit(X_train, y_train)

print("Best parameters:", random_search.best_params_)
print(f"Best CV score: {random_search.best_score_:.4f}")

# –ù–∞–π–∫—Ä–∞—â–∞ –º–æ–¥–µ–ª—å
best_rf = random_search.best_estimator_
test_score = best_rf.score(X_test, y_test)
print(f"Test score: {test_score:.4f}")
```

---

## OOB Score vs Cross-Validation

### Out-of-Bag Error

```python
# OOB score (–±–µ–∑–∫–æ—à—Ç–æ–≤–Ω–æ –ø—ñ–¥ —á–∞—Å –Ω–∞–≤—á–∞–Ω–Ω—è)
rf = RandomForestClassifier(
    n_estimators=100,
    oob_score=True,
    random_state=42
)
rf.fit(X_train, y_train)

print(f"OOB Score: {rf.oob_score_:.4f}")

# OOB –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –ø—Ä–∏–∫–ª–∞–¥—É
oob_predictions = rf.oob_decision_function_
print(f"OOB predictions shape: {oob_predictions.shape}")
```

### Cross-Validation

```python
from sklearn.model_selection import cross_val_score

# Cross-validation (—Ç–æ—á–Ω—ñ—à–µ, –∞–ª–µ –ø–æ–≤—ñ–ª—å–Ω—ñ—à–µ)
rf = RandomForestClassifier(n_estimators=100, random_state=42)
cv_scores = cross_val_score(rf, X_train, y_train, cv=5, scoring='accuracy')

print(f"CV Scores: {cv_scores}")
print(f"CV Mean: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")
```

### –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è

| –ú–µ—Ç–æ–¥ | –®–≤–∏–¥–∫—ñ—Å—Ç—å | –¢–æ—á–Ω—ñ—Å—Ç—å –æ—Ü—ñ–Ω–∫–∏ | –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è |
|-------|-----------|-----------------|--------------|
| **OOB Score** | ‚úÖ –®–≤–∏–¥–∫–æ (–ø—ñ–¥ —á–∞—Å –Ω–∞–≤—á–∞–Ω–Ω—è) | ‚ö†Ô∏è –î–æ–±—Ä–∞ | –®–≤–∏–¥–∫–∞ –æ—Ü—ñ–Ω–∫–∞, –≤–µ–ª–∏–∫—ñ –¥–∞–Ω—ñ |
| **Cross-Validation** | ‚ùå –ü–æ–≤—ñ–ª—å–Ω–æ (–Ω–∞–≤—á–∞—î k —Ä–∞–∑—ñ–≤) | ‚úÖ –ù–∞–π—Ç–æ—á–Ω—ñ—à–∞ | –û—Å—Ç–∞—Ç–æ—á–Ω–∞ –æ—Ü—ñ–Ω–∫–∞, –ø—ñ–¥–±—ñ—Ä –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ |

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—è:**
- **OOB** –¥–ª—è —à–≤–∏–¥–∫–æ–≥–æ –º–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥—É —Ç–∞ –≤–µ–ª–∏–∫–∏—Ö –¥–∞–Ω–∏—Ö
- **CV** –¥–ª—è —Ñ—ñ–Ω–∞–ª—å–Ω–æ–≥–æ –ø—ñ–¥–±–æ—Ä—É –≥—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤

---

## Feature Importance

### –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫

**–í–∞–∂–ª–∏–≤—ñ—Å—Ç—å –æ–∑–Ω–∞–∫–∏** = —Å–µ—Ä–µ–¥–Ω—î –∑–º–µ–Ω—à–µ–Ω–Ω—è impurity –ø—Ä–∏ —Ä–æ–∑–±–∏—Ç—Ç—ñ –ø–æ —Ü—ñ–π –æ–∑–Ω–∞—Ü—ñ, —É—Å–µ—Ä–µ–¥–Ω–µ–Ω–µ –ø–æ –≤—Å—ñ—Ö –¥–µ—Ä–µ–≤–∞—Ö.

```python
# Feature importance
importances = rf.feature_importances_
indices = np.argsort(importances)[::-1]

# –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–µ –≤—ñ–¥—Ö–∏–ª–µ–Ω–Ω—è (–º—ñ–∂ –¥–µ—Ä–µ–≤–∞–º–∏)
std = np.std([tree.feature_importances_ for tree in rf.estimators_], axis=0)

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 6))
plt.bar(range(X.shape[1]), importances[indices], 
        yerr=std[indices], align='center')
plt.xticks(range(X.shape[1]), 
           [data.feature_names[i] for i in indices], 
           rotation=90)
plt.xlabel('Features', fontsize=12)
plt.ylabel('Importance', fontsize=12)
plt.title('Feature Importance with Error Bars', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.show()

# –í—ñ–¥–±—ñ—Ä –Ω–∞–π–≤–∞–∂–ª–∏–≤—ñ—à–∏—Ö –æ–∑–Ω–∞–∫
n_important = 10
important_features = indices[:n_important]
print(f"Selected {n_important} most important features")
print([data.feature_names[i] for i in important_features])
```

### Permutation Importance

**–ë—ñ–ª—å—à –Ω–∞–¥—ñ–π–Ω–∏–π –º–µ—Ç–æ–¥:**

```python
from sklearn.inspection import permutation_importance

# –û–±—á–∏—Å–ª–µ–Ω–Ω—è
perm_importance = permutation_importance(
    rf, X_test, y_test,
    n_repeats=10,
    random_state=42,
    n_jobs=-1
)

# –†–µ–∑—É–ª—å—Ç–∞—Ç–∏
sorted_idx = perm_importance.importances_mean.argsort()[::-1]

plt.figure(figsize=(12, 6))
plt.boxplot(perm_importance.importances[sorted_idx].T,
            labels=[data.feature_names[i] for i in sorted_idx],
            vert=False)
plt.xlabel('Permutation Importance', fontsize=12)
plt.title('Permutation Feature Importance', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.show()
```

**–ü–µ—Ä–µ–≤–∞–≥–∏ Permutation Importance:**
- ‚úÖ –ù–µ –º–∞—î bias –¥–æ –æ–∑–Ω–∞–∫ –∑ –±–∞–≥–∞—Ç—å–º–∞ –∑–Ω–∞—á–µ–Ω–Ω—è–º–∏
- ‚úÖ –ü—Ä–∞—Ü—é—î –Ω–∞ test –¥–∞–Ω–∏—Ö (–≤–∏–º—ñ—Ä—é—î —Ä–µ–∞–ª—å–Ω—É –≤–∞–∂–ª–∏–≤—ñ—Å—Ç—å)
- ‚úÖ Model-agnostic (–ø—Ä–∞—Ü—é—î –¥–ª—è –±—É–¥—å-—è–∫–æ—ó –º–æ–¥–µ–ª—ñ)

---

## –ü–µ—Ä–µ–≤–∞–≥–∏ —Ç–∞ –Ω–µ–¥–æ–ª—ñ–∫–∏

### –ü–µ—Ä–µ–≤–∞–≥–∏ ‚úì

| –ü–µ—Ä–µ–≤–∞–≥–∞ | –ü–æ—è—Å–Ω–µ–Ω–Ω—è |
|----------|-----------|
| **–í–∏—Å–æ–∫–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å** | –û–¥–∏–Ω –∑ –Ω–∞–π–∫—Ä–∞—â–∏—Ö out-of-the-box –∞–ª–≥–æ—Ä–∏—Ç–º—ñ–≤ |
| **Robust** | –°—Ç—ñ–π–∫–∏–π –¥–æ overfitting (–≤ –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—ñ –∑ –æ–¥–Ω–∏–º –¥–µ—Ä–µ–≤–æ–º) |
| **–£–Ω—ñ–≤–µ—Ä—Å–∞–ª—å–Ω—ñ—Å—Ç—å** | –ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è + —Ä–µ–≥—Ä–µ—Å—ñ—è |
| **–ù–µ –ø–æ—Ç—Ä–µ–±—É—î –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—ó** | –°—Ç—ñ–π–∫–∏–π –¥–æ –º–∞—Å—à—Ç–∞–±—É –æ–∑–Ω–∞–∫ |
| **Feature importance** | –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∏–π –≤—ñ–¥–±—ñ—Ä –≤–∞–∂–ª–∏–≤–∏—Ö –æ–∑–Ω–∞–∫ |
| **OOB validation** | –ë–µ–∑–∫–æ—à—Ç–æ–≤–Ω–∞ –æ—Ü—ñ–Ω–∫–∞ –±–µ–∑ test set |
| **–ü–∞—Ä–∞–ª–µ–ª—ñ–∑–∞—Ü—ñ—è** | –®–≤–∏–¥–∫–µ –Ω–∞–≤—á–∞–Ω–Ω—è –Ω–∞ –±–∞–≥–∞—Ç—å–æ—Ö —è–¥—Ä–∞—Ö |
| **–ú—ñ–∫—Å-–¥–∞–Ω—ñ** | –ß–∏—Å–ª–æ–≤—ñ + –∫–∞—Ç–µ–≥–æ—Ä—ñ–∞–ª—å–Ω—ñ |
| **–ú–∞–ª–æ –≥—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤** | –õ–µ–≥–∫–æ –Ω–∞–ª–∞—à—Ç—É–≤–∞—Ç–∏ |

### –ù–µ–¥–æ–ª—ñ–∫–∏ ‚úó

| –ù–µ–¥–æ–ª—ñ–∫ | –ü–æ—è—Å–Ω–µ–Ω–Ω—è |
|---------|-----------|
| **–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å** | –í–∞–∂—á–µ –ø–æ—è—Å–Ω–∏—Ç–∏ –Ω—ñ–∂ –æ–¥–Ω–µ –¥–µ—Ä–µ–≤–æ |
| **–†–æ–∑–º—ñ—Ä –º–æ–¥–µ–ª—ñ** | –ë–∞–≥–∞—Ç–æ –¥–µ—Ä–µ–≤ ‚Üí –±–∞–≥–∞—Ç–æ –ø–∞–º'—è—Ç—ñ |
| **–ü–æ–≤—ñ–ª—å–Ω–µ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è** | –ü–æ—Ç—Ä—ñ–±–Ω–æ –ø—Ä–æ–π—Ç–∏ –≤—Å—ñ –¥–µ—Ä–µ–≤–∞ |
| **–ï–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü—ñ—è** | –ù–µ –º–æ–∂–µ –ø–µ—Ä–µ–¥–±–∞—á–∞—Ç–∏ –∑–∞ –º–µ–∂–∞–º–∏ train –¥–∞–Ω–∏—Ö |
| **Bias –¥–æ –∫–∞—Ç–µ–≥–æ—Ä—ñ–∞–ª—å–Ω–∏—Ö** | –û–∑–Ω–∞–∫–∏ –∑ –±–∞–≥–∞—Ç—å–º–∞ –∑–Ω–∞—á–µ–Ω–Ω—è–º–∏ –º–∞—é—Ç—å –≤–∏—â–∏–π importance |
| **–Ü–º–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—ñ–≤** | –ü–æ—Ç—Ä–µ–±—É—î –¥–æ–¥–∞—Ç–∫–æ–≤–æ—ó –æ–±—Ä–æ–±–∫–∏ |
| **–ù–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω–∏–π –¥–ª—è –¥—É–∂–µ –≤–µ–ª–∏–∫–∏—Ö –¥–∞–Ω–∏—Ö** | XGBoost/LightGBM —à–≤–∏–¥—à—ñ |

---

## Random Forest vs Decision Tree

| –ö—Ä–∏—Ç–µ—Ä—ñ–π | Decision Tree | Random Forest |
|----------|---------------|---------------|
| **–¢–æ—á–Ω—ñ—Å—Ç—å** | ‚ö†Ô∏è –°–µ—Ä–µ–¥–Ω—è | ‚úÖ –í–∏—Å–æ–∫–∞ |
| **Overfitting** | ‚ö†Ô∏è –í–∏—Å–æ–∫–∏–π —Ä–∏–∑–∏–∫ | ‚úÖ –ù–∏–∑—å–∫–∏–π —Ä–∏–∑–∏–∫ |
| **–°—Ç–∞–±—ñ–ª—å–Ω—ñ—Å—Ç—å** | ‚ùå –ù–µ—Å—Ç–∞–±—ñ–ª—å–Ω–∏–π | ‚úÖ –°—Ç–∞–±—ñ–ª—å–Ω–∏–π |
| **–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å** | ‚úÖ –í–∏—Å–æ–∫–∞ | ‚ö†Ô∏è –°–µ—Ä–µ–¥–Ω—è |
| **–®–≤–∏–¥–∫—ñ—Å—Ç—å –Ω–∞–≤—á–∞–Ω–Ω—è** | ‚úÖ –®–≤–∏–¥–∫–æ | ‚ö†Ô∏è –ü–æ–≤—ñ–ª—å–Ω—ñ—à–µ |
| **–®–≤–∏–¥–∫—ñ—Å—Ç—å –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è** | ‚úÖ –î—É–∂–µ —à–≤–∏–¥–∫–æ | ‚ö†Ô∏è –ü–æ–≤—ñ–ª—å–Ω—ñ—à–µ |
| **–†–æ–∑–º—ñ—Ä –º–æ–¥–µ–ª—ñ** | ‚úÖ –ú–∞–ª–∏–π | ‚ùå –í–µ–ª–∏–∫–∏–π |

### –ü—Ä–∏–∫–ª–∞–¥ –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è

```python
from sklearn.tree import DecisionTreeClassifier

# –û–¥–Ω–µ –¥–µ—Ä–µ–≤–æ
tree = DecisionTreeClassifier(random_state=42)
tree.fit(X_train, y_train)
tree_acc = tree.score(X_test, y_test)

# Random Forest
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)
rf_acc = rf.score(X_test, y_test)

print(f"Decision Tree Accuracy: {tree_acc:.4f}")
print(f"Random Forest Accuracy: {rf_acc:.4f}")
print(f"Improvement: {(rf_acc - tree_acc) * 100:.2f}%")
```

**–¢–∏–ø–æ–≤—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏:**
```
Decision Tree Accuracy: 0.8800
Random Forest Accuracy: 0.9500
Improvement: 7.00%
```

---

## –ù–µ–∑–±–∞–ª–∞–Ω—Å–æ–≤–∞–Ω—ñ –∫–ª–∞—Å–∏

### –ü—Ä–æ–±–ª–µ–º–∞

```python
# –ù–µ–∑–±–∞–ª–∞–Ω—Å–æ–≤–∞–Ω—ñ –¥–∞–Ω—ñ
# –ö–ª–∞—Å 0: 9000 –ø—Ä–∏–∫–ª–∞–¥—ñ–≤
# –ö–ª–∞—Å 1: 1000 –ø—Ä–∏–∫–ª–∞–¥—ñ–≤

# Random Forest –º–æ–∂–µ —ñ–≥–Ω–æ—Ä—É–≤–∞—Ç–∏ minority class
```

### –†—ñ—à–µ–Ω–Ω—è

#### 1. Class Weights

```python
# –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∏–π –ø—ñ–¥–±—ñ—Ä –≤–∞–≥
rf = RandomForestClassifier(
    n_estimators=100,
    class_weight='balanced',  # –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ
    random_state=42
)

# –ê–±–æ –≤—Ä—É—á–Ω—É
rf = RandomForestClassifier(
    n_estimators=100,
    class_weight={0: 1, 1: 9},  # –ö–ª–∞—Å 1 –≤ 9 —Ä–∞–∑—ñ–≤ –≤–∞–∂–ª–∏–≤—ñ—à–∏–π
    random_state=42
)

rf.fit(X_train, y_train)
```

#### 2. Balanced Random Forest

```python
from imblearn.ensemble import BalancedRandomForestClassifier

# –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –±–∞–ª–∞–Ω—Å—É—î –∫–æ–∂–µ–Ω bootstrap
brf = BalancedRandomForestClassifier(
    n_estimators=100,
    sampling_strategy='auto',  # –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–µ –±–∞–ª–∞–Ω—Å—É–≤–∞–Ω–Ω—è
    replacement=True,
    random_state=42
)

brf.fit(X_train, y_train)
```

#### 3. Resampling –ø–µ—Ä–µ–¥ –Ω–∞–≤—á–∞–Ω–Ω—è–º

```python
from imblearn.over_sampling import SMOTE

# SMOTE –¥–ª—è oversampling
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

# –ù–∞–≤—á–∞–Ω–Ω—è –Ω–∞ –∑–±–∞–ª–∞–Ω—Å–æ–≤–∞–Ω–∏—Ö –¥–∞–Ω–∏—Ö
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_resampled, y_resampled)
```

---

## –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –æ–∫—Ä–µ–º–æ–≥–æ –¥–µ—Ä–µ–≤–∞

```python
from sklearn.tree import plot_tree

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –æ–¥–Ω–æ–≥–æ –¥–µ—Ä–µ–≤–∞ –∑ –ª—ñ—Å—É
estimator = rf.estimators_[0]  # –ü–µ—Ä—à–µ –¥–µ—Ä–µ–≤–æ

plt.figure(figsize=(20, 10))
plot_tree(
    estimator,
    feature_names=data.feature_names,
    class_names=data.target_names,
    filled=True,
    rounded=True,
    fontsize=8
)
plt.title('First Decision Tree from Random Forest', 
          fontsize=16, fontweight='bold')
plt.tight_layout()
plt.show()
```

---

## –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –∑ —ñ–Ω—à–∏–º–∏ –∞–Ω—Å–∞–º–±–ª—è–º–∏

| –ú–µ—Ç–æ–¥ | –ü—ñ–¥—Ö—ñ–¥ | –ü–µ—Ä–µ–≤–∞–≥–∏ | –ù–µ–¥–æ–ª—ñ–∫–∏ |
|-------|--------|----------|----------|
| **Random Forest** | –ü–∞—Ä–∞–ª–µ–ª—å–Ω—ñ –Ω–µ–∑–∞–ª–µ–∂–Ω—ñ –¥–µ—Ä–µ–≤–∞ | –®–≤–∏–¥–∫–æ, robust, –º–∞–ª–æ overfitting | –í–µ–ª–∏–∫–∞ –º–æ–¥–µ–ª—å |
| **Gradient Boosting** | –ü–æ—Å–ª—ñ–¥–æ–≤–Ω—ñ –¥–µ—Ä–µ–≤–∞ –≤–∏–ø—Ä–∞–≤–ª—è—é—Ç—å –ø–æ–º–∏–ª–∫–∏ | –í–∏—â–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å | –ü–æ–≤—ñ–ª—å–Ω—ñ—à–µ, overfitting |
| **AdaBoost** | –ü–æ—Å–ª—ñ–¥–æ–≤–Ω–æ –∑–≤–∞–∂—É—î –ø–æ–º–∏–ª–∫–∏ | –ü—Ä–æ—Å—Ç–æ—Ç–∞ | –ß—É—Ç–ª–∏–≤–∏–π –¥–æ outliers |
| **XGBoost** | –û–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∏–π Gradient Boosting | –î—É–∂–µ —à–≤–∏–¥–∫–æ, –≤–∏—Å–æ–∫–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å | –°–∫–ª–∞–¥–Ω—ñ—à–µ –Ω–∞–ª–∞—à—Ç—É–≤–∞—Ç–∏ |

```python
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import AdaBoostClassifier
import time

# –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è
models = {
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),
    'AdaBoost': AdaBoostClassifier(n_estimators=100, random_state=42)
}

results = {}

for name, model in models.items():
    start_time = time.time()
    model.fit(X_train, y_train)
    train_time = time.time() - start_time
    
    accuracy = model.score(X_test, y_test)
    
    results[name] = {
        'Accuracy': accuracy,
        'Train Time': train_time
    }
    
    print(f"{name}:")
    print(f"  Accuracy: {accuracy:.4f}")
    print(f"  Train Time: {train_time:.2f}s\n")
```

---

## –ü—Ä–∞–∫—Ç–∏—á–Ω—ñ –ø–æ—Ä–∞–¥–∏ üí°

1. **–ü–æ—á–Ω–∏ –∑ –¥–µ—Ñ–æ–ª—Ç–∞–º–∏** ‚Äî 100 –¥–µ—Ä–µ–≤, sqrt features –ø—Ä–∞—Ü—é—î –¥–æ–±—Ä–µ
2. **n_jobs=-1** ‚Äî –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π –≤—Å—ñ —è–¥—Ä–∞ –¥–ª—è —à–≤–∏–¥–∫–æ—Å—Ç—ñ
3. **OOB score** ‚Äî –±–µ–∑–∫–æ—à—Ç–æ–≤–Ω–∞ –≤–∞–ª—ñ–¥–∞—Ü—ñ—è –¥–ª—è –≤–µ–ª–∏–∫–∏—Ö –¥–∞–Ω–∏—Ö
4. **–ù–µ –ø–µ—Ä–µ–±—ñ–ª—å—à—É–π n_estimators** ‚Äî –ø—ñ—Å–ª—è 100-500 –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è –º—ñ–Ω—ñ–º–∞–ª—å–Ω–µ
5. **max_depth=10-30** ‚Äî –æ–±–º–µ–∂ –¥–ª—è –∑–∞–ø–æ–±—ñ–≥–∞–Ω–Ω—è overfitting
6. **Feature importance** ‚Äî –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–π –¥–ª—è –≤—ñ–¥–±–æ—Ä—É –æ–∑–Ω–∞–∫
7. **–ù–µ –Ω–æ—Ä–º–∞–ª—ñ–∑—É–π** ‚Äî Random Forest –Ω–µ –ø–æ—Ç—Ä–µ–±—É—î –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—ó
8. **Class weights** ‚Äî –¥–ª—è –Ω–µ–∑–±–∞–ª–∞–Ω—Å–æ–≤–∞–Ω–∏—Ö –∫–ª–∞—Å—ñ–≤
9. **Baseline** ‚Äî –≤—ñ–¥–º—ñ–Ω–Ω–∏–π —Å—Ç–∞—Ä—Ç–æ–≤–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º
10. **–ü–æ—Ä—ñ–≤–Ω—é–π –∑ XGBoost** ‚Äî —è–∫—â–æ RF –Ω–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ

---

## –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ Random Forest

### –Ü–¥–µ–∞–ª—å–Ω–æ –ø—ñ–¥—Ö–æ–¥–∏—Ç—å ‚úì

- –ü–æ—Ç—Ä—ñ–±–Ω–∞ **–≤–∏—Å–æ–∫–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å** –Ω–∞ —Ç–∞–±–ª–∏—á–Ω–∏—Ö –¥–∞–Ω–∏—Ö
- **Robust baseline** –±–µ–∑ –±–∞–≥–∞—Ç–æ feature engineering
- –ù–µ–ª—ñ–Ω—ñ–π–Ω—ñ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ
- –ú—ñ–∫—Å —á–∏—Å–ª–æ–≤–∏—Ö —Ç–∞ –∫–∞—Ç–µ–≥–æ—Ä—ñ–∞–ª—å–Ω–∏—Ö –æ–∑–Ω–∞–∫
- **Feature importance** –≤–∞–∂–ª–∏–≤—ñ
- –°–µ—Ä–µ–¥–Ω—ñ –¥–∞—Ç–∞—Å–µ—Ç–∏ (10k-1M –ø—Ä–∏–∫–ª–∞–¥—ñ–≤)
- –ü–∞—Ä–∞–ª–µ–ª—ñ–∑–∞—Ü—ñ—è –¥–æ—Å—Ç—É–ø–Ω–∞ (–±–∞–≥–∞—Ç–æ —è–¥–µ—Ä)

### –ö—Ä–∞—â–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ —ñ–Ω—à–µ ‚úó

- **–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å –∫—Ä–∏—Ç–∏—á–Ω–∞** ‚Üí Decision Tree
- **–î—É–∂–µ –≤–µ–ª–∏–∫—ñ –¥–∞–Ω—ñ** ‚Üí XGBoost, LightGBM
- –õ—ñ–Ω—ñ–π–Ω—ñ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ ‚Üí Linear models
- **–ó–æ–±—Ä–∞–∂–µ–Ω–Ω—è, —Ç–µ–∫—Å—Ç, –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ** ‚Üí Neural Networks
- –ü–æ—Ç—Ä—ñ–±–Ω–∞ –∫–∞–ª—ñ–±—Ä–æ–≤–∞–Ω–∞ –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å ‚Üí Logistic Regression + calibration
- **–ï–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–æ –º–∞–ª–æ –¥–∞–Ω–∏—Ö** ‚Üí Regularized linear models

---

## –ü–æ—à–∏—Ä–µ–Ω—ñ –ø–æ–º–∏–ª–∫–∏ ‚ùå

### 1. –ó–∞–Ω–∞–¥—Ç–æ –±–∞–≥–∞—Ç–æ –¥–µ—Ä–µ–≤ –±–µ–∑ –ø—Ä–∏—á–∏–Ω–∏

```python
# ‚ùå –ù–ï–ü–†–ê–í–ò–õ–¨–ù–û (–º–∞—Ä–Ω–∞ —Ç—Ä–∞—Ç–∞ —á–∞—Å—É)
rf = RandomForestClassifier(n_estimators=10000)

# ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û (–¥–æ—Å—Ç–∞—Ç–Ω—å–æ 100-500)
rf = RandomForestClassifier(n_estimators=100)
```

### 2. –ù–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –ø–∞—Ä–∞–ª–µ–ª—ñ–∑–∞—Ü—ñ—é

```python
# ‚ùå –ù–ï–ü–†–ê–í–ò–õ–¨–ù–û (–ø–æ–≤—ñ–ª—å–Ω–æ)
rf = RandomForestClassifier(n_estimators=100)

# ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û (–≤–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ –≤—Å—ñ —è–¥—Ä–∞)
rf = RandomForestClassifier(n_estimators=100, n_jobs=-1)
```

### 3. –ù–æ—Ä–º–∞–ª—ñ–∑—É–≤–∞—Ç–∏ –¥–∞–Ω—ñ (–Ω–µ–ø–æ—Ç—Ä—ñ–±–Ω–æ)

```python
# ‚ùå –ù–ï–ü–û–¢–†–Ü–ë–ù–û
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
rf.fit(X_scaled, y)

# ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û (–≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ)
rf.fit(X, y)
```

### 4. –ó–∞–±—É—Ç–∏ –ø—Ä–æ random_state

```python
# ‚ùå –ù–ï–ü–†–ê–í–ò–õ–¨–ù–û (–Ω–µ—Å—Ç–∞–±—ñ–ª—å–Ω—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏)
rf = RandomForestClassifier(n_estimators=100)

# ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û (–≤—ñ–¥—Ç–≤–æ—Ä—é–≤–∞–Ω—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏)
rf = RandomForestClassifier(n_estimators=100, random_state=42)
```

---

## –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ç–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è

```python
import joblib

# –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ
joblib.dump(rf, 'random_forest_model.pkl')

# –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è
loaded_rf = joblib.load('random_forest_model.pkl')

# –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è
predictions = loaded_rf.predict(X_new)
```

---

## –ü–æ–≤'—è–∑–∞–Ω—ñ —Ç–µ–º–∏

- [[01_Decision_Trees]] ‚Äî –±–∞–∑–æ–≤–∏–π –±—É–¥—ñ–≤–µ–ª—å–Ω–∏–π –±–ª–æ–∫
- [[03_Gradient_Boosting]] ‚Äî –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∏–π –∞–Ω—Å–∞–º–±–ª—å
- [[05_Ensemble_Methods]] ‚Äî —Ç–µ–æ—Ä—ñ—è –∞–Ω—Å–∞–º–±–ª—ñ–≤
- [[06_Feature_Importance]] ‚Äî –≤—ñ–¥–±—ñ—Ä –æ–∑–Ω–∞–∫
- [[Cross_Validation]] ‚Äî –æ—Ü—ñ–Ω–∫–∞ –º–æ–¥–µ–ª—ñ
- [[Hyperparameter_Tuning]] ‚Äî –ø—ñ–¥–±—ñ—Ä –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤

## –†–µ—Å—É—Ä—Å–∏

- [Scikit-learn: Random Forest](https://scikit-learn.org/stable/modules/ensemble.html#forest)
- [Leo Breiman: Random Forests (Original Paper)](https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf)
- [StatQuest: Random Forests](https://www.youtube.com/watch?v=J4Wdy0Wc_xQ)
- [Random Forest Interpretation](https://explained.ai/rf-importance/)

---

## –ö–ª—é—á–æ–≤—ñ –≤–∏—Å–Ω–æ–≤–∫–∏

> Random Forest ‚Äî —Ü–µ –∞–Ω—Å–∞–º–±–ª—å –±–∞–≥–∞—Ç—å–æ—Ö Decision Trees, –Ω–∞–≤—á–µ–Ω–∏—Ö –Ω–∞ —Ä—ñ–∑–Ω–∏—Ö –ø—ñ–¥–≤–∏–±—ñ—Ä–∫–∞—Ö –¥–∞–Ω–∏—Ö —Ç–∞ –æ–∑–Ω–∞–∫, –∑ –æ–±'—î–¥–Ω–∞–Ω–Ω—è–º –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω—å —á–µ—Ä–µ–∑ –≥–æ–ª–æ—Å—É–≤–∞–Ω–Ω—è –∞–±–æ —É—Å–µ—Ä–µ–¥–Ω–µ–Ω–Ω—è.

**–û—Å–Ω–æ–≤–Ω—ñ –ø—Ä–∏–Ω—Ü–∏–ø–∏:**
- **Bagging:** bootstrap sampling –¥–ª—è —Ä—ñ–∑–Ω–æ–º–∞–Ω—ñ—Ç–Ω–æ—Å—Ç—ñ –¥–∞–Ω–∏—Ö
- **Random feature selection:** –≤–∏–ø–∞–¥–∫–æ–≤—ñ –æ–∑–Ω–∞–∫–∏ –¥–ª—è –¥–µ–∫–æ—Ä–µ–ª—è—Ü—ñ—ó –¥–µ—Ä–µ–≤
- **Voting/Averaging:** –æ–±'—î–¥–Ω–∞–Ω–Ω—è –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω—å –∑–º–µ–Ω—à—É—î variance

**–§–æ—Ä–º—É–ª–∞ variance reduction:**
$$\text{Var}(\text{average}) = \rho \sigma^2 + \frac{1-\rho}{N}\sigma^2$$

**–ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏:**
- –í–∏—Å–æ–∫–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å + robust + —Ç–∞–±–ª–∏—á–Ω—ñ –¥–∞–Ω—ñ = Random Forest ‚úì
- –ë—ñ–ª—å—à–µ —Ç–æ—á–Ω–æ—Å—Ç—ñ –ø–æ—Ç—Ä—ñ–±–Ω–æ ‚Üí XGBoost/LightGBM
- –Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å –≤–∞–∂–ª–∏–≤–∞ ‚Üí Decision Tree

**–ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è:**
- n_estimators: 100-500
- max_depth: 10-30
- max_features: 'sqrt' (classification), 'log2' –∞–±–æ p/3 (regression)
- n_jobs: -1 (–ø–∞—Ä–∞–ª–µ–ª—ñ–∑–∞—Ü—ñ—è)

---

#ml #supervised-learning #ensemble #random-forest #bagging #classification #regression
