# Gradient Boosting (–ì—Ä–∞–¥—ñ—î–Ω—Ç–Ω–∏–π –±—É—Å—Ç–∏–Ω–≥)

## –©–æ —Ü–µ?

**Gradient Boosting** ‚Äî —Ü–µ –ø–æ—Ç—É–∂–Ω–∏–π –∞–Ω—Å–∞–º–±–ª–µ–≤–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º, —è–∫–∏–π **–ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ** –±—É–¥—É—î —Å–ª–∞–±–∫—ñ –º–æ–¥–µ–ª—ñ (–∑–∞–∑–≤–∏—á–∞–π –¥–µ—Ä–µ–≤–∞), –¥–µ –∫–æ–∂–Ω–∞ –Ω–∞—Å—Ç—É–ø–Ω–∞ –º–æ–¥–µ–ª—å –≤–∏–ø—Ä–∞–≤–ª—è—î –ø–æ–º–∏–ª–∫–∏ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ—Ö, —Ä—É—Ö–∞—é—á–∏—Å—å —É –Ω–∞–ø—Ä—è–º–∫—É –≥—Ä–∞–¥—ñ—î–Ω—Ç–∞ —Ñ—É–Ω–∫—Ü—ñ—ó –≤—Ç—Ä–∞—Ç.

**–ì–æ–ª–æ–≤–Ω–∞ —ñ–¥–µ—è:** –Ω–∞–≤—á–∞—Ç–∏ –Ω–æ–≤—ñ –º–æ–¥–µ–ª—ñ –Ω–∞ –ø–æ–º–∏–ª–∫–∞—Ö (residuals) –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ—Ö –º–æ–¥–µ–ª–µ–π, –ø–æ—Å—Ç—É–ø–æ–≤–æ –ø–æ–∫—Ä–∞—â—É—é—á–∏ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è.

## –ù–∞–≤—ñ—â–æ –ø–æ—Ç—Ä—ñ–±–µ–Ω?

- üèÜ **–ù–∞–π–≤–∏—â–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å** ‚Äî SOTA –Ω–∞ —Ç–∞–±–ª–∏—á–Ω–∏—Ö –¥–∞–Ω–∏—Ö (Kaggle winner)
- üéØ **–ì–Ω—É—á–∫—ñ—Å—Ç—å** ‚Äî —Ä—ñ–∑–Ω—ñ —Ñ—É–Ω–∫—Ü—ñ—ó –≤—Ç—Ä–∞—Ç –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö –∑–∞–¥–∞—á
- üìä **–£–Ω—ñ–≤–µ—Ä—Å–∞–ª—å–Ω—ñ—Å—Ç—å** ‚Äî —Ä–µ–≥—Ä–µ—Å—ñ—è, –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è, —Ä–∞–Ω–∂—É–≤–∞–Ω–Ω—è
- üí° **Feature importance** ‚Äî –∞–Ω–∞–ª—ñ–∑ –≤–∞–∂–ª–∏–≤–æ—Å—Ç—ñ –æ–∑–Ω–∞–∫
- üîß **–ö–æ–Ω—Ç—Ä–æ–ª—å** ‚Äî –±–∞–≥–∞—Ç–æ –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ –¥–ª—è fine-tuning
- ‚ö° **–°—É—á–∞—Å–Ω—ñ —Ä–µ–∞–ª—ñ–∑–∞—Ü—ñ—ó** ‚Äî XGBoost, LightGBM, CatBoost

## –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏?

**–ü–æ—Ç—Ä—ñ–±–Ω–æ:**
- –ü–æ—Ç—Ä—ñ–±–Ω–∞ **–º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å** –Ω–∞ —Ç–∞–±–ª–∏—á–Ω–∏—Ö –¥–∞–Ω–∏—Ö
- **Kaggle competitions** ‚Äî —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è —Ç–∞–±–ª–∏—á–Ω–∏—Ö –¥–∞–Ω–∏—Ö
- –°–∫–ª–∞–¥–Ω—ñ –Ω–µ–ª—ñ–Ω—ñ–π–Ω—ñ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ
- **Production ML** ‚Äî –≤–∏—Å–æ–∫–∞ —è–∫—ñ—Å—Ç—å –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω—å
- –ì–æ—Ç–æ–≤–Ω—ñ—Å—Ç—å –≤–∏—Ç—Ä–∞—Ç–∏—Ç–∏ —á–∞—Å –Ω–∞ **tuning**

**–ù–µ –ø–æ—Ç—Ä—ñ–±–Ω–æ:**
- **–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å –∫—Ä–∏—Ç–∏—á–Ω–∞** ‚Üí Decision Tree, Linear Models
- **–î—É–∂–µ —à–≤–∏–¥–∫–∏–π baseline** ‚Üí Random Forest (–ª–µ–≥—à–µ –≤ tuning)
- –ó–æ–±—Ä–∞–∂–µ–Ω–Ω—è, —Ç–µ–∫—Å—Ç, –∞—É–¥—ñ–æ ‚Üí Deep Learning
- **–î—É–∂–µ –º–∞–ª—ñ –¥–∞–Ω—ñ** (< 1000 –∑—Ä–∞–∑–∫—ñ–≤) ‚Üí Linear Models
- –ü–æ—Ç—Ä—ñ–±–Ω–∞ —à–≤–∏–¥–∫—ñ—Å—Ç—å inference ‚Üí Linear Models

---

## –í—ñ–¥–º—ñ–Ω–Ω—ñ—Å—Ç—å –≤—ñ–¥ Random Forest

### Random Forest (Bagging)

```
–ü–∞—Ä–∞–ª–µ–ª—å–Ω–∏–π –ø—ñ–¥—Ö—ñ–¥:

Tree 1 ‚îÄ‚îê
Tree 2 ‚îÄ‚î§
Tree 3 ‚îÄ‚îº‚îÄ‚Üí Average/Vote ‚Üí Prediction
Tree 4 ‚îÄ‚î§
Tree 5 ‚îÄ‚îò

–ö–æ–∂–Ω–µ –¥–µ—Ä–µ–≤–æ –Ω–µ–∑–∞–ª–µ–∂–Ω–µ
–ù–∞–≤—á–∞—é—Ç—å—Å—è –ø–∞—Ä–∞–ª–µ–ª—å–Ω–æ
```

### Gradient Boosting (Boosting)

```
–ü–æ—Å–ª—ñ–¥–æ–≤–Ω–∏–π –ø—ñ–¥—Ö—ñ–¥:

Data ‚Üí Tree 1 ‚Üí Residuals‚ÇÅ ‚Üí Tree 2 ‚Üí Residuals‚ÇÇ ‚Üí Tree 3 ‚Üí ... ‚Üí Final
         ‚Üì                      ‚Üì                      ‚Üì
       Pred‚ÇÅ                  Pred‚ÇÇ                  Pred‚ÇÉ

–ö–æ–∂–Ω–µ –¥–µ—Ä–µ–≤–æ –∑–∞–ª–µ–∂–∏—Ç—å –≤—ñ–¥ –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ–≥–æ
–ù–∞–≤—á–∞—é—Ç—å—Å—è –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ
```

**–ö–ª—é—á–æ–≤–∞ —Ä—ñ–∑–Ω–∏—Ü—è:**
- **Random Forest:** –Ω–µ–∑–∞–ª–µ–∂–Ω—ñ –¥–µ—Ä–µ–≤–∞, –≤–∏–ø—Ä–∞–≤–ª—è—é—Ç—å variance
- **Gradient Boosting:** –∑–∞–ª–µ–∂–Ω—ñ –¥–µ—Ä–µ–≤–∞, –≤–∏–ø—Ä–∞–≤–ª—è—é—Ç—å bias

---

## –Ø–∫ –ø—Ä–∞—Ü—é—î Gradient Boosting?

### –Ü–Ω—Ç—É—ó—Ü—ñ—è

**–ó–∞–¥–∞—á–∞:** –ø–µ—Ä–µ–¥–±–∞—á–∏—Ç–∏ –∑–∞—Ä–ø–ª–∞—Ç—É.

**–ö—Ä–æ–∫ 1:** –ü—Ä–æ—Å—Ç–µ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è (—Å–µ—Ä–µ–¥–Ω—î)

```
–†–µ–∞–ª—å–Ω–∞ –∑–∞—Ä–ø–ª–∞—Ç–∞: [50k, 60k, 70k, 80k]
–ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è (—Å–µ—Ä–µ–¥–Ω—î): [65k, 65k, 65k, 65k]
–ü–æ–º–∏–ª–∫–∏ (residuals): [-15k, -5k, +5k, +15k]
```

**–ö—Ä–æ–∫ 2:** –ù–∞–≤—á–∏—Ç–∏ –¥–µ—Ä–µ–≤–æ –Ω–∞ –ø–æ–º–∏–ª–∫–∞—Ö

```
–ú–æ–¥–µ–ª—å 2 –ø–µ—Ä–µ–¥–±–∞—á–∞—î –ø–æ–º–∏–ª–∫–∏: [-14k, -6k, +6k, +14k]
–ù–æ–≤–µ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è: 65k + (-14k) = 51k, 65k + (-6k) = 59k, ...
–ù–æ–≤—ñ –ø–æ–º–∏–ª–∫–∏: [-1k, +1k, -1k, +1k]  ‚Üê –ú–µ–Ω—à—ñ!
```

**–ö—Ä–æ–∫ 3:** –ü–æ–≤—Ç–æ—Ä—é—î–º–æ...

```
–ú–æ–¥–µ–ª—å 3 –ø–µ—Ä–µ–¥–±–∞—á–∞—î –Ω–æ–≤—ñ –ø–æ–º–∏–ª–∫–∏: [-1k, +1k, -1k, +1k]
–§—ñ–Ω–∞–ª—å–Ω–µ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è: [50k, 60k, 70k, 80k]  ‚Üê –Ü–¥–µ–∞–ª—å–Ω–æ!
```

### –ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞

**–ó–∞–≥–∞–ª—å–Ω–∞ —Ñ–æ—Ä–º–∞:**

$$F_M(x) = F_0(x) + \sum_{m=1}^{M} \nu \cdot h_m(x)$$

–¥–µ:
- $F_M(x)$ ‚Äî —Ñ—ñ–Ω–∞–ª—å–Ω–µ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –ø—ñ—Å–ª—è $M$ —ñ—Ç–µ—Ä–∞—Ü—ñ–π
- $F_0(x)$ ‚Äî –ø–æ—á–∞—Ç–∫–æ–≤–µ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è (–∑–∞–∑–≤–∏—á–∞–π –∫–æ–Ω—Å—Ç–∞–Ω—Ç–∞)
- $h_m(x)$ ‚Äî $m$-—Ç–µ –¥–µ—Ä–µ–≤–æ (—Å–ª–∞–±–∫–∏–π learner)
- $\nu$ ‚Äî **learning rate** (0 < ŒΩ ‚â§ 1)
- $M$ ‚Äî –∫—ñ–ª—å–∫—ñ—Å—Ç—å –¥–µ—Ä–µ–≤

### –ê–ª–≥–æ—Ä–∏—Ç–º (–¥–ª—è —Ä–µ–≥—Ä–µ—Å—ñ—ó)

**–í—Ö—ñ–¥:** –¥–∞—Ç–∞—Å–µ—Ç $(x_i, y_i)$, —Ñ—É–Ω–∫—Ü—ñ—è –≤—Ç—Ä–∞—Ç $L$, –∫—ñ–ª—å–∫—ñ—Å—Ç—å —ñ—Ç–µ—Ä–∞—Ü—ñ–π $M$

**1. –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è:** –ø–æ—á–∞—Ç–∫–æ–≤–µ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è (–∫–æ–Ω—Å—Ç–∞–Ω—Ç–∞)
$$F_0(x) = \arg\min_\gamma \sum_{i=1}^{n} L(y_i, \gamma)$$

–î–ª—è MSE: $F_0(x) = \text{mean}(y) = \bar{y}$

**2. –î–ª—è $m = 1$ –¥–æ $M$:**

   **a) –û–±—á–∏—Å–ª–∏—Ç–∏ –ø—Å–µ–≤–¥–æ-residuals (–Ω–µ–≥–∞—Ç–∏–≤–Ω–∏–π –≥—Ä–∞–¥—ñ—î–Ω—Ç):**
   $$r_{im} = -\left[\frac{\partial L(y_i, F(x_i))}{\partial F(x_i)}\right]_{F=F_{m-1}}$$
   
   –î–ª—è MSE: $r_{im} = y_i - F_{m-1}(x_i)$ (–ø—Ä–æ—Å—Ç–æ –ø–æ–º–∏–ª–∫–∏)

   **b) –ù–∞–≤—á–∏—Ç–∏ –¥–µ—Ä–µ–≤–æ $h_m(x)$ –ø–µ—Ä–µ–¥–±–∞—á–∞—Ç–∏ residuals $r_m$**

   **c) –û–Ω–æ–≤–∏—Ç–∏ –º–æ–¥–µ–ª—å:**
   $$F_m(x) = F_{m-1}(x) + \nu \cdot h_m(x)$$

**3. –í–∏—Ö—ñ–¥:** $F_M(x)$

---

## Learning Rate (—à–≤–∏–¥–∫—ñ—Å—Ç—å –Ω–∞–≤—á–∞–Ω–Ω—è)

### –©–æ —Ü–µ?

**Learning rate** $\nu$ –∫–æ–Ω—Ç—Ä–æ–ª—é—î, –Ω–∞—Å–∫—ñ–ª—å–∫–∏ –º–∏ –¥–æ–≤—ñ—Ä—è—î–º–æ –∫–æ–∂–Ω–æ–º—É –Ω–æ–≤–æ–º—É –¥–µ—Ä–µ–≤—É.

$$F_m(x) = F_{m-1}(x) + \nu \cdot h_m(x)$$

### –ï—Ñ–µ–∫—Ç —Ä—ñ–∑–Ω–∏—Ö $\nu$

```
ŒΩ = 1.0 (–∞–≥—Ä–µ—Å–∏–≤–Ω–µ –Ω–∞–≤—á–∞–Ω–Ω—è):
    –®–≤–∏–¥–∫–∞ –∑–±—ñ–∂–Ω—ñ—Å—Ç—å
    –í–∏—Å–æ–∫–∏–π —Ä–∏–∑–∏–∫ overfitting
    –ú–∞–ª–æ –¥–µ—Ä–µ–≤ –ø–æ—Ç—Ä—ñ–±–Ω–æ

ŒΩ = 0.1 (–ø–æ–º—ñ—Ä–Ω–µ –Ω–∞–≤—á–∞–Ω–Ω—è):
    –°–µ—Ä–µ–¥–Ω—è —à–≤–∏–¥–∫—ñ—Å—Ç—å
    –ë–∞–ª–∞–Ω—Å –º—ñ–∂ —Ç–æ—á–Ω—ñ—Å—Ç—é —Ç–∞ overfitting
    –†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–æ –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º

ŒΩ = 0.01 (–ø–æ–≤—ñ–ª—å–Ω–µ –Ω–∞–≤—á–∞–Ω–Ω—è):
    –î—É–∂–µ –ø–æ–≤—ñ–ª—å–Ω–∞ –∑–±—ñ–∂–Ω—ñ—Å—Ç—å
    –î—É–∂–µ —Ä–æ–±–∞—Å—Ç–Ω–æ –¥–æ overfitting
    –ë–∞–≥–∞—Ç–æ –¥–µ—Ä–µ–≤ –ø–æ—Ç—Ä—ñ–±–Ω–æ
```

### Trade-off: Learning Rate vs Number of Trees

```
–í–∏—Å–æ–∫–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å –¥–æ—Å—è–≥–∞—î—Ç—å—Å—è —á–µ—Ä–µ–∑:
    –ú–∞–ª–∏–π ŒΩ + –±–∞–≥–∞—Ç–æ –¥–µ—Ä–µ–≤ (M)
    –∞–±–æ
    –í–µ–ª–∏–∫–∏–π ŒΩ + –º–∞–ª–æ –¥–µ—Ä–µ–≤ (M)

–ê–ª–µ:
    –ú–∞–ª–∏–π ŒΩ + –±–∞–≥–∞—Ç–æ M ‚Üí –∫—Ä–∞—â–µ —É–∑–∞–≥–∞–ª—å–Ω–µ–Ω–Ω—è ‚úì
    –í–µ–ª–∏–∫–∏–π ŒΩ + –º–∞–ª–æ M ‚Üí —Ä–∏–∑–∏–∫ overfitting ‚úó
```

**–ü—Ä–∞–∫—Ç–∏—á–Ω–µ –ø—Ä–∞–≤–∏–ª–æ:**
- $\nu = 0.1$ —Ç–∞ $M = 100-500$ ‚Äî —Ö–æ—Ä–æ—à–∏–π —Å—Ç–∞—Ä—Ç
- $\nu = 0.01$ —Ç–∞ $M = 1000-5000$ ‚Äî –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ—ó —è–∫–æ—Å—Ç—ñ

---

## –§—É–Ω–∫—Ü—ñ—ó –≤—Ç—Ä–∞—Ç

### –î–ª—è —Ä–µ–≥—Ä–µ—Å—ñ—ó

#### 1. MSE (L2 Loss)

$$L(y, F(x)) = \frac{1}{2}(y - F(x))^2$$

**–ì—Ä–∞–¥—ñ—î–Ω—Ç (residuals):**
$$r = y - F(x)$$

**–í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è:** –∑–≤–∏—á–∞–π–Ω–∞ —Ä–µ–≥—Ä–µ—Å—ñ—è, —á—É—Ç–ª–∏–≤–∞ –¥–æ –≤–∏–∫–∏–¥—ñ–≤

#### 2. MAE (L1 Loss)

$$L(y, F(x)) = |y - F(x)|$$

**–ì—Ä–∞–¥—ñ—î–Ω—Ç:**
$$r = \text{sign}(y - F(x))$$

**–í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è:** —Ä–æ–±–∞—Å—Ç–Ω–∞ –¥–æ –≤–∏–∫–∏–¥—ñ–≤

#### 3. Huber Loss (–∫–æ–º–±—ñ–Ω–∞—Ü—ñ—è L1 + L2)

$$L_\delta(y, F) = \begin{cases}
\frac{1}{2}(y - F)^2 & \text{—è–∫—â–æ } |y - F| \leq \delta \\
\delta(|y - F| - \frac{\delta}{2}) & \text{—ñ–Ω–∞–∫—à–µ}
\end{cases}$$

**–í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è:** –∫–æ–º–ø—Ä–æ–º—ñ—Å –º—ñ–∂ MSE —Ç–∞ MAE

### –î–ª—è –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó

#### 1. Log Loss (Binary Cross-Entropy)

$$L(y, F(x)) = -[y \log(p) + (1-y) \log(1-p)]$$

–¥–µ $p = \sigma(F(x)) = \frac{1}{1 + e^{-F(x)}}$

**–ì—Ä–∞–¥—ñ—î–Ω—Ç:**
$$r = y - p$$

**–í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è:** –±—ñ–Ω–∞—Ä–Ω–∞ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è

#### 2. Multinomial Deviance

–î–ª—è –±–∞–≥–∞—Ç–æ–∫–ª–∞—Å–æ–≤–æ—ó –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó (softmax).

---

## Regularization (—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—è)

### 1. Shrinkage (Learning Rate)

$$F_m(x) = F_{m-1}(x) + \nu \cdot h_m(x)$$

–ú–∞–ª–∏–π $\nu$ ‚Üí —Å–∏–ª—å–Ω—ñ—à–∞ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—è

### 2. Subsampling (Stochastic GB)

**–í–∏–ø–∞–¥–∫–æ–≤–∞ –ø—ñ–¥–º–Ω–æ–∂–∏–Ω–∞ –¥–∞–Ω–∏—Ö** –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –¥–µ—Ä–µ–≤–∞:

```python
GradientBoostingClassifier(subsample=0.8)  # 80% –¥–∞–Ω–∏—Ö –Ω–∞ –¥–µ—Ä–µ–≤–æ
```

**–ü–µ—Ä–µ–≤–∞–≥–∏:**
- –ó–º–µ–Ω—à—É—î overfitting
- –ü—Ä–∏—Å–∫–æ—Ä—é—î –Ω–∞–≤—á–∞–Ω–Ω—è
- –î–æ–¥–∞—î —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–Ω—ñ—Å—Ç—å (—è–∫ —É SGD)

**–¢–∏–ø–æ–≤—ñ –∑–Ω–∞—á–µ–Ω–Ω—è:** 0.5 - 1.0

### 3. Tree Constraints

**–û–±–º–µ–∂–µ–Ω–Ω—è —Å–∫–ª–∞–¥–Ω–æ—Å—Ç—ñ –¥–µ—Ä–µ–≤:**
- `max_depth` ‚Äî –≥–ª–∏–±–∏–Ω–∞ –¥–µ—Ä–µ–≤ (–∑–∞–∑–≤–∏—á–∞–π 3-10)
- `min_samples_split` ‚Äî –º—ñ–Ω. –∑—Ä–∞–∑–∫—ñ–≤ –¥–ª—è —Ä–æ–∑–±–∏—Ç—Ç—è
- `min_samples_leaf` ‚Äî –º—ñ–Ω. –∑—Ä–∞–∑–∫—ñ–≤ —É –ª–∏—Å—Ç–∫—É
- `max_features` ‚Äî –ø—ñ–¥–º–Ω–æ–∂–∏–Ω–∞ –æ–∑–Ω–∞–∫

**Gradient Boosting –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î –ú–Ü–õ–ö–Ü –¥–µ—Ä–µ–≤–∞!**
- Random Forest: –≥–ª–∏–±–æ–∫—ñ –¥–µ—Ä–µ–≤–∞ (max_depth=None)
- Gradient Boosting: –º—ñ–ª–∫—ñ –¥–µ—Ä–µ–≤–∞ (max_depth=3-5)

### 4. Early Stopping

**–ó—É–ø–∏–Ω–∫–∞ –ø—Ä–∏ –ø–æ–≥—ñ—Ä—à–µ–Ω–Ω—ñ –Ω–∞ validation set:**

```python
gb = GradientBoostingClassifier(
    n_estimators=1000,
    validation_fraction=0.1,  # 10% –¥–ª—è –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó
    n_iter_no_change=50,      # –ó—É–ø–∏–Ω–∫–∞ –ø—ñ—Å–ª—è 50 —ñ—Ç–µ—Ä–∞—Ü—ñ–π –±–µ–∑ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è
    tol=1e-4
)
```

---

## –ö–æ–¥ (scikit-learn)

### –ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score

# –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –¥–∞–Ω–∏—Ö
X, y = make_classification(
    n_samples=1000,
    n_features=20,
    n_informative=15,
    n_redundant=5,
    random_state=42
)

# –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Gradient Boosting Classifier
gb_clf = GradientBoostingClassifier(
    n_estimators=100,         # –ö—ñ–ª—å–∫—ñ—Å—Ç—å –¥–µ—Ä–µ–≤
    learning_rate=0.1,        # –®–≤–∏–¥–∫—ñ—Å—Ç—å –Ω–∞–≤—á–∞–Ω–Ω—è (ŒΩ)
    max_depth=3,              # –ì–ª–∏–±–∏–Ω–∞ –¥–µ—Ä–µ–≤ (–º—ñ–ª–∫—ñ!)
    min_samples_split=2,
    min_samples_leaf=1,
    subsample=0.8,            # Stochastic GB (80% –¥–∞–Ω–∏—Ö)
    max_features='sqrt',      # –ü—ñ–¥–º–Ω–æ–∂–∏–Ω–∞ –æ–∑–Ω–∞–∫
    random_state=42,
    verbose=0
)

# –ù–∞–≤—á–∞–Ω–Ω—è
gb_clf.fit(X_train, y_train)

# –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
y_pred = gb_clf.predict(X_test)
y_pred_proba = gb_clf.predict_proba(X_test)[:, 1]

# –ú–µ—Ç—Ä–∏–∫–∏
print("=== Gradient Boosting Classification ===")
print(f"Train Accuracy: {gb_clf.score(X_train, y_train):.4f}")
print(f"Test Accuracy: {accuracy_score(y_test, y_pred):.4f}")
print(f"ROC-AUC: {roc_auc_score(y_test, y_pred_proba):.4f}")

print("\n" + classification_report(y_test, y_pred))

# Feature Importance
importances = gb_clf.feature_importances_
indices = np.argsort(importances)[::-1]

print("\n=== Top 10 Features ===")
for i in range(min(10, len(importances))):
    print(f"{i+1}. Feature {indices[i]}: {importances[indices[i]]:.4f}")
```

### –†–µ–≥—Ä–µ—Å—ñ—è

```python
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.datasets import make_regression
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

# –î–∞–Ω—ñ
X, y = make_regression(
    n_samples=500,
    n_features=10,
    n_informative=8,
    noise=10,
    random_state=42
)

# –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Gradient Boosting Regressor
gb_reg = GradientBoostingRegressor(
    n_estimators=200,
    learning_rate=0.1,
    max_depth=4,
    subsample=0.8,
    loss='squared_error',    # –∞–±–æ 'absolute_error', 'huber'
    random_state=42,
    verbose=0
)

# –ù–∞–≤—á–∞–Ω–Ω—è
gb_reg.fit(X_train, y_train)

# –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
y_pred_train = gb_reg.predict(X_train)
y_pred_test = gb_reg.predict(X_test)

# –ú–µ—Ç—Ä–∏–∫–∏
print("=== Gradient Boosting Regression ===")
print(f"Train R¬≤: {r2_score(y_train, y_pred_train):.4f}")
print(f"Test R¬≤: {r2_score(y_test, y_pred_test):.4f}")
print(f"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_test)):.2f}")
print(f"MAE: {mean_absolute_error(y_test, y_pred_test):.2f}")

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred_test, alpha=0.5)
plt.plot([y_test.min(), y_test.max()], 
         [y_test.min(), y_test.max()], 
         'r--', lw=2, label='Perfect Prediction')
plt.xlabel('True Values', fontsize=12)
plt.ylabel('Predictions', fontsize=12)
plt.title('Gradient Boosting Regression', fontsize=14, fontweight='bold')
plt.legend(fontsize=11)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

---

## Staged Predictions (–ø–æ–µ—Ç–∞–ø–Ω—ñ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è)

### –ú–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥ –Ω–∞–≤—á–∞–Ω–Ω—è

**–û—Ç—Ä–∏–º–∞–Ω–Ω—è –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω—å –ø—ñ—Å–ª—è –∫–æ–∂–Ω–æ–≥–æ –¥–µ—Ä–µ–≤–∞:**

```python
import matplotlib.pyplot as plt

# –ù–∞–≤—á–∞–Ω–Ω—è
gb = GradientBoostingRegressor(n_estimators=200, random_state=42)
gb.fit(X_train, y_train)

# –ü–æ–µ—Ç–∞–ø–Ω—ñ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
train_scores = []
test_scores = []

for i, (train_pred, test_pred) in enumerate(zip(
    gb.staged_predict(X_train),
    gb.staged_predict(X_test)
)):
    train_scores.append(mean_squared_error(y_train, train_pred))
    test_scores.append(mean_squared_error(y_test, test_pred))

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
plt.figure(figsize=(12, 6))
plt.plot(range(1, len(train_scores) + 1), train_scores, 
         label='Train MSE', linewidth=2)
plt.plot(range(1, len(test_scores) + 1), test_scores, 
         label='Test MSE', linewidth=2)
plt.xlabel('Number of Trees', fontsize=12)
plt.ylabel('MSE', fontsize=12)
plt.title('Gradient Boosting: MSE vs Number of Trees', 
          fontsize=14, fontweight='bold')
plt.legend(fontsize=11)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# –û–ø—Ç–∏–º–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –¥–µ—Ä–µ–≤
optimal_n_trees = np.argmin(test_scores) + 1
print(f"Optimal number of trees: {optimal_n_trees}")
print(f"Best Test MSE: {test_scores[optimal_n_trees - 1]:.2f}")
```

**–°–ø–æ—Å—Ç–µ—Ä–µ–∂–µ–Ω–Ω—è:**
- Train MSE –º–æ–Ω–æ—Ç–æ–Ω–Ω–æ –∑–º–µ–Ω—à—É—î—Ç—å—Å—è
- Test MSE —Å–ø–æ—á–∞—Ç–∫—É –∑–º–µ–Ω—à—É—î—Ç—å—Å—è, –ø–æ—Ç—ñ–º –º–æ–∂–µ –∑—Ä–æ—Å—Ç–∞—Ç–∏ (overfitting)
- –û–ø—Ç–∏–º–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –¥–µ—Ä–µ–≤ ‚Äî –º—ñ–Ω—ñ–º—É–º Test MSE

---

## –ü—ñ–¥–±—ñ—Ä –≥—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤

### Grid Search

```python
from sklearn.model_selection import GridSearchCV

# –°—ñ—Ç–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤
param_grid = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.01, 0.1, 0.3],
    'max_depth': [3, 4, 5],
    'subsample': [0.8, 1.0],
    'max_features': ['sqrt', 'log2', None]
}

# Grid Search
grid_search = GridSearchCV(
    GradientBoostingClassifier(random_state=42),
    param_grid,
    cv=5,
    scoring='roc_auc',
    n_jobs=-1,
    verbose=1
)

grid_search.fit(X_train, y_train)

print("Best parameters:")
print(grid_search.best_params_)
print(f"\nBest CV score: {grid_search.best_score_:.4f}")

# –¢–µ—Å—Ç
best_gb = grid_search.best_estimator_
print(f"Test score: {best_gb.score(X_test, y_test):.4f}")
```

### Randomized Search

```python
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint, uniform

param_distributions = {
    'n_estimators': randint(50, 500),
    'learning_rate': uniform(0.01, 0.3),
    'max_depth': randint(3, 10),
    'subsample': uniform(0.5, 0.5),  # 0.5-1.0
    'min_samples_split': randint(2, 20),
    'min_samples_leaf': randint(1, 10),
    'max_features': ['sqrt', 'log2', None]
}

random_search = RandomizedSearchCV(
    GradientBoostingClassifier(random_state=42),
    param_distributions,
    n_iter=50,
    cv=5,
    scoring='roc_auc',
    random_state=42,
    n_jobs=-1,
    verbose=1
)

random_search.fit(X_train, y_train)
print("Best parameters:", random_search.best_params_)
```

---

## XGBoost, LightGBM, CatBoost

### –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è —Ä–µ–∞–ª—ñ–∑–∞—Ü—ñ–π

| –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞ | scikit-learn GB | XGBoost | LightGBM | CatBoost |
|----------------|-----------------|---------|----------|----------|
| **–®–≤–∏–¥–∫—ñ—Å—Ç—å** | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |
| **–¢–æ—á–Ω—ñ—Å—Ç—å** | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **–ö–∞—Ç–µ–≥–æ—Ä—ñ–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ** | ‚ùå | ‚ö†Ô∏è | ‚ö†Ô∏è | ‚úÖ‚úÖ |
| **GPU –ø—ñ–¥—Ç—Ä–∏–º–∫–∞** | ‚ùå | ‚úÖ | ‚úÖ | ‚úÖ |
| **Regularization** | –ë–∞–∑–æ–≤–∞ | ‚úÖ‚úÖ | ‚úÖ | ‚úÖ |
| **–í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è** | –ù–∞–≤—á–∞–Ω–Ω—è | Production | –î—É–∂–µ –≤–µ–ª–∏–∫—ñ –¥–∞–Ω—ñ | –ö–∞—Ç–µ–≥–æ—Ä—ñ–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ |

### XGBoost

**Extreme Gradient Boosting** ‚Äî –Ω–∞–π–ø–æ–ø—É–ª—è—Ä–Ω—ñ—à–∞ —Ä–µ–∞–ª—ñ–∑–∞—Ü—ñ—è.

```python
import xgboost as xgb

# XGBoost Classifier
xgb_clf = xgb.XGBClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=5,
    subsample=0.8,
    colsample_bytree=0.8,     # Feature sampling
    reg_alpha=0.1,            # L1 regularization
    reg_lambda=1.0,           # L2 regularization
    random_state=42,
    eval_metric='logloss'
)

# –ù–∞–≤—á–∞–Ω–Ω—è –∑ early stopping
xgb_clf.fit(
    X_train, y_train,
    eval_set=[(X_test, y_test)],
    early_stopping_rounds=10,
    verbose=False
)

print(f"Best iteration: {xgb_clf.best_iteration}")
print(f"Test Accuracy: {xgb_clf.score(X_test, y_test):.4f}")
```

### LightGBM

**Light Gradient Boosting Machine** ‚Äî –Ω–∞–π—à–≤–∏–¥—à–∞ —Ä–µ–∞–ª—ñ–∑–∞—Ü—ñ—è.

```python
import lightgbm as lgb

# LightGBM Classifier
lgb_clf = lgb.LGBMClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=5,
    num_leaves=31,            # –£–Ω—ñ–∫–∞–ª—å–Ω–æ –¥–ª—è LightGBM
    subsample=0.8,
    colsample_bytree=0.8,
    reg_alpha=0.1,
    reg_lambda=1.0,
    random_state=42
)

lgb_clf.fit(
    X_train, y_train,
    eval_set=[(X_test, y_test)],
    callbacks=[lgb.early_stopping(10)]
)

print(f"Best iteration: {lgb_clf.best_iteration_}")
print(f"Test Accuracy: {lgb_clf.score(X_test, y_test):.4f}")
```

### CatBoost

**Categorical Boosting** ‚Äî –Ω–∞–π–∫—Ä–∞—â–µ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä—ñ–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö.

```python
from catboost import CatBoostClassifier

# CatBoost Classifier
cat_clf = CatBoostClassifier(
    iterations=100,
    learning_rate=0.1,
    depth=5,
    subsample=0.8,
    l2_leaf_reg=1.0,
    random_state=42,
    verbose=False
)

cat_clf.fit(
    X_train, y_train,
    eval_set=(X_test, y_test),
    early_stopping_rounds=10
)

print(f"Best iteration: {cat_clf.get_best_iteration()}")
print(f"Test Accuracy: {cat_clf.score(X_test, y_test):.4f}")
```

---

## –ü–µ—Ä–µ–≤–∞–≥–∏ —Ç–∞ –Ω–µ–¥–æ–ª—ñ–∫–∏

### –ü–µ—Ä–µ–≤–∞–≥–∏ ‚úì

| –ü–µ—Ä–µ–≤–∞–≥–∞ | –ü–æ—è—Å–Ω–µ–Ω–Ω—è |
|----------|-----------|
| **–ù–∞–π–≤–∏—â–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å** | SOTA –Ω–∞ —Ç–∞–±–ª–∏—á–Ω–∏—Ö –¥–∞–Ω–∏—Ö |
| **–ì–Ω—É—á–∫—ñ—Å—Ç—å** | –†—ñ–∑–Ω—ñ —Ñ—É–Ω–∫—Ü—ñ—ó –≤—Ç—Ä–∞—Ç |
| **Feature importance** | –ê–Ω–∞–ª—ñ–∑ –≤–∞–∂–ª–∏–≤–æ—Å—Ç—ñ –æ–∑–Ω–∞–∫ |
| **Missing values** | –ú–æ–∂–µ –ø—Ä–∞—Ü—é–≤–∞—Ç–∏ (XGBoost, LightGBM) |
| **Regularization** | –ë–∞–≥–∞—Ç–æ —Å–ø–æ—Å–æ–±—ñ–≤ –∫–æ–Ω—Ç—Ä–æ–ª—é overfitting |
| **–ù–µ–ª—ñ–Ω—ñ–π–Ω—ñ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ** | –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –≤–∏—è–≤–ª—è—î |
| **–†–æ–±–∞—Å—Ç–Ω—ñ—Å—Ç—å** | –ú–µ–Ω—à —á—É—Ç–ª–∏–≤—ñ –¥–æ –≤–∏–∫–∏–¥—ñ–≤ –∑–∞ –ª—ñ–Ω—ñ–π–Ω—ñ –º–æ–¥–µ–ª—ñ |

### –ù–µ–¥–æ–ª—ñ–∫–∏ ‚úó

| –ù–µ–¥–æ–ª—ñ–∫ | –ü–æ—è—Å–Ω–µ–Ω–Ω—è |
|---------|-----------|
| **–°–∫–ª–∞–¥–Ω–∏–π tuning** | –ë–∞–≥–∞—Ç–æ –≥—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ |
| **–ü–æ–≤—ñ–ª—å–Ω–µ –Ω–∞–≤—á–∞–Ω–Ω—è** | –ü–æ—Å–ª—ñ–¥–æ–≤–Ω–∞ –ø—Ä–∏—Ä–æ–¥–∞ |
| **Overfitting** | –õ–µ–≥–∫–æ –ø–µ—Ä–µ–æ–±—É—á–∏—Ç–∏—Å—è –±–µ–∑ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—ó |
| **–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å** | –ß–æ—Ä–Ω–∞ —Å–∫—Ä–∏–Ω—å–∫–∞ |
| **–ù–µ –ø–∞—Ä–∞–ª–µ–ª–∏—Ç—å—Å—è** | –ù–∞ –≤—ñ–¥–º—ñ–Ω—É –≤—ñ–¥ Random Forest |
| **–ü–æ—Ç—Ä–µ–±—É—î –ø—ñ–¥–≥–æ—Ç–æ–≤–∫–∏** | –ö–∞—Ç–µ–≥–æ—Ä—ñ–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ, –º–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è (—ñ–Ω–æ–¥—ñ) |
| **–ß—É—Ç–ª–∏–≤—ñ—Å—Ç—å –¥–æ —à—É–º—É** | –£ –¥–∞–Ω–∏—Ö –∑ label noise |

---

## Random Forest vs Gradient Boosting

### –ö–æ–ª–∏ Random Forest –∫—Ä–∞—â–µ?

‚úÖ –ü–æ—Ç—Ä—ñ–±–µ–Ω **—à–≤–∏–¥–∫–∏–π baseline**
‚úÖ –ú–∞–ª–æ —á–∞—Å—É –Ω–∞ tuning
‚úÖ –ü–∞—Ä–∞–ª–µ–ª—ñ–∑–∞—Ü—ñ—è –≤–∞–∂–ª–∏–≤–∞
‚úÖ –î—É–∂–µ —à—É–º–Ω—ñ –¥–∞–Ω—ñ
‚úÖ –†–æ–±–∞—Å—Ç–Ω—ñ—Å—Ç—å > —Ç–æ—á–Ω—ñ—Å—Ç—å

### –ö–æ–ª–∏ Gradient Boosting –∫—Ä–∞—â–µ?

‚úÖ –ü–æ—Ç—Ä—ñ–±–Ω–∞ **–º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å**
‚úÖ –Ñ —á–∞—Å –Ω–∞ fine-tuning
‚úÖ Kaggle competition
‚úÖ Production ML system
‚úÖ –¢–æ—á–Ω—ñ—Å—Ç—å > —à–≤–∏–¥–∫—ñ—Å—Ç—å –Ω–∞–≤—á–∞–Ω–Ω—è

### –ü–æ—Ä—ñ–≤–Ω—è–ª—å–Ω–∞ —Ç–∞–±–ª–∏—Ü—è

| –ö—Ä–∏—Ç–µ—Ä—ñ–π | Random Forest | Gradient Boosting |
|----------|---------------|-------------------|
| **–¢–æ—á–Ω—ñ—Å—Ç—å** | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **–®–≤–∏–¥–∫—ñ—Å—Ç—å –Ω–∞–≤—á–∞–Ω–Ω—è** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |
| **Tuning —Å–∫–ª–∞–¥–Ω—ñ—Å—Ç—å** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê |
| **Overfitting —Ä–∏–∑–∏–∫** | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **–ü–∞—Ä–∞–ª–µ–ª—ñ–∑–∞—Ü—ñ—è** | ‚úÖ –¢–∞–∫ | ‚ùå –ù—ñ (—Å–∫–ª–∞–¥–Ω–æ) |
| **–í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è** | Baseline | Production |

---

## –ü—Ä–∞–∫—Ç–∏—á–Ω—ñ –ø–æ—Ä–∞–¥–∏ üí°

1. **–ü–æ—á–Ω–∏ –∑ –º–∞–ª–æ–≥–æ learning rate** ‚Äî 0.1 —Ç–∞ 100-200 –¥–µ—Ä–µ–≤
2. **–ú—ñ–ª–∫—ñ –¥–µ—Ä–µ–≤–∞** ‚Äî max_depth=3-5 (–Ω–µ —è–∫ —É RF!)
3. **Subsample** ‚Äî 0.8 –¥–ª—è stochastic GB
4. **Early stopping** ‚Äî –∑–∞–≤–∂–¥–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π
5. **Staged predictions** ‚Äî –º–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥ –Ω–∞–≤—á–∞–Ω–Ω—è
6. **XGBoost/LightGBM** –¥–ª—è production ‚Äî —à–≤–∏–¥—à–µ —Ç–∞ —Ç–æ—á–Ω—ñ—à–µ
7. **CatBoost** –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä—ñ–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö
8. **Feature engineering** ‚Äî GB –ª—é–±–∏—Ç—å —è–∫—ñ—Å–Ω—ñ –æ–∑–Ω–∞–∫–∏
9. **Grid/Random Search** ‚Äî —ñ–Ω–≤–µ—Å—Ç—É–π —á–∞—Å —É tuning
10. **–ü–æ—Ä—ñ–≤–Ω—è–π –∑ Random Forest** ‚Äî —ñ–Ω–æ–¥—ñ RF –¥–æ—Å—Ç–∞—Ç–Ω—å–æ

---

## –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ Gradient Boosting

### –Ü–¥–µ–∞–ª—å–Ω–æ –ø—ñ–¥—Ö–æ–¥–∏—Ç—å ‚úì

- **Kaggle competitions** ‚Äî —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è —Ç–∞–±–ª–∏—á–Ω–∏—Ö –¥–∞–Ω–∏—Ö
- **Production ML** –∑ –≤–∏—Å–æ–∫–∏–º–∏ –≤–∏–º–æ–≥–∞–º–∏ –¥–æ —Ç–æ—á–Ω–æ—Å—Ç—ñ
- –°–∫–ª–∞–¥–Ω—ñ –Ω–µ–ª—ñ–Ω—ñ–π–Ω—ñ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ
- –Ñ —á–∞—Å —Ç–∞ —Ä–µ—Å—É—Ä—Å–∏ –¥–ª—è **tuning**
- –¢–∞–±–ª–∏—á–Ω—ñ –¥–∞–Ω—ñ (structured data)
- –ü–æ—Ç—Ä—ñ–±–Ω–∞ **–º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ —è–∫—ñ—Å—Ç—å** –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω—å

### –ö—Ä–∞—â–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ —ñ–Ω—à–µ ‚úó

- **–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å** ‚Üí Decision Tree, Linear Models
- **–®–≤–∏–¥–∫–∏–π baseline** ‚Üí Random Forest
- –ó–æ–±—Ä–∞–∂–µ–Ω–Ω—è/–¢–µ–∫—Å—Ç ‚Üí Deep Learning
- **–î—É–∂–µ –º–∞–ª—ñ –¥–∞–Ω—ñ** ‚Üí Linear Models, SVM
- –ù–µ–º–∞—î —á–∞—Å—É –Ω–∞ tuning ‚Üí Random Forest
- **–†–µ–∞–ª-—Ç–∞–π–º inference** ‚Üí Linear Models

---

## –†–µ–∞–ª—å–Ω–∏–π –ø—Ä–∏–∫–ª–∞–¥: –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –≤—ñ–¥—Ç–æ–∫—É –∫–ª—ñ—î–Ω—Ç—ñ–≤

```python
import pandas as pd
import numpy as np
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import classification_report, roc_auc_score, roc_curve
import matplotlib.pyplot as plt

# –°–∏–Ω—Ç–µ—Ç–∏—á–Ω—ñ –¥–∞–Ω—ñ (customer churn)
np.random.seed(42)
n_samples = 5000

data = {
    'tenure_months': np.random.randint(1, 72, n_samples),
    'monthly_charges': np.random.uniform(20, 120, n_samples),
    'total_charges': np.random.uniform(100, 8000, n_samples),
    'contract_type': np.random.choice(['Month-to-month', 'One year', 'Two year'], n_samples),
    'payment_method': np.random.choice(['Auto', 'Manual'], n_samples),
    'customer_service_calls': np.random.randint(0, 10, n_samples),
    'num_products': np.random.randint(1, 5, n_samples),
}

# Target (—Å–∏–º—É–ª—é—î–º–æ churn)
churn_prob = (
    (data['tenure_months'] < 12) * 0.3 +
    (data['monthly_charges'] > 80) * 0.2 +
    (data['customer_service_calls'] > 5) * 0.3 +
    np.random.uniform(0, 0.2, n_samples)
)
data['churn'] = (churn_prob > 0.5).astype(int)

df = pd.DataFrame(data)

# Encoding –∫–∞—Ç–µ–≥–æ—Ä—ñ–∞–ª—å–Ω–∏—Ö –æ–∑–Ω–∞–∫
df['contract_month_to_month'] = (df['contract_type'] == 'Month-to-month').astype(int)
df['contract_one_year'] = (df['contract_type'] == 'One year').astype(int)
df['payment_auto'] = (df['payment_method'] == 'Auto').astype(int)

# –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö
X = df.drop(['churn', 'contract_type', 'payment_method'], axis=1)
y = df['churn']

print(f"Dataset shape: {X.shape}")
print(f"Churn rate: {y.mean():.2%}")

# –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Gradient Boosting –∑ –æ–ø—Ç–∏–º–∞–ª—å–Ω–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
gb = GradientBoostingClassifier(
    n_estimators=200,
    learning_rate=0.1,
    max_depth=5,
    min_samples_split=20,
    min_samples_leaf=10,
    subsample=0.8,
    max_features='sqrt',
    random_state=42,
    verbose=0
)

# –ù–∞–≤—á–∞–Ω–Ω—è
print("\nTraining Gradient Boosting...")
gb.fit(X_train, y_train)

# Cross-validation
cv_scores = cross_val_score(gb, X_train, y_train, cv=5, scoring='roc_auc')
print(f"CV ROC-AUC: {cv_scores.mean():.4f} (+/- {cv_scores.std()*2:.4f})")

# –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
y_pred = gb.predict(X_test)
y_pred_proba = gb.predict_proba(X_test)[:, 1]

# –ú–µ—Ç—Ä–∏–∫–∏
print("\n" + "="*60)
print("=== Model Performance ===")
print("="*60)
print(f"Train Accuracy: {gb.score(X_train, y_train):.4f}")
print(f"Test Accuracy: {gb.score(X_test, y_test):.4f}")
print(f"ROC-AUC: {roc_auc_score(y_test, y_pred_proba):.4f}")

print("\n" + classification_report(y_test, y_pred, 
                                   target_names=['No Churn', 'Churn']))

# Feature Importance
print("\n" + "="*60)
print("=== Top 5 Most Important Features ===")
print("="*60)
feature_importance = pd.DataFrame({
    'feature': X.columns,
    'importance': gb.feature_importances_
}).sort_values('importance', ascending=False)

for idx, row in feature_importance.head(5).iterrows():
    print(f"{row['feature']}: {row['importance']:.4f}")

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó
fig, axes = plt.subplots(2, 2, figsize=(16, 12))

# 1. Feature Importance
axes[0, 0].barh(feature_importance['feature'][:10][::-1], 
                feature_importance['importance'][:10][::-1])
axes[0, 0].set_xlabel('Importance', fontsize=12)
axes[0, 0].set_title('Top 10 Feature Importances', 
                     fontsize=14, fontweight='bold')
axes[0, 0].grid(True, alpha=0.3, axis='x')

# 2. Learning Curve (Staged Predictions)
train_scores_staged = []
test_scores_staged = []

for train_pred, test_pred in zip(gb.staged_predict_proba(X_train),
                                  gb.staged_predict_proba(X_test)):
    train_scores_staged.append(roc_auc_score(y_train, train_pred[:, 1]))
    test_scores_staged.append(roc_auc_score(y_test, test_pred[:, 1]))

axes[0, 1].plot(range(1, len(train_scores_staged) + 1), 
                train_scores_staged, label='Train', linewidth=2)
axes[0, 1].plot(range(1, len(test_scores_staged) + 1), 
                test_scores_staged, label='Test', linewidth=2)
axes[0, 1].set_xlabel('Number of Trees', fontsize=12)
axes[0, 1].set_ylabel('ROC-AUC', fontsize=12)
axes[0, 1].set_title('Learning Curve', fontsize=14, fontweight='bold')
axes[0, 1].legend(fontsize=11)
axes[0, 1].grid(True, alpha=0.3)

# 3. ROC Curve
fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)
auc = roc_auc_score(y_test, y_pred_proba)

axes[1, 0].plot(fpr, tpr, linewidth=2, label=f'ROC Curve (AUC={auc:.3f})')
axes[1, 0].plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random')
axes[1, 0].set_xlabel('False Positive Rate', fontsize=12)
axes[1, 0].set_ylabel('True Positive Rate', fontsize=12)
axes[1, 0].set_title('ROC Curve', fontsize=14, fontweight='bold')
axes[1, 0].legend(fontsize=11)
axes[1, 0].grid(True, alpha=0.3)

# 4. Prediction Distribution
axes[1, 1].hist(y_pred_proba[y_test == 0], bins=30, alpha=0.6, 
                label='No Churn', color='blue', edgecolor='black')
axes[1, 1].hist(y_pred_proba[y_test == 1], bins=30, alpha=0.6, 
                label='Churn', color='red', edgecolor='black')
axes[1, 1].set_xlabel('Predicted Probability', fontsize=12)
axes[1, 1].set_ylabel('Frequency', fontsize=12)
axes[1, 1].set_title('Prediction Distribution', fontsize=14, fontweight='bold')
axes[1, 1].legend(fontsize=11)
axes[1, 1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

---

## –ü–æ—à–∏—Ä–µ–Ω—ñ –ø–æ–º–∏–ª–∫–∏ ‚ùå

### 1. –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –≥–ª–∏–±–æ–∫–∏—Ö –¥–µ—Ä–µ–≤

```python
# ‚ùå –ù–ï–ü–†–ê–í–ò–õ–¨–ù–û (—è–∫ —É Random Forest)
gb = GradientBoostingClassifier(max_depth=None)

# ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û (–º—ñ–ª–∫—ñ –¥–µ—Ä–µ–≤–∞!)
gb = GradientBoostingClassifier(max_depth=3)  # –∞–±–æ 4, 5
```

### 2. –í–∏—Å–æ–∫–∏–π learning rate –±–µ–∑ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—ó

```python
# ‚ùå –†–ò–ó–ò–ö OVERFITTING
gb = GradientBoostingClassifier(
    learning_rate=1.0,
    n_estimators=50
)

# ‚úÖ –ö–†–ê–©–ï
gb = GradientBoostingClassifier(
    learning_rate=0.1,
    n_estimators=200,
    subsample=0.8
)
```

### 3. –ù–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ early stopping

```python
# ‚ùå –ú–û–ñ–õ–ò–í–ò–ô OVERFITTING
gb = GradientBoostingClassifier(n_estimators=1000)
gb.fit(X_train, y_train)

# ‚úÖ –ó EARLY STOPPING
import xgboost as xgb
xgb_clf = xgb.XGBClassifier(n_estimators=1000)
xgb_clf.fit(
    X_train, y_train,
    eval_set=[(X_test, y_test)],
    early_stopping_rounds=50
)
```

### 4. –ù–µ –º–æ–Ω—ñ—Ç–æ—Ä–∏—Ç–∏ –Ω–∞–≤—á–∞–Ω–Ω—è

```python
# ‚úÖ –ó–ê–í–ñ–î–ò –ú–û–ù–Ü–¢–û–†–ò–¢–ò
train_scores = []
test_scores = []

for pred_train, pred_test in zip(gb.staged_predict(X_train),
                                   gb.staged_predict(X_test)):
    train_scores.append(accuracy_score(y_train, pred_train))
    test_scores.append(accuracy_score(y_test, pred_test))

# –Ø–∫—â–æ test_scores –∑—Ä–æ—Å—Ç–∞—î ‚Üí overfitting!
```

---

## –ü–æ–≤'—è–∑–∞–Ω—ñ —Ç–µ–º–∏

- [[01_Decision_Trees]] ‚Äî –±–∞–∑–æ–≤—ñ –±—É–¥—ñ–≤–µ–ª—å–Ω—ñ –±–ª–æ–∫–∏
- [[02_Random_Forest]] ‚Äî –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∏–π –∞–Ω—Å–∞–º–±–ª—å
- [[04_AdaBoost]] ‚Äî —ñ–Ω—à–∏–π boosting –∞–ª–≥–æ—Ä–∏—Ç–º
- [[05_Ensemble_Methods]] ‚Äî —Ç–µ–æ—Ä—ñ—è –∞–Ω—Å–∞–º–±–ª—ñ–≤
- [[05_Gradient_Descent]] ‚Äî –∫–æ–Ω—Ü–µ–ø—Ü—ñ—è –≥—Ä–∞–¥—ñ—î–Ω—Ç–∞

## –†–µ—Å—É—Ä—Å–∏

- [Scikit-learn: Gradient Boosting](https://scikit-learn.org/stable/modules/ensemble.html#gradient-boosting)
- [XGBoost Documentation](https://xgboost.readthedocs.io/)
- [LightGBM Documentation](https://lightgbm.readthedocs.io/)
- [CatBoost Documentation](https://catboost.ai/docs/)
- [Original Paper: Friedman (2001)](https://jerryfriedman.su.domains/ftp/trebst.pdf)

---

## –ö–ª—é—á–æ–≤—ñ –≤–∏—Å–Ω–æ–≤–∫–∏

> Gradient Boosting –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ –±—É–¥—É—î —Å–ª–∞–±–∫—ñ –º–æ–¥–µ–ª—ñ (–º—ñ–ª–∫—ñ –¥–µ—Ä–µ–≤–∞), –¥–µ –∫–æ–∂–Ω–∞ –Ω–∞—Å—Ç—É–ø–Ω–∞ –º–æ–¥–µ–ª—å –≤–∏–ø—Ä–∞–≤–ª—è—î –ø–æ–º–∏–ª–∫–∏ (residuals) –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ—Ö, —Ä—É—Ö–∞—é—á–∏—Å—å —É –Ω–∞–ø—Ä—è–º–∫—É –≥—Ä–∞–¥—ñ—î–Ω—Ç–∞ —Ñ—É–Ω–∫—Ü—ñ—ó –≤—Ç—Ä–∞—Ç.

**–û—Å–Ω–æ–≤–Ω—ñ –ø—Ä–∏–Ω—Ü–∏–ø–∏:**
- **–ü–æ—Å–ª—ñ–¥–æ–≤–Ω–µ –Ω–∞–≤—á–∞–Ω–Ω—è** ‚Äî –∫–æ–∂–Ω–µ –¥–µ—Ä–µ–≤–æ –∑–∞–ª–µ–∂–∏—Ç—å –≤—ñ–¥ –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ–≥–æ
- **Gradient descent –≤ —Ñ—É–Ω–∫—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ–º—É –ø—Ä–æ—Å—Ç–æ—Ä—ñ** ‚Äî –º—ñ–Ω—ñ–º—ñ–∑–∞—Ü—ñ—è loss —á–µ—Ä–µ–∑ –¥–æ–¥–∞–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª–µ–π
- **Learning rate** –∫–æ–Ω—Ç—Ä–æ–ª—é—î —à–≤–∏–¥–∫—ñ—Å—Ç—å –Ω–∞–≤—á–∞–Ω–Ω—è —Ç–∞ overfitting
- **–ú—ñ–ª–∫—ñ –¥–µ—Ä–µ–≤–∞** (max_depth=3-5) —è–∫ —Å–ª–∞–±–∫—ñ learners

**–§–æ—Ä–º—É–ª–∞:**
$$F_M(x) = F_0(x) + \sum_{m=1}^{M} \nu \cdot h_m(x)$$

**–ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏:**
- –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å –Ω–∞ —Ç–∞–±–ª–∏—á–Ω–∏—Ö –¥–∞–Ω–∏—Ö = Gradient Boosting ‚úì
- –®–≤–∏–¥–∫–∏–π baseline –±–µ–∑ tuning = Random Forest ‚úì
- Production ML –∑ XGBoost/LightGBM = Gradient Boosting ‚úì

**–ù–∞–π–≤–∞–∂–ª–∏–≤—ñ—à–µ:**
- learning_rate=0.1, max_depth=3-5, subsample=0.8
- –ó–∞–≤–∂–¥–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π early stopping
- XGBoost/LightGBM –¥–ª—è production

---

#ml #supervised-learning #ensemble #gradient-boosting #boosting #xgboost #lightgbm #catboost #kaggle #tree-based
