# Decision Trees (–î–µ—Ä–µ–≤–∞ —Ä—ñ—à–µ–Ω—å)

## –©–æ —Ü–µ?

**Decision Trees (–î–µ—Ä–µ–≤–∞ —Ä—ñ—à–µ–Ω—å)** ‚Äî —Ü–µ –∞–ª–≥–æ—Ä–∏—Ç–º supervised learning, —è–∫–∏–π –ø—Ä–∏–π–º–∞—î —Ä—ñ—à–µ–Ω–Ω—è –Ω–∞ –æ—Å–Ω–æ–≤—ñ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ –∑–∞–ø–∏—Ç–∞–Ω—å (—É–º–æ–≤) –ø—Ä–æ –æ–∑–Ω–∞–∫–∏ –¥–∞–Ω–∏—Ö, —Ñ–æ—Ä–º—É—é—á–∏ –¥–µ—Ä–µ–≤–æ–ø–æ–¥—ñ–±–Ω—É —Å—Ç—Ä—É–∫—Ç—É—Ä—É.

**–ì–æ–ª–æ–≤–Ω–∞ —ñ–¥–µ—è:** —Ä–æ–∑–±–∏–≤–∞—Ç–∏ –¥–∞–Ω—ñ –Ω–∞ –ø—ñ–¥–≥—Ä—É–ø–∏ –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ –ø—Ä–æ—Å—Ç–∏—Ö –ø—Ä–∞–≤–∏–ª —Ç–∏–ø—É "—è–∫—â–æ-—Ç–æ" –¥–æ —Ç–∏—Ö –ø—ñ—Ä, –ø–æ–∫–∏ –Ω–µ –æ—Ç—Ä–∏–º–∞—î–º–æ –¥–æ—Å—Ç–∞—Ç–Ω—å–æ –æ–¥–Ω–æ—Ä—ñ–¥–Ω—ñ –≥—Ä—É–ø–∏ –¥–ª—è –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è.

## –ù–∞–≤—ñ—â–æ –ø–æ—Ç—Ä—ñ–±–Ω—ñ?

- üå≥ **–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å** ‚Äî –ª–µ–≥–∫–æ –ø–æ—è—Å–Ω–∏—Ç–∏ —Ä—ñ—à–µ–Ω–Ω—è (–≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –¥–µ—Ä–µ–≤–∞)
- üéØ **–£–Ω—ñ–≤–µ—Ä—Å–∞–ª—å–Ω—ñ—Å—Ç—å** ‚Äî —Ä–µ–≥—Ä–µ—Å—ñ—è —Ç–∞ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è
- üîß **–ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∞ –ø—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö** ‚Äî –Ω–µ –ø–æ—Ç—Ä—ñ–±–Ω–∞ –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è
- üìä **–ù–µ–ª—ñ–Ω—ñ–π–Ω—ñ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ** ‚Äî –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –≤–∏—è–≤–ª—è—î —Å–∫–ª–∞–¥–Ω—ñ –ø–∞—Ç—Ç–µ—Ä–Ω–∏
- üöÄ **–®–≤–∏–¥–∫—ñ—Å—Ç—å** ‚Äî —à–≤–∏–¥–∫—ñ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è (O(log n))
- üí° **Feature importance** ‚Äî –ø–æ–∫–∞–∑—É—î –≤–∞–∂–ª–∏–≤—ñ—Å—Ç—å –æ–∑–Ω–∞–∫

## –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏?

**–ü–æ—Ç—Ä—ñ–±–Ω–æ:**
- **–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å –∫—Ä–∏—Ç–∏—á–Ω–∞** ‚Äî –º–µ–¥–∏—Ü–∏–Ω–∞, –±–∞–Ω–∫—ñ–≤—Å—å–∫–∞ —Å–ø—Ä–∞–≤–∞, —é—Ä–∏—Å–ø—Ä—É–¥–µ–Ω—Ü—ñ—è
- –ö–∞—Ç–µ–≥–æ—Ä—ñ–∞–ª—å–Ω—ñ —Ç–∞ —á–∏—Å–ª–æ–≤—ñ –æ–∑–Ω–∞–∫–∏ —Ä–∞–∑–æ–º
- **–ù–µ–ª—ñ–Ω—ñ–π–Ω—ñ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ** —Ç–∞ —Å–∫–ª–∞–¥–Ω—ñ –≤–∑–∞—î–º–æ–¥—ñ—ó
- –ü–æ—Ç—Ä—ñ–±–Ω–∞ **–≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è** –ø—Ä–æ—Ü–µ—Å—É –ø—Ä–∏–π–Ω—è—Ç—Ç—è —Ä—ñ—à–µ–Ω–Ω—è
- –î–∞–Ω—ñ –º–∞—é—Ç—å **–ø—Ä–∏—Ä–æ–¥–Ω—É —ñ—î—Ä–∞—Ä—Ö—ñ—á–Ω—É —Å—Ç—Ä—É–∫—Ç—É—Ä—É**

**–ù–µ –ø–æ—Ç—Ä—ñ–±–Ω–æ:**
- –ü–æ—Ç—Ä—ñ–±–Ω–∞ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å ‚Üí **Random Forest, Gradient Boosting**
- –õ—ñ–Ω—ñ–π–Ω–∞ –∑–∞–ª–µ–∂–Ω—ñ—Å—Ç—å ‚Üí Linear/Logistic Regression
- –î—É–∂–µ –≤–∏—Å–æ–∫–æ—Ä–æ–∑–º—ñ—Ä–Ω—ñ –¥–∞–Ω—ñ ‚Üí dimensionality reduction + —ñ–Ω—à—ñ –º–µ—Ç–æ–¥–∏

---

## –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–µ—Ä–µ–≤–∞

### –¢–µ—Ä–º—ñ–Ω–æ–ª–æ–≥—ñ—è

```
                    [Root Node]               ‚Üê –ö–æ—Ä—ñ–Ω—å (–≤—Å—è –≤–∏–±—ñ—Ä–∫–∞)
                    Age <= 30?
                   /          \
                 Yes           No
                /                \
        [Internal Node]      [Internal Node]  ‚Üê –í–Ω—É—Ç—Ä—ñ—à–Ω—ñ –≤—É–∑–ª–∏
        Income <= 50k?        Income <= 70k?
         /        \              /         \
       Yes        No           Yes         No
       /            \           /             \
  [Leaf]         [Leaf]     [Leaf]         [Leaf]  ‚Üê –õ–∏—Å—Ç–∫–∏ (–ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è)
  Class: 0       Class: 1   Class: 1       Class: 0
```

**–ö–æ–º–ø–æ–Ω–µ–Ω—Ç–∏:**
- **Root Node** (–∫–æ—Ä—ñ–Ω—å) ‚Äî –ø–µ—Ä—à–∏–π –≤—É–∑–æ–ª, –º—ñ—Å—Ç–∏—Ç—å –≤—Å—ñ –¥–∞–Ω—ñ
- **Internal Nodes** (–≤–Ω—É—Ç—Ä—ñ—à–Ω—ñ –≤—É–∑–ª–∏) ‚Äî —É–º–æ–≤–∏ —Ä–æ–∑–±–∏—Ç—Ç—è
- **Branches** (–≥—ñ–ª–∫–∏) ‚Äî —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ —É–º–æ–≤ (Yes/No)
- **Leaf Nodes** (–ª–∏—Å—Ç–∫–∏) ‚Äî —Ñ—ñ–Ω–∞–ª—å–Ω—ñ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
- **Depth** (–≥–ª–∏–±–∏–Ω–∞) ‚Äî –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –¥–æ–≤–∂–∏–Ω–∞ —à–ª—è—Ö—É –≤—ñ–¥ –∫–æ—Ä–µ–Ω—è –¥–æ –ª–∏—Å—Ç–∫–∞

### –ü—Ä–∏–∫–ª–∞–¥: –°—Ö–≤–∞–ª–µ–Ω–Ω—è –∫—Ä–µ–¥–∏—Ç—É

```
                    –ó–∞—Ä–ø–ª–∞—Ç–∞ <= 50k?
                   /                \
                 –¢–∞–∫                  –ù—ñ
                /                      \
        –ö—Ä–µ–¥–∏—Ç–Ω–∏–π                  –í—ñ–∫ <= 25?
        —Ä–µ–π—Ç–∏–Ω–≥ <= 600?            /        \
         /        \               –¢–∞–∫       –ù—ñ
       –¢–∞–∫        –ù—ñ              /           \
       /            \        –í—ñ–¥–º–æ–≤–∏—Ç–∏    –°—Ö–≤–∞–ª–∏—Ç–∏
  –í—ñ–¥–º–æ–≤–∏—Ç–∏    –°—Ö–≤–∞–ª–∏—Ç–∏    (Class: 0)   (Class: 1)
  (Class: 0)   (Class: 1)
```

**–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü—ñ—è —à–ª—è—Ö—ñ–≤:**
- **–®–ª—è—Ö 1:** –ó–∞—Ä–ø–ª–∞—Ç–∞ > 50k AND –í—ñ–∫ > 25 ‚Üí **–°—Ö–≤–∞–ª–∏—Ç–∏** ‚úì
- **–®–ª—è—Ö 2:** –ó–∞—Ä–ø–ª–∞—Ç–∞ > 50k AND –í—ñ–∫ ‚â§ 25 ‚Üí **–í—ñ–¥–º–æ–≤–∏—Ç–∏** ‚úó
- **–®–ª—è—Ö 3:** –ó–∞—Ä–ø–ª–∞—Ç–∞ ‚â§ 50k AND –†–µ–π—Ç–∏–Ω–≥ > 600 ‚Üí **–°—Ö–≤–∞–ª–∏—Ç–∏** ‚úì
- **–®–ª—è—Ö 4:** –ó–∞—Ä–ø–ª–∞—Ç–∞ ‚â§ 50k AND –†–µ–π—Ç–∏–Ω–≥ ‚â§ 600 ‚Üí **–í—ñ–¥–º–æ–≤–∏—Ç–∏** ‚úó

---

## –Ø–∫ –±—É–¥—É—î—Ç—å—Å—è –¥–µ—Ä–µ–≤–æ?

### –ñ–∞–¥—ñ–±–Ω–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º (Greedy Algorithm)

**–†–µ–∫—É—Ä—Å–∏–≤–Ω–∏–π –ø—Ä–æ—Ü–µ—Å:**

1. **–ü–æ—á–∞—Ç–∏ –∑ –∫–æ—Ä–µ–Ω—è** ‚Äî —É—Å—ñ –¥–∞–Ω—ñ –≤ –æ–¥–Ω–æ–º—É –≤—É–∑–ª—ñ
2. **–ó–Ω–∞–π—Ç–∏ –Ω–∞–π–∫—Ä–∞—â—É –æ–∑–Ω–∞–∫—É —Ç–∞ –ø–æ—Ä—ñ–≥** –¥–ª—è —Ä–æ–∑–±–∏—Ç—Ç—è:
   - –ü–µ—Ä–µ–±—Ä–∞—Ç–∏ –≤—Å—ñ –æ–∑–Ω–∞–∫–∏
   - –î–ª—è –∫–æ–∂–Ω–æ—ó –æ–∑–Ω–∞–∫–∏ –ø–µ—Ä–µ–±—Ä–∞—Ç–∏ –º–æ–∂–ª–∏–≤—ñ –ø–æ—Ä–æ–≥–∏
   - –í–∏–±—Ä–∞—Ç–∏ —Ä–æ–∑–±–∏—Ç—Ç—è –∑ –Ω–∞–π–±—ñ–ª—å—à–∏–º information gain / –Ω–∞–π–º–µ–Ω—à–æ—é impurity
3. **–†–æ–∑–¥—ñ–ª–∏—Ç–∏ –¥–∞–Ω—ñ** –Ω–∞ –¥–≤—ñ –≥—Ä—É–ø–∏ (–ª—ñ–≤—É —Ç–∞ –ø—Ä–∞–≤—É –≥—ñ–ª–∫–∏)
4. **–†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –ø–æ–≤—Ç–æ—Ä–∏—Ç–∏** –¥–ª—è –∫–æ–∂–Ω–æ—ó –≥—ñ–ª–∫–∏
5. **–ó—É–ø–∏–Ω–∏—Ç–∏—Å—è** –∫–æ–ª–∏ –≤–∏–∫–æ–Ω–∞–Ω–æ –∫—Ä–∏—Ç–µ—Ä—ñ–π –∑—É–ø–∏–Ω–∫–∏

### –ö—Ä–∏—Ç–µ—Ä—ñ—ó –∑—É–ø–∏–Ω–∫–∏

–ê–ª–≥–æ—Ä–∏—Ç–º –∑—É–ø–∏–Ω—è—î—Ç—å—Å—è, –∫–æ–ª–∏:

- **–î–æ—Å—è–≥–Ω—É—Ç–∞ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –≥–ª–∏–±–∏–Ω–∞** (`max_depth`)
- **–ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∑—Ä–∞–∑–∫—ñ–≤ —É –≤—É–∑–ª—ñ** (`min_samples_split`)
- **–ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∑—Ä–∞–∑–∫—ñ–≤ —É –ª–∏—Å—Ç–∫—É** (`min_samples_leaf`)
- **–ß–∏—Å—Ç–æ—Ç–∞ 100%** ‚Äî –≤—Å—ñ –∑—Ä–∞–∑–∫–∏ –æ–¥–Ω–æ–≥–æ –∫–ª–∞—Å—É
- **–ù–µ –º–æ–∂–Ω–∞ –ø–æ–∫—Ä–∞—â–∏—Ç–∏** ‚Äî information gain = 0

---

## –ö—Ä–∏—Ç–µ—Ä—ñ—ó —Ä–æ–∑–±–∏—Ç—Ç—è (Splitting Criteria)

### –î–ª—è –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó

## 1. Gini Impurity (Gini Index)

### –§–æ—Ä–º—É–ª–∞

$$\text{Gini}(D) = 1 - \sum_{i=1}^{C} p_i^2$$

–¥–µ:
- $D$ ‚Äî –≤—É–∑–æ–ª (–Ω–∞–±—ñ—Ä –¥–∞–Ω–∏—Ö)
- $C$ ‚Äî –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∫–ª–∞—Å—ñ–≤
- $p_i$ ‚Äî –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å –∫–ª–∞—Å—É $i$ —É –≤—É–∑–ª—ñ

### –î—ñ–∞–ø–∞–∑–æ–Ω

- **Gini = 0** ‚Üí –≤—É–∑–æ–ª **—á–∏—Å—Ç–∏–π** (–≤—Å—ñ –∑—Ä–∞–∑–∫–∏ –æ–¥–Ω–æ–≥–æ –∫–ª–∞—Å—É) ‚úì
- **Gini ‚Üí max** ‚Üí –≤—É–∑–æ–ª **–Ω–µ—á–∏—Å—Ç–∏–π** (—Ä—ñ–≤–Ω–æ–º—ñ—Ä–Ω–∏–π —Ä–æ–∑–ø–æ–¥—ñ–ª –∫–ª–∞—Å—ñ–≤) ‚úó

### –ü—Ä–∏–∫–ª–∞–¥

**–í—É–∑–æ–ª:** 100 –∑—Ä–∞–∑–∫—ñ–≤, 60 –∫–ª–∞—Å—É A, 40 –∫–ª–∞—Å—É B

$$\text{Gini} = 1 - (0.6^2 + 0.4^2) = 1 - (0.36 + 0.16) = 1 - 0.52 = 0.48$$

**–ü—ñ—Å–ª—è —Ä–æ–∑–±–∏—Ç—Ç—è:**

**–õ—ñ–≤–∞ –≥—ñ–ª–∫–∞:** 70 –∑—Ä–∞–∑–∫—ñ–≤, 55 –∫–ª–∞—Å—É A, 15 –∫–ª–∞—Å—É B
$$\text{Gini}_{\text{left}} = 1 - \left(\frac{55}{70}\right)^2 - \left(\frac{15}{70}\right)^2 = 1 - 0.617 - 0.046 = 0.337$$

**–ü—Ä–∞–≤–∞ –≥—ñ–ª–∫–∞:** 30 –∑—Ä–∞–∑–∫—ñ–≤, 5 –∫–ª–∞—Å—É A, 25 –∫–ª–∞—Å—É B
$$\text{Gini}_{\text{right}} = 1 - \left(\frac{5}{30}\right)^2 - \left(\frac{25}{30}\right)^2 = 1 - 0.028 - 0.694 = 0.278$$

**–ó–≤–∞–∂–µ–Ω–∏–π Gini –ø—ñ—Å–ª—è —Ä–æ–∑–±–∏—Ç—Ç—è:**
$$\text{Gini}_{\text{split}} = \frac{70}{100} \times 0.337 + \frac{30}{100} \times 0.278 = 0.236 + 0.083 = 0.319$$

**Information Gain (–∑–º–µ–Ω—à–µ–Ω–Ω—è Gini):**
$$\Delta \text{Gini} = 0.48 - 0.319 = 0.161$$

‚úì –†–æ–∑–±–∏—Ç—Ç—è **–ø–æ–∫—Ä–∞—â—É—î —á–∏—Å—Ç–æ—Ç—É**!

---

## 2. Entropy (Information Gain)

### –§–æ—Ä–º—É–ª–∞ Entropy

$$\text{Entropy}(D) = -\sum_{i=1}^{C} p_i \log_2(p_i)$$

–¥–µ:
- $p_i$ ‚Äî –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å –∫–ª–∞—Å—É $i$

### –î—ñ–∞–ø–∞–∑–æ–Ω

- **Entropy = 0** ‚Üí –≤—É–∑–æ–ª —á–∏—Å—Ç–∏–π (–≤—Å—ñ –æ–¥–Ω–æ–≥–æ –∫–ª–∞—Å—É) ‚úì
- **Entropy ‚Üí max** ‚Üí –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –Ω–µ–≤–∏–∑–Ω–∞—á–µ–Ω—ñ—Å—Ç—å ‚úó

### Information Gain (–Ü–Ω—Ñ–æ—Ä–º–∞—Ü—ñ–π–Ω–∏–π –≤–∏–≥—Ä–∞—à)

$$\text{IG}(D, A) = \text{Entropy}(D) - \sum_{v \in \text{Values}(A)} \frac{|D_v|}{|D|} \text{Entropy}(D_v)$$

–¥–µ:
- $A$ ‚Äî –æ–∑–Ω–∞–∫–∞ –¥–ª—è —Ä–æ–∑–±–∏—Ç—Ç—è
- $D_v$ ‚Äî –ø—ñ–¥–º–Ω–æ–∂–∏–Ω–∞ –¥–∞–Ω–∏—Ö –ø—ñ—Å–ª—è —Ä–æ–∑–±–∏—Ç—Ç—è –∑–∞ –∑–Ω–∞—á–µ–Ω–Ω—è–º $v$

### –ü—Ä–∏–∫–ª–∞–¥

**–¢–æ–π —Å–∞–º–∏–π –≤—É–∑–æ–ª:** 60 A, 40 B

$$\text{Entropy} = -0.6 \log_2(0.6) - 0.4 \log_2(0.4)$$
$$= -0.6 \times (-0.737) - 0.4 \times (-1.322)$$
$$= 0.442 + 0.529 = 0.971$$

**–ü—ñ—Å–ª—è —Ä–æ–∑–±–∏—Ç—Ç—è:**

$$\text{Entropy}_{\text{left}} = -\frac{55}{70} \log_2\left(\frac{55}{70}\right) - \frac{15}{70} \log_2\left(\frac{15}{70}\right) = 0.779$$

$$\text{Entropy}_{\text{right}} = -\frac{5}{30} \log_2\left(\frac{5}{30}\right) - \frac{25}{30} \log_2\left(\frac{25}{30}\right) = 0.650$$

**–ó–≤–∞–∂–µ–Ω–∞ Entropy –ø—ñ—Å–ª—è —Ä–æ–∑–±–∏—Ç—Ç—è:**
$$\text{Entropy}_{\text{split}} = 0.7 \times 0.779 + 0.3 \times 0.650 = 0.545 + 0.195 = 0.740$$

**Information Gain:**
$$\text{IG} = 0.971 - 0.740 = 0.231$$

‚úì –í–∏–±–∏—Ä–∞—î–º–æ —Ä–æ–∑–±–∏—Ç—Ç—è –∑ **–Ω–∞–π–±—ñ–ª—å—à–∏–º IG**!

---

## 3. Gini vs Entropy: —â–æ –≤–∏–±—Ä–∞—Ç–∏?

| –ö—Ä–∏—Ç–µ—Ä—ñ–π | Gini Impurity | Entropy (IG) |
|----------|---------------|--------------|
| **–û–±—á–∏—Å–ª–µ–Ω–Ω—è** | –®–≤–∏–¥—à–µ (–±–µ–∑ –ª–æ–≥–∞—Ä–∏—Ñ–º—ñ–≤) | –ü–æ–≤—ñ–ª—å–Ω—ñ—à–µ |
| **–ß—É—Ç–ª–∏–≤—ñ—Å—Ç—å** | –ú–µ–Ω—à —á—É—Ç–ª–∏–≤–∞ –¥–æ –∑–º—ñ–Ω | –ë—ñ–ª—å—à —á—É—Ç–ª–∏–≤–∞ |
| **–†–µ–∑—É–ª—å—Ç–∞—Ç–∏** | –ó–∞–∑–≤–∏—á–∞–π –¥—É–∂–µ —Å—Ö–æ–∂—ñ | –ó–∞–∑–≤–∏—á–∞–π –¥—É–∂–µ —Å—Ö–æ–∂—ñ |
| **–ó–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º** | **sklearn** | CART, ID3, C4.5 |
| **–í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è** | **–†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–æ** (—à–≤–∏–¥—à–µ) | –¢—Ä–∞–¥–∏—Ü—ñ–π–Ω–∏–π –ø—ñ–¥—Ö—ñ–¥ |

**–í–∏—Å–Ω–æ–≤–æ–∫:** –ù–∞ –ø—Ä–∞–∫—Ç–∏—Ü—ñ —Ä—ñ–∑–Ω–∏—Ü—è –º—ñ–Ω—ñ–º–∞–ª—å–Ω–∞. **Gini** —Ç—Ä–æ—Ö–∏ —à–≤–∏–¥—à–µ, —Ç–æ–º—É –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º —É scikit-learn.

---

### –î–ª—è —Ä–µ–≥—Ä–µ—Å—ñ—ó

## 1. MSE (Mean Squared Error)

### –§–æ—Ä–º—É–ª–∞

$$\text{MSE}(D) = \frac{1}{|D|} \sum_{i \in D} (y_i - \bar{y})^2$$

–¥–µ:
- $\bar{y}$ ‚Äî —Å–µ—Ä–µ–¥–Ω—î –∑–Ω–∞—á–µ–Ω–Ω—è $y$ —É –≤—É–∑–ª—ñ

### –ü—Ä–∏–∫–ª–∞–¥

**–í—É–∑–æ–ª:** [10, 20, 30, 40, 50]
$$\bar{y} = 30$$
$$\text{MSE} = \frac{1}{5}[(10-30)^2 + (20-30)^2 + (30-30)^2 + (40-30)^2 + (50-30)^2]$$
$$= \frac{1}{5}[400 + 100 + 0 + 100 + 400] = \frac{1000}{5} = 200$$

**–ü—ñ—Å–ª—è —Ä–æ–∑–±–∏—Ç—Ç—è (x ‚â§ 25):**

**–õ—ñ–≤–∞:** [10, 20] ‚Üí $\bar{y}_L = 15$ ‚Üí MSE = 25
**–ü—Ä–∞–≤–∞:** [30, 40, 50] ‚Üí $\bar{y}_R = 40$ ‚Üí MSE = 66.67

**–ó–≤–∞–∂–µ–Ω–∞ MSE:**
$$\text{MSE}_{\text{split}} = \frac{2}{5} \times 25 + \frac{3}{5} \times 66.67 = 10 + 40 = 50$$

**–ó–º–µ–Ω—à–µ–Ω–Ω—è MSE:**
$$\Delta \text{MSE} = 200 - 50 = 150$$ ‚úì

---

## 2. MAE (Mean Absolute Error)

### –§–æ—Ä–º—É–ª–∞

$$\text{MAE}(D) = \frac{1}{|D|} \sum_{i \in D} |y_i - \text{median}(D)|$$

**–í—ñ–¥–º—ñ–Ω–Ω—ñ—Å—Ç—å –≤—ñ–¥ MSE:**
- –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î **median** –∑–∞–º—ñ—Å—Ç—å mean
- –ú–µ–Ω—à —á—É—Ç–ª–∏–≤–∞ –¥–æ **outliers**
- **L1** –Ω–æ—Ä–º–∞ –∑–∞–º—ñ—Å—Ç—å L2

### –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ MAE?

‚úÖ –ë–∞–≥–∞—Ç–æ –≤–∏–∫–∏–¥—ñ–≤ —É —Ü—ñ–ª—å–æ–≤—ñ–π –∑–º—ñ–Ω–Ω—ñ–π
‚úÖ –ü–æ—Ç—Ä—ñ–±–Ω–∞ —Ä–æ–±–∞—Å—Ç–Ω—ñ—Å—Ç—å
‚úÖ Outliers –Ω–µ –ø–æ–≤–∏–Ω–Ω—ñ —Å–∏–ª—å–Ω–æ –≤–ø–ª–∏–≤–∞—Ç–∏

---

## –ü—Ä–æ—Å—Ç–∏–π –ø—Ä–∏–∫–ª–∞–¥: –ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è "–ì—Ä–∞—Ç–∏ —É —Ç–µ–Ω—ñ—Å?"

### –î–∞–Ω—ñ

| –î–µ–Ω—å | –ü–æ–≥–æ–¥–∞ | –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ | –í–æ–ª–æ–≥—ñ—Å—Ç—å | –í—ñ—Ç–µ—Ä | –ì—Ä–∞—Ç–∏? |
|------|---------|-------------|-----------|-------|--------|
| 1 | –°–æ–Ω—è—á–Ω–æ | –°–ø–µ–∫–æ—Ç–Ω–æ | –í–∏—Å–æ–∫–∞ | –ù—ñ | –ù—ñ |
| 2 | –°–æ–Ω—è—á–Ω–æ | –°–ø–µ–∫–æ—Ç–Ω–æ | –í–∏—Å–æ–∫–∞ | –¢–∞–∫ | –ù—ñ |
| 3 | –•–º–∞—Ä–Ω–æ | –°–ø–µ–∫–æ—Ç–Ω–æ | –í–∏—Å–æ–∫–∞ | –ù—ñ | –¢–∞–∫ |
| 4 | –î–æ—â | –ü–æ–º—ñ—Ä–Ω–æ | –í–∏—Å–æ–∫–∞ | –ù—ñ | –¢–∞–∫ |
| 5 | –î–æ—â | –ü—Ä–æ—Ö–æ–ª–æ–¥–Ω–æ | –ù–æ—Ä–º–∞ | –ù—ñ | –¢–∞–∫ |
| 6 | –î–æ—â | –ü—Ä–æ—Ö–æ–ª–æ–¥–Ω–æ | –ù–æ—Ä–º–∞ | –¢–∞–∫ | –ù—ñ |
| 7 | –•–º–∞—Ä–Ω–æ | –ü—Ä–æ—Ö–æ–ª–æ–¥–Ω–æ | –ù–æ—Ä–º–∞ | –¢–∞–∫ | –¢–∞–∫ |
| 8 | –°–æ–Ω—è—á–Ω–æ | –ü–æ–º—ñ—Ä–Ω–æ | –í–∏—Å–æ–∫–∞ | –ù—ñ | –ù—ñ |
| 9 | –°–æ–Ω—è—á–Ω–æ | –ü—Ä–æ—Ö–æ–ª–æ–¥–Ω–æ | –ù–æ—Ä–º–∞ | –ù—ñ | –¢–∞–∫ |
| 10 | –î–æ—â | –ü–æ–º—ñ—Ä–Ω–æ | –ù–æ—Ä–º–∞ | –ù—ñ | –¢–∞–∫ |

**–†–æ–∑–ø–æ–¥—ñ–ª:** 5 "–¢–∞–∫", 5 "–ù—ñ"

### –ö—Ä–æ–∫ 1: –û–±—á–∏—Å–ª–∏—Ç–∏ Entropy –∫–æ—Ä–µ–Ω—è

$$\text{Entropy}_{\text{root}} = -\frac{5}{10} \log_2\left(\frac{5}{10}\right) - \frac{5}{10} \log_2\left(\frac{5}{10}\right) = 1.0$$

### –ö—Ä–æ–∫ 2: –ó–Ω–∞–π—Ç–∏ –Ω–∞–π–∫—Ä–∞—â—É –æ–∑–Ω–∞–∫—É

**–°–ø—Ä–æ–±—É—î–º–æ "–ü–æ–≥–æ–¥–∞":**

**–°–æ–Ω—è—á–Ω–æ** (3 –¥–Ω—ñ): 1 "–¢–∞–∫", 2 "–ù—ñ"
$$\text{Entropy}_{\text{–°–æ–Ω—è—á–Ω–æ}} = -\frac{1}{3} \log_2\left(\frac{1}{3}\right) - \frac{2}{3} \log_2\left(\frac{2}{3}\right) = 0.918$$

**–•–º–∞—Ä–Ω–æ** (2 –¥–Ω—ñ): 2 "–¢–∞–∫", 0 "–ù—ñ"
$$\text{Entropy}_{\text{–•–º–∞—Ä–Ω–æ}} = 0$$ (—á–∏—Å—Ç–æ!)

**–î–æ—â** (5 –¥–Ω—ñ–≤): 2 "–¢–∞–∫", 3 "–ù—ñ"
$$\text{Entropy}_{\text{–î–æ—â}} = -\frac{2}{5} \log_2\left(\frac{2}{5}\right) - \frac{3}{5} \log_2\left(\frac{3}{5}\right) = 0.971$$

**–ó–≤–∞–∂–µ–Ω–∞ Entropy:**
$$\text{Entropy}_{\text{split}} = \frac{3}{10} \times 0.918 + \frac{2}{10} \times 0 + \frac{5}{10} \times 0.971 = 0.761$$

**Information Gain:**
$$\text{IG}_{\text{–ü–æ–≥–æ–¥–∞}} = 1.0 - 0.761 = 0.239$$

–ê–Ω–∞–ª–æ–≥—ñ—á–Ω–æ –æ–±—á–∏—Å–ª—é—î–º–æ –¥–ª—è —ñ–Ω—à–∏—Ö –æ–∑–Ω–∞–∫ —ñ –≤–∏–±–∏—Ä–∞—î–º–æ –∑ –Ω–∞–π–±—ñ–ª—å—à–∏–º IG.

### –†–µ–∑—É–ª—å—Ç–∞—Ç –¥–µ—Ä–µ–≤–∞

```
                    –ü–æ–≥–æ–¥–∞?
                 /      |      \
            –°–æ–Ω—è—á–Ω–æ  –•–º–∞—Ä–Ω–æ    –î–æ—â
               |        |        |
           –í–æ–ª–æ–≥—ñ—Å—Ç—å  [–¢–∞–∫]   –í—ñ—Ç–µ—Ä?
            /    \            /    \
        –í–∏—Å–æ–∫–∞ –ù–æ—Ä–º–∞        –ù—ñ    –¢–∞–∫
          |      |          |      |
        [–ù—ñ]   [–¢–∞–∫]      [–¢–∞–∫]  [–ù—ñ]
```

---

## –°–∫–ª–∞–¥–Ω–∏–π –ø—Ä–∏–∫–ª–∞–¥: –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –∑–∞—Ä–ø–ª–∞—Ç–∏

### –î–∞–Ω—ñ

200 –ø—Ä–∞—Ü—ñ–≤–Ω–∏–∫—ñ–≤ –∑ –æ–∑–Ω–∞–∫–∞–º–∏:

| –û–∑–Ω–∞–∫–∞ | –¢–∏–ø | –î—ñ–∞–ø–∞–∑–æ–Ω |
|--------|-----|----------|
| Years_Experience | –ß–∏—Å–ª–æ–≤–∞ | 0-20 |
| Education_Level | –ö–∞—Ç–µ–≥–æ—Ä—ñ–∞–ª—å–Ω–∞ | [School, Bachelor, Master, PhD] |
| Age | –ß–∏—Å–ª–æ–≤–∞ | 22-65 |
| City | –ö–∞—Ç–µ–≥–æ—Ä—ñ–∞–ª—å–Ω–∞ | [Kyiv, Lviv, Dnipro] |
| **Salary** | –ß–∏—Å–ª–æ–≤–∞ (target) | 20-200 —Ç–∏—Å. $ |

### –î–µ—Ä–µ–≤–æ —Ä–µ–≥—Ä–µ—Å—ñ—ó

```
                Years_Experience <= 5?
                /                    \
              –¢–∞–∫                     –ù—ñ
             /                          \
    Education = School?          Years_Experience <= 10?
      /              \              /                  \
    –¢–∞–∫              –ù—ñ           –¢–∞–∫                  –ù—ñ
     |                |            |                    |
  [Pred: 35k]    [Pred: 55k]  [Pred: 85k]         Education = PhD?
                                                    /            \
                                                  –¢–∞–∫            –ù—ñ
                                                   |              |
                                               [Pred: 150k]   [Pred: 110k]
```

**–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü—ñ—è:**
- **–î–æ—Å–≤—ñ–¥ ‚â§ 5 —Ä–æ–∫—ñ–≤ + School ‚Üí 35k**
- **–î–æ—Å–≤—ñ–¥ ‚â§ 5 —Ä–æ–∫—ñ–≤ + –≤–∏—â–∞ –æ—Å–≤—ñ—Ç–∞ ‚Üí 55k**
- **–î–æ—Å–≤—ñ–¥ 5-10 —Ä–æ–∫—ñ–≤ ‚Üí 85k**
- **–î–æ—Å–≤—ñ–¥ > 10 —Ä–æ–∫—ñ–≤ + PhD ‚Üí 150k**
- **–î–æ—Å–≤—ñ–¥ > 10 —Ä–æ–∫—ñ–≤ + —ñ–Ω—à–∞ –æ—Å–≤—ñ—Ç–∞ ‚Üí 110k**

---

## –ö–æ–¥ (Python + scikit-learn)

### –ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# 1. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö
iris = load_iris()
X = iris.data
y = iris.target

# –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# 2. –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ
dt_clf = DecisionTreeClassifier(
    criterion='gini',        # –∞–±–æ 'entropy'
    max_depth=3,             # –û–±–º–µ–∂–µ–Ω–Ω—è –≥–ª–∏–±–∏–Ω–∏
    min_samples_split=2,     # –ú—ñ–Ω. –∑—Ä–∞–∑–∫—ñ–≤ –¥–ª—è —Ä–æ–∑–±–∏—Ç—Ç—è
    min_samples_leaf=1,      # –ú—ñ–Ω. –∑—Ä–∞–∑–∫—ñ–≤ —É –ª–∏—Å—Ç–∫—É
    random_state=42
)

# 3. –ù–∞–≤—á–∞–Ω–Ω—è
dt_clf.fit(X_train, y_train)

# 4. –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
y_pred = dt_clf.predict(X_test)

# 5. –û—Ü—ñ–Ω–∫–∞
print("=== Metrics ===")
print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")
print("\nClassification Report:")
print(classification_report(y_test, y_pred, target_names=iris.target_names))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
print("\nConfusion Matrix:")
print(cm)

# 6. –í–∞–∂–ª–∏–≤—ñ—Å—Ç—å –æ–∑–Ω–∞–∫
print("\n=== Feature Importance ===")
for name, importance in zip(iris.feature_names, dt_clf.feature_importances_):
    print(f"{name}: {importance:.4f}")

# 7. –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –¥–µ—Ä–µ–≤–∞
plt.figure(figsize=(20, 10))
plot_tree(
    dt_clf,
    feature_names=iris.feature_names,
    class_names=iris.target_names,
    filled=True,
    rounded=True,
    fontsize=10
)
plt.title('Decision Tree - Iris Classification', fontsize=16, fontweight='bold')
plt.tight_layout()
plt.show()
```

### –†–µ–≥—Ä–µ—Å—ñ—è

```python
from sklearn.tree import DecisionTreeRegressor
from sklearn.datasets import make_regression
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

# –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –¥–∞–Ω–∏—Ö
X, y = make_regression(
    n_samples=200,
    n_features=1,
    noise=20,
    random_state=42
)

# –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# –ú–æ–¥–µ–ª—å
dt_reg = DecisionTreeRegressor(
    criterion='squared_error',  # –∞–±–æ 'absolute_error', 'friedman_mse'
    max_depth=5,
    min_samples_split=10,
    min_samples_leaf=5,
    random_state=42
)

# –ù–∞–≤—á–∞–Ω–Ω—è
dt_reg.fit(X_train, y_train)

# –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
y_pred = dt_reg.predict(X_test)

# –û—Ü—ñ–Ω–∫–∞
print("=== Regression Metrics ===")
print(f"R¬≤: {r2_score(y_test, y_pred):.4f}")
print(f"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.2f}")
print(f"MAE: {mean_absolute_error(y_test, y_pred):.2f}")

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
X_plot = np.linspace(X.min(), X.max(), 300).reshape(-1, 1)
y_plot = dt_reg.predict(X_plot)

plt.figure(figsize=(12, 6))
plt.scatter(X_train, y_train, alpha=0.5, s=30, label='Train', color='blue')
plt.scatter(X_test, y_test, alpha=0.5, s=50, label='Test', color='green')
plt.plot(X_plot, y_plot, color='red', linewidth=2, label='Decision Tree')
plt.xlabel('X', fontsize=12)
plt.ylabel('y', fontsize=12)
plt.title('Decision Tree Regression', fontsize=14, fontweight='bold')
plt.legend(fontsize=11)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

---

## –ì—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∏

### –û—Å–Ω–æ–≤–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –∫–æ–Ω—Ç—Ä–æ–ª—é —Å–∫–ª–∞–¥–Ω–æ—Å—Ç—ñ

| –ü–∞—Ä–∞–º–µ—Ç—Ä | –û–ø–∏—Å | –¢–∏–ø–æ–≤—ñ –∑–Ω–∞—á–µ–Ω–Ω—è | –ï—Ñ–µ–∫—Ç |
|----------|------|-----------------|-------|
| **max_depth** | –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –≥–ª–∏–±–∏–Ω–∞ –¥–µ—Ä–µ–≤–∞ | 3-10 (–∞–±–æ None) | –û–±–º–µ–∂—É—î —Å–∫–ª–∞–¥–Ω—ñ—Å—Ç—å |
| **min_samples_split** | –ú—ñ–Ω. –∑—Ä–∞–∑–∫—ñ–≤ –¥–ª—è —Ä–æ–∑–±–∏—Ç—Ç—è | 2-20 | –ó–∞–ø–æ–±—ñ–≥–∞—î overfitting |
| **min_samples_leaf** | –ú—ñ–Ω. –∑—Ä–∞–∑–∫—ñ–≤ —É –ª–∏—Å—Ç–∫—É | 1-10 | –ó–≥–ª–∞–¥–∂—É—î –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è |
| **max_features** | –ú–∞–∫—Å. –æ–∑–Ω–∞–∫ –ø—Ä–∏ —Ä–æ–∑–±–∏—Ç—Ç—ñ | 'sqrt', 'log2', None | Feature subsampling |
| **max_leaf_nodes** | –ú–∞–∫—Å. –∫—ñ–ª—å–∫—ñ—Å—Ç—å –ª–∏—Å—Ç–∫—ñ–≤ | 10-100 | –û–±–º–µ–∂—É—î —Å–∫–ª–∞–¥–Ω—ñ—Å—Ç—å |

### –ö—Ä–∏—Ç–µ—Ä—ñ—ó —Ä–æ–∑–±–∏—Ç—Ç—è

| –ü–∞—Ä–∞–º–µ—Ç—Ä | –ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è | –†–µ–≥—Ä–µ—Å—ñ—è |
|----------|--------------|----------|
| **criterion** | 'gini', 'entropy' | 'squared_error', 'absolute_error' |

### –Ü–Ω—à—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏

```python
DecisionTreeClassifier(
    splitter='best',           # –∞–±–æ 'random' (–¥–ª—è —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–Ω–æ—Å—Ç—ñ)
    class_weight='balanced',   # –í–∞–≥–∏ –∫–ª–∞—Å—ñ–≤ –¥–ª—è –Ω–µ–∑–±–∞–ª–∞–Ω—Å–æ–≤–∞–Ω–∏—Ö –¥–∞–Ω–∏—Ö
    min_impurity_decrease=0.0, # –ú—ñ–Ω. –∑–º–µ–Ω—à–µ–Ω–Ω—è impurity –¥–ª—è —Ä–æ–∑–±–∏—Ç—Ç—è
    ccp_alpha=0.0              # Cost-complexity pruning
)
```

---

## –ü—ñ–¥–±—ñ—Ä –≥—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤

### Grid Search CV

```python
from sklearn.model_selection import GridSearchCV

# –°—ñ—Ç–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤
param_grid = {
    'max_depth': [3, 5, 7, 10, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'criterion': ['gini', 'entropy']
}

# Grid Search
grid_search = GridSearchCV(
    DecisionTreeClassifier(random_state=42),
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    verbose=1
)

grid_search.fit(X_train, y_train)

# –ö—Ä–∞—â—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏
print("Best parameters:")
print(grid_search.best_params_)
print(f"\nBest cross-validation score: {grid_search.best_score_:.4f}")

# –ö—Ä–∞—â–∞—è –º–æ–¥–µ–ª—å
best_dt = grid_search.best_estimator_

# –û—Ü—ñ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç—ñ
test_score = best_dt.score(X_test, y_test)
print(f"Test score: {test_score:.4f}")
```

### Randomized Search (—à–≤–∏–¥—à–µ)

```python
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint

# –†–æ–∑–ø–æ–¥—ñ–ª–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤
param_distributions = {
    'max_depth': randint(3, 20),
    'min_samples_split': randint(2, 20),
    'min_samples_leaf': randint(1, 10),
    'criterion': ['gini', 'entropy']
}

# Randomized Search
random_search = RandomizedSearchCV(
    DecisionTreeClassifier(random_state=42),
    param_distributions,
    n_iter=50,              # –ö—ñ–ª—å–∫—ñ—Å—Ç—å –∫–æ–º–±—ñ–Ω–∞—Ü—ñ–π
    cv=5,
    scoring='accuracy',
    random_state=42,
    n_jobs=-1
)

random_search.fit(X_train, y_train)
print("Best parameters:", random_search.best_params_)
```

---

## Overfitting —Ç–∞ Pruning

### –ü—Ä–æ–±–ª–µ–º–∞ Overfitting

**–ë–µ–∑ –æ–±–º–µ–∂–µ–Ω—å –¥–µ—Ä–µ–≤–æ –º–æ–∂–µ:**
- –†–æ–∑–¥—ñ–ª—è—Ç–∏—Å—è –¥–æ —Ç–∏—Ö –ø—ñ—Ä, –ø–æ–∫–∏ –∫–æ–∂–µ–Ω –ª–∏—Å—Ç –Ω–µ –º–∞—Ç–∏–º–µ 1 –∑—Ä–∞–∑–æ–∫
- –Ü–¥–µ–∞–ª—å–Ω–æ "–∑–∞–ø–∞–º'—è—Ç–∞—î" —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ (Train Acc = 100%)
- –ü–æ–≥–∞–Ω–æ —É–∑–∞–≥–∞–ª—å–Ω—é–≤–∞—Ç–∏–º–µ—Ç—å—Å—è –Ω–∞ –Ω–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö

```
Overfitted Tree:

Train Accuracy: 100%
Test Accuracy: 65%  ‚Üê –ü–û–ì–ê–ù–û!

–î–µ—Ä–µ–≤–æ –¥—É–∂–µ –≥–ª–∏–±–æ–∫–µ –∑ –±–∞–≥–∞—Ç—å–º–∞ –ª–∏—Å—Ç–∫–∞–º–∏,
–∫–æ–∂–µ–Ω –∑ —è–∫–∏—Ö –ø—Ä–∏—Å—Ç–æ—Å–æ–≤–∞–Ω–∏–π –¥–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∏—Ö –ø—Ä–∏–∫–ª–∞–¥—ñ–≤.
```

### Pre-Pruning (–û–±–º–µ–∂–µ–Ω–Ω—è –ø—ñ–¥ —á–∞—Å –ø–æ–±—É–¥–æ–≤–∏)

**–í—Å—Ç–∞–Ω–æ–≤–ª—é—î–º–æ –æ–±–º–µ–∂–µ–Ω–Ω—è –î–û –Ω–∞–≤—á–∞–Ω–Ω—è:**

```python
# –û–±–º–µ–∂–µ–Ω–Ω—è —Å–∫–ª–∞–¥–Ω–æ—Å—Ç—ñ
dt = DecisionTreeClassifier(
    max_depth=5,              # –ú–∞–∫—Å. –≥–ª–∏–±–∏–Ω–∞
    min_samples_split=10,     # –ú—ñ–Ω. –∑—Ä–∞–∑–∫—ñ–≤ –¥–ª—è —Ä–æ–∑–±–∏—Ç—Ç—è
    min_samples_leaf=5,       # –ú—ñ–Ω. –∑—Ä–∞–∑–∫—ñ–≤ —É –ª–∏—Å—Ç–∫—É
    max_leaf_nodes=20         # –ú–∞–∫—Å. –ª–∏—Å—Ç–∫—ñ–≤
)
```

### Post-Pruning (Cost-Complexity Pruning)

**–°–ø–æ—á–∞—Ç–∫—É –±—É–¥—É—î–º–æ –ø–æ–≤–Ω–µ –¥–µ—Ä–µ–≤–æ, –ø–æ—Ç—ñ–º "–æ–±—Ä—ñ–∑–∞—î–º–æ" –Ω–µ–ø–æ—Ç—Ä—ñ–±–Ω—ñ –≥—ñ–ª–∫–∏:**

```python
# 1. –ù–∞–≤—á–∏—Ç–∏ –ø–æ–≤–Ω–µ –¥–µ—Ä–µ–≤–æ
dt_full = DecisionTreeClassifier(random_state=42)
dt_full.fit(X_train, y_train)

# 2. –ó–Ω–∞–π—Ç–∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω–∏–π ccp_alpha —á–µ—Ä–µ–∑ cross-validation
path = dt_full.cost_complexity_pruning_path(X_train, y_train)
ccp_alphas = path.ccp_alphas
impurities = path.impurities

# 3. –¢—Ä–µ–Ω—É–≤–∞—Ç–∏ –¥–µ—Ä–µ–≤–∞ –∑ —Ä—ñ–∑–Ω–∏–º–∏ alpha
train_scores = []
test_scores = []

for ccp_alpha in ccp_alphas:
    dt = DecisionTreeClassifier(random_state=42, ccp_alpha=ccp_alpha)
    dt.fit(X_train, y_train)
    train_scores.append(dt.score(X_train, y_train))
    test_scores.append(dt.score(X_test, y_test))

# 4. –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Alpha vs Accuracy
axes[0].plot(ccp_alphas, train_scores, marker='o', label='Train', linewidth=2)
axes[0].plot(ccp_alphas, test_scores, marker='s', label='Test', linewidth=2)
axes[0].set_xlabel('ccp_alpha', fontsize=12)
axes[0].set_ylabel('Accuracy', fontsize=12)
axes[0].set_title('Accuracy vs ccp_alpha', fontsize=14, fontweight='bold')
axes[0].legend(fontsize=11)
axes[0].grid(True, alpha=0.3)

# Alpha vs Tree Size
node_counts = [dt.tree_.node_count for dt in 
               [DecisionTreeClassifier(random_state=42, ccp_alpha=alpha).fit(X_train, y_train) 
                for alpha in ccp_alphas]]

axes[1].plot(ccp_alphas, node_counts, marker='o', linewidth=2)
axes[1].set_xlabel('ccp_alpha', fontsize=12)
axes[1].set_ylabel('Number of Nodes', fontsize=12)
axes[1].set_title('Tree Size vs ccp_alpha', fontsize=14, fontweight='bold')
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# 5. –í–∏–±—Ä–∞—Ç–∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω–∏–π alpha
optimal_idx = np.argmax(test_scores)
optimal_alpha = ccp_alphas[optimal_idx]
print(f"Optimal ccp_alpha: {optimal_alpha:.6f}")

# 6. –§—ñ–Ω–∞–ª—å–Ω–∞ –º–æ–¥–µ–ª—å
dt_pruned = DecisionTreeClassifier(random_state=42, ccp_alpha=optimal_alpha)
dt_pruned.fit(X_train, y_train)
print(f"Test Accuracy (pruned): {dt_pruned.score(X_test, y_test):.4f}")
```

---

## Feature Importance

### –Ø–∫ –æ–±—á–∏—Å–ª—é—î—Ç—å—Å—è?

**–í–∞–∂–ª–∏–≤—ñ—Å—Ç—å –æ–∑–Ω–∞–∫–∏** = —Å—É–º–∞ –∑–º–µ–Ω—à–µ–Ω—å impurity, –∑–≤–∞–∂–µ–Ω–∏—Ö –Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∑—Ä–∞–∑–∫—ñ–≤:

$$\text{Importance}(f) = \frac{\sum_{t \in \text{splits using } f} n_t \Delta I_t}{\sum_{t \in \text{all splits}} n_t \Delta I_t}$$

–¥–µ:
- $n_t$ ‚Äî –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∑—Ä–∞–∑–∫—ñ–≤ —É –≤—É–∑–ª—ñ $t$
- $\Delta I_t$ ‚Äî –∑–º–µ–Ω—à–µ–Ω–Ω—è impurity –ø—ñ—Å–ª—è —Ä–æ–∑–±–∏—Ç—Ç—è

**–í–ª–∞—Å—Ç–∏–≤–æ—Å—Ç—ñ:**
- –°—É–º–∞ –≤—Å—ñ—Ö importance = 1.0
- –í–∏—â–∞ importance ‚Üí –æ–∑–Ω–∞–∫–∞ –≤–∞–∂–ª–∏–≤—ñ—à–∞ –¥–ª—è –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω—å
- –í–∞–∂–ª–∏–≤—ñ—Å—Ç—å 0 ‚Üí –æ–∑–Ω–∞–∫–∞ –Ω–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞–ª–∞—Å—å

### –ö–æ–¥

```python
# Feature Importance
importances = dt_clf.feature_importances_
indices = np.argsort(importances)[::-1]

print("Feature ranking:")
for i in range(X.shape[1]):
    print(f"{i+1}. {iris.feature_names[indices[i]]}: {importances[indices[i]]:.4f}")

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
plt.figure(figsize=(10, 6))
plt.bar(range(X.shape[1]), importances[indices], align='center')
plt.xticks(range(X.shape[1]), [iris.feature_names[i] for i in indices], rotation=45)
plt.xlabel('Feature', fontsize=12)
plt.ylabel('Importance', fontsize=12)
plt.title('Feature Importances', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.show()
```

---

## –ü–µ—Ä–µ–≤–∞–≥–∏ —Ç–∞ –Ω–µ–¥–æ–ª—ñ–∫–∏

### –ü–µ—Ä–µ–≤–∞–≥–∏ ‚úì

| –ü–µ—Ä–µ–≤–∞–≥–∞ | –ü–æ—è—Å–Ω–µ–Ω–Ω—è |
|----------|-----------|
| **–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å** | –õ–µ–≥–∫–æ –≤—ñ–∑—É–∞–ª—ñ–∑—É–≤–∞—Ç–∏ —Ç–∞ –ø–æ—è—Å–Ω–∏—Ç–∏ |
| **–ù–µ –ø–æ—Ç—Ä—ñ–±–Ω–∞ –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è** | –ü—Ä–∞—Ü—é—î –∑ –æ–∑–Ω–∞–∫–∞–º–∏ —Ä—ñ–∑–Ω–∏—Ö –º–∞—Å—à—Ç–∞–±—ñ–≤ |
| **–ö–∞—Ç–µ–≥–æ—Ä—ñ–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ** | –û–±—Ä–æ–±–ª—è—î –±–µ–∑ One-Hot Encoding |
| **–ù–µ–ª—ñ–Ω—ñ–π–Ω—ñ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ** | –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –≤–∏—è–≤–ª—è—î |
| **Feature interactions** | –í–∏—è–≤–ª—è—î –≤–∑–∞—î–º–æ–¥—ñ—ó –º—ñ–∂ –æ–∑–Ω–∞–∫–∞–º–∏ |
| **–®–≤–∏–¥–∫—ñ—Å—Ç—å –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è** | O(log n) ‚Äî –¥—É–∂–µ —à–≤–∏–¥–∫–æ |
| **Feature importance** | –ü–æ–∫–∞–∑—É—î –≤–∞–∂–ª–∏–≤—ñ—Å—Ç—å –æ–∑–Ω–∞–∫ |
| **–£–Ω—ñ–≤–µ—Ä—Å–∞–ª—å–Ω—ñ—Å—Ç—å** | –†–µ–≥—Ä–µ—Å—ñ—è —Ç–∞ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è |
| **–†–æ–±–∞—Å—Ç–Ω—ñ—Å—Ç—å –¥–æ –≤–∏–∫–∏–¥—ñ–≤** | –ú–µ–Ω—à —á—É—Ç–ª–∏–≤—ñ –∑–∞ –ª—ñ–Ω—ñ–π–Ω—ñ –º–æ–¥–µ–ª—ñ |

### –ù–µ–¥–æ–ª—ñ–∫–∏ ‚úó

| –ù–µ–¥–æ–ª—ñ–∫ | –ü–æ—è—Å–Ω–µ–Ω–Ω—è |
|---------|-----------|
| **Overfitting** | –õ–µ–≥–∫–æ –ø–µ—Ä–µ–Ω–∞–≤—á–∞—é—Ç—å—Å—è –±–µ–∑ –æ–±–º–µ–∂–µ–Ω—å |
| **–ù–µ—Å—Ç–∞–±—ñ–ª—å–Ω—ñ—Å—Ç—å** | –ú–∞–ª—ñ –∑–º—ñ–Ω–∏ –¥–∞–Ω–∏—Ö ‚Üí —Ä—ñ–∑–Ω—ñ –¥–µ—Ä–µ–≤–∞ |
| **Bias –≤ –æ–∑–Ω–∞–∫–∞—Ö** | –ù–∞–¥–∞—î –ø–µ—Ä–µ–≤–∞–≥—É –æ–∑–Ω–∞–∫–∞–º –∑ –±—ñ–ª—å—à–µ –∑–Ω–∞—á–µ–Ω—å |
| **–ù–µ –µ–∫—Å—Ç—Ä–∞–ø–æ–ª—é—é—Ç—å** | –ü–æ–≥–∞–Ω–æ –∑–∞ –º–µ–∂–∞–º–∏ —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö |
| **–õ—ñ–Ω—ñ–π–Ω—ñ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ** | –ù–µ–µ—Ñ–µ–∫—Ç–∏–≤–Ω—ñ –¥–ª—è –ø—Ä–æ—Å—Ç–∏—Ö –ª—ñ–Ω—ñ–π–Ω–∏—Ö –ø–∞—Ç—Ç–µ—Ä–Ω—ñ–≤ |
| **XOR problem** | –°–∫–ª–∞–¥–Ω–æ –∑ –¥—ñ–∞–≥–æ–Ω–∞–ª—å–Ω–∏–º–∏ –≥—Ä–∞–Ω–∏—Ü—è–º–∏ |
| **–¢–æ—á–Ω—ñ—Å—Ç—å** | –ü–æ—Å—Ç—É–ø–∞—é—Ç—å—Å—è –∞–Ω—Å–∞–º–±–ª—è–º (RF, GBM) |

---

## –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –∑ —ñ–Ω—à–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏

| –ú–æ–¥–µ–ª—å | –Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å | –¢–æ—á–Ω—ñ—Å—Ç—å | –®–≤–∏–¥–∫—ñ—Å—Ç—å | –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö |
|--------|-------------------|----------|-----------|------------------|
| **Decision Tree** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| Linear Regression | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |
| Random Forest | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| Gradient Boosting | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| Neural Networks | ‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê‚≠ê |

---

## –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ Decision Trees

### –Ü–¥–µ–∞–ª—å–Ω–æ –ø—ñ–¥—Ö–æ–¥–∏—Ç—å ‚úì

- **–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å –∫—Ä–∏—Ç–∏—á–Ω–∞** ‚Äî –º–µ–¥–∏—Ü–∏–Ω–∞, —Ñ—ñ–Ω–∞–Ω—Å–∏, —é—Ä–∏—Å–ø—Ä—É–¥–µ–Ω—Ü—ñ—è
- –ü–æ—Ç—Ä—ñ–±–Ω–∞ **–≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è** –ø—Ä–æ—Ü–µ—Å—É –ø—Ä–∏–π–Ω—è—Ç—Ç—è —Ä—ñ—à–µ–Ω—å
- –ö–∞—Ç–µ–≥–æ—Ä—ñ–∞–ª—å–Ω—ñ + —á–∏—Å–ª–æ–≤—ñ –æ–∑–Ω–∞–∫–∏
- **–®–≤–∏–¥–∫–∏–π baseline** –ø–µ—Ä–µ–¥ –∞–Ω—Å–∞–º–±–ª—è–º–∏
- –ú–∞–ª—ñ/—Å–µ—Ä–µ–¥–Ω—ñ –¥–∞—Ç–∞—Å–µ—Ç–∏
- **–î–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö** ‚Äî feature importance

### –ö—Ä–∞—â–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ —ñ–Ω—à–µ ‚úó

- –ü–æ—Ç—Ä—ñ–±–Ω–∞ **–º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å** ‚Üí **Random Forest, XGBoost**
- –õ—ñ–Ω—ñ–π–Ω–∞ –∑–∞–ª–µ–∂–Ω—ñ—Å—Ç—å ‚Üí Linear/Logistic Regression
- –î—É–∂–µ –≤–µ–ª–∏–∫—ñ –¥–∞–Ω—ñ ‚Üí Linear models, Neural Networks
- –ü–æ—Ç—Ä—ñ–±–Ω–∞ **—Å—Ç–∞–±—ñ–ª—å–Ω—ñ—Å—Ç—å** ‚Üí –ê–Ω—Å–∞–º–±–ª—ñ

---

## –ü—Ä–∞–∫—Ç–∏—á–Ω—ñ –ø–æ—Ä–∞–¥–∏ üí°

1. **–ü–æ—á–Ω–∏ –∑ –æ–±–º–µ–∂–µ–Ω—å** ‚Äî –≤—Å—Ç–∞–Ω–æ–≤–∏ `max_depth=5-7` –æ–¥—Ä–∞–∑—É
2. **Grid Search** ‚Äî –∑–Ω–∞–π–¥–∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ñ –≥—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∏
3. **–í—ñ–∑—É–∞–ª—ñ–∑—É–π –¥–µ—Ä–µ–≤–æ** ‚Äî –ø–µ—Ä–µ–≤—ñ—Ä, —á–∏ –º–∞—î —Å–µ–Ω—Å
4. **Feature importance** ‚Äî –≤–∏–¥–∞–ª–∏ –Ω–µ–ø–æ—Ç—Ä—ñ–±–Ω—ñ –æ–∑–Ω–∞–∫–∏
5. **Cost-Complexity Pruning** ‚Äî –¥–ª—è —Ñ—ñ–Ω–∞–ª—å–Ω–æ–≥–æ tuning
6. **–ü–æ—Ä—ñ–≤–Ω—è–π –∑ Random Forest** ‚Äî —á–∞—Å—Ç–æ RF –∫—Ä–∞—â–µ
7. **class_weight='balanced'** –¥–ª—è –Ω–µ–∑–±–∞–ª–∞–Ω—Å–æ–≤–∞–Ω–∏—Ö –∫–ª–∞—Å—ñ–≤
8. **min_samples_leaf** ‚Äî –∑–±—ñ–ª—å—à –¥–ª—è –∑–≥–ª–∞–¥–∂—É–≤–∞–Ω–Ω—è
9. **–ù–µ –¥–æ–≤—ñ—Ä—è–π –æ–¥–Ω–æ–º—É –¥–µ—Ä–µ–≤—É** ‚Äî –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π –∞–Ω—Å–∞–º–±–ª—ñ
10. **–ï–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—É–π** ‚Äî —Å–ø—Ä–æ–±—É–π —Ä—ñ–∑–Ω—ñ –∫—Ä–∏—Ç–µ—Ä—ñ—ó (gini vs entropy)

---

## –ü–æ—à–∏—Ä–µ–Ω—ñ –ø–æ–º–∏–ª–∫–∏ ‚ùå

### 1. –ù–µ –æ–±–º–µ–∂—É–≤–∞—Ç–∏ –≥–ª–∏–±–∏–Ω—É

```python
# ‚ùå –ù–ï–ü–†–ê–í–ò–õ–¨–ù–û
dt = DecisionTreeClassifier()  # Overfitting –º–∞–π–∂–µ –≥–∞—Ä–∞–Ω—Ç–æ–≤–∞–Ω–∏–π

# ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û
dt = DecisionTreeClassifier(max_depth=5, min_samples_leaf=5)
```

### 2. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –¥–ª—è –µ–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü—ñ—ó

```python
# Decision Trees –ù–ï –ï–ö–°–¢–†–ê–ü–û–õ–Æ–Æ–¢–¨
# –Ø–∫—â–æ train: X = [0, 10], –∞ predict: X = 15
# –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –±—É–¥–µ —Å–µ—Ä–µ–¥–Ω—ñ–º –Ω–∞–π–±–ª–∏–∂—á–æ–≥–æ –ª–∏—Å—Ç–∫–∞, –Ω–µ –ø—Ä–æ–¥–æ–≤–∂–µ–Ω–Ω—è–º —Ç—Ä–µ–Ω–¥—É
```

### 3. –Ü–≥–Ω–æ—Ä—É–≤–∞—Ç–∏ –Ω–µ–∑–±–∞–ª–∞–Ω—Å–æ–≤–∞–Ω—ñ –∫–ª–∞—Å–∏

```python
# ‚ùå –ù–ï–ü–†–ê–í–ò–õ–¨–ù–û
dt = DecisionTreeClassifier()

# ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û
dt = DecisionTreeClassifier(class_weight='balanced')
```

### 4. –ù–µ –≤—ñ–∑—É–∞–ª—ñ–∑—É–≤–∞—Ç–∏ –¥–µ—Ä–µ–≤–æ

```python
# –ó–∞–≤–∂–¥–∏ –¥–∏–≤–∏—Å—å, —â–æ –ø–æ–±—É–¥—É–≤–∞–ª–æ –¥–µ—Ä–µ–≤–æ!
plot_tree(dt, filled=True, feature_names=feature_names)
```

---

## –†–µ–∞–ª—å–Ω–∏–π –ø—Ä–∏–∫–ª–∞–¥: –î—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –∑–∞—Ö–≤–æ—Ä—é–≤–∞–Ω—å

```python
import pandas as pd
from sklearn.tree import DecisionTreeClassifier, export_text
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

# –°–∏–Ω—Ç–µ—Ç–∏—á–Ω—ñ –º–µ–¥–∏—á–Ω—ñ –¥–∞–Ω—ñ
data = {
    'Age': [25, 45, 35, 50, 60, 30, 40, 55, 28, 52],
    'BP': [120, 140, 130, 150, 160, 125, 145, 155, 122, 148],  # Blood Pressure
    'Cholesterol': [200, 240, 220, 260, 280, 210, 250, 270, 205, 255],
    'BMI': [22, 28, 25, 30, 32, 23, 29, 31, 21, 30],
    'Disease': [0, 1, 0, 1, 1, 0, 1, 1, 0, 1]  # 0=Healthy, 1=Disease
}

df = pd.DataFrame(data)

X = df.drop('Disease', axis=1)
y = df['Disease']

# –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# –ú–æ–¥–µ–ª—å –∑ –æ–±–º–µ–∂–µ–Ω–Ω—è–º–∏
dt = DecisionTreeClassifier(
    max_depth=3,
    min_samples_split=2,
    criterion='gini',
    random_state=42
)

dt.fit(X_train, y_train)

# –¢–µ–∫—Å—Ç–æ–≤–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—è –¥–µ—Ä–µ–≤–∞
tree_rules = export_text(dt, feature_names=list(X.columns))
print("=== Decision Tree Rules ===")
print(tree_rules)

# –ü—Ä–∏–∫–ª–∞–¥ –≤–∏—Ö–æ–¥—É:
# |--- Cholesterol <= 225.00
# |   |--- Age <= 32.50
# |   |   |--- class: 0 (Healthy)
# |   |--- Age >  32.50
# |   |   |--- class: 1 (Disease)
# |--- Cholesterol >  225.00
# |   |--- class: 1 (Disease)

# –û—Ü—ñ–Ω–∫–∞
y_pred = dt.predict(X_test)
print("\n" + classification_report(y_test, y_pred, 
                                   target_names=['Healthy', 'Disease']))

# Feature Importance
print("\n=== Feature Importance ===")
for feature, importance in zip(X.columns, dt.feature_importances_):
    if importance > 0:
        print(f"{feature}: {importance:.4f}")
```

---

## –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è Decision Boundaries

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_moons
from sklearn.tree import DecisionTreeClassifier

# –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –¥–∞–Ω–∏—Ö
X, y = make_moons(n_samples=200, noise=0.2, random_state=42)

# –†—ñ–∑–Ω—ñ max_depth
depths = [2, 5, 10, None]
fig, axes = plt.subplots(2, 2, figsize=(14, 12))
axes = axes.ravel()

for idx, depth in enumerate(depths):
    # –ú–æ–¥–µ–ª—å
    dt = DecisionTreeClassifier(max_depth=depth, random_state=42)
    dt.fit(X, y)
    
    # –°—ñ—Ç–∫–∞ –¥–ª—è –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó
    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5
    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5
    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),
                         np.linspace(y_min, y_max, 200))
    
    # –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –Ω–∞ —Å—ñ—Ç—Ü—ñ
    Z = dt.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    
    # –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
    axes[idx].contourf(xx, yy, Z, alpha=0.3, cmap='viridis')
    axes[idx].scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', 
                     edgecolors='k', s=50)
    axes[idx].set_title(f'max_depth={depth}\nTrain Acc={dt.score(X, y):.3f}',
                       fontsize=12, fontweight='bold')
    axes[idx].set_xlabel('Feature 1')
    axes[idx].set_ylabel('Feature 2')

plt.tight_layout()
plt.show()
```

---

## –ü–æ–≤'—è–∑–∞–Ω—ñ —Ç–µ–º–∏

- [[02_Random_Forest]] ‚Äî –∞–Ω—Å–∞–º–±–ª—å –¥–µ—Ä–µ–≤
- [[03_Gradient_Boosting]] ‚Äî –ø–æ—Å–ª—ñ–¥–æ–≤–Ω—ñ –¥–µ—Ä–µ–≤–∞
- [[05_Ensemble_Methods]] ‚Äî –∫–æ–º–±—ñ–Ω—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª–µ–π
- [[06_Feature_Importance]] ‚Äî –∞–Ω–∞–ª—ñ–∑ –æ–∑–Ω–∞–∫
- [[Information_Theory]] ‚Äî Entropy, Information Gain

## –†–µ—Å—É—Ä—Å–∏

- [Scikit-learn: Decision Trees](https://scikit-learn.org/stable/modules/tree.html)
- [CART Algorithm](https://en.wikipedia.org/wiki/Decision_tree_learning)
- [StatQuest: Decision Trees](https://www.youtube.com/watch?v=7VeUPuFGJHk)
- [Visualization Tool](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/)

---

## –ö–ª—é—á–æ–≤—ñ –≤–∏—Å–Ω–æ–≤–∫–∏

> Decision Trees –±—É–¥—É—é—Ç—å –¥–µ—Ä–µ–≤–æ–ø–æ–¥—ñ–±–Ω—É —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Ä—ñ—à–µ–Ω—å —á–µ—Ä–µ–∑ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω—ñ —Ä–æ–∑–±–∏—Ç—Ç—è –¥–∞–Ω–∏—Ö –∑–∞ –æ–∑–Ω–∞–∫–∞–º–∏ –¥–ª—è –º–∞–∫—Å–∏–º—ñ–∑–∞—Ü—ñ—ó information gain –∞–±–æ –º—ñ–Ω—ñ–º—ñ–∑–∞—Ü—ñ—ó impurity.

**–û—Å–Ω–æ–≤–Ω—ñ –ø—Ä–∏–Ω—Ü–∏–ø–∏:**
- –ñ–∞–¥—ñ–±–Ω–∏–π —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º
- –í–∏–±—ñ—Ä –Ω–∞–π–∫—Ä–∞—â–æ–≥–æ —Ä–æ–∑–±–∏—Ç—Ç—è –Ω–∞ –∫–æ–∂–Ω–æ–º—É –∫—Ä–æ—Ü—ñ
- –ö—Ä–∏—Ç–µ—Ä—ñ—ó: Gini Impurity, Entropy (–∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è), MSE (—Ä–µ–≥—Ä–µ—Å—ñ—è)
- –ü–æ—Ç—Ä–µ–±—É—î –æ–±–º–µ–∂–µ–Ω—å –¥–ª—è –∑–∞–ø–æ–±—ñ–≥–∞–Ω–Ω—è overfitting

**–§–æ—Ä–º—É–ª–∏:**
- **Gini:** $1 - \sum p_i^2$
- **Entropy:** $-\sum p_i \log_2(p_i)$
- **Information Gain:** $\text{Entropy}_{\text{parent}} - \text{weighted Entropy}_{\text{children}}$

**–ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏:**
- –Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å + –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è + –∫–∞—Ç–µ–≥–æ—Ä—ñ–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ = Decision Trees ‚úì
- –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å ‚Üí –ê–Ω—Å–∞–º–±–ª—ñ (Random Forest, XGBoost) ‚úì

**–ù–∞–π–≤–∞–∂–ª–∏–≤—ñ—à–µ:**
- –ó–ê–í–ñ–î–ò –æ–±–º–µ–∂—É–π —Å–∫–ª–∞–¥–Ω—ñ—Å—Ç—å (`max_depth`, `min_samples_leaf`)
- –í—ñ–∑—É–∞–ª—ñ–∑—É–π –¥–µ—Ä–µ–≤–æ –¥–ª—è —Ä–æ–∑—É–º—ñ–Ω–Ω—è
- –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π —è–∫ baseline, –∞–ª–µ —Ä–æ–∑–≥–ª—è–¥–∞–π –∞–Ω—Å–∞–º–±–ª—ñ –¥–ª—è production

---

#ml #supervised-learning #classification #regression #decision-trees #interpretability #tree-based
