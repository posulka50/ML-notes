# Decision Trees (–î–µ—Ä–µ–≤–∞ —Ä—ñ—à–µ–Ω—å)

## –©–æ —Ü–µ?

**–î–µ—Ä–µ–≤–æ —Ä—ñ—à–µ–Ω—å** ‚Äî —Ü–µ –∞–ª–≥–æ—Ä–∏—Ç–º supervised learning, —è–∫–∏–π –ø—Ä–∏–π–º–∞—î —Ä—ñ—à–µ–Ω–Ω—è —à–ª—è—Ö–æ–º –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ–≥–æ —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö –Ω–∞ –æ—Å–Ω–æ–≤—ñ –ø–∏—Ç–∞–Ω—å –ø—Ä–æ –æ–∑–Ω–∞–∫–∏, —Ñ–æ—Ä–º—É—é—á–∏ –¥–µ—Ä–µ–≤–æ–ø–æ–¥—ñ–±–Ω—É —Å—Ç—Ä—É–∫—Ç—É—Ä—É.

**–ì–æ–ª–æ–≤–Ω–∞ —ñ–¥–µ—è:** –º–æ–¥–µ–ª—å —è–∫ —Å–µ—Ä—ñ—è if-else –ø—Ä–∞–≤–∏–ª, —è–∫—ñ –ª–µ–≥–∫–æ –∑—Ä–æ–∑—É–º—ñ—Ç–∏ –ª—é–¥–∏–Ω—ñ ‚Äî "—è–∫—â–æ –≤—ñ–∫ > 30 –¢–ê –¥–æ—Ö—ñ–¥ > 50k, —Ç–æ —Å—Ö–≤–∞–ª–∏—Ç–∏ –∫—Ä–µ–¥–∏—Ç".

## –ù–∞–≤—ñ—â–æ –ø–æ—Ç—Ä—ñ–±–Ω—ñ?

- üå≥ **–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å** ‚Äî –º–æ–∂–Ω–∞ –≤—ñ–∑—É–∞–ª—ñ–∑—É–≤–∞—Ç–∏ —Ç–∞ –ø–æ—è—Å–Ω–∏—Ç–∏ –∫–æ–∂–Ω–µ —Ä—ñ—à–µ–Ω–Ω—è
- üéØ **–£–Ω—ñ–≤–µ—Ä—Å–∞–ª—å–Ω—ñ—Å—Ç—å** ‚Äî –ø—Ä–∞—Ü—é—î –¥–ª—è —Ä–µ–≥—Ä–µ—Å—ñ—ó —Ç–∞ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó
- üìä **–ù–µ–ª—ñ–Ω—ñ–π–Ω—ñ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ** ‚Äî –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –≤–∏—è–≤–ª—è—î —Å–∫–ª–∞–¥–Ω—ñ –ø–∞—Ç–µ—Ä–Ω–∏
- üîß **–ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∞ –ø—ñ–¥–≥–æ—Ç–æ–≤–∫–∞** ‚Äî –Ω–µ –ø–æ—Ç—Ä—ñ–±–Ω–∞ –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è –∞–±–æ encoding
- üí° **Feature importance** ‚Äî –ø–æ–∫–∞–∑—É—î –Ω–∞–π–≤–∞–∂–ª–∏–≤—ñ—à—ñ –æ–∑–Ω–∞–∫–∏
- üöÄ **–®–≤–∏–¥–∫—ñ—Å—Ç—å** ‚Äî —à–≤–∏–¥–∫–µ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è

## –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏?

**–ü–æ—Ç—Ä—ñ–±–Ω–æ:**
- **–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å –∫—Ä–∏—Ç–∏—á–Ω–∞** ‚Äî –º–µ–¥–∏—Ü–∏–Ω–∞, —Ñ—ñ–Ω–∞–Ω—Å–∏, –ø—Ä–∞–≤–æ
- –î–∞–Ω—ñ **–∑–º—ñ—à–∞–Ω–∏—Ö —Ç–∏–ø—ñ–≤** ‚Äî —á–∏—Å–ª–æ–≤—ñ + –∫–∞—Ç–µ–≥–æ—Ä—ñ–∞–ª—å–Ω—ñ
- **–ù–µ–ª—ñ–Ω—ñ–π–Ω—ñ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ** ‚Äî —Å–∫–ª–∞–¥–Ω—ñ interaction effects
- **Feature importance** ‚Äî –ø–æ—Ç—Ä—ñ–±–Ω–æ –∑—Ä–æ–∑—É–º—ñ—Ç–∏, —â–æ –≤–ø–ª–∏–≤–∞—î
- –®–≤–∏–¥–∫–∏–π baseline –ø–µ—Ä–µ–¥ –∞–Ω—Å–∞–º–±–ª—è–º–∏

**–ù–µ –ø–æ—Ç—Ä—ñ–±–Ω–æ:**
- –ü–æ—Ç—Ä—ñ–±–Ω–∞ **–≤–∏—Å–æ–∫–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å** ‚Üí Random Forest, XGBoost
- –õ—ñ–Ω—ñ–π–Ω—ñ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ ‚Üí Linear/Logistic Regression
- –î—É–∂–µ **–≤–µ–ª–∏–∫—ñ –¥–∞—Ç–∞—Å–µ—Ç–∏** ‚Üí –º–æ–∂–µ –±—É—Ç–∏ –ø–æ–≤—ñ–ª—å–Ω–æ
- **–ï–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü—ñ—è** ‚Üí –ø–æ–≥–∞–Ω–æ –ø—Ä–∞—Ü—é—î –∑–∞ –º–µ–∂–∞–º–∏ train –¥–∞–Ω–∏—Ö

---

## –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–µ—Ä–µ–≤–∞

### –ê–Ω–∞—Ç–æ–º—ñ—è –¥–µ—Ä–µ–≤–∞

```
                    [Root Node]
                    –î–æ—Ö—ñ–¥ ‚â§ 50k?
                    /          \
                 –¢–∞–∫            –ù—ñ
                /                  \
        [Internal Node]        [Internal Node]
        –í—ñ–∫ ‚â§ 25?              –û—Å–≤—ñ—Ç–∞?
          /    \                /      \
       –¢–∞–∫      –ù—ñ          –í–∏—â–∞    –°–µ—Ä–µ–¥–Ω—è
       /          \           |         |
   [Leaf]      [Leaf]     [Leaf]    [Leaf]
   –í—ñ–¥–º–æ–≤–∞    –°—Ö–≤–∞–ª–∏—Ç–∏   –°—Ö–≤–∞–ª–∏—Ç–∏  –í—ñ–¥–º–æ–≤–∞
   (Class 0)  (Class 1)  (Class 1)  (Class 0)
```

### –ö–æ–º–ø–æ–Ω–µ–Ω—Ç–∏

| –ö–æ–º–ø–æ–Ω–µ–Ω—Ç | –û–ø–∏—Å | –ü—Ä–∏–∫–ª–∞–¥ |
|-----------|------|---------|
| **Root Node** | –ü–µ—Ä—à–∏–π –ø–æ–¥—ñ–ª (–∫–æ—Ä—ñ–Ω—å) | –î–æ—Ö—ñ–¥ ‚â§ 50k? |
| **Internal Node** | –ü—Ä–æ–º—ñ–∂–Ω–∏–π –ø–æ–¥—ñ–ª | –í—ñ–∫ ‚â§ 25? |
| **Leaf Node** | –ö—ñ–Ω—Ü–µ–≤–µ —Ä—ñ—à–µ–Ω–Ω—è | –°—Ö–≤–∞–ª–∏—Ç–∏ –∫—Ä–µ–¥–∏—Ç |
| **Branch** | –ì—ñ–ª–∫–∞ (—Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–∏—Ç–∞–Ω–Ω—è) | –¢–∞–∫/–ù—ñ, ‚â§/> |
| **Depth** | –ö—ñ–ª—å–∫—ñ—Å—Ç—å —Ä—ñ–≤–Ω—ñ–≤ –≤—ñ–¥ –∫–æ—Ä–µ–Ω—è | Depth = 2 |
| **Split** | –ü—Ä–∞–≤–∏–ª–æ –ø–æ–¥—ñ–ª—É | Feature + threshold |

---

## –Ø–∫ –ø—Ä–∞—Ü—é—î? –ê–ª–≥–æ—Ä–∏—Ç–º –ø–æ–±—É–¥–æ–≤–∏

### –†–µ–∫—É—Ä—Å–∏–≤–Ω–∏–π –ø–æ–¥—ñ–ª (Recursive Binary Splitting)

**–ê–ª–≥–æ—Ä–∏—Ç–º (Top-Down Greedy):**

1. **–ü–æ—á–∞—Ç–∏ –∑ –∫–æ—Ä–Ω—è** (–≤—Å—ñ –¥–∞–Ω—ñ –≤ –æ–¥–Ω–æ–º—É –≤—É–∑–ª—ñ)
2. **–î–ª—è –∫–æ–∂–Ω–æ—ó –æ–∑–Ω–∞–∫–∏ —Ç–∞ –∫–æ–∂–Ω–æ–≥–æ –ø–æ—Ä–æ–≥—É:**
   - –†–æ–∑–¥—ñ–ª–∏—Ç–∏ –¥–∞–Ω—ñ –Ω–∞ –¥–≤—ñ –≥—Ä—É–ø–∏
   - –û–±—á–∏—Å–ª–∏—Ç–∏ —è–∫—ñ—Å—Ç—å –ø–æ–¥—ñ–ª—É (–∫—Ä–∏—Ç–µ—Ä—ñ–π)
3. **–í–∏–±—Ä–∞—Ç–∏ –Ω–∞–π–∫—Ä–∞—â–∏–π –ø–æ–¥—ñ–ª** (–º—ñ–Ω—ñ–º—ñ–∑—É—î impurity)
4. **–†–µ–∫—É—Ä—Å–∏–≤–Ω–æ** –ø–æ–≤—Ç–æ—Ä–∏—Ç–∏ –¥–ª—è –ª—ñ–≤–æ—ó —Ç–∞ –ø—Ä–∞–≤–æ—ó —á–∞—Å—Ç–∏–Ω
5. **–ó—É–ø–∏–Ω–∏—Ç–∏—Å—å**, –∫–æ–ª–∏ –¥–æ—Å—è–≥–Ω—É—Ç–æ –∫—Ä–∏—Ç–µ—Ä—ñ–π:
   - –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –≥–ª–∏–±–∏–Ω–∞
   - –ú—ñ–Ω—ñ–º—É–º –ø—Ä–∏–∫–ª–∞–¥—ñ–≤ —É –ª–∏—Å—Ç—ñ
   - –í—Å—ñ –ø—Ä–∏–∫–ª–∞–¥–∏ –æ–¥–Ω–æ–≥–æ –∫–ª–∞—Å—É (pure node)

### –ö—Ä–∏—Ç–µ—Ä—ñ—ó —è–∫–æ—Å—Ç—ñ –ø–æ–¥—ñ–ª—É

## 1. –î–ª—è –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó

### A) Gini Impurity (–Ü–Ω–¥–µ–∫—Å –î–∂–∏–Ω—ñ)

**–§–æ—Ä–º—É–ª–∞:**
$$\text{Gini}(S) = 1 - \sum_{i=1}^{C} p_i^2$$

–¥–µ:
- $S$ ‚Äî –º–Ω–æ–∂–∏–Ω–∞ –ø—Ä–∏–∫–ª–∞–¥—ñ–≤ —É –≤—É–∑–ª—ñ
- $C$ ‚Äî –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∫–ª–∞—Å—ñ–≤
- $p_i$ ‚Äî —á–∞—Å—Ç–∫–∞ –∫–ª–∞—Å—É $i$ —É –≤—É–∑–ª—ñ

**–Ü–Ω—Ç—É—ó—Ü—ñ—è:** –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å –ø–æ–º–∏–ª–∫–æ–≤–æ –∫–ª–∞—Å–∏—Ñ—ñ–∫—É–≤–∞—Ç–∏ –≤–∏–ø–∞–¥–∫–æ–≤–∏–π –ø—Ä–∏–∫–ª–∞–¥, —è–∫–±–∏ –π–æ–≥–æ –∫–ª–∞—Å –≤–∏–±—Ä–∞–ª–∏ –≤–∏–ø–∞–¥–∫–æ–≤–æ –∑–≥—ñ–¥–Ω–æ –∑ —Ä–æ–∑–ø–æ–¥—ñ–ª–æ–º —É –≤—É–∑–ª—ñ.

**–î—ñ–∞–ø–∞–∑–æ–Ω:** $[0, 0.5]$ –¥–ª—è –±—ñ–Ω–∞—Ä–Ω–æ—ó –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó
- **Gini = 0** ‚Üí pure node (–≤—Å—ñ –ø—Ä–∏–∫–ª–∞–¥–∏ –æ–¥–Ω–æ–≥–æ –∫–ª–∞—Å—É) ‚úì
- **Gini = 0.5** ‚Üí –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ impurity (50/50)

**–ü—Ä–∏–∫–ª–∞–¥ (–±—ñ–Ω–∞—Ä–Ω–∞ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è):**

–í—É–∑–æ–ª –∑ 100 –ø—Ä–∏–∫–ª–∞–¥–∞–º–∏: 70 –∫–ª–∞—Å A, 30 –∫–ª–∞—Å B

$$\text{Gini} = 1 - (0.7^2 + 0.3^2) = 1 - (0.49 + 0.09) = 1 - 0.58 = 0.42$$

**–ü—ñ—Å–ª—è –ø–æ–¥—ñ–ª—É:**

–õ—ñ–≤–∏–π –≤—É–∑–æ–ª (60 –ø—Ä–∏–∫–ª–∞–¥—ñ–≤): 55 A, 5 B
$$\text{Gini}_L = 1 - (0.917^2 + 0.083^2) = 1 - 0.848 = 0.152$$

–ü—Ä–∞–≤–∏–π –≤—É–∑–æ–ª (40 –ø—Ä–∏–∫–ª–∞–¥—ñ–≤): 15 A, 25 B
$$\text{Gini}_R = 1 - (0.375^2 + 0.625^2) = 1 - 0.531 = 0.469$$

**Weighted Gini:**
$$\text{Gini}_{\text{split}} = \frac{60}{100} \times 0.152 + \frac{40}{100} \times 0.469 = 0.091 + 0.188 = 0.279$$

**Information Gain:**
$$\Delta\text{Gini} = 0.42 - 0.279 = 0.141$$

–ß–∏–º –±—ñ–ª—å—à–µ $\Delta\text{Gini}$, —Ç–∏–º –∫—Ä–∞—â–∏–π –ø–æ–¥—ñ–ª! ‚úì

---

### B) Entropy (–ï–Ω—Ç—Ä–æ–ø—ñ—è) —Ç–∞ Information Gain

**–§–æ—Ä–º—É–ª–∞ –µ–Ω—Ç—Ä–æ–ø—ñ—ó:**
$$\text{Entropy}(S) = -\sum_{i=1}^{C} p_i \log_2(p_i)$$

**–Ü–Ω—Ç—É—ó—Ü—ñ—è:** –º—ñ—Ä–∞ –Ω–µ–≤–∏–∑–Ω–∞—á–µ–Ω–æ—Å—Ç—ñ –∞–±–æ "–±–µ–∑–ª–∞–¥—É" —É –≤—É–∑–ª—ñ.

**–î—ñ–∞–ø–∞–∑–æ–Ω:** $[0, 1]$ –¥–ª—è –±—ñ–Ω–∞—Ä–Ω–æ—ó –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó
- **Entropy = 0** ‚Üí pure node (–≤—Å—ñ –æ–¥–Ω–∞–∫–æ–≤—ñ)
- **Entropy = 1** ‚Üí –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –Ω–µ–≤–∏–∑–Ω–∞—á–µ–Ω—ñ—Å—Ç—å (50/50)

**–ü—Ä–∏–∫–ª–∞–¥:**

–¢–æ–π –∂–µ –≤—É–∑–æ–ª: 70 A, 30 B

$$\text{Entropy} = -(0.7 \log_2(0.7) + 0.3 \log_2(0.3))$$
$$= -(0.7 \times (-0.515) + 0.3 \times (-1.737))$$
$$= -(-0.361 - 0.521) = 0.882$$

**–ü—ñ—Å–ª—è –ø–æ–¥—ñ–ª—É:**

–õ—ñ–≤–∏–π: 55 A, 5 B
$$\text{Entropy}_L = -(0.917 \log_2(0.917) + 0.083 \log_2(0.083))$$
$$= 0.408$$

–ü—Ä–∞–≤–∏–π: 15 A, 25 B
$$\text{Entropy}_R = -(0.375 \log_2(0.375) + 0.625 \log_2(0.625))$$
$$= 0.954$$

**Weighted Entropy:**
$$\text{Entropy}_{\text{split}} = 0.6 \times 0.408 + 0.4 \times 0.954 = 0.626$$

**Information Gain:**
$$\text{IG} = 0.882 - 0.626 = 0.256$$

–ß–∏–º –±—ñ–ª—å—à–µ IG, —Ç–∏–º –∫—Ä–∞—â–∏–π –ø–æ–¥—ñ–ª!

---

### –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è Gini vs Entropy

| –ö—Ä–∏—Ç–µ—Ä—ñ–π | Gini Impurity | Entropy |
|----------|---------------|---------|
| **–§–æ—Ä–º—É–ª–∞** | $1 - \sum p_i^2$ | $-\sum p_i \log_2(p_i)$ |
| **–û–±—á–∏—Å–ª–µ–Ω–Ω—è** | –®–≤–∏–¥—à–µ ‚úì | –ü–æ–≤—ñ–ª—å–Ω—ñ—à–µ (log) |
| **–†–µ–∑—É–ª—å—Ç–∞—Ç–∏** | –î—É–∂–µ —Å—Ö–æ–∂—ñ | –î—É–∂–µ —Å—Ö–æ–∂—ñ |
| **–ó–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º** | scikit-learn | - |
| **–ü–µ—Ä–µ–≤–∞–≥–∏** | –ü—Ä–æ—Å—Ç–∞, —à–≤–∏–¥–∫–∞ | –¢–µ–æ—Ä–µ—Ç–∏—á–Ω–æ –æ–±“ë—Ä—É–Ω—Ç–æ–≤–∞–Ω–∞ |
| **–í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è** | –ë—ñ–ª—å—à –ø–æ–ø—É–ª—è—Ä–Ω–∞ | Information Theory |

**–í–∏—Å–Ω–æ–≤–æ–∫:** –ù–∞ –ø—Ä–∞–∫—Ç–∏—Ü—ñ —Ä—ñ–∑–Ω–∏—Ü—è –º—ñ–Ω—ñ–º–∞–ª—å–Ω–∞. Gini —à–≤–∏–¥—à–µ, Entropy —Ç–µ–æ—Ä–µ—Ç–∏—á–Ω–æ "—á–∏—Å—Ç—ñ—à–∞".

---

## 2. –î–ª—è —Ä–µ–≥—Ä–µ—Å—ñ—ó

### MSE (Mean Squared Error)

**–§–æ—Ä–º—É–ª–∞:**
$$\text{MSE}(S) = \frac{1}{|S|} \sum_{i \in S} (y_i - \bar{y})^2$$

–¥–µ $\bar{y}$ ‚Äî —Å–µ—Ä–µ–¥–Ω—î –∑–Ω–∞—á–µ–Ω–Ω—è $y$ —É –≤—É–∑–ª—ñ.

**–ü—ñ—Å–ª—è –ø–æ–¥—ñ–ª—É:**
$$\text{MSE}_{\text{split}} = \frac{|S_L|}{|S|} \text{MSE}(S_L) + \frac{|S_R|}{|S|} \text{MSE}(S_R)$$

**Reduction in MSE:**
$$\Delta\text{MSE} = \text{MSE}(S) - \text{MSE}_{\text{split}}$$

**–ü—Ä–∏–∫–ª–∞–¥:**

–í—É–∑–æ–ª –∑ 6 –ø—Ä–∏–∫–ª–∞–¥–∞–º–∏: $y = [10, 20, 15, 30, 25, 18]$

–°–µ—Ä–µ–¥–Ω—î: $\bar{y} = 19.67$

$$\text{MSE} = \frac{1}{6}[(10-19.67)^2 + (20-19.67)^2 + ... + (18-19.67)^2]$$
$$= \frac{1}{6}[93.5 + 0.11 + 21.8 + 106.7 + 28.4 + 2.8] = 42.2$$

**–ü–æ–¥—ñ–ª:** $x \leq 17$

–õ—ñ–≤–∏–π ($x \leq 17$): $y = [10, 15]$, $\bar{y}_L = 12.5$
$$\text{MSE}_L = \frac{1}{2}[(10-12.5)^2 + (15-12.5)^2] = 6.25$$

–ü—Ä–∞–≤–∏–π ($x > 17$): $y = [20, 30, 25, 18]$, $\bar{y}_R = 23.25$
$$\text{MSE}_R = \frac{1}{4}[(20-23.25)^2 + (30-23.25)^2 + (25-23.25)^2 + (18-23.25)^2]$$
$$= 23.19$$

**Weighted MSE:**
$$\text{MSE}_{\text{split}} = \frac{2}{6} \times 6.25 + \frac{4}{6} \times 23.19 = 2.08 + 15.46 = 17.54$$

**Reduction:**
$$\Delta\text{MSE} = 42.2 - 17.54 = 24.66$$ ‚úì

–¶–µ–π –ø–æ–¥—ñ–ª —Å—É—Ç—Ç—î–≤–æ –∑–º–µ–Ω—à—É—î MSE!

---

## –ü—Ä–æ—Å—Ç–∏–π –ø—Ä–∏–∫–ª–∞–¥: –°—Ö–≤–∞–ª–µ–Ω–Ω—è –∫—Ä–µ–¥–∏—Ç—É

### –î–∞–Ω—ñ

| –î–æ—Ö—ñ–¥ (—Ç–∏—Å. $) | –í—ñ–∫ | –ë–æ—Ä–≥ (—Ç–∏—Å. $) | –°—Ö–≤–∞–ª–µ–Ω–æ |
|----------------|-----|---------------|----------|
| 30 | 25 | 5 | 0 (–ù—ñ) |
| 50 | 35 | 10 | 1 (–¢–∞–∫) |
| 40 | 28 | 15 | 0 (–ù—ñ) |
| 70 | 45 | 8 | 1 (–¢–∞–∫) |
| 60 | 40 | 20 | 0 (–ù—ñ) |
| 80 | 50 | 5 | 1 (–¢–∞–∫) |

### –ü–æ–±—É–¥–æ–≤–∞ –¥–µ—Ä–µ–≤–∞

**–ö—Ä–æ–∫ 1: –í–∏–±—Ä–∞—Ç–∏ –ø–µ—Ä—à–∏–π –ø–æ–¥—ñ–ª (root)**

–†–æ–∑–≥–ª—è–Ω–µ–º–æ –≤—Å—ñ –º–æ–∂–ª–∏–≤—ñ –ø–æ–¥—ñ–ª–∏:

**–î–æ—Ö—ñ–¥ ‚â§ 45:**
- –õ—ñ–≤–æ—Ä—É—á: 30, 40 ‚Üí 2 –≤—ñ–¥–º–æ–≤–∏
- –ü—Ä–∞–≤–æ—Ä—É—á: 50, 70, 60, 80 ‚Üí 2 —Å—Ö–≤–∞–ª–µ–Ω–Ω—è, 1 –≤—ñ–¥–º–æ–≤–∞
- Gini = ...

**–î–æ—Ö—ñ–¥ ‚â§ 55:**
- –õ—ñ–≤–æ—Ä—É—á: 30, 50, 40 ‚Üí 1 —Å—Ö–≤–∞–ª–µ–Ω–Ω—è, 2 –≤—ñ–¥–º–æ–≤–∏
- –ü—Ä–∞–≤–æ—Ä—É—á: 70, 60, 80 ‚Üí 2 —Å—Ö–≤–∞–ª–µ–Ω–Ω—è, 1 –≤—ñ–¥–º–æ–≤–∞
- Gini_left = $1 - (1/3)^2 - (2/3)^2 = 0.444$
- Gini_right = $1 - (2/3)^2 - (1/3)^2 = 0.444$
- Weighted Gini = $3/6 \times 0.444 + 3/6 \times 0.444 = 0.444$

**–ë–æ—Ä–≥ ‚â§ 12:**
- –õ—ñ–≤–æ—Ä—É—á: 5, 10, 8, 5 ‚Üí 3 —Å—Ö–≤–∞–ª–µ–Ω–Ω—è, 1 –≤—ñ–¥–º–æ–≤–∞
- –ü—Ä–∞–≤–æ—Ä—É—á: 15, 20 ‚Üí 2 –≤—ñ–¥–º–æ–≤–∏
- Gini_left = $1 - (3/4)^2 - (1/4)^2 = 0.375$
- Gini_right = $1 - 0 - 1 = 0$ (pure!)
- Weighted Gini = $4/6 \times 0.375 + 2/6 \times 0 = 0.25$ ‚úì

**–ù–∞–π–∫—Ä–∞—â–∏–π –ø–æ–¥—ñ–ª:** –ë–æ—Ä–≥ ‚â§ 12 (Gini = 0.25)

### –†–µ–∑—É–ª—å—Ç—É—é—á–µ –¥–µ—Ä–µ–≤–æ

```
            [Root]
          –ë–æ—Ä–≥ ‚â§ 12?
          /        \
       –¢–∞–∫          –ù—ñ
       /              \
  [Internal]       [Leaf]
  –î–æ—Ö—ñ–¥ ‚â§ 55?     –í—ñ–¥–º–æ–≤–∞
    /      \
  –¢–∞–∫       –ù—ñ
  /           \
[Leaf]      [Leaf]
–í—ñ–¥–º–æ–≤–∞   –°—Ö–≤–∞–ª–∏—Ç–∏
```

---

## –°–∫–ª–∞–¥–Ω–∏–π –ø—Ä–∏–∫–ª–∞–¥: –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è —Ü—ñ–Ω–∏ –±—É–¥–∏–Ω–∫—É (—Ä–µ–≥—Ä–µ—Å—ñ—è)

### –î–∞–Ω—ñ

| –ü–ª–æ—â–∞ (–º¬≤) | –ö—ñ–º–Ω–∞—Ç | –†–∞–π–æ–Ω | –¶—ñ–Ω–∞ (—Ç–∏—Å. $) |
|------------|--------|-------|---------------|
| 50 | 1 | A | 100 |
| 80 | 2 | B | 150 |
| 100 | 3 | A | 200 |
| 120 | 3 | B | 250 |
| 150 | 4 | A | 300 |
| 180 | 4 | B | 350 |

### –ü–æ–±—É–¥–æ–≤–∞ –¥–µ—Ä–µ–≤–∞ —Ä–µ–≥—Ä–µ—Å—ñ—ó

**Root node:** –í—Å—ñ 6 –ø—Ä–∏–∫–ª–∞–¥—ñ–≤, $\bar{y} = 225$

**MSE(root) = $\frac{1}{6}[(100-225)^2 + (150-225)^2 + ... + (350-225)^2]$**
**= $\frac{1}{6}[15625 + 5625 + 625 + 625 + 5625 + 15625] = 7291.67$**

**–ö–∞–Ω–¥–∏–¥–∞—Ç–∏ –ø–æ–¥—ñ–ª—É:**

**–ü–ª–æ—â–∞ ‚â§ 90:**
- –õ—ñ–≤–∏–π: 50, 80 ‚Üí –¶—ñ–Ω–∏: 100, 150, $\bar{y}_L = 125$
- –ü—Ä–∞–≤–∏–π: 100, 120, 150, 180 ‚Üí –¶—ñ–Ω–∏: 200, 250, 300, 350, $\bar{y}_R = 275$
- MSE_L = 625
- MSE_R = 3125
- Weighted MSE = $2/6 \times 625 + 4/6 \times 3125 = 2291.67$
- Reduction = $7291.67 - 2291.67 = 5000$ ‚úì

**–ö—ñ–º–Ω–∞—Ç ‚â§ 2:**
- –õ—ñ–≤–∏–π: 1, 2 ‚Üí –¶—ñ–Ω–∏: 100, 150, $\bar{y}_L = 125$
- –ü—Ä–∞–≤–∏–π: 3, 3, 4, 4 ‚Üí –¶—ñ–Ω–∏: 200, 250, 300, 350, $\bar{y}_R = 275$
- (–ê–Ω–∞–ª–æ–≥—ñ—á–Ω–æ –¥–æ –ü–ª–æ—â–∞ ‚â§ 90)

**–ù–∞–π–∫—Ä–∞—â–∏–π –ø–æ–¥—ñ–ª:** –ü–ª–æ—â–∞ ‚â§ 90

### –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è

–î–ª—è –Ω–æ–≤–æ–≥–æ –±—É–¥–∏–Ω–∫—É: –ü–ª–æ—â–∞ = 110 –º¬≤, –ö—ñ–º–Ω–∞—Ç = 3

```
–ü–ª–æ—â–∞ ‚â§ 90?
    ‚Üí –ù—ñ (110 > 90)
    ‚Üí –ô–¥–µ–º–æ –ø—Ä–∞–≤–æ—Ä—É—á
    ‚Üí –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è: $\bar{y}_R = 275$ —Ç–∏—Å. $
```

---

## –ö–æ–¥ (Python + scikit-learn)

### –ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.datasets import make_classification

# 1. –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –¥–∞–Ω–∏—Ö
X, y = make_classification(
    n_samples=500,
    n_features=2,
    n_informative=2,
    n_redundant=0,
    n_classes=2,
    random_state=42
)

# –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# 2. –ú–æ–¥–µ–ª—å Decision Tree
tree_clf = DecisionTreeClassifier(
    criterion='gini',           # –∞–±–æ 'entropy'
    max_depth=3,                # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –≥–ª–∏–±–∏–Ω–∞
    min_samples_split=20,       # –ú—ñ–Ω—ñ–º—É–º –¥–ª—è –ø–æ–¥—ñ–ª—É
    min_samples_leaf=10,        # –ú—ñ–Ω—ñ–º—É–º —É –ª–∏—Å—Ç—ñ
    random_state=42
)

tree_clf.fit(X_train, y_train)

# 3. –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
y_pred = tree_clf.predict(X_test)
y_proba = tree_clf.predict_proba(X_test)

# 4. –û—Ü—ñ–Ω–∫–∞
print("=== Classification Metrics ===")
print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")
print(f"\n{classification_report(y_test, y_pred)}")
print(f"\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred))

# 5. Feature Importance
print("\n=== Feature Importance ===")
for i, importance in enumerate(tree_clf.feature_importances_):
    print(f"Feature {i}: {importance:.4f}")

# 6. –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–µ—Ä–µ–≤–∞
print(f"\n=== Tree Structure ===")
print(f"Number of leaves: {tree_clf.get_n_leaves()}")
print(f"Tree depth: {tree_clf.get_depth()}")

# 7. –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –¥–µ—Ä–µ–≤–∞
plt.figure(figsize=(20, 10))
plot_tree(
    tree_clf,
    feature_names=['Feature 0', 'Feature 1'],
    class_names=['Class 0', 'Class 1'],
    filled=True,
    rounded=True,
    fontsize=10
)
plt.title('Decision Tree Visualization', fontsize=16, fontweight='bold')
plt.tight_layout()
plt.savefig('decision_tree.png', dpi=300, bbox_inches='tight')
plt.show()
```

### –†–µ–≥—Ä–µ—Å—ñ—è

```python
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.datasets import make_regression

# –î–∞–Ω—ñ
X, y = make_regression(
    n_samples=200,
    n_features=1,
    noise=20,
    random_state=42
)

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# –ú–æ–¥–µ–ª—å
tree_reg = DecisionTreeRegressor(
    max_depth=5,
    min_samples_split=20,
    min_samples_leaf=10,
    random_state=42
)

tree_reg.fit(X_train, y_train)

# –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
y_pred = tree_reg.predict(X_test)

# –ú–µ—Ç—Ä–∏–∫–∏
print("=== Regression Metrics ===")
print(f"R¬≤: {r2_score(y_test, y_pred):.4f}")
print(f"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.2f}")
print(f"MAE: {mean_absolute_error(y_test, y_pred):.2f}")

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
X_plot = np.linspace(X.min(), X.max(), 300).reshape(-1, 1)
y_plot = tree_reg.predict(X_plot)

plt.figure(figsize=(12, 6))
plt.scatter(X_train, y_train, alpha=0.4, s=30, label='Train')
plt.scatter(X_test, y_test, alpha=0.6, s=50, label='Test', color='green')
plt.plot(X_plot, y_plot, color='red', linewidth=2, label='Decision Tree')
plt.xlabel('X', fontsize=12)
plt.ylabel('y', fontsize=12)
plt.title('Decision Tree Regression', fontsize=14, fontweight='bold')
plt.legend(fontsize=11)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

---

## –ì—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∏ Decision Trees

### –û—Å–Ω–æ–≤–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏

| –ü–∞—Ä–∞–º–µ—Ç—Ä | –û–ø–∏—Å | –î—ñ–∞–ø–∞–∑–æ–Ω | –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó |
|----------|------|----------|--------------|
| **max_depth** | –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –≥–ª–∏–±–∏–Ω–∞ –¥–µ—Ä–µ–≤–∞ | 1-‚àû | 3-10 –¥–ª—è –ø—Ä–æ—Å—Ç–∏—Ö, 10-20 –¥–ª—è —Å–∫–ª–∞–¥–Ω–∏—Ö |
| **min_samples_split** | –ú—ñ–Ω—ñ–º—É–º –ø—Ä–∏–∫–ª–∞–¥—ñ–≤ –¥–ª—è –ø–æ–¥—ñ–ª—É | 2-‚àû | 10-50 –∑–∞–ª–µ–∂–Ω–æ –≤—ñ–¥ —Ä–æ–∑–º—ñ—Ä—É –¥–∞–Ω–∏—Ö |
| **min_samples_leaf** | –ú—ñ–Ω—ñ–º—É–º –ø—Ä–∏–∫–ª–∞–¥—ñ–≤ —É –ª–∏—Å—Ç—ñ | 1-‚àû | 5-20 |
| **max_features** | –ú–∞–∫—Å. –æ–∑–Ω–∞–∫ –¥–ª—è –ø–æ–¥—ñ–ª—É | int, float, auto | sqrt(n) –¥–ª—è –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó |
| **criterion** | –ö—Ä–∏—Ç–µ—Ä—ñ–π –ø–æ–¥—ñ–ª—É | gini, entropy | gini –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º |
| **max_leaf_nodes** | –ú–∞–∫—Å. –∫—ñ–ª—å–∫—ñ—Å—Ç—å –ª–∏—Å—Ç–∫—ñ–≤ | 2-‚àû | –û–±–º–µ–∂—É—î —Å–∫–ª–∞–¥–Ω—ñ—Å—Ç—å |
| **min_impurity_decrease** | –ú—ñ–Ω. –∑–º–µ–Ω—à–µ–Ω–Ω—è impurity | 0.0-‚àû | 0.01-0.1 –¥–ª—è pruning |

### –í–ø–ª–∏–≤ –≥—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤

```python
# –ï–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç –∑ —Ä—ñ–∑–Ω–∏–º–∏ max_depth
depths = [1, 2, 3, 5, 10, 20, None]
train_scores = []
test_scores = []

for depth in depths:
    tree = DecisionTreeClassifier(max_depth=depth, random_state=42)
    tree.fit(X_train, y_train)
    
    train_score = tree.score(X_train, y_train)
    test_score = tree.score(X_test, y_test)
    
    train_scores.append(train_score)
    test_scores.append(test_score)
    
    print(f"Depth: {depth:>4} | Train: {train_score:.4f} | Test: {test_score:.4f} | Overfitting: {train_score - test_score:.4f}")

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
plt.figure(figsize=(10, 6))
plt.plot([str(d) for d in depths], train_scores, 'o-', linewidth=2, label='Train Score')
plt.plot([str(d) for d in depths], test_scores, 's-', linewidth=2, label='Test Score')
plt.xlabel('Max Depth', fontsize=12)
plt.ylabel('Accuracy', fontsize=12)
plt.title('Effect of max_depth on Performance', fontsize=14, fontweight='bold')
plt.legend(fontsize=11)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

**–¢–∏–ø–æ–≤–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:**

```
Depth:    1 | Train: 0.8200 | Test: 0.8100 | Overfitting: 0.0100
Depth:    2 | Train: 0.8900 | Test: 0.8750 | Overfitting: 0.0150
Depth:    3 | Train: 0.9300 | Test: 0.9000 | Overfitting: 0.0300
Depth:    5 | Train: 0.9700 | Test: 0.8900 | Overfitting: 0.0800
Depth:   10 | Train: 0.9950 | Test: 0.8500 | Overfitting: 0.1450  ‚Üê Overfitting!
Depth:   20 | Train: 1.0000 | Test: 0.8200 | Overfitting: 0.1800  ‚Üê –°–∏–ª—å–Ω–∏–π overfitting!
Depth: None | Train: 1.0000 | Test: 0.8000 | Overfitting: 0.2000  ‚Üê –ù–∞–π–≥—ñ—Ä—à–µ!
```

**–í–∏—Å–Ω–æ–≤–æ–∫:** max_depth = 3-5 ‚Äî –æ–ø—Ç–∏–º–∞–ª—å–Ω–∏–π –±–∞–ª–∞–Ω—Å!

---

## Pruning (–û–±—Ä—ñ–∑–∞–Ω–Ω—è –¥–µ—Ä–µ–≤–∞)

### –ü—Ä–æ–±–ª–µ–º–∞: Overfitting

```
Overfitting –¥–µ—Ä–µ–≤–æ:
                [Root]
              /    |    \
           /       |       \
        /          |          \
     [...]       [...]       [...]
    /  |  \     /  |  \     /  |  \
  [Leaf][...] [Leaf][...] [Leaf][...]
  
–ó–∞–Ω–∞–¥—Ç–æ —Å–∫–ª–∞–¥–Ω–µ ‚Üí –∑–∞–ø–∞–º'—è—Ç–æ–≤—É—î noise
```

### –†—ñ—à–µ–Ω–Ω—è 1: Pre-pruning (Early Stopping)

**–ó—É–ø–∏–Ω–∫–∞ –ø—ñ–¥ —á–∞—Å –ø–æ–±—É–¥–æ–≤–∏** —á–µ—Ä–µ–∑ –≥—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∏:

```python
tree_pruned = DecisionTreeClassifier(
    max_depth=5,                    # –û–±–º–µ–∂–∏—Ç–∏ –≥–ª–∏–±–∏–Ω—É
    min_samples_split=20,           # –ú—ñ–Ω—ñ–º—É–º –¥–ª—è –ø–æ–¥—ñ–ª—É
    min_samples_leaf=10,            # –ú—ñ–Ω—ñ–º—É–º —É –ª–∏—Å—Ç—ñ
    max_leaf_nodes=20,              # –ú–∞–∫—Å. –ª–∏—Å—Ç–∫—ñ–≤
    min_impurity_decrease=0.01,    # –ú—ñ–Ω. –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è
    random_state=42
)
```

### –†—ñ—à–µ–Ω–Ω—è 2: Post-pruning (Cost Complexity Pruning)

**–û–±—Ä—ñ–∑–∞–Ω–Ω—è –ø—ñ—Å–ª—è –ø–æ–±—É–¥–æ–≤–∏** ‚Äî scikit-learn –ø—ñ–¥—Ç—Ä–∏–º—É—î —á–µ—Ä–µ–∑ `ccp_alpha`:

```python
# 1. –ó–Ω–∞–π—Ç–∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω–∏–π ccp_alpha
path = tree_clf.cost_complexity_pruning_path(X_train, y_train)
ccp_alphas = path.ccp_alphas
impurities = path.impurities

# 2. –¢—Ä–µ–Ω—É–≤–∞—Ç–∏ –¥–µ—Ä–µ–≤–∞ –∑ —Ä—ñ–∑–Ω–∏–º–∏ alpha
train_scores = []
test_scores = []

for ccp_alpha in ccp_alphas:
    tree = DecisionTreeClassifier(ccp_alpha=ccp_alpha, random_state=42)
    tree.fit(X_train, y_train)
    train_scores.append(tree.score(X_train, y_train))
    test_scores.append(tree.score(X_test, y_test))

# 3. –í–∏–±—Ä–∞—Ç–∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω–∏–π alpha
best_alpha = ccp_alphas[np.argmax(test_scores)]
print(f"Best ccp_alpha: {best_alpha:.6f}")

# 4. –§—ñ–Ω–∞–ª—å–Ω–∞ –º–æ–¥–µ–ª—å
tree_final = DecisionTreeClassifier(ccp_alpha=best_alpha, random_state=42)
tree_final.fit(X_train, y_train)

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
fig, axes = plt.subplots(1, 2, figsize=(16, 6))

# Impurity vs alpha
axes[0].plot(ccp_alphas, impurities, marker='o')
axes[0].set_xlabel('ccp_alpha', fontsize=12)
axes[0].set_ylabel('Impurity', fontsize=12)
axes[0].set_title('Impurity vs alpha', fontsize=14, fontweight='bold')
axes[0].grid(True, alpha=0.3)

# Scores vs alpha
axes[1].plot(ccp_alphas, train_scores, marker='o', label='Train')
axes[1].plot(ccp_alphas, test_scores, marker='s', label='Test')
axes[1].axvline(x=best_alpha, color='red', linestyle='--', label=f'Best alpha={best_alpha:.4f}')
axes[1].set_xlabel('ccp_alpha', fontsize=12)
axes[1].set_ylabel('Accuracy', fontsize=12)
axes[1].set_title('Accuracy vs alpha', fontsize=14, fontweight='bold')
axes[1].legend(fontsize=11)
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

---

## Feature Importance

### –Ø–∫ –æ–±—á–∏—Å–ª—é—î—Ç—å—Å—è?

**–î–ª—è –∫–æ–∂–Ω–æ—ó –æ–∑–Ω–∞–∫–∏:**
$$\text{Importance}(f) = \sum_{t \in \text{splits using } f} \frac{N_t}{N} \times \Delta\text{Impurity}_t$$

–¥–µ:
- $N_t$ ‚Äî –∫—ñ–ª—å–∫—ñ—Å—Ç—å –ø—Ä–∏–∫–ª–∞–¥—ñ–≤ —É –≤—É–∑–ª—ñ $t$
- $N$ ‚Äî –∑–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –ø—Ä–∏–∫–ª–∞–¥—ñ–≤
- $\Delta\text{Impurity}_t$ ‚Äî –∑–º–µ–Ω—à–µ–Ω–Ω—è impurity –≤—ñ–¥ –ø–æ–¥—ñ–ª—É

**–ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è:** —Å—É–º–∞ –≤—Å—ñ—Ö importance = 1.0

### –ö–æ–¥

```python
import pandas as pd

# Feature importance
feature_names = ['–î–æ—Ö—ñ–¥', '–í—ñ–∫', '–ë–æ—Ä–≥', '–ö—Ä–µ–¥–∏—Ç–Ω–∏–π_—Ä–µ–π—Ç–∏–Ω–≥']
importances = tree_clf.feature_importances_

# DataFrame
feature_importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': importances
}).sort_values('Importance', ascending=False)

print(feature_importance_df)

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
plt.figure(figsize=(10, 6))
plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])
plt.xlabel('Importance', fontsize=12)
plt.ylabel('Feature', fontsize=12)
plt.title('Feature Importance', fontsize=14, fontweight='bold')
plt.grid(True, alpha=0.3, axis='x')
plt.tight_layout()
plt.show()
```

**–ü—Ä–∏–∫–ª–∞–¥ –≤–∏—Ö–æ–¥—É:**

```
          Feature  Importance
0          –ë–æ—Ä–≥      0.5200
1    –î–æ—Ö—ñ–¥         0.2800
2          –í—ñ–∫       0.1500
3  –ö—Ä–µ–¥–∏—Ç–Ω–∏–π_—Ä–µ–π—Ç–∏–Ω–≥  0.0500
```

**–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü—ñ—è:**
- **–ë–æ—Ä–≥** ‚Äî –Ω–∞–π–≤–∞–∂–ª–∏–≤—ñ—à–∞ –æ–∑–Ω–∞–∫–∞ (52%)
- **–î–æ—Ö—ñ–¥** ‚Äî –¥—Ä—É–≥–∞ –∑–∞ –≤–∞–∂–ª–∏–≤—ñ—Å—Ç—é (28%)
- **–ö—Ä–µ–¥–∏—Ç–Ω–∏–π —Ä–µ–π—Ç–∏–Ω–≥** ‚Äî –º–∞–π–∂–µ –Ω–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è (5%)

---

## Decision Boundaries (–ú–µ–∂—ñ —Ä—ñ—à–µ–Ω–Ω—è)

### –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –¥–ª—è 2D –¥–∞–Ω–∏—Ö

```python
from matplotlib.colors import ListedColormap

def plot_decision_boundary(model, X, y, title="Decision Boundary"):
    """–í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –º–µ–∂ —Ä—ñ—à–µ–Ω–Ω—è –¥–ª—è 2D –¥–∞–Ω–∏—Ö"""
    h = 0.02  # –ö—Ä–æ–∫ —Å—ñ—Ç–∫–∏
    
    # –°—Ç–≤–æ—Ä–∏—Ç–∏ —Å—ñ—Ç–∫—É
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))
    
    # –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –¥–ª—è –≤—Å—ñ—î—ó —Å—ñ—Ç–∫–∏
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    
    # –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
    plt.figure(figsize=(10, 8))
    plt.contourf(xx, yy, Z, alpha=0.3, cmap=ListedColormap(['#FFAAAA', '#AAAAFF']))
    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=ListedColormap(['#FF0000', '#0000FF']), 
                edgecolor='black', s=50, alpha=0.7)
    plt.xlabel('Feature 0', fontsize=12)
    plt.ylabel('Feature 1', fontsize=12)
    plt.title(title, fontsize=14, fontweight='bold')
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()

# –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è
tree_shallow = DecisionTreeClassifier(max_depth=2, random_state=42)
tree_shallow.fit(X_train, y_train)
plot_decision_boundary(tree_shallow, X_train, y_train, "Decision Tree (depth=2)")

tree_deep = DecisionTreeClassifier(max_depth=10, random_state=42)
tree_deep.fit(X_train, y_train)
plot_decision_boundary(tree_deep, X_train, y_train, "Decision Tree (depth=10)")
```

**–°–ø–æ—Å—Ç–µ—Ä–µ–∂–µ–Ω–Ω—è:**
- **Depth=2:** –ø—Ä–æ—Å—Ç—ñ –ø—Ä—è–º–æ–∫—É—Ç–Ω—ñ –æ–±–ª–∞—Å—Ç—ñ (underfitting –º–æ–∂–ª–∏–≤–æ)
- **Depth=10:** –¥—É–∂–µ —Å–∫–ª–∞–¥–Ω—ñ –º–µ–∂—ñ (overfitting!)

Decision Trees —Å—Ç–≤–æ—Ä—é—é—Ç—å **–ø—Ä—è–º–æ–∫—É—Ç–Ω—ñ** (axis-aligned) –º–µ–∂—ñ —Ä—ñ—à–µ–Ω–Ω—è!

---

## –ü–µ—Ä–µ–≤–∞–≥–∏ —Ç–∞ –Ω–µ–¥–æ–ª—ñ–∫–∏

### –ü–µ—Ä–µ–≤–∞–≥–∏ ‚úì

| –ü–µ—Ä–µ–≤–∞–≥–∞ | –ü–æ—è—Å–Ω–µ–Ω–Ω—è |
|----------|-----------|
| **–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å** | –õ–µ–≥–∫–æ –≤—ñ–∑—É–∞–ª—ñ–∑—É–≤–∞—Ç–∏ —Ç–∞ –ø–æ—è—Å–Ω–∏—Ç–∏ |
| **–ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∞ –ø—ñ–¥–≥–æ—Ç–æ–≤–∫–∞** | –ù–µ –ø–æ—Ç—Ä—ñ–±–Ω–∞ –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è, encoding |
| **–ù–µ–ª—ñ–Ω—ñ–π–Ω–æ—Å—Ç—ñ** | –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –≤–∏—è–≤–ª—è—î —Å–∫–ª–∞–¥–Ω—ñ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ |
| **–ó–º—ñ—à–∞–Ω—ñ —Ç–∏–ø–∏** | –ü—Ä–∞—Ü—é—î –∑ —á–∏—Å–ª–æ–≤–∏–º–∏ + –∫–∞—Ç–µ–≥–æ—Ä—ñ–∞–ª—å–Ω–∏–º–∏ |
| **Feature importance** | –ü–æ–∫–∞–∑—É—î –Ω–∞–π–≤–∞–∂–ª–∏–≤—ñ—à—ñ –æ–∑–Ω–∞–∫–∏ |
| **–ü—Ä–æ–ø—É—â–µ–Ω—ñ –¥–∞–Ω—ñ** | –ú–æ–∂–µ –ø—Ä–∞—Ü—é–≤–∞—Ç–∏ –∑ missing values |
| **–®–≤–∏–¥–∫—ñ—Å—Ç—å** | –®–≤–∏–¥–∫–µ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è O(log n) |
| **–ù–µ —á—É—Ç–ª–∏–≤–∏–π –¥–æ –≤–∏–∫–∏–¥—ñ–≤** | –†–æ–∑–¥—ñ–ª—è—î –ø–æ —Ä–∞–Ω–≥–∞–º, –Ω–µ –ø–æ –∑–Ω–∞—á–µ–Ω–Ω—è–º |
| **–ë–µ–∑ –ø—Ä–∏–ø—É—â–µ–Ω—å** | –ù–µ –ø—Ä–∏–ø—É—Å–∫–∞—î —Ä–æ–∑–ø–æ–¥—ñ–ª –¥–∞–Ω–∏—Ö |

### –ù–µ–¥–æ–ª—ñ–∫–∏ ‚úó

| –ù–µ–¥–æ–ª—ñ–∫ | –ü–æ—è—Å–Ω–µ–Ω–Ω—è |
|---------|-----------|
| **Overfitting** | –õ–µ–≥–∫–æ –ø–µ—Ä–µ–Ω–∞–≤—á–∞—î—Ç—å—Å—è –±–µ–∑ –æ–±–º–µ–∂–µ–Ω—å |
| **–ù–µ—Å—Ç–∞–±—ñ–ª—å–Ω—ñ—Å—Ç—å** | –ú–∞–ª—ñ –∑–º—ñ–Ω–∏ –¥–∞–Ω–∏—Ö ‚Üí —ñ–Ω—à–µ –¥–µ—Ä–µ–≤–æ |
| **Bias –¥–æ –±–∞–≥–∞—Ç–∏—Ö –∫–ª–∞—Å—ñ–≤** | –î–æ–º—ñ–Ω—É–≤–∞–Ω–Ω—è —á–∞—Å—Ç—ñ—à–∏—Ö –∫–ª–∞—Å—ñ–≤ |
| **Axis-aligned splits** | –¢—ñ–ª—å–∫–∏ –ø—Ä—è–º–æ–∫—É—Ç–Ω—ñ –º–µ–∂—ñ |
| **–ù–µ –µ–∫—Å—Ç—Ä–∞–ø–æ–ª—é—î** | –ü–æ–≥–∞–Ω–æ –∑–∞ –º–µ–∂–∞–º–∏ train –¥–∞–Ω–∏—Ö |
| **–õ–æ–∫–∞–ª—å–Ω–∏–π –æ–ø—Ç–∏–º—É–º** | Greedy –∞–ª–≥–æ—Ä–∏—Ç–º (–Ω–µ –≥–ª–æ–±–∞–ª—å–Ω–∏–π) |
| **–í–µ–ª–∏–∫—ñ –¥–µ—Ä–µ–≤–∞** | –ú–æ–∂—É—Ç—å –±—É—Ç–∏ –¥—É–∂–µ —Å–∫–ª–∞–¥–Ω—ñ |

---

## –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –∑ —ñ–Ω—à–∏–º–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–∞–º–∏

### Decision Trees vs Linear Models

| –ö—Ä–∏—Ç–µ—Ä—ñ–π | Decision Trees | Linear Regression/Logistic |
|----------|----------------|----------------------------|
| **–õ—ñ–Ω—ñ–π–Ω—ñ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ** | ‚ùå –ù–µ–µ—Ñ–µ–∫—Ç–∏–≤–Ω–æ | ‚úÖ –Ü–¥–µ–∞–ª—å–Ω–æ |
| **–ù–µ–ª—ñ–Ω—ñ–π–Ω—ñ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ** | ‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ | ‚ùå –ü–æ—Ç—Ä–µ–±—É—î feature engineering |
| **–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å** | ‚úÖ –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è | ‚úÖ –ö–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç–∏ |
| **–ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö** | ‚úÖ –ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∞ | ‚ùå –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è, encoding |
| **Overfitting** | ‚ùå –õ–µ–≥–∫–æ | ‚úÖ –ú–µ–Ω—à–µ (–∑ regularization) |
| **–ï–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü—ñ—è** | ‚ùå –ü–æ–≥–∞–Ω–æ | ‚úÖ –ö—Ä–∞—â–µ |
| **–¢–æ—á–Ω—ñ—Å—Ç—å** | ‚ùå –°–µ—Ä–µ–¥–Ω—è | ‚ùå –ù–∏–∑—å–∫–∞ –¥–ª—è –Ω–µ–ª—ñ–Ω—ñ–π–Ω–∏—Ö |

### Decision Trees vs Ensemble Methods

| –ö—Ä–∏—Ç–µ—Ä—ñ–π | Single Tree | Random Forest | Gradient Boosting |
|----------|-------------|---------------|-------------------|
| **–¢–æ—á–Ω—ñ—Å—Ç—å** | ‚ùå –°–µ—Ä–µ–¥–Ω—è | ‚úÖ –í–∏—Å–æ–∫–∞ | ‚úÖ –î—É–∂–µ –≤–∏—Å–æ–∫–∞ |
| **Overfitting** | ‚ùå –í–∏—Å–æ–∫–∏–π —Ä–∏–∑–∏–∫ | ‚úÖ –ù–∏–∑—å–∫–∏–π | ‚ö†Ô∏è –ú–æ–∂–ª–∏–≤–∏–π |
| **–®–≤–∏–¥–∫—ñ—Å—Ç—å** | ‚úÖ –î—É–∂–µ —à–≤–∏–¥–∫–æ | ‚ö†Ô∏è –°–µ—Ä–µ–¥–Ω—å–æ | ‚ùå –ü–æ–≤—ñ–ª—å–Ω–æ |
| **–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å** | ‚úÖ –í–∏—Å–æ–∫–∞ | ‚ùå –ù–∏–∑—å–∫–∞ | ‚ùå –ù–∏–∑—å–∫–∞ |
| **–°—Ç–∞–±—ñ–ª—å–Ω—ñ—Å—Ç—å** | ‚ùå –ù–µ—Å—Ç–∞–±—ñ–ª—å–Ω–∏–π | ‚úÖ –°—Ç–∞–±—ñ–ª—å–Ω–∏–π | ‚úÖ –°—Ç–∞–±—ñ–ª—å–Ω–∏–π |

---

## –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ Decision Trees

### –Ü–¥–µ–∞–ª—å–Ω–æ –ø—ñ–¥—Ö–æ–¥–∏—Ç—å ‚úì

- **–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å –∫—Ä–∏—Ç–∏—á–Ω–∞** ‚Äî –º–µ–¥–∏—Ü–∏–Ω–∞, –ø—Ä–∞–≤–æ, —Ñ—ñ–Ω–∞–Ω—Å–∏
- **–®–≤–∏–¥–∫–∏–π baseline** ‚Äî –ø–µ—Ä–µ–¥ —Å–∫–ª–∞–¥–Ω—ñ—à–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏
- **–ï–∫—Å–ø–ª–æ—Ä–∞—Ç–æ—Ä–Ω–∏–π –∞–Ω–∞–ª—ñ–∑** ‚Äî –∑—Ä–æ–∑—É–º—ñ—Ç–∏ –¥–∞–Ω—ñ
- **Feature importance** ‚Äî –∑–Ω–∞–π—Ç–∏ –∫–ª—é—á–æ–≤—ñ –æ–∑–Ω–∞–∫–∏
- **–ó–º—ñ—à–∞–Ω—ñ —Ç–∏–ø–∏ –¥–∞–Ω–∏—Ö** ‚Äî —á–∏—Å–ª–æ–≤—ñ + –∫–∞—Ç–µ–≥–æ—Ä—ñ–∞–ª—å–Ω—ñ
- **–ù–µ–≤–µ–ª–∏–∫—ñ/—Å–µ—Ä–µ–¥–Ω—ñ –¥–∞—Ç–∞—Å–µ—Ç–∏**
- **Presentation –¥–ª—è stakeholders** ‚Äî –ª–µ–≥–∫–æ –ø–æ—è—Å–Ω–∏—Ç–∏

### –ö—Ä–∞—â–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ —ñ–Ω—à–µ ‚úó

- **–ü–æ—Ç—Ä—ñ–±–Ω–∞ –≤–∏—Å–æ–∫–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å** ‚Üí Random Forest, XGBoost
- **–õ—ñ–Ω—ñ–π–Ω—ñ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ** ‚Üí Linear/Logistic Regression
- **–î—É–∂–µ –≤–µ–ª–∏–∫—ñ –¥–∞–Ω—ñ** ‚Üí SGD, online learning
- **–ï–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü—ñ—è** ‚Üí –æ–±–µ—Ä–µ–∂–Ω–æ, –¥–µ—Ä–µ–≤–∞ –ø–æ–≥–∞–Ω–æ –µ–∫—Å—Ç—Ä–∞–ø–æ–ª—é—é—Ç—å
- **–í–∏—Ä–æ–±–Ω–∏—Ü—Ç–≤–æ (production)** ‚Üí –∞–Ω—Å–∞–º–±–ª—ñ —Å—Ç–∞–±—ñ–ª—å–Ω—ñ—à—ñ

---

## –ü—Ä–∞–∫—Ç–∏—á–Ω—ñ –ø–æ—Ä–∞–¥–∏ üí°

1. **–ü–æ—á–Ω–∏ –∑ –Ω–µ–≤–µ–ª–∏–∫–æ—é –≥–ª–∏–±–∏–Ω–æ—é** (3-5) ‚Äî —É–Ω–∏–∫–Ω–∏ overfitting
2. **–í—ñ–∑—É–∞–ª—ñ–∑—É–π –¥–µ—Ä–µ–≤–æ** ‚Äî –∑—Ä–æ–∑—É–º—ñ–π –ª–æ–≥—ñ–∫—É –º–æ–¥–µ–ª—ñ
3. **Feature importance** ‚Äî –∑–Ω–∞–π–¥–∏ –∫–ª—é—á–æ–≤—ñ –æ–∑–Ω–∞–∫–∏
4. **Cross-validation** –¥–ª—è –ø—ñ–¥–±–æ—Ä—É –≥—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤
5. **Pruning** ‚Äî –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π ccp_alpha –∞–±–æ –æ–±–º–µ–∂—É–π –≥–ª–∏–±–∏–Ω—É
6. **–ù–µ –Ω–æ—Ä–º–∞–ª—ñ–∑—É–π –¥–∞–Ω—ñ** ‚Äî –¥–µ—Ä–µ–≤–∞ –Ω–µ –ø–æ—Ç—Ä–µ–±—É—é—Ç—å
7. **min_samples_leaf** ‚Äî –∑–±—ñ–ª—å—à–∏ –¥–ª—è –∑–≥–ª–∞–¥–∂—É–≤–∞–Ω–Ω—è
8. **Grid Search** –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤
9. **–ü–æ—Ä—ñ–≤–Ω—è–π –∑ Random Forest** ‚Äî –∑–∞–≤–∂–¥–∏ –ø–µ—Ä–µ–≤—ñ—Ä –∞–Ω—Å–∞–º–±–ª—å
10. **–î–æ–∫—É–º–µ–Ω—Ç—É–π —Ä—ñ—à–µ–Ω–Ω—è** ‚Äî –∑–±–µ—Ä—ñ–≥–∞–π –ø—Ä–∞–≤–∏–ª–∞ –¥–µ—Ä–µ–≤–∞

---

## –ü–æ—à–∏—Ä–µ–Ω—ñ –ø–æ–º–∏–ª–∫–∏ ‚ùå

### 1. –î–æ–∑–≤–æ–ª–∏—Ç–∏ –ø–æ–≤–Ω—ñ—Å—Ç—é —Ä–æ—Å—Ç–∏

```python
# ‚ùå –ù–ï–ü–†–ê–í–ò–õ–¨–ù–û
tree = DecisionTreeClassifier()  # –ë–µ–∑ –æ–±–º–µ–∂–µ–Ω—å ‚Üí overfitting!

# ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û
tree = DecisionTreeClassifier(
    max_depth=5,
    min_samples_split=20,
    min_samples_leaf=10
)
```

### 2. –ù–µ –ø–µ—Ä–µ–≤—ñ—Ä—è—Ç–∏ overfitting

```python
# ‚ùå –ù–ï–ü–†–ê–í–ò–õ–¨–ù–û
# –î–∏–≤–∏—Ç–∏—Å—å —Ç—ñ–ª—å–∫–∏ –Ω–∞ train accuracy

# ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û
print(f"Train accuracy: {tree.score(X_train, y_train):.4f}")
print(f"Test accuracy: {tree.score(X_test, y_test):.4f}")
# –Ø–∫—â–æ Train >> Test ‚Üí overfitting!
```

### 3. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –¥–ª—è –µ–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü—ñ—ó

```python
# ‚ùå –ù–ï–ë–ï–ó–ü–ï–ß–ù–û
# –ü–µ—Ä–µ–¥–±–∞—á–∞—Ç–∏ –¥–∞–ª–µ–∫–æ –∑–∞ –º–µ–∂–∞–º–∏ train –¥–∞–Ω–∏—Ö
# –î–µ—Ä–µ–≤–æ –ø–æ–≤–µ—Ä–Ω–µ –Ω–∞–π–±–ª–∏–∂—á–∏–π –ª–∏—Å—Ç, –∞ –Ω–µ –µ–∫—Å—Ç—Ä–∞–ø–æ–ª—é—î!

# ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û
if X_new > X_train.max():
    print("WARNING: Extrapolation! Use with caution.")
```

### 4. –ù–æ—Ä–º–∞–ª—ñ–∑—É–≤–∞—Ç–∏ –¥–∞–Ω—ñ

```python
# ‚ùå –ú–ê–†–ù–û (–Ω–µ —à–∫–æ–¥–∏—Ç—å, –∞–ª–µ –Ω–µ –ø–æ—Ç—Ä—ñ–±–Ω–æ)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
tree.fit(X_scaled, y)

# ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û (–∑–µ–∫–æ–Ω–æ–º–∏—Ç–∏ —á–∞—Å)
tree.fit(X, y)  # –ë–µ–∑ –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—ó
```

---

## –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –¥–µ—Ä–µ–≤–∞

### –ú–µ—Ç–æ–¥ 1: plot_tree (scikit-learn)

```python
from sklearn.tree import plot_tree

plt.figure(figsize=(20, 10))
plot_tree(
    tree_clf,
    feature_names=['Feature_1', 'Feature_2', 'Feature_3'],
    class_names=['Class_0', 'Class_1'],
    filled=True,           # –ó–∞–ø–æ–≤–Ω–∏—Ç–∏ –∫–æ–ª—å–æ—Ä–æ–º
    rounded=True,          # –û–∫—Ä—É–≥–ª–µ–Ω—ñ —Ä–∞–º–∫–∏
    fontsize=10,
    proportion=True,       # –ü—Ä–æ–ø–æ—Ä—Ü—ñ—ó –∫–ª–∞—Å—ñ–≤
    impurity=True,         # –ü–æ–∫–∞–∑–∞—Ç–∏ impurity
    precision=2            # –¢–æ—á–Ω—ñ—Å—Ç—å —á–∏—Å–µ–ª
)
plt.title('Decision Tree', fontsize=16, fontweight='bold')
plt.tight_layout()
plt.savefig('tree_visualization.png', dpi=300, bbox_inches='tight')
plt.show()
```

### –ú–µ—Ç–æ–¥ 2: Graphviz (–±—ñ–ª—å—à –∫—Ä–∞—Å–∏–≤–æ)

```python
from sklearn.tree import export_graphviz
import graphviz

# –ï–∫—Å–ø–æ—Ä—Ç —É DOT —Ñ–æ—Ä–º–∞—Ç
dot_data = export_graphviz(
    tree_clf,
    out_file=None,
    feature_names=['Feature_1', 'Feature_2'],
    class_names=['Class_0', 'Class_1'],
    filled=True,
    rounded=True,
    special_characters=True
)

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
graph = graphviz.Source(dot_data)
graph.render('decision_tree', format='png', cleanup=True)
graph.view()
```

### –ú–µ—Ç–æ–¥ 3: –¢–µ–∫—Å—Ç–æ–≤–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—è

```python
from sklearn.tree import export_text

tree_rules = export_text(
    tree_clf,
    feature_names=['Feature_1', 'Feature_2']
)

print(tree_rules)
```

**–ü—Ä–∏–∫–ª–∞–¥ –≤–∏—Ö–æ–¥—É:**

```
|--- Feature_1 <= 0.50
|   |--- Feature_2 <= -0.30
|   |   |--- class: 0
|   |--- Feature_2 >  -0.30
|   |   |--- class: 1
|--- Feature_1 >  0.50
|   |--- class: 1
```

---

## –†–æ–±–æ—Ç–∞ –∑ –∫–∞—Ç–µ–≥–æ—Ä—ñ–∞–ª—å–Ω–∏–º–∏ –∑–º—ñ–Ω–Ω–∏–º–∏

### Scikit-learn –≤–∏–º–∞–≥–∞—î —á–∏—Å–ª–æ–≤—ñ –¥–∞–Ω—ñ

```python
import pandas as pd
from sklearn.preprocessing import LabelEncoder

# –î–∞–Ω—ñ
df = pd.DataFrame({
    '–ö–æ–ª—ñ—Ä': ['—á–µ—Ä–≤–æ–Ω–∏–π', '–∑–µ–ª–µ–Ω–∏–π', '—Å–∏–Ω—ñ–π', '—á–µ—Ä–≤–æ–Ω–∏–π'],
    '–†–æ–∑–º—ñ—Ä': ['S', 'M', 'L', 'M'],
    '–¶—ñ–Ω–∞': [10, 20, 30, 15],
    '–ö—É–ø–∏–ª–∏': [0, 1, 1, 0]
})

# Encoding –∫–∞—Ç–µ–≥–æ—Ä—ñ–∞–ª—å–Ω–∏—Ö –∑–º—ñ–Ω–Ω–∏—Ö
le_color = LabelEncoder()
le_size = LabelEncoder()

df['–ö–æ–ª—ñ—Ä_encoded'] = le_color.fit_transform(df['–ö–æ–ª—ñ—Ä'])
df['–†–æ–∑–º—ñ—Ä_encoded'] = le_size.fit_transform(df['–†–æ–∑–º—ñ—Ä'])

# –ù–∞–≤—á–∞–Ω–Ω—è
X = df[['–ö–æ–ª—ñ—Ä_encoded', '–†–æ–∑–º—ñ—Ä_encoded', '–¶—ñ–Ω–∞']]
y = df['–ö—É–ø–∏–ª–∏']

tree = DecisionTreeClassifier(max_depth=3, random_state=42)
tree.fit(X, y)
```

**–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞:** One-Hot Encoding –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä—ñ–π –±–µ–∑ –ø–æ—Ä—è–¥–∫—É

```python
# One-Hot Encoding
df_encoded = pd.get_dummies(df, columns=['–ö–æ–ª—ñ—Ä', '–†–æ–∑–º—ñ—Ä'], drop_first=False)

X = df_encoded.drop('–ö—É–ø–∏–ª–∏', axis=1)
y = df_encoded['–ö—É–ø–∏–ª–∏']

tree.fit(X, y)
```

---

## –ü–æ–≤'—è–∑–∞–Ω—ñ —Ç–µ–º–∏

- [[02_Random_Forest]] ‚Äî –∞–Ω—Å–∞–º–±–ª—å –¥–µ—Ä–µ–≤
- [[03_Gradient_Boosting]] ‚Äî –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–∏–π –∞–Ω—Å–∞–º–±–ª—å
- [[05_Ensemble_Methods]] ‚Äî –∑–∞–≥–∞–ª—å–Ω–∞ —Ç–µ–æ—Ä—ñ—è
- [[06_Feature_Importance]] ‚Äî –∞–Ω–∞–ª—ñ–∑ –≤–∞–∂–ª–∏–≤–æ—Å—Ç—ñ –æ–∑–Ω–∞–∫
- [[Cross_Validation]] ‚Äî –æ—Ü—ñ–Ω–∫–∞ –º–æ–¥–µ–ª–µ–π
- [[Hyperparameter_Tuning]] ‚Äî Grid Search, Random Search

## –†–µ—Å—É—Ä—Å–∏

- [Scikit-learn: Decision Trees](https://scikit-learn.org/stable/modules/tree.html)
- [StatQuest: Decision Trees](https://www.youtube.com/watch?v=7VeUPuFGJHk)
- [CART: Classification and Regression Trees](https://www.amazon.com/Classification-Regression-Wadsworth-Statistics-Probability/dp/0412048418)
- [Interactive visualization](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/)

---

## –ö–ª—é—á–æ–≤—ñ –≤–∏—Å–Ω–æ–≤–∫–∏

> Decision Trees ‚Äî —ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ –º–æ–¥–µ–ª—ñ, —â–æ –±—É–¥—É—é—Ç—å –¥–µ—Ä–µ–≤–æ–ø–æ–¥—ñ–±–Ω—ñ –ø—Ä–∞–≤–∏–ª–∞ —Ä—ñ—à–µ–Ω—å —á–µ—Ä–µ–∑ —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–∏–π –ø–æ–¥—ñ–ª –¥–∞–Ω–∏—Ö.

**–û—Å–Ω–æ–≤–Ω—ñ –ø—Ä–∏–Ω—Ü–∏–ø–∏:**
- –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ —Ä–æ–∑–¥—ñ–ª—è—î –¥–∞–Ω—ñ –∑–∞ –æ–∑–Ω–∞–∫–∞–º–∏
- –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î Gini Impurity –∞–±–æ Entropy –¥–ª—è –≤–∏–±–æ—Ä—É –ø–æ–¥—ñ–ª—ñ–≤
- –°—Ç–≤–æ—Ä—é—î –ø—Ä–æ—Å—Ç—ñ if-else –ø—Ä–∞–≤–∏–ª–∞
- –õ–µ–≥–∫–æ –≤—ñ–∑—É–∞–ª—ñ–∑—É–≤–∞—Ç–∏ —Ç–∞ —ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç—É–≤–∞—Ç–∏

**–ê–ª–≥–æ—Ä–∏—Ç–º:**
1. –í–∏–±—Ä–∞—Ç–∏ –Ω–∞–π–∫—Ä–∞—â–∏–π –ø–æ–¥—ñ–ª (–º—ñ–Ω—ñ–º—ñ–∑—É—î impurity)
2. –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –∑–∞—Å—Ç–æ—Å—É–≤–∞—Ç–∏ –¥–æ –ø—ñ–¥–≤—É–∑–ª—ñ–≤
3. –ó—É–ø–∏–Ω–∏—Ç–∏—Å—å –ø—Ä–∏ –¥–æ—Å—è–≥–Ω–µ–Ω–Ω—ñ –∫—Ä–∏—Ç–µ—Ä—ñ—é

**–ö—Ä–∏—Ç–µ—Ä—ñ—ó –ø–æ–¥—ñ–ª—É:**
- **–ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è:** Gini Impurity –∞–±–æ Entropy
- **–†–µ–≥—Ä–µ—Å—ñ—è:** MSE (Mean Squared Error)

**–ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏:**
- –Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å + –∑–º—ñ—à–∞–Ω—ñ –¥–∞–Ω—ñ + feature importance = Decision Trees ‚úì

**–í–∞–∂–ª–∏–≤–æ:**
- –ó–ê–í–ñ–î–ò –æ–±–º–µ–∂—É–π –≥–ª–∏–±–∏–Ω—É (max_depth)
- –ü–µ—Ä–µ–≤—ñ—Ä—è–π –Ω–∞ overfitting (train vs test)
- –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π —è–∫ baseline –ø–µ—Ä–µ–¥ –∞–Ω—Å–∞–º–±–ª—è–º–∏
- –í—ñ–∑—É–∞–ª—ñ–∑—É–π –¥–µ—Ä–µ–≤–æ –¥–ª—è —ñ–Ω—Å–∞–π—Ç—ñ–≤

---

#ml #supervised-learning #decision-trees #classification #regression #interpretability
