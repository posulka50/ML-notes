# Gradient Descent (–ì—Ä–∞–¥—ñ—î–Ω—Ç–Ω–∏–π —Å–ø—É—Å–∫)

## –©–æ —Ü–µ?

**Gradient Descent** ‚Äî —Ü–µ **—ñ—Ç–µ—Ä–∞—Ç–∏–≤–Ω–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó**, —è–∫–∏–π –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –¥–ª—è –∑–Ω–∞—Ö–æ–¥–∂–µ–Ω–Ω—è –º—ñ–Ω—ñ–º—É–º—É —Ñ—É–Ω–∫—Ü—ñ—ó —à–ª—è—Ö–æ–º —Ä—É—Ö—É –≤ –Ω–∞–ø—Ä—è–º–∫—É –Ω–∞–π—à–≤–∏–¥—à–æ–≥–æ —Å–ø–∞–¥–∞–Ω–Ω—è (–ø—Ä–æ—Ç–∏–ª–µ–∂–Ω–æ–º—É –¥–æ –≥—Ä–∞–¥—ñ—î–Ω—Ç–∞).

**–ì–æ–ª–æ–≤–Ω–∞ —ñ–¥–µ—è:** –ø–æ—á–∏–Ω–∞—î–º–æ –∑ –≤–∏–ø–∞–¥–∫–æ–≤–æ—ó —Ç–æ—á–∫–∏ —Ç–∞ –∫—Ä–æ–∫ –∑–∞ –∫—Ä–æ–∫–æ–º —Ä—É—Ö–∞—î–º–æ—Å—è "–≤–Ω–∏–∑ –ø–æ —Å—Ö–∏–ª—É" –¥–æ –º—ñ–Ω—ñ–º—É–º—É —Ñ—É–Ω–∫—Ü—ñ—ó –≤—Ç—Ä–∞—Ç.

## –ù–∞–≤—ñ—â–æ –ø–æ—Ç—Ä—ñ–±–µ–Ω?

- üéØ **–ù–∞–≤—á–∞–Ω–Ω—è ML-–º–æ–¥–µ–ª–µ–π** ‚Äî –º—ñ–Ω—ñ–º—ñ–∑–∞—Ü—ñ—è —Ñ—É–Ω–∫—Ü—ñ—ó –≤—Ç—Ä–∞—Ç
- üìä **–í–µ–ª–∏–∫—ñ –¥–∞–Ω—ñ** ‚Äî —à–≤–∏–¥—à–µ –∑–∞ –∞–Ω–∞–ª—ñ—Ç–∏—á–Ω—ñ —Ä—ñ—à–µ–Ω–Ω—è (Normal Equation)
- üß† **Neural Networks** ‚Äî —î–¥–∏–Ω–∏–π –ø—Ä–∞–∫—Ç–∏—á–Ω–∏–π —Å–ø–æ—Å—ñ–± –Ω–∞–≤—á–∞–Ω–Ω—è
- ‚ö° **–ú–∞—Å—à—Ç–∞–±–æ–≤–∞–Ω—ñ—Å—Ç—å** ‚Äî –ø—Ä–∞—Ü—é—î –∑ –º—ñ–ª—å–π–æ–Ω–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤
- üîß **–£–Ω—ñ–≤–µ—Ä—Å–∞–ª—å–Ω—ñ—Å—Ç—å** ‚Äî –ø—Ä–∞—Ü—é—î –¥–ª—è –±—É–¥—å-—è–∫–æ—ó –¥–∏—Ñ–µ—Ä–µ–Ω—Ü—ñ–π–æ–≤–Ω–æ—ó —Ñ—É–Ω–∫—Ü—ñ—ó

## –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏?

**–ü–æ—Ç—Ä—ñ–±–Ω–æ:**
- **–í–µ–ª–∏–∫—ñ –¥–∞—Ç–∞—Å–µ—Ç–∏** ‚Äî $n > 100,000$ —Å–ø–æ—Å—Ç–µ—Ä–µ–∂–µ–Ω—å
- **–ë–∞–≥–∞—Ç–æ –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤** ‚Äî $p > 10,000$ –æ–∑–Ω–∞–∫
- –ù–µ–º–∞—î –∞–Ω–∞–ª—ñ—Ç–∏—á–Ω–æ–≥–æ —Ä—ñ—à–µ–Ω–Ω—è (Neural Networks, Logistic Regression)
- **Online learning** ‚Äî –º–æ–¥–µ–ª—å –æ–Ω–æ–≤–ª—é—î—Ç—å—Å—è –Ω–∞ –Ω–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö
- –û–±–º–µ–∂–µ–Ω–Ω—è –ø–æ –ø–∞–º'—è—Ç—ñ ‚Äî –Ω–µ –º–æ–∂–Ω–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –≤—Å—ñ –¥–∞–Ω—ñ

**–ù–µ –ø–æ—Ç—Ä—ñ–±–Ω–æ:**
- **–ú–∞–ª—ñ –¥–∞–Ω—ñ** ‚Äî Normal Equation —à–≤–∏–¥—à–µ
- –ú–∞–ª–æ –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ ‚Äî –∞–Ω–∞–ª—ñ—Ç–∏—á–Ω–µ —Ä—ñ—à–µ–Ω–Ω—è –ø—Ä–æ—Å—Ç—ñ—à–µ
- –ü–æ—Ç—Ä—ñ–±–Ω–∞ **–≥–∞—Ä–∞–Ω—Ç—ñ—è –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ –º—ñ–Ω—ñ–º—É–º—É** (–æ–ø—É–∫–ª—ñ —Ñ—É–Ω–∫—Ü—ñ—ó)

---

## –í—ñ–∑—É–∞–ª—å–Ω–∞ —ñ–Ω—Ç—É—ó—Ü—ñ—è

### 1D –≤–∏–ø–∞–¥–æ–∫

```
Loss J(Œ∏)
    |
    |    ‚ï±‚ï≤
    |   ‚ï±  ‚ï≤
    |  ‚ï±    ‚ï≤
    | ‚ï±  ‚Ä¢   ‚ï≤
    |‚ï±    ‚Üì   ‚ï≤
    |      ‚Ä¢   ‚ï≤
    |       ‚Üì   ‚ï≤
    |        ‚Ä¢‚Üí  ‚ï≤
    |         ‚òÖ   ‚ï≤
    |______________‚ï≤_____ Œ∏
                   min
                   
‚Ä¢ ‚Äî –ø–æ—Ç–æ—á–Ω–∞ –ø–æ–∑–∏—Ü—ñ—è
‚Üì/‚Üí ‚Äî –Ω–∞–ø—Ä—è–º–æ–∫ —Ä—É—Ö—É (–ø—Ä–æ—Ç–∏–ª–µ–∂–Ω–∏–π –≥—Ä–∞–¥—ñ—î–Ω—Ç—É)
‚òÖ ‚Äî –º—ñ–Ω—ñ–º—É–º
```

### 2D –≤–∏–ø–∞–¥–æ–∫ (–ø–æ–≤–µ—Ä—Ö–Ω—è)

```
–í–∏–¥ –∑–≤–µ—Ä—Ö—É (–∫–æ–Ω—Ç—É—Ä–Ω—ñ –ª—ñ–Ω—ñ—ó):

        –í–∏—Å–æ–∫—ñ –≤—Ç—Ä–∞—Ç–∏
             ‚ï±‚îÄ‚îÄ‚ï≤
            ‚ï±    ‚ï≤
           ‚ï±  ‚Ä¢   ‚ï≤
          ‚îÇ    ‚Üò   ‚îÇ
          ‚îÇ     ‚Ä¢  ‚îÇ
          ‚îÇ      ‚Üò ‚îÇ
          ‚îÇ       ‚Ä¢‚îÇ
           ‚ï≤      ‚òÖ‚ï±  ‚Üê –ú—ñ–Ω—ñ–º—É–º
            ‚ï≤____‚ï±
            
‚Ä¢ ‚Äî –∫—Ä–æ–∫–∏ –≥—Ä–∞–¥—ñ—î–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫—É
‚òÖ ‚Äî –æ–ø—Ç–∏–º–∞–ª—å–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏
```

**–ì—Ä–∞–¥—ñ—î–Ω—Ç –≤–∫–∞–∑—É—î –Ω–∞–ø—Ä—è–º–æ–∫ –Ω–∞–π—à–≤–∏–¥—à–æ–≥–æ –ó–†–û–°–¢–ê–ù–ù–Ø, —Ç–æ–º—É –º–∏ –π–¥–µ–º–æ –≤ –ü–†–û–¢–ò–õ–ï–ñ–ù–û–ú–£ –Ω–∞–ø—Ä—è–º–∫—É.**

---

## –ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞

### –§—É–Ω–∫—Ü—ñ—è –≤—Ç—Ä–∞—Ç

–î–ª—è –ª—ñ–Ω—ñ–π–Ω–æ—ó —Ä–µ–≥—Ä–µ—Å—ñ—ó:
$$J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)})^2$$

–¥–µ:
- $J(\theta)$ ‚Äî —Ñ—É–Ω–∫—Ü—ñ—è –≤—Ç—Ä–∞—Ç (MSE)
- $\theta = [\theta_0, \theta_1, ..., \theta_n]$ ‚Äî –≤–µ–∫—Ç–æ—Ä –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤
- $h_\theta(x) = \theta^T x$ ‚Äî –≥—ñ–ø–æ—Ç–µ–∑–∞ (–ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è)
- $m$ ‚Äî –∫—ñ–ª—å–∫—ñ—Å—Ç—å –ø—Ä–∏–∫–ª–∞–¥—ñ–≤

### –ì—Ä–∞–¥—ñ—î–Ω—Ç

**–ì—Ä–∞–¥—ñ—î–Ω—Ç** ‚Äî —Ü–µ –≤–µ–∫—Ç–æ—Ä —á–∞—Å—Ç–∫–æ–≤–∏—Ö –ø–æ—Ö—ñ–¥–Ω–∏—Ö:

$$\nabla J(\theta) = \begin{bmatrix}
\frac{\partial J}{\partial \theta_0} \\
\frac{\partial J}{\partial \theta_1} \\
\vdots \\
\frac{\partial J}{\partial \theta_n}
\end{bmatrix}$$

–î–ª—è –ª—ñ–Ω—ñ–π–Ω–æ—ó —Ä–µ–≥—Ä–µ—Å—ñ—ó:
$$\frac{\partial J}{\partial \theta_j} = \frac{1}{m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)}$$

### –ü—Ä–∞–≤–∏–ª–æ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è

**–û–¥–Ω–æ—á–∞—Å–Ω–æ** –¥–ª—è –≤—Å—ñ—Ö $j$:
$$\theta_j := \theta_j - \alpha \frac{\partial J}{\partial \theta_j}$$

–¥–µ $\alpha$ (alpha) ‚Äî **learning rate** (—à–≤–∏–¥–∫—ñ—Å—Ç—å –Ω–∞–≤—á–∞–Ω–Ω—è).

### –í–µ–∫—Ç–æ—Ä–Ω–∞ —Ñ–æ—Ä–º–∞

$$\theta := \theta - \alpha \nabla J(\theta)$$

**–ü–æ–∫—Ä–æ–∫–æ–≤–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º:**
1. –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É–≤–∞—Ç–∏ $\theta$ –≤–∏–ø–∞–¥–∫–æ–≤–∏–º–∏ –∑–Ω–∞—á–µ–Ω–Ω—è–º–∏
2. –û–±—á–∏—Å–ª–∏—Ç–∏ –≥—Ä–∞–¥—ñ—î–Ω—Ç $\nabla J(\theta)$
3. –û–Ω–æ–≤–∏—Ç–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏: $\theta := \theta - \alpha \nabla J(\theta)$
4. –ü–æ–≤—Ç–æ—Ä—é–≤–∞—Ç–∏ –∫—Ä–æ–∫–∏ 2-3 –¥–æ –∑–±—ñ–∂–Ω–æ—Å—Ç—ñ

---

## –¢–∏–ø–∏ Gradient Descent

## 1. Batch Gradient Descent (–ü–∞–∫–µ—Ç–Ω–∏–π)

### –Ø–∫ –ø—Ä–∞—Ü—é—î?

–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î **–≤—Å—ñ** —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ –Ω–∞ –∫–æ–∂–Ω—ñ–π —ñ—Ç–µ—Ä–∞—Ü—ñ—ó.

$$\theta := \theta - \alpha \frac{1}{m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) x^{(i)}$$

### –ö–æ–¥

```python
def batch_gradient_descent(X, y, theta, alpha, iterations):
    """
    Batch Gradient Descent
    
    X: –º–∞—Ç—Ä–∏—Ü—è –æ–∑–Ω–∞–∫ (m x n)
    y: –≤–µ–∫—Ç–æ—Ä —Ü—ñ–ª—å–æ–≤–∏—Ö –∑–Ω–∞—á–µ–Ω—å (m,)
    theta: –ø–æ—á–∞—Ç–∫–æ–≤—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ (n,)
    alpha: learning rate
    iterations: –∫—ñ–ª—å–∫—ñ—Å—Ç—å —ñ—Ç–µ—Ä–∞—Ü—ñ–π
    """
    m = len(y)
    cost_history = []
    
    for i in range(iterations):
        # –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
        predictions = X.dot(theta)
        
        # –ü–æ–º–∏–ª–∫–∏
        errors = predictions - y
        
        # –ì—Ä–∞–¥—ñ—î–Ω—Ç (–≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î –í–°–Ü –¥–∞–Ω—ñ)
        gradient = (1/m) * X.T.dot(errors)
        
        # –û–Ω–æ–≤–ª–µ–Ω–Ω—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤
        theta = theta - alpha * gradient
        
        # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ñ—É–Ω–∫—Ü—ñ—ó –≤—Ç—Ä–∞—Ç
        cost = (1/(2*m)) * np.sum(errors**2)
        cost_history.append(cost)
        
        if i % 100 == 0:
            print(f"Iteration {i}: Cost = {cost:.4f}")
    
    return theta, cost_history
```

### –ü–µ—Ä–µ–≤–∞–≥–∏ ‚úì

- ‚úÖ **–°—Ç–∞–±—ñ–ª—å–Ω–∞ –∑–±—ñ–∂–Ω—ñ—Å—Ç—å** ‚Äî –ø–ª–∞–≤–Ω–∏–π —Å–ø—É—Å–∫ –¥–æ –º—ñ–Ω—ñ–º—É–º—É
- ‚úÖ **–¢–æ—á–Ω–∏–π –≥—Ä–∞–¥—ñ—î–Ω—Ç** ‚Äî –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î –≤—Å—ñ –¥–∞–Ω—ñ
- ‚úÖ **–¢–µ–æ—Ä–µ—Ç–∏—á–Ω—ñ –≥–∞—Ä–∞–Ω—Ç—ñ—ó** ‚Äî –∑–±—ñ–≥–∞—î—Ç—å—Å—è –¥–ª—è –æ–ø—É–∫–ª–∏—Ö —Ñ—É–Ω–∫—Ü—ñ–π

### –ù–µ–¥–æ–ª—ñ–∫–∏ ‚úó

- ‚ùå **–ü–æ–≤—ñ–ª—å–Ω–∏–π** ‚Äî –æ–±—á–∏—Å–ª—é—î –≥—Ä–∞–¥—ñ—î–Ω—Ç –Ω–∞ –≤—Å—ñ—Ö –¥–∞–Ω–∏—Ö
- ‚ùå **–ë–∞–≥–∞—Ç–æ –ø–∞–º'—è—Ç—ñ** ‚Äî –ø–æ—Ç—Ä—ñ–±–Ω–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –≤–µ—Å—å –¥–∞—Ç–∞—Å–µ—Ç
- ‚ùå **–ù–µ –ø—Ä–∞—Ü—é—î –¥–ª—è online learning**

### –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏?

- –ú–∞–ª—ñ/—Å–µ—Ä–µ–¥–Ω—ñ –¥–∞—Ç–∞—Å–µ—Ç–∏ ($m < 10,000$)
- –î–æ—Å—Ç–∞—Ç–Ω—å–æ –ø–∞–º'—è—Ç—ñ
- –ü–æ—Ç—Ä—ñ–±–Ω–∞ —Å—Ç–∞–±—ñ–ª—å–Ω–∞ –∑–±—ñ–∂–Ω—ñ—Å—Ç—å

---

## 2. Stochastic Gradient Descent (SGD, –°—Ç–æ—Ö–∞—Å—Ç–∏—á–Ω–∏–π)

### –Ø–∫ –ø—Ä–∞—Ü—é—î?

–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î **–æ–¥–∏–Ω –≤–∏–ø–∞–¥–∫–æ–≤–∏–π** –ø—Ä–∏–∫–ª–∞–¥ –Ω–∞ –∫–æ–∂–Ω—ñ–π —ñ—Ç–µ—Ä–∞—Ü—ñ—ó.

$$\theta := \theta - \alpha (h_\theta(x^{(i)}) - y^{(i)}) x^{(i)}$$

### –ö–æ–¥

```python
def stochastic_gradient_descent(X, y, theta, alpha, epochs):
    """
    Stochastic Gradient Descent
    
    epochs: –∫—ñ–ª—å–∫—ñ—Å—Ç—å –ø—Ä–æ—Ö–æ–¥—ñ–≤ —á–µ—Ä–µ–∑ –≤–µ—Å—å –¥–∞—Ç–∞—Å–µ—Ç
    """
    m = len(y)
    cost_history = []
    
    for epoch in range(epochs):
        # –ü–µ—Ä–µ–º—ñ—à–∞—Ç–∏ –¥–∞–Ω—ñ
        indices = np.random.permutation(m)
        X_shuffled = X[indices]
        y_shuffled = y[indices]
        
        for i in range(m):
            # –û–¥–∏–Ω –ø—Ä–∏–∫–ª–∞–¥
            xi = X_shuffled[i:i+1]
            yi = y_shuffled[i:i+1]
            
            # –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
            prediction = xi.dot(theta)
            
            # –ü–æ–º–∏–ª–∫–∞
            error = prediction - yi
            
            # –ì—Ä–∞–¥—ñ—î–Ω—Ç (—Ç—ñ–ª—å–∫–∏ –¥–ª—è –û–î–ù–û–ì–û –ø—Ä–∏–∫–ª–∞–¥—É)
            gradient = xi.T.dot(error)
            
            # –û–Ω–æ–≤–ª–µ–Ω–Ω—è
            theta = theta - alpha * gradient.flatten()
        
        # Cost –Ω–∞ –≤—Å—å–æ–º—É –¥–∞—Ç–∞—Å–µ—Ç—ñ (–¥–ª—è –º–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥—É)
        predictions = X.dot(theta)
        cost = (1/(2*m)) * np.sum((predictions - y)**2)
        cost_history.append(cost)
        
        if epoch % 10 == 0:
            print(f"Epoch {epoch}: Cost = {cost:.4f}")
    
    return theta, cost_history
```

### –ü–µ—Ä–µ–≤–∞–≥–∏ ‚úì

- ‚úÖ **–î—É–∂–µ —à–≤–∏–¥–∫–∏–π** ‚Äî –æ–Ω–æ–≤–ª–µ–Ω–Ω—è –ø—ñ—Å–ª—è –∫–æ–∂–Ω–æ–≥–æ –ø—Ä–∏–∫–ª–∞–¥—É
- ‚úÖ **–ú–∞–ª–æ –ø–∞–º'—è—Ç—ñ** ‚Äî –ø–æ –æ–¥–Ω–æ–º—É –ø—Ä–∏–∫–ª–∞–¥—É
- ‚úÖ **Online learning** ‚Äî –º–æ–∂–µ –æ–Ω–æ–≤–ª—é–≤–∞—Ç–∏—Å—å –Ω–∞ –Ω–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö
- ‚úÖ **–ú–æ–∂–µ –≤—Ç—ñ–∫–∞—Ç–∏ –∑ –ª–æ–∫–∞–ª—å–Ω–∏—Ö –º—ñ–Ω—ñ–º—É–º—ñ–≤** ‚Äî —á–µ—Ä–µ–∑ —à—É–º

### –ù–µ–¥–æ–ª—ñ–∫–∏ ‚úó

- ‚ùå **–ù–µ—Å—Ç–∞–±—ñ–ª—å–Ω–∞ –∑–±—ñ–∂–Ω—ñ—Å—Ç—å** ‚Äî "—à—É–º" –≤ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è—Ö
- ‚ùå **–ù–µ –∑–±—ñ–≥–∞—î—Ç—å—Å—è —Ç–æ—á–Ω–æ** ‚Äî –∫–æ–ª–∏–≤–∞—î—Ç—å—Å—è –Ω–∞–≤–∫–æ–ª–æ –º—ñ–Ω—ñ–º—É–º—É
- ‚ùå **–ü–æ—Ç—Ä—ñ–±–µ–Ω learning rate decay** ‚Äî –∑–º–µ–Ω—à—É–≤–∞—Ç–∏ Œ±

### –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏?

- –î—É–∂–µ –≤–µ–ª–∏–∫—ñ –¥–∞—Ç–∞—Å–µ—Ç–∏ ($m > 1,000,000$)
- Online learning
- –û–±–º–µ–∂–µ–Ω–Ω—è –ø–æ –ø–∞–º'—è—Ç—ñ
- –®–≤–∏–¥–∫—ñ –Ω–∞–±–ª–∏–∂–µ–Ω—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏

---

## 3. Mini-Batch Gradient Descent (–ú—ñ–Ω—ñ-–ø–∞–∫–µ—Ç–Ω–∏–π)

### –Ø–∫ –ø—Ä–∞—Ü—é—î?

–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î **–Ω–µ–≤–µ–ª–∏–∫—É –ø—ñ–¥–º–Ω–æ–∂–∏–Ω—É** –¥–∞–Ω–∏—Ö (batch) –Ω–∞ –∫–æ–∂–Ω—ñ–π —ñ—Ç–µ—Ä–∞—Ü—ñ—ó.

$$\theta := \theta - \alpha \frac{1}{b} \sum_{i \in \text{batch}} (h_\theta(x^{(i)}) - y^{(i)}) x^{(i)}$$

–¥–µ $b$ ‚Äî batch size (–∑–∞–∑–≤–∏—á–∞–π 32, 64, 128, 256).

### –ö–æ–¥

```python
def mini_batch_gradient_descent(X, y, theta, alpha, epochs, batch_size=32):
    """
    Mini-Batch Gradient Descent
    """
    m = len(y)
    cost_history = []
    
    for epoch in range(epochs):
        # –ü–µ—Ä–µ–º—ñ—à–∞—Ç–∏ –¥–∞–Ω—ñ
        indices = np.random.permutation(m)
        X_shuffled = X[indices]
        y_shuffled = y[indices]
        
        # –Ü—Ç–µ—Ä–∞—Ü—ñ—è –ø–æ –º—ñ–Ω—ñ-–±–∞—Ç—á–∞–º
        for i in range(0, m, batch_size):
            # –í–∏—Ç—è–≥—Ç–∏ batch
            X_batch = X_shuffled[i:i+batch_size]
            y_batch = y_shuffled[i:i+batch_size]
            
            batch_m = len(y_batch)
            
            # –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
            predictions = X_batch.dot(theta)
            
            # –ü–æ–º–∏–ª–∫–∏
            errors = predictions - y_batch
            
            # –ì—Ä–∞–¥—ñ—î–Ω—Ç (–Ω–∞ batch)
            gradient = (1/batch_m) * X_batch.T.dot(errors)
            
            # –û–Ω–æ–≤–ª–µ–Ω–Ω—è
            theta = theta - alpha * gradient
        
        # Cost –Ω–∞ –≤—Å—å–æ–º—É –¥–∞—Ç–∞—Å–µ—Ç—ñ
        predictions = X.dot(theta)
        cost = (1/(2*m)) * np.sum((predictions - y)**2)
        cost_history.append(cost)
        
        if epoch % 10 == 0:
            print(f"Epoch {epoch}: Cost = {cost:.4f}")
    
    return theta, cost_history
```

### –ü–µ—Ä–µ–≤–∞–≥–∏ ‚úì

- ‚úÖ **–ë–∞–ª–∞–Ω—Å —à–≤–∏–¥–∫–æ—Å—Ç—ñ —Ç–∞ —Å—Ç–∞–±—ñ–ª—å–Ω–æ—Å—Ç—ñ**
- ‚úÖ **–ï—Ñ–µ–∫—Ç–∏–≤–Ω–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è GPU** ‚Äî –≤–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω—ñ –æ–ø–µ—Ä–∞—Ü—ñ—ó
- ‚úÖ **–ú–µ–Ω—à–µ —à—É–º—É** –Ω—ñ–∂ SGD
- ‚úÖ **–®–≤–∏–¥—à–µ** –Ω—ñ–∂ Batch GD
- ‚úÖ **–ù–∞–π–ø–æ–ø—É–ª—è—Ä–Ω—ñ—à–∏–π** –Ω–∞ –ø—Ä–∞–∫—Ç–∏—Ü—ñ

### –ù–µ–¥–æ–ª—ñ–∫–∏ ‚úó

- ‚ùå –ü–æ—Ç—Ä—ñ–±–µ–Ω –≤–∏–±—ñ—Ä batch_size
- ‚ùå –í—Å–µ —â–µ –º–∞—î –¥–µ—è–∫–∏–π —à—É–º

### –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏?

- **–ú–∞–π–∂–µ –∑–∞–≤–∂–¥–∏!** (—Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è Deep Learning)
- –í–µ–ª–∏–∫—ñ –¥–∞—Ç–∞—Å–µ—Ç–∏
- –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è GPU/TPU
- –ë–∞–ª–∞–Ω—Å –º—ñ–∂ —à–≤–∏–¥–∫—ñ—Å—Ç—é —Ç–∞ —Ç–æ—á–Ω—ñ—Å—Ç—é

---

## –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è —Ç–∏–ø—ñ–≤

| –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞ | Batch GD | Stochastic GD | Mini-Batch GD |
|----------------|----------|---------------|---------------|
| **–ü—Ä–∏–∫–ª–∞–¥—ñ–≤ –∑–∞ —ñ—Ç–µ—Ä–∞—Ü—ñ—é** | –í—Å—ñ ($m$) | 1 | $b$ (32-256) |
| **–®–≤–∏–¥–∫—ñ—Å—Ç—å —ñ—Ç–µ—Ä–∞—Ü—ñ—ó** | –ü–æ–≤—ñ–ª—å–Ω–∞ | –î—É–∂–µ —à–≤–∏–¥–∫–∞ | –°–µ—Ä–µ–¥–Ω—è |
| **–ó–±—ñ–∂–Ω—ñ—Å—Ç—å** | –°—Ç–∞–±—ñ–ª—å–Ω–∞ | –ù–µ—Å—Ç–∞–±—ñ–ª—å–Ω–∞ | –ü–æ–º—ñ—Ä–Ω–∞ |
| **–ü–∞–º'—è—Ç—å** | –ë–∞–≥–∞—Ç–æ | –ú–∞–ª–æ | –°–µ—Ä–µ–¥–Ω—å–æ |
| **GPU –ø—Ä–∏—Å–∫–æ—Ä–µ–Ω–Ω—è** | –¢–∞–∫ | –ù—ñ | **–¢–∞–∫ ‚úì** |
| **Online learning** | ‚ùå | ‚úÖ | ‚úÖ |
| **–í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è** | –†—ñ–¥–∫–æ | –Ü–Ω–æ–¥—ñ | **–ù–∞–π—á–∞—Å—Ç—ñ—à–µ** |

### –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è —à–ª—è—Ö—É –¥–æ –º—ñ–Ω—ñ–º—É–º—É

```
Batch GD:                SGD:                Mini-Batch GD:
–ü–ª–∞–≤–Ω–∏–π —à–ª—è—Ö            –®—É–º–Ω–∏–π —à–ª—è—Ö         –ü–æ–º—ñ—Ä–Ω–∏–π —à—É–º

     ‚Ä¢                      ‚Ä¢                    ‚Ä¢
      ‚Üò                    ‚Üô‚Üò                   ‚Üò
       ‚Ä¢                  ‚Üó  ‚Üò                   ‚Üò‚Ä¢
        ‚Üò                ‚Üô    ‚Üò                   ‚Üò
         ‚Ä¢              ‚Üó      ‚Üò                   ‚Ä¢
          ‚Üò            ‚Üô        ‚Üò                   ‚Üò
           ‚òÖ          ‚Üó          ‚òÖ                   ‚òÖ
```

---

## Learning Rate (Œ±)

### –©–æ —Ü–µ?

**Learning rate** –∫–æ–Ω—Ç—Ä–æ–ª—é—î **—Ä–æ–∑–º—ñ—Ä –∫—Ä–æ–∫—É** –ø—Ä–∏ –æ–Ω–æ–≤–ª–µ–Ω–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤.

$$\theta := \theta - \alpha \nabla J(\theta)$$

### –ï—Ñ–µ–∫—Ç —Ä—ñ–∑–Ω–∏—Ö Œ±

```
Œ± –∑–∞–Ω–∞–¥—Ç–æ –º–∞–ª–µ (0.001):     Œ± –æ–ø—Ç–∏–º–∞–ª—å–Ω–µ (0.1):      Œ± –∑–∞–Ω–∞–¥—Ç–æ –≤–µ–ª–∏–∫–µ (10):

Cost                        Cost                      Cost
 |  ‚Ä¢                        |  ‚Ä¢                      |  ‚Ä¢ ‚Üò ‚Üó
 | ‚Ä¢                         | ‚Ä¢                       |      ‚Ä¢ ‚Üò ‚Üó
 |‚Ä¢                          |‚Ä¢                        |          ‚Ä¢
 |‚Ä¢                          | ‚Üò                       |  –ù–µ –∑–±—ñ–≥–∞—î—Ç—å—Å—è!
 | ‚Üò                         |  ‚òÖ                      |  
 |  ‚Ä¢                        |                         |
 |_____ iterations           |_____ iterations         |_____ iterations

–î—É–∂–µ –ø–æ–≤—ñ–ª—å–Ω–æ              –®–≤–∏–¥–∫–æ —Ç–∞ –µ—Ñ–µ–∫—Ç–∏–≤–Ω–æ       –†–æ–∑–±—ñ–∂–Ω—ñ—Å—Ç—å
```

### –Ø–∫ –≤–∏–±—Ä–∞—Ç–∏ Œ±?

#### 1. Grid Search

```python
learning_rates = [0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1.0]

for alpha in learning_rates:
    theta, cost_history = gradient_descent(X, y, theta_init, alpha, 1000)
    
    print(f"Œ± = {alpha}: Final cost = {cost_history[-1]:.4f}")
    
    plt.plot(cost_history, label=f'Œ±={alpha}')

plt.xlabel('Iterations')
plt.ylabel('Cost')
plt.legend()
plt.show()
```

#### 2. Learning Rate Decay (–∑–º–µ–Ω—à–µ–Ω–Ω—è Œ±)

**–°—Ç—Ä–∞—Ç–µ–≥—ñ—ó:**

**Step Decay:**
$$\alpha_t = \alpha_0 \times \gamma^{\lfloor t / k \rfloor}$$

–¥–µ $\gamma = 0.1-0.5$, $k$ ‚Äî –∫—ñ–ª—å–∫—ñ—Å—Ç—å –µ–ø–æ—Ö –¥–æ –∑–º–µ–Ω—à–µ–Ω–Ω—è.

```python
def step_decay(initial_lr, epoch, drop_rate=0.5, epochs_drop=10):
    return initial_lr * (drop_rate ** (epoch // epochs_drop))
```

**Exponential Decay:**
$$\alpha_t = \alpha_0 \times e^{-kt}$$

```python
def exponential_decay(initial_lr, epoch, k=0.1):
    return initial_lr * np.exp(-k * epoch)
```

**1/t Decay:**
$$\alpha_t = \frac{\alpha_0}{1 + kt}$$

```python
def inverse_time_decay(initial_lr, epoch, k=0.01):
    return initial_lr / (1 + k * epoch)
```

**Cosine Annealing:**
$$\alpha_t = \alpha_{min} + \frac{1}{2}(\alpha_{max} - \alpha_{min})(1 + \cos(\frac{t\pi}{T}))$$

---

## –ö—Ä–∏—Ç–µ—Ä—ñ—ó –∑—É–ø–∏–Ω–∫–∏

### –ö–æ–ª–∏ –∑—É–ø–∏–Ω—è—Ç–∏—Å—è?

1. **–§—ñ–∫—Å–æ–≤–∞–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —ñ—Ç–µ—Ä–∞—Ü—ñ–π**
   ```python
   for i in range(max_iterations):
       ...
   ```

2. **–ó–º—ñ–Ω–∞ cost –º–∞–ª–∞**
   ```python
   if abs(cost_history[-1] - cost_history[-2]) < epsilon:
       break
   ```

3. **Gradient –º–∞–ª–∏–π**
   ```python
   if np.linalg.norm(gradient) < epsilon:
       break
   ```

4. **Validation loss –∑—Ä–æ—Å—Ç–∞—î** (early stopping)
   ```python
   if val_cost > best_val_cost:
       patience_counter += 1
       if patience_counter > patience:
           break
   ```

---

## –ü—Ä–∞–∫—Ç–∏—á–Ω–∏–π –ø—Ä–∏–∫–ª–∞–¥

### Linear Regression –∑ —Ä—ñ–∑–Ω–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ GD

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# 1. –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –¥–∞–Ω–∏—Ö
X, y = make_regression(n_samples=1000, n_features=1, noise=20, random_state=42)

# –î–æ–¥–∞—î–º–æ bias term (—Å—Ç–æ–≤–ø–µ—Ü—å –æ–¥–∏–Ω–∏—Ü—å)
X_bias = np.c_[np.ones((X.shape[0], 1)), X]

# –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è
X_train, X_test, y_train, y_test = train_test_split(
    X_bias, y, test_size=0.2, random_state=42
)

# –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è (–í–ê–ñ–õ–ò–í–û –¥–ª—è GD!)
scaler = StandardScaler()
X_train[:, 1:] = scaler.fit_transform(X_train[:, 1:])
X_test[:, 1:] = scaler.transform(X_test[:, 1:])

# –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤
theta_init = np.random.randn(X_train.shape[1])

# 2. Batch GD
print("=== Batch Gradient Descent ===")
theta_batch, cost_batch = batch_gradient_descent(
    X_train, y_train, theta_init.copy(), alpha=0.01, iterations=1000
)

# 3. Stochastic GD
print("\n=== Stochastic Gradient Descent ===")
theta_sgd, cost_sgd = stochastic_gradient_descent(
    X_train, y_train, theta_init.copy(), alpha=0.01, epochs=100
)

# 4. Mini-Batch GD
print("\n=== Mini-Batch Gradient Descent ===")
theta_minibatch, cost_minibatch = mini_batch_gradient_descent(
    X_train, y_train, theta_init.copy(), alpha=0.01, epochs=100, batch_size=32
)

# 5. –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
fig, axes = plt.subplots(1, 2, figsize=(15, 5))

# Cost history
axes[0].plot(cost_batch, label='Batch GD', linewidth=2)
axes[0].plot(cost_sgd, label='Stochastic GD', linewidth=2, alpha=0.7)
axes[0].plot(cost_minibatch, label='Mini-Batch GD', linewidth=2)
axes[0].set_xlabel('Iterations/Epochs', fontsize=12)
axes[0].set_ylabel('Cost', fontsize=12)
axes[0].set_title('Cost History Comparison', fontsize=14, fontweight='bold')
axes[0].legend(fontsize=11)
axes[0].grid(True, alpha=0.3)
axes[0].set_yscale('log')

# Predictions
X_plot = np.linspace(X_test[:, 1].min(), X_test[:, 1].max(), 100).reshape(-1, 1)
X_plot_bias = np.c_[np.ones((X_plot.shape[0], 1)), X_plot]

y_pred_batch = X_plot_bias.dot(theta_batch)
y_pred_sgd = X_plot_bias.dot(theta_sgd)
y_pred_minibatch = X_plot_bias.dot(theta_minibatch)

axes[1].scatter(X_test[:, 1], y_test, alpha=0.5, s=30, label='Test Data')
axes[1].plot(X_plot, y_pred_batch, label='Batch GD', linewidth=2)
axes[1].plot(X_plot, y_pred_sgd, label='Stochastic GD', linewidth=2, alpha=0.7)
axes[1].plot(X_plot, y_pred_minibatch, label='Mini-Batch GD', linewidth=2)
axes[1].set_xlabel('X', fontsize=12)
axes[1].set_ylabel('y', fontsize=12)
axes[1].set_title('Predictions Comparison', fontsize=14, fontweight='bold')
axes[1].legend(fontsize=11)
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# –§—ñ–Ω–∞–ª—å–Ω–∏–π cost
print("\n=== Final Costs ===")
print(f"Batch GD: {cost_batch[-1]:.4f}")
print(f"Stochastic GD: {cost_sgd[-1]:.4f}")
print(f"Mini-Batch GD: {cost_minibatch[-1]:.4f}")
```

---

## –û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó Gradient Descent

### 1. Momentum

**–Ü–¥–µ—è:** –î–æ–¥–∞—î–º–æ "—ñ–Ω–µ—Ä—Ü—ñ—é" ‚Äî –≤—Ä–∞—Ö–æ–≤—É—î–º–æ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–π –Ω–∞–ø—Ä—è–º–æ–∫ —Ä—É—Ö—É.

$$v_t = \beta v_{t-1} + \alpha \nabla J(\theta)$$
$$\theta := \theta - v_t$$

–¥–µ $\beta \in [0, 1]$ (–∑–∞–∑–≤–∏—á–∞–π 0.9).

```python
def gradient_descent_momentum(X, y, theta, alpha, iterations, beta=0.9):
    m = len(y)
    v = np.zeros_like(theta)  # Velocity
    cost_history = []
    
    for i in range(iterations):
        predictions = X.dot(theta)
        errors = predictions - y
        gradient = (1/m) * X.T.dot(errors)
        
        # Momentum update
        v = beta * v + alpha * gradient
        theta = theta - v
        
        cost = (1/(2*m)) * np.sum(errors**2)
        cost_history.append(cost)
    
    return theta, cost_history
```

**–ü–µ—Ä–µ–≤–∞–≥–∏:**
- ‚úÖ –®–≤–∏–¥—à–∞ –∑–±—ñ–∂–Ω—ñ—Å—Ç—å
- ‚úÖ –ú–µ–Ω—à–µ –∫–æ–ª–∏–≤–∞–Ω—å
- ‚úÖ –ú–æ–∂–µ –ø–æ–¥–æ–ª–∞—Ç–∏ –ø–ª–∞—Ç–æ

### 2. Nesterov Accelerated Gradient (NAG)

**–Ü–¥–µ—è:** "–î–∏–≤–∏–º–æ—Å—å –≤–ø–µ—Ä–µ–¥" –ø–µ—Ä–µ–¥ –æ–±—á–∏—Å–ª–µ–Ω–Ω—è–º –≥—Ä–∞–¥—ñ—î–Ω—Ç–∞.

$$v_t = \beta v_{t-1} + \alpha \nabla J(\theta - \beta v_{t-1})$$
$$\theta := \theta - v_t$$

### 3. AdaGrad

**–Ü–¥–µ—è:** –ê–¥–∞–ø—Ç–∏–≤–Ω–∏–π learning rate –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞.

$$G_t = G_{t-1} + (\nabla J(\theta))^2$$
$$\theta := \theta - \frac{\alpha}{\sqrt{G_t + \epsilon}} \nabla J(\theta)$$

### 4. RMSProp

**–Ü–¥–µ—è:** –ï–∫—Å–ø–æ–Ω–µ–Ω—Ü—ñ–∞–ª—å–Ω–æ –∑–≥–ª–∞–¥–∂–µ–Ω–∞ –≤–µ—Ä—Å—ñ—è AdaGrad.

$$E[g^2]_t = \beta E[g^2]_{t-1} + (1-\beta) (\nabla J(\theta))^2$$
$$\theta := \theta - \frac{\alpha}{\sqrt{E[g^2]_t + \epsilon}} \nabla J(\theta)$$

### 5. Adam (Adaptive Moment Estimation)

**–ù–∞–π–ø–æ–ø—É–ª—è—Ä–Ω—ñ—à–∏–π –æ–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä –¥–ª—è Deep Learning!**

–ö–æ–º–±—ñ–Ω—É—î Momentum + RMSProp:

$$m_t = \beta_1 m_{t-1} + (1-\beta_1) \nabla J(\theta)$$ (–ø–µ—Ä—à–∏–π –º–æ–º–µ–Ω—Ç)
$$v_t = \beta_2 v_{t-1} + (1-\beta_2) (\nabla J(\theta))^2$$ (–¥—Ä—É–≥–∏–π –º–æ–º–µ–Ω—Ç)

Bias correction:
$$\hat{m}_t = \frac{m_t}{1 - \beta_1^t}$$
$$\hat{v}_t = \frac{v_t}{1 - \beta_2^t}$$

Update:
$$\theta := \theta - \frac{\alpha}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t$$

**–ì—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º:**
- $\alpha = 0.001$
- $\beta_1 = 0.9$
- $\beta_2 = 0.999$
- $\epsilon = 10^{-8}$

```python
def adam_optimizer(X, y, theta, alpha=0.001, iterations=1000, 
                   beta1=0.9, beta2=0.999, epsilon=1e-8):
    m = len(y)
    mt = np.zeros_like(theta)  # First moment
    vt = np.zeros_like(theta)  # Second moment
    cost_history = []
    
    for t in range(1, iterations + 1):
        predictions = X.dot(theta)
        errors = predictions - y
        gradient = (1/m) * X.T.dot(errors)
        
        # Update biased first moment
        mt = beta1 * mt + (1 - beta1) * gradient
        
        # Update biased second moment
        vt = beta2 * vt + (1 - beta2) * (gradient ** 2)
        
        # Bias correction
        mt_hat = mt / (1 - beta1 ** t)
        vt_hat = vt / (1 - beta2 ** t)
        
        # Update parameters
        theta = theta - alpha * mt_hat / (np.sqrt(vt_hat) + epsilon)
        
        cost = (1/(2*m)) * np.sum(errors**2)
        cost_history.append(cost)
    
    return theta, cost_history
```

---

## Scikit-learn —Ä–µ–∞–ª—ñ–∑–∞—Ü—ñ—è

### SGDRegressor (Linear Regression)

```python
from sklearn.linear_model import SGDRegressor

# SGD –¥–ª—è —Ä–µ–≥—Ä–µ—Å—ñ—ó
sgd_reg = SGDRegressor(
    max_iter=1000,
    tol=1e-3,
    penalty='l2',        # Ridge regularization
    alpha=0.0001,        # Regularization strength
    learning_rate='invscaling',  # Learning rate strategy
    eta0=0.01,           # Initial learning rate
    random_state=42
)

sgd_reg.fit(X_train, y_train)
y_pred = sgd_reg.predict(X_test)

print(f"Coefficients: {sgd_reg.coef_}")
print(f"Intercept: {sgd_reg.intercept_}")
```

### SGDClassifier (Logistic Regression)

```python
from sklearn.linear_model import SGDClassifier

# SGD –¥–ª—è –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó
sgd_clf = SGDClassifier(
    loss='log',          # Logistic loss
    max_iter=1000,
    tol=1e-3,
    learning_rate='adaptive',
    eta0=0.01,
    random_state=42
)

sgd_clf.fit(X_train, y_train)
y_pred = sgd_clf.predict(X_test)
```

---

## –ü–µ—Ä–µ–≤–∞–≥–∏ —Ç–∞ –Ω–µ–¥–æ–ª—ñ–∫–∏

### –ü–µ—Ä–µ–≤–∞–≥–∏ ‚úì

| –ü–µ—Ä–µ–≤–∞–≥–∞ | –ü–æ—è—Å–Ω–µ–Ω–Ω—è |
|----------|-----------|
| **–ú–∞—Å—à—Ç–∞–±–æ–≤–∞–Ω—ñ—Å—Ç—å** | –ü—Ä–∞—Ü—é—î –∑ –º—ñ–ª—å–π–æ–Ω–∞–º–∏ –ø—Ä–∏–∫–ª–∞–¥—ñ–≤ |
| **–ï—Ñ–µ–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å –ø–∞–º'—è—Ç—ñ** | –ù–µ –ø–æ—Ç—Ä—ñ–±–Ω–æ –∑–∞–≤–∞–Ω—Ç–∞–∂—É–≤–∞—Ç–∏ –≤—Å—ñ –¥–∞–Ω—ñ |
| **Online learning** | –û–Ω–æ–≤–ª–µ–Ω–Ω—è –Ω–∞ –Ω–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö |
| **–£–Ω—ñ–≤–µ—Ä—Å–∞–ª—å–Ω—ñ—Å—Ç—å** | –ü—Ä–∞—Ü—é—î –¥–ª—è –±—É–¥—å-—è–∫–æ—ó –¥–∏—Ñ–µ—Ä–µ–Ω—Ü—ñ–π–æ–≤–Ω–æ—ó —Ñ—É–Ω–∫—Ü—ñ—ó |
| **Parallelization** | Mini-batch –¥–æ–±—Ä–µ –ø–∞—Ä–∞–ª–µ–ª–∏—Ç—å—Å—è |
| **GPU acceleration** | –®–≤–∏–¥–∫–µ –æ–±—á–∏—Å–ª–µ–Ω–Ω—è –Ω–∞ GPU |

### –ù–µ–¥–æ–ª—ñ–∫–∏ ‚úó

| –ù–µ–¥–æ–ª—ñ–∫ | –ü–æ—è—Å–Ω–µ–Ω–Ω—è |
|---------|-----------|
| **–í–∏–±—ñ—Ä –≥—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤** | Œ±, batch_size –ø–æ—Ç—Ä—ñ–±–Ω–æ –Ω–∞–ª–∞—à—Ç–æ–≤—É–≤–∞—Ç–∏ |
| **–õ–æ–∫–∞–ª—å–Ω—ñ –º—ñ–Ω—ñ–º—É–º–∏** | –ú–æ–∂–µ –∑–∞—Å—Ç—Ä—è–≥—Ç–∏ (–¥–ª—è –Ω–µ–æ–ø—É–∫–ª–∏—Ö —Ñ—É–Ω–∫—Ü—ñ–π) |
| **–ü–æ—Ç—Ä–µ–±—É—î –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—ó** | –ß—É—Ç–ª–∏–≤–∏–π –¥–æ –º–∞—Å—à—Ç–∞–±—É –æ–∑–Ω–∞–∫ |
| **–°—Ç–æ—Ö–∞—Å—Ç–∏—á–Ω—ñ—Å—Ç—å** | SGD –Ω–µ—Å—Ç–∞–±—ñ–ª—å–Ω–∏–π |
| **–ü–æ–≤—ñ–ª—å–Ω—ñ—à–µ –∑–∞ –∞–Ω–∞–ª—ñ—Ç–∏—á–Ω—ñ –º–µ—Ç–æ–¥–∏** | –î–ª—è –º–∞–ª–∏—Ö –¥–∞–Ω–∏—Ö |

---

## –ü—Ä–∞–∫—Ç–∏—á–Ω—ñ –ø–æ—Ä–∞–¥–∏ üí°

1. **–ó–ê–í–ñ–î–ò –Ω–æ—Ä–º–∞–ª—ñ–∑—É–π –¥–∞–Ω—ñ** ‚Äî StandardScaler –ø–µ—Ä–µ–¥ GD
2. **–ü–æ—á–Ω–∏ –∑ Adam** ‚Äî –Ω–∞–π–∫—Ä–∞—â–∏–π —É–Ω—ñ–≤–µ—Ä—Å–∞–ª—å–Ω–∏–π –æ–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä
3. **–í—ñ–∑—É–∞–ª—ñ–∑—É–π cost history** ‚Äî –ø–µ—Ä–µ–≤—ñ—Ä –∑–±—ñ–∂–Ω—ñ—Å—Ç—å
4. **Grid search –¥–ª—è Œ±** ‚Äî –∑–Ω–∞–π–¥–∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω–∏–π learning rate
5. **Mini-batch = 32-256** ‚Äî —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∏–π –≤–∏–±—ñ—Ä
6. **–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π momentum** ‚Äî —à–≤–∏–¥—à–∞ –∑–±—ñ–∂–Ω—ñ—Å—Ç—å
7. **Learning rate decay** ‚Äî –¥–ª—è —Ñ—ñ–Ω–∞–ª—å–Ω–æ–≥–æ tuning
8. **Early stopping** ‚Äî –∑—É–ø–∏–Ω—è–π—Å—è –ø—Ä–∏ –∑–±—ñ–ª—å—à–µ–Ω–Ω—ñ val loss
9. **Checkpointing** ‚Äî –∑–±–µ—Ä—ñ–≥–∞–π –∫—Ä–∞—â—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏
10. **–ü–µ—Ä–µ–º—ñ—à—É–π –¥–∞–Ω—ñ** ‚Äî shuffle –Ω–∞ –∫–æ–∂–Ω—ñ–π –µ–ø–æ—Å—ñ

---

## –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ Gradient Descent

### –Ü–¥–µ–∞–ª—å–Ω–æ –ø—ñ–¥—Ö–æ–¥–∏—Ç—å ‚úì

- **–í–µ–ª–∏–∫—ñ –¥–∞—Ç–∞—Å–µ—Ç–∏** ‚Äî n > 100,000
- **Neural Networks** ‚Äî —î–¥–∏–Ω–∏–π –ø—Ä–∞–∫—Ç–∏—á–Ω–∏–π –º–µ—Ç–æ–¥
- **Online learning** ‚Äî –ø–æ—Å—Ç—ñ–π–Ω–µ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è
- **–ë–∞–≥–∞—Ç–æ –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤** ‚Äî p > 10,000
- **GPU –¥–æ—Å—Ç—É–ø–Ω–∏–π** ‚Äî –ø—Ä–∏—Å–∫–æ—Ä–µ–Ω–Ω—è –æ–±—á–∏—Å–ª–µ–Ω—å

### –ö—Ä–∞—â–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ —ñ–Ω—à–µ ‚úó

- **–ú–∞–ª—ñ –¥–∞–Ω—ñ** ‚Äî Normal Equation —à–≤–∏–¥—à–µ —Ç–∞ —Ç–æ—á–Ω—ñ—à–µ
- **–û–ø—É–∫–ª—ñ —Ñ—É–Ω–∫—Ü—ñ—ó + –º–∞–ª–æ –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤** ‚Äî –∞–Ω–∞–ª—ñ—Ç–∏—á–Ω–µ —Ä—ñ—à–µ–Ω–Ω—è
- **–ü–æ—Ç—Ä—ñ–±–Ω–∞ –≥–∞—Ä–∞–Ω—Ç—ñ—è –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ –º—ñ–Ω—ñ–º—É–º—É** ‚Äî —Å–ø–µ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω—ñ –º–µ—Ç–æ–¥–∏

---

## –ü–æ–≤'—è–∑–∞–Ω—ñ —Ç–µ–º–∏

- [[01_Linear_Regression]] ‚Äî Normal Equation vs Gradient Descent
- [[02_Logistic_Regression]] ‚Äî –Ω–∞–≤—á–∞–Ω–Ω—è —á–µ—Ä–µ–∑ GD
- [[Neural_Networks]] ‚Äî –æ—Å–Ω–æ–≤–∞ backpropagation
- [[Optimization_Algorithms]] ‚Äî Adam, RMSProp, etc.
- [[01_Feature_Scaling]] ‚Äî –∫—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è GD

## –†–µ—Å—É—Ä—Å–∏

- [Andrew Ng: Gradient Descent](https://www.coursera.org/learn/machine-learning)
- [CS231n: Optimization](http://cs231n.stanford.edu/)
- [An Overview of Gradient Descent Optimization Algorithms](https://ruder.io/optimizing-gradient-descent/)
- [Scikit-learn: SGD](https://scikit-learn.org/stable/modules/sgd.html)

---

## –ö–ª—é—á–æ–≤—ñ –≤–∏—Å–Ω–æ–≤–∫–∏

> Gradient Descent ‚Äî —ñ—Ç–µ—Ä–∞—Ç–∏–≤–Ω–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó, —è–∫–∏–π —Ä—É—Ö–∞—î—Ç—å—Å—è –≤ –Ω–∞–ø—Ä—è–º–∫—É –ø—Ä–æ—Ç–∏–ª–µ–∂–Ω–æ–º—É –¥–æ –≥—Ä–∞–¥—ñ—î–Ω—Ç–∞ –¥–ª—è –º—ñ–Ω—ñ–º—ñ–∑–∞—Ü—ñ—ó —Ñ—É–Ω–∫—Ü—ñ—ó –≤—Ç—Ä–∞—Ç.

**–û—Å–Ω–æ–≤–Ω—ñ —Ç–∏–ø–∏:**
- **Batch GD:** –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î –≤—Å—ñ –¥–∞–Ω—ñ ‚Äî —Å—Ç–∞–±—ñ–ª—å–Ω–∏–π, –ø–æ–≤—ñ–ª—å–Ω–∏–π
- **Stochastic GD:** –æ–¥–∏–Ω –ø—Ä–∏–∫–ª–∞–¥ ‚Äî —à–≤–∏–¥–∫–∏–π, –Ω–µ—Å—Ç–∞–±—ñ–ª—å–Ω–∏–π
- **Mini-Batch GD:** –∫–æ–º–ø—Ä–æ–º—ñ—Å ‚Äî **–Ω–∞–π–ø–æ–ø—É–ª—è—Ä–Ω—ñ—à–∏–π** ‚úì

**–ü—Ä–∞–≤–∏–ª–æ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è:**
$$\theta := \theta - \alpha \nabla J(\theta)$$

**–û–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä–∏:**
- **SGD:** –±–∞–∑–æ–≤–∏–π
- **Momentum:** —à–≤–∏–¥—à–∞ –∑–±—ñ–∂–Ω—ñ—Å—Ç—å
- **Adam:** **–Ω–∞–π–∫—Ä–∞—â–∏–π –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º** ‚úì

**–ö–ª—é—á–æ–≤—ñ –ø—Ä–∏–Ω—Ü–∏–ø–∏:**
- –ó–∞–≤–∂–¥–∏ –Ω–æ—Ä–º–∞–ª—ñ–∑—É–π –¥–∞–Ω—ñ
- –ü—ñ–¥–±–∏—Ä–∞–π learning rate —á–µ—Ä–µ–∑ grid search
- –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π mini-batch –¥–ª—è –±–∞–ª–∞–Ω—Å—É
- Adam ‚Äî –Ω–∞–π–±–µ–∑–ø–µ—á–Ω—ñ—à–∏–π –≤–∏–±—ñ—Ä

**–ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏:**
- –í–µ–ª–∏–∫—ñ –¥–∞–Ω—ñ + Neural Networks + GPU = Gradient Descent ‚úì

---

#ml #optimization #gradient-descent #sgd #adam #deep-learning #supervised-learning
