# Regularization (–†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—è)

## –©–æ —Ü–µ?

**–†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—è** ‚Äî —Ü–µ —Ç–µ—Ö–Ω—ñ–∫–∞ –¥–æ–¥–∞–≤–∞–Ω–Ω—è —à—Ç—Ä–∞—Ñ—É (penalty) –¥–æ —Ñ—É–Ω–∫—Ü—ñ—ó –≤—Ç—Ä–∞—Ç –º–æ–¥–µ–ª—ñ –¥–ª—è **–∑–∞–ø–æ–±—ñ–≥–∞–Ω–Ω—è overfitting** (–ø–µ—Ä–µ–Ω–∞–≤—á–∞–Ω–Ω—é) —à–ª—è—Ö–æ–º –æ–±–º–µ–∂–µ–Ω–Ω—è —Å–∫–ª–∞–¥–Ω–æ—Å—Ç—ñ –º–æ–¥–µ–ª—ñ.

**–ì–æ–ª–æ–≤–Ω–∞ —ñ–¥–µ—è:** –∑–Ω–∞–π—Ç–∏ –±–∞–ª–∞–Ω—Å –º—ñ–∂ —Ç–æ—á–Ω—ñ—Å—Ç—é –Ω–∞ —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö —ñ –∑–¥–∞—Ç–Ω—ñ—Å—Ç—é –º–æ–¥–µ–ª—ñ —É–∑–∞–≥–∞–ª—å–Ω—é–≤–∞—Ç–∏ –Ω–∞ –Ω–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö.

## –ù–∞–≤—ñ—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–∞?

- üéØ **–ë–æ—Ä–æ—Ç—å–±–∞ –∑ overfitting** ‚Äî –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–ø–∞–º'—è—Ç–æ–≤—É—î —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ
- üìâ **–ó–º–µ–Ω—à–µ–Ω–Ω—è variance** ‚Äî —Å—Ç–∞–±—ñ–ª—å–Ω—ñ—à—ñ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
- üîç **Feature selection** ‚Äî –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∏–π –≤—ñ–¥–±—ñ—Ä –≤–∞–∂–ª–∏–≤–∏—Ö –æ–∑–Ω–∞–∫ (L1)
- ‚öñÔ∏è **–ú—É–ª—å—Ç–∏–∫–æ–ª—ñ–Ω–µ–∞—Ä–Ω—ñ—Å—Ç—å** ‚Äî —Ä–æ–±–æ—Ç–∞ –∑ –∫–æ—Ä–µ–ª—å–æ–≤–∞–Ω–∏–º–∏ –æ–∑–Ω–∞–∫–∞–º–∏ (L2)
- üéöÔ∏è **–ö–æ–Ω—Ç—Ä–æ–ª—å —Å–∫–ª–∞–¥–Ω–æ—Å—Ç—ñ** ‚Äî –ø—Ä–æ—Å—Ç—ñ—à—ñ, –±—ñ–ª—å—à —ñ–Ω—Ç–µ—Ä–ø—Ä–∏—Ç–æ–≤–∞–Ω—ñ –º–æ–¥–µ–ª—ñ

## –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏?

**–ü–æ—Ç—Ä—ñ–±–Ω–æ:**

- **Overfitting** ‚Äî train score >> test score
- –ë–∞–≥–∞—Ç–æ –æ–∑–Ω–∞–∫ –ø–æ—Ä—ñ–≤–Ω—è–Ω–æ –∑ –∫—ñ–ª—å–∫—ñ—Å—Ç—é —Å–ø–æ—Å—Ç–µ—Ä–µ–∂–µ–Ω—å ($p > n$ –∞–±–æ $p \approx n$)
- **–ú—É–ª—å—Ç–∏–∫–æ–ª—ñ–Ω–µ–∞—Ä–Ω—ñ—Å—Ç—å** ‚Äî —Å–∏–ª—å–Ω–∞ –∫–æ—Ä–µ–ª—è—Ü—ñ—è –º—ñ–∂ –æ–∑–Ω–∞–∫–∞–º–∏
- –ü–æ—Ç—Ä—ñ–±–Ω–∞ **–∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∞ feature selection**
- –ö–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç–∏ –º–æ–¥–µ–ª—ñ –∑–∞–Ω–∞–¥—Ç–æ –≤–µ–ª–∏–∫—ñ

**–ù–µ –ø–æ—Ç—Ä—ñ–±–Ω–æ:**

- **Underfitting** ‚Äî –º–æ–¥–µ–ª—å –∑–∞–Ω–∞–¥—Ç–æ –ø—Ä–æ—Å—Ç–∞
- –î—É–∂–µ –º–∞–ª–æ –æ–∑–Ω–∞–∫ ($p << n$)
- Tree-based –º–æ–¥–µ–ª—ñ (–≤–æ–Ω–∏ –º–∞—é—Ç—å –≤–ª–∞—Å–Ω—É —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—é)

---

## –ü—Ä–æ–±–ª–µ–º–∞ Overfitting

### Bias-Variance Trade-off

```
High Bias               Balanced            High Variance
(Underfitting)                             (Overfitting)

    y                      y                    y
    |  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ               | ‚ï±‚îÄ‚îÄ‚ï≤              | ‚ï±‚ï≤‚ï±‚ï≤‚ï±‚ï≤
    | /                    |‚ï±    ‚ï≤             |‚ï±    ‚ï≤ ‚ï≤
    |/                     /      ‚ï≤            ‚ï±      ‚ï≤ ‚ï≤
    |________              |_______‚ï≤           |________‚ï≤‚ï≤
         x                      x                   x

Train MSE: –í–∏—Å–æ–∫–∏–π      –°–µ—Ä–µ–¥–Ω—ñ–π            –ù–∏–∑—å–∫–∏–π
Test MSE:  –í–∏—Å–æ–∫–∏–π      –ù–∏–∑—å–∫–∏–π             –í–∏—Å–æ–∫–∏–π
```

**Overfitting –≤–∏–Ω–∏–∫–∞—î –∫–æ–ª–∏:**
- –ú–æ–¥–µ–ª—å –∑–∞–Ω–∞–¥—Ç–æ —Å–∫–ª–∞–¥–Ω–∞ (–±–∞–≥–∞—Ç–æ –æ–∑–Ω–∞–∫)
- –ú–∞–ª–æ —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö
- Noise –≤ –¥–∞–Ω–∏—Ö —ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç—É—î—Ç—å—Å—è —è–∫ —Å–∏–≥–Ω–∞–ª

---

## –¢–∏–ø–∏ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—ó

## 1. Ridge Regression (L2 Regularization)

### –§—É–Ω–∫—Ü—ñ—è –≤—Ç—Ä–∞—Ç

$$J(\beta) = \text{MSE} + \lambda \sum_{j=1}^{p} \beta_j^2 = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 + \lambda \sum_{j=1}^{p} \beta_j^2$$

–¥–µ:
- $\lambda$ (lambda) ‚Äî **—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ–π–Ω–∏–π –ø–∞—Ä–∞–º–µ—Ç—Ä** ($\lambda \geq 0$)
- $\sum \beta_j^2$ ‚Äî **L2 norm** (—Å—É–º–∞ –∫–≤–∞–¥—Ä–∞—Ç—ñ–≤ –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç—ñ–≤)

### –Ø–∫ –ø—Ä–∞—Ü—é—î?

**–î–æ–¥–∞—î–º–æ —à—Ç—Ä–∞—Ñ –∑–∞ –≤–µ–ª–∏–∫—ñ –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç–∏:**
—è
- –ú–æ–¥–µ–ª—å –Ω–∞–º–∞–≥–∞—î—Ç—å—Å—è –º—ñ–Ω—ñ–º—ñ–∑—É–≤–∞—Ç–∏ MSE
- –ê–ª–µ —Ç–∞–∫–æ–∂ –Ω–∞–º–∞–≥–∞—î—Ç—å—Å—è –∑–º–µ–Ω—à–∏—Ç–∏ –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç–∏
- **Trade-off:** —Ç–æ—á–Ω—ñ—Å—Ç—å vs –ø—Ä–æ—Å—Ç–æ—Ç–∞

### –ï—Ñ–µ–∫—Ç –Ω–∞ –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç–∏

```
Œª = 0:      Œ≤ –º–æ–∂–µ –±—É—Ç–∏ –¥—É–∂–µ –≤–µ–ª–∏–∫–∏–º
Œª = 0.01:   Œ≤ —Ç—Ä–æ—Ö–∏ –∑–º–µ–Ω—à—É—î—Ç—å—Å—è
Œª = 1:      Œ≤ –∑–Ω–∞—á–Ω–æ –∑–º–µ–Ω—à—É—î—Ç—å—Å—è
Œª = 100:    Œ≤ ‚âà 0 (–º–∞–π–∂–µ –≤—Å—ñ –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç–∏ –±–ª–∏–∑—å–∫—ñ –¥–æ 0)
```

**Ridge –ù–ï –∑–∞–Ω—É–ª—è—î –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç–∏!** –í–æ–Ω–∏ —Å—Ç–∞—é—Ç—å –º–∞–ª–∏–º–∏, –∞–ª–µ –Ω–µ 0.

### –í–ª–∞—Å—Ç–∏–≤–æ—Å—Ç—ñ

| –í–ª–∞—Å—Ç–∏–≤—ñ—Å—Ç—å | –û–ø–∏—Å |
|-------------|------|
| **–ù–µ –∑–∞–Ω—É–ª—è—î Œ≤** | –í—Å—ñ –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç–∏ –∑–∞–ª–∏—à–∞—é—Ç—å—Å—è –≤ –º–æ–¥–µ–ª—ñ |
| **–ì—Ä—É–ø–∞ –∫–æ—Ä–µ–ª—è—Ü—ñ–π** | –ö–æ—Ä–µ–ª—å–æ–≤–∞–Ω—ñ –æ–∑–Ω–∞–∫–∏ –æ—Ç—Ä–∏–º—É—é—Ç—å —Å—Ö–æ–∂—ñ –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç–∏ |
| **–ú—É–ª—å—Ç–∏–∫–æ–ª—ñ–Ω–µ–∞—Ä–Ω—ñ—Å—Ç—å** | –î—É–∂–µ –¥–æ–±—Ä–µ —Å–ø—Ä–∞–≤–ª—è—î—Ç—å—Å—è |
| **–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å** | –ú–µ–Ω—à–∞, –Ω—ñ–∂ Lasso |

### –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏?

‚úÖ **–ú—É–ª—å—Ç–∏–∫–æ–ª—ñ–Ω–µ–∞—Ä–Ω—ñ—Å—Ç—å** ‚Äî —Å–∏–ª—å–Ω–∞ –∫–æ—Ä–µ–ª—è—Ü—ñ—è –º—ñ–∂ –æ–∑–Ω–∞–∫–∞–º–∏
‚úÖ –í—Å—ñ –æ–∑–Ω–∞–∫–∏ –ø–æ—Ç–µ–Ω—Ü—ñ–π–Ω–æ –∫–æ—Ä–∏—Å–Ω—ñ
‚úÖ –ü–æ—Ç—Ä—ñ–±–Ω–∞ —Å—Ç–∞–±—ñ–ª—å–Ω—ñ—Å—Ç—å –º–æ–¥–µ–ª—ñ
‚úÖ $p > n$ (–±—ñ–ª—å—à–µ –æ–∑–Ω–∞–∫, –Ω—ñ–∂ —Å–ø–æ—Å—Ç–µ—Ä–µ–∂–µ–Ω—å)

---

## 2. Lasso Regression (L1 Regularization)

### –§—É–Ω–∫—Ü—ñ—è –≤—Ç—Ä–∞—Ç

$$J(\beta) = \text{MSE} + \lambda \sum_{j=1}^{p} |\beta_j| = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 + \lambda \sum_{j=1}^{p} |\beta_j|$$

–¥–µ:
- $\sum |\beta_j|$ ‚Äî **L1 norm** (—Å—É–º–∞ –∞–±—Å–æ–ª—é—Ç–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω—å)

### –Ø–∫ –ø—Ä–∞—Ü—é—î?

**–î–æ–¥–∞—î–º–æ —à—Ç—Ä–∞—Ñ –∑–∞ –∞–±—Å–æ–ª—é—Ç–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç—ñ–≤:**
- Lasso **–∑–∞–Ω—É–ª—è—î** –¥–µ—è–∫—ñ –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç–∏
- **–ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∞ feature selection**
- –ó–∞–ª–∏—à–∞—î —Ç—ñ–ª—å–∫–∏ –Ω–∞–π–≤–∞–∂–ª–∏–≤—ñ—à—ñ –æ–∑–Ω–∞–∫–∏

### –ï—Ñ–µ–∫—Ç –Ω–∞ –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç–∏

```
Œª = 0:      Œ≤‚ÇÅ=5, Œ≤‚ÇÇ=3, Œ≤‚ÇÉ=2, Œ≤‚ÇÑ=1
Œª = 0.5:    Œ≤‚ÇÅ=4.5, Œ≤‚ÇÇ=2.5, Œ≤‚ÇÉ=1, Œ≤‚ÇÑ=0  ‚Üê Œ≤‚ÇÑ –∑–∞–Ω—É–ª–∏–≤—Å—è!
Œª = 1:      Œ≤‚ÇÅ=4, Œ≤‚ÇÇ=1.5, Œ≤‚ÇÉ=0, Œ≤‚ÇÑ=0
Œª = 5:      Œ≤‚ÇÅ=2, Œ≤‚ÇÇ=0, Œ≤‚ÇÉ=0, Œ≤‚ÇÑ=0
Œª = 10:     Œ≤‚ÇÅ=0, Œ≤‚ÇÇ=0, Œ≤‚ÇÉ=0, Œ≤‚ÇÑ=0      ‚Üê –í—Å—ñ –Ω—É–ª—ñ
```

**Lasso –∑–∞–Ω—É–ª—è—î –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç–∏ = –≤–∏–¥–∞–ª—è—î –æ–∑–Ω–∞–∫–∏!**

### –í–ª–∞—Å—Ç–∏–≤–æ—Å—Ç—ñ

| –í–ª–∞—Å—Ç–∏–≤—ñ—Å—Ç—å | –û–ø–∏—Å |
|-------------|------|
| **–ó–∞–Ω—É–ª—è—î Œ≤** | –î–µ—è–∫—ñ –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç–∏ —Å—Ç–∞—é—Ç—å —Ä—ñ–≤–Ω–æ 0 |
| **Feature selection** | –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –≤—ñ–¥–±–∏—Ä–∞—î –≤–∞–∂–ª–∏–≤—ñ –æ–∑–Ω–∞–∫–∏ |
| **Sparse models** | –ú–æ–¥–µ–ª—å –∑ –º–∞–ª–æ—é –∫—ñ–ª—å–∫—ñ—Å—Ç—é –æ–∑–Ω–∞–∫ |
| **–ö–æ—Ä–µ–ª—å–æ–≤–∞–Ω—ñ –æ–∑–Ω–∞–∫–∏** | –í–∏–±–∏—Ä–∞—î –æ–¥–Ω—É, —ñ–≥–Ω–æ—Ä—É—î —ñ–Ω—à—ñ |

### –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏?

‚úÖ **Feature selection** ‚Äî –ø–æ—Ç—Ä—ñ–±–Ω–æ –≤—ñ–¥—ñ–±—Ä–∞—Ç–∏ –Ω–∞–π–≤–∞–∂–ª–∏–≤—ñ—à—ñ –æ–∑–Ω–∞–∫–∏
‚úÖ –ë–∞–≥–∞—Ç–æ –Ω–µ–ø–æ—Ç—Ä—ñ–±–Ω–∏—Ö –æ–∑–Ω–∞–∫
‚úÖ –ü–æ—Ç—Ä—ñ–±–Ω–∞ **—ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å** ‚Äî –ø—Ä–æ—Å—Ç–∞ –º–æ–¥–µ–ª—å
‚úÖ Sparse data (–±–∞–≥–∞—Ç–æ –Ω—É–ª—ñ–≤)

---

## 3. Elastic Net (L1 + L2)

### –§—É–Ω–∫—Ü—ñ—è –≤—Ç—Ä–∞—Ç

$$J(\beta) = \text{MSE} + \lambda \left[ \alpha \sum_{j=1}^{p} |\beta_j| + (1-\alpha) \sum_{j=1}^{p} \beta_j^2 \right]$$

–¥–µ:
- $\alpha \in [0, 1]$ ‚Äî **mixing parameter**
  - $\alpha = 0$ ‚Üí Ridge
  - $\alpha = 1$ ‚Üí Lasso
  - $\alpha = 0.5$ ‚Üí —Ä—ñ–≤–Ω–∏–π –±–∞–ª–∞–Ω—Å

### –Ø–∫ –ø—Ä–∞—Ü—é—î?

**–ö–æ–º–±—ñ–Ω—É—î –ø–µ—Ä–µ–≤–∞–≥–∏ Ridge —Ç–∞ Lasso:**
- L1 (Lasso): feature selection
- L2 (Ridge): —Å—Ç–∞–±—ñ–ª—å–Ω—ñ—Å—Ç—å –ø—Ä–∏ –∫–æ—Ä–µ–ª—è—Ü—ñ—è—Ö

### –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏?

‚úÖ –ö–æ—Ä–µ–ª—å–æ–≤–∞–Ω—ñ –æ–∑–Ω–∞–∫–∏ + –ø–æ—Ç—Ä—ñ–±–Ω–∞ feature selection
‚úÖ $p > n$ –∑ –≥—Ä—É–ø–∞–º–∏ –∫–æ—Ä–µ–ª—å–æ–≤–∞–Ω–∏—Ö –æ–∑–Ω–∞–∫
‚úÖ –ù–µ–≤–ø–µ–≤–Ω–µ–Ω—ñ—Å—Ç—å –º—ñ–∂ Ridge —Ç–∞ Lasso

---

## –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –º–µ—Ç–æ–¥—ñ–≤

| –ö—Ä–∏—Ç–µ—Ä—ñ–π | Ridge (L2) | Lasso (L1) | Elastic Net |
|----------|------------|------------|-------------|
| **Penalty** | $\sum \beta_j^2$ | $\sum \|\beta_j\|$ | $\alpha L1 + (1-\alpha)L2$ |
| **–ó–∞–Ω—É–ª—è—î Œ≤** | ‚ùå –ù—ñ | ‚úÖ –¢–∞–∫ | ‚úÖ –¢–∞–∫ (—á–∞—Å—Ç–∫–æ–≤–æ) |
| **Feature selection** | ‚ùå –ù—ñ | ‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ | ‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ |
| **–ú—É–ª—å—Ç–∏–∫–æ–ª—ñ–Ω–µ–∞—Ä–Ω—ñ—Å—Ç—å** | ‚úÖ –î—É–∂–µ –¥–æ–±—Ä–µ | ‚ö†Ô∏è –í–∏–±–∏—Ä–∞—î –æ–¥–Ω—É –æ–∑–Ω–∞–∫—É | ‚úÖ –î–æ–±—Ä–µ |
| **–ö–æ—Ä–µ–ª—å–æ–≤–∞–Ω—ñ –æ–∑–Ω–∞–∫–∏** | –£—Å–µ—Ä–µ–¥–Ω—é—î | –í–∏–±–∏—Ä–∞—î –æ–¥–Ω—É | –ì—Ä—É–ø—É—î —Å—Ö–æ–∂—ñ |
| **–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å** | –°–µ—Ä–µ–¥–Ω—è | –í–∏—Å–æ–∫–∞ | –í–∏—Å–æ–∫–∞ |
| **–û–±—á–∏—Å–ª–µ–Ω–Ω—è** | –®–≤–∏–¥–∫–æ | –ü–æ–≤—ñ–ª—å–Ω—ñ—à–µ | –ü–æ–≤—ñ–ª—å–Ω—ñ—à–µ |

### –í—ñ–∑—É–∞–ª—å–Ω–µ –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è

```
Constraint regions (–¥–ª—è 2 –æ–∑–Ω–∞–∫):

Ridge (L2):              Lasso (L1):           Elastic Net:
    Œ≤‚ÇÇ                      Œ≤‚ÇÇ                    Œ≤‚ÇÇ
     |                       |                     |
     ‚óè                      ‚ï±‚îÇ‚ï≤                   ‚ï±‚óè‚ï≤
    ‚ï± ‚ï≤                   ‚ï±  ‚îÇ  ‚ï≤               ‚ï±  |  ‚ï≤
   ‚ï±   ‚ï≤                 ‚óè‚îÄ‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚îÄ‚óè             ‚óè‚îÄ‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚îÄ‚óè
  |     |                  ‚ï≤ | ‚ï±                ‚ï≤  |  ‚ï±
  |     |                   ‚ï≤‚îÇ‚ï±                  ‚ï≤ ‚îÇ ‚ï±
  ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚îÄ Œ≤‚ÇÅ              ‚óè‚îÄ‚îÄ‚îÄ‚îÄ Œ≤‚ÇÅ             ‚óè‚îÄ‚îÄ‚îÄ‚îÄ Œ≤‚ÇÅ
   ‚ï≤   ‚ï±
    ‚ï≤ ‚ï±
     ‚óè

–ö–æ–ª–æ/–ï–ª—ñ–ø—Å           –†–æ–º–±                   –ü—Ä–æ–º—ñ–∂–Ω–∞ —Ñ–æ—Ä–º–∞
(smooth)            (–≥–æ—Å—Ç—Ä—ñ –∫—É—Ç–∏)          (smooth + –∫—É—Ç–∏)
Œ≤ —Ä—ñ–¥–∫–æ = 0         Œ≤ —á–∞—Å—Ç–æ = 0            Œ≤ —ñ–Ω–æ–¥—ñ = 0
```

**–ß–æ–º—É Lasso –∑–∞–Ω—É–ª—è—î:**
- –û–ø—Ç–∏–º—É–º —á–∞—Å—Ç–æ –ø–æ–ø–∞–¥–∞—î –Ω–∞ –∫—É—Ç —Ä–æ–º–±–∞ (–æ—Å—ñ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç)
- –ù–∞ –æ—Å—è—Ö –æ–¥–Ω–∞ –∑ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç = 0

---

## –í–∏–±—ñ—Ä Œª (—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ–π–Ω–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞)

### –ï—Ñ–µ–∫—Ç Œª

```
Œª = 0:      –ù–µ–º–∞—î —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—ó (—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞ —Ä–µ–≥—Ä–µ—Å—ñ—è)
            ‚Üí –º–æ–∂–µ –±—É—Ç–∏ overfitting

Œª –º–∞–ª–∏–π:    –õ–µ–≥–∫–∞ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—è
            ‚Üí —Ç—Ä–æ—Ö–∏ –∑–º–µ–Ω—à—É—î overfitting

Œª —Å–µ—Ä–µ–¥–Ω—ñ–π: –ë–∞–ª–∞–Ω—Å –º—ñ–∂ bias —Ç–∞ variance
            ‚Üí –û–ü–¢–ò–ú–ê–õ–¨–ù–û ‚úì

Œª –≤–µ–ª–∏–∫–∏–π:  –°–∏–ª—å–Ω–∞ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—è
            ‚Üí –º–æ–∂–µ –±—É—Ç–∏ underfitting
```

### –ü—ñ–¥–±—ñ—Ä Œª —á–µ—Ä–µ–∑ Cross-Validation

**–ù–∞–π–∫—Ä–∞—â–∏–π –º–µ—Ç–æ–¥:** Grid Search CV –∞–±–æ RandomizedSearchCV

```
Test Error
    |
    |        ‚ï±‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    |       ‚ï±
    |      ‚ï±
    |     ‚ï±
    |    ‚ï±
    |___‚ï±____________
    0   optimal Œª    Œª
```

**–û–ø—Ç–∏–º–∞–ª—å–Ω–∏–π Œª:** –º—ñ–Ω—ñ–º—É–º test error –Ω–∞ CV.

---

## –ö–æ–¥ (Python + scikit-learn)

### Ridge Regression

```python
from sklearn.linear_model import Ridge
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

# –î–∞–Ω—ñ
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Ridge –º–æ–¥–µ–ª—å
ridge = Ridge(alpha=1.0)  # alpha = Œª
ridge.fit(X_train, y_train)

# –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
y_pred = ridge.predict(X_test)

# –ú–µ—Ç—Ä–∏–∫–∏
print(f"R¬≤: {r2_score(y_test, y_pred):.4f}")
print(f"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.2f}")

# –ö–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç–∏
print(f"\nIntercept: {ridge.intercept_:.4f}")
print("Coefficients:")
for i, coef in enumerate(ridge.coef_):
    print(f"  Œ≤{i}: {coef:.4f}")
```

### Lasso Regression

```python
from sklearn.linear_model import Lasso

# Lasso –º–æ–¥–µ–ª—å
lasso = Lasso(alpha=1.0)
lasso.fit(X_train, y_train)

# –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
y_pred = lasso.predict(X_test)

# –ú–µ—Ç—Ä–∏–∫–∏
print(f"R¬≤: {r2_score(y_test, y_pred):.4f}")

# –ö–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç–∏ (–¥–µ—è–∫—ñ –±—É–¥—É—Ç—å = 0)
print("\nCoefficients:")
for i, coef in enumerate(lasso.coef_):
    if coef != 0:  # –ü–æ–∫–∞–∑—É—î–º–æ —Ç—ñ–ª—å–∫–∏ –Ω–µ–Ω—É–ª—å–æ–≤—ñ
        print(f"  Œ≤{i}: {coef:.4f}")
    else:
        print(f"  Œ≤{i}: 0.0000 (ELIMINATED)")

# –°–∫—ñ–ª—å–∫–∏ –æ–∑–Ω–∞–∫ –∑–∞–ª–∏—à–∏–ª–æ—Å—å
n_features_selected = np.sum(lasso.coef_ != 0)
print(f"\nFeatures selected: {n_features_selected} / {len(lasso.coef_)}")
```

### Elastic Net

```python
from sklearn.linear_model import ElasticNet

# Elastic Net
elastic = ElasticNet(
    alpha=1.0,      # Œª
    l1_ratio=0.5    # Œ± (mixing: 0=Ridge, 1=Lasso)
)
elastic.fit(X_train, y_train)

# –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
y_pred = elastic.predict(X_test)

print(f"R¬≤: {r2_score(y_test, y_pred):.4f}")
```

### –ü—ñ–¥–±—ñ—Ä –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ Œª (Cross-Validation)

```python
from sklearn.linear_model import RidgeCV, LassoCV, ElasticNetCV
from sklearn.model_selection import cross_val_score

# Ridge –∑ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∏–º –ø—ñ–¥–±–æ—Ä–æ–º alpha
alphas = [0.001, 0.01, 0.1, 1, 10, 100, 1000]

ridge_cv = RidgeCV(alphas=alphas, cv=5)
ridge_cv.fit(X_train, y_train)

print(f"Best alpha (Ridge): {ridge_cv.alpha_}")
print(f"R¬≤ on test: {ridge_cv.score(X_test, y_test):.4f}")

# Lasso CV
lasso_cv = LassoCV(alphas=alphas, cv=5, random_state=42)
lasso_cv.fit(X_train, y_train)

print(f"\nBest alpha (Lasso): {lasso_cv.alpha_}")
print(f"R¬≤ on test: {lasso_cv.score(X_test, y_test):.4f}")

# Elastic Net CV
elastic_cv = ElasticNetCV(
    alphas=alphas,
    l1_ratio=[0.1, 0.5, 0.7, 0.9, 0.95, 0.99],
    cv=5,
    random_state=42
)
elastic_cv.fit(X_train, y_train)

print(f"\nBest alpha (Elastic): {elastic_cv.alpha_}")
print(f"Best l1_ratio: {elastic_cv.l1_ratio_}")
print(f"R¬≤ on test: {elastic_cv.score(X_test, y_test):.4f}")
```

### –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –µ—Ñ–µ–∫—Ç—É —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—ó

```python
import matplotlib.pyplot as plt

# –†—ñ–∑–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è alpha
alphas = np.logspace(-3, 3, 100)
coefs_ridge = []
coefs_lasso = []

for alpha in alphas:
    # Ridge
    ridge = Ridge(alpha=alpha)
    ridge.fit(X_train, y_train)
    coefs_ridge.append(ridge.coef_)
    
    # Lasso
    lasso = Lasso(alpha=alpha, max_iter=10000)
    lasso.fit(X_train, y_train)
    coefs_lasso.append(lasso.coef_)

coefs_ridge = np.array(coefs_ridge)
coefs_lasso = np.array(coefs_lasso)

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
fig, axes = plt.subplots(1, 2, figsize=(16, 6))

# Ridge
for i in range(coefs_ridge.shape[1]):
    axes[0].plot(alphas, coefs_ridge[:, i], label=f'Feature {i}')
axes[0].set_xscale('log')
axes[0].set_xlabel('Alpha (Œª)', fontsize=12)
axes[0].set_ylabel('Coefficients', fontsize=12)
axes[0].set_title('Ridge: Coefficients vs Regularization', 
                  fontsize=14, fontweight='bold')
axes[0].axhline(y=0, color='black', linestyle='--', linewidth=1)
axes[0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')
axes[0].grid(True, alpha=0.3)

# Lasso
for i in range(coefs_lasso.shape[1]):
    axes[1].plot(alphas, coefs_lasso[:, i], label=f'Feature {i}')
axes[1].set_xscale('log')
axes[1].set_xlabel('Alpha (Œª)', fontsize=12)
axes[1].set_ylabel('Coefficients', fontsize=12)
axes[1].set_title('Lasso: Coefficients vs Regularization', 
                  fontsize=14, fontweight='bold')
axes[1].axhline(y=0, color='black', linestyle='--', linewidth=1)
axes[1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –º–æ–¥–µ–ª–µ–π

```python
from sklearn.preprocessing import StandardScaler

# –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è (–í–ê–ñ–õ–ò–í–û –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—ó!)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# –ú–æ–¥–µ–ª—ñ
models = {
    'No Regularization': Ridge(alpha=0),
    'Ridge (Œ±=0.1)': Ridge(alpha=0.1),
    'Ridge (Œ±=1)': Ridge(alpha=1.0),
    'Ridge (Œ±=10)': Ridge(alpha=10),
    'Lasso (Œ±=0.1)': Lasso(alpha=0.1),
    'Lasso (Œ±=1)': Lasso(alpha=1.0),
    'Elastic Net': ElasticNet(alpha=1.0, l1_ratio=0.5)
}

# –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è —Ç–∞ –æ—Ü—ñ–Ω–∫–∞
results = []

for name, model in models.items():
    model.fit(X_train_scaled, y_train)
    
    train_score = model.score(X_train_scaled, y_train)
    test_score = model.score(X_test_scaled, y_test)
    
    n_nonzero = np.sum(model.coef_ != 0)
    
    results.append({
        'Model': name,
        'Train R¬≤': train_score,
        'Test R¬≤': test_score,
        'Overfitting': train_score - test_score,
        'Features': n_nonzero
    })

# –†–µ–∑—É–ª—å—Ç–∞—Ç–∏
import pandas as pd
results_df = pd.DataFrame(results)
print(results_df.to_string(index=False))
```

---

## –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—è –¥–ª—è Logistic Regression

### Ridge (L2)

```python
from sklearn.linear_model import LogisticRegression

# C = 1/Œª (inverse regularization strength)
# –ú–µ–Ω—à–µ C ‚Üí –±—ñ–ª—å—à–∞ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—è
log_reg_ridge = LogisticRegression(
    penalty='l2',
    C=1.0,          # C=1 ‚Üí Œª=1
    solver='lbfgs',
    max_iter=1000
)

log_reg_ridge.fit(X_train, y_train)
```

### Lasso (L1)

```python
log_reg_lasso = LogisticRegression(
    penalty='l1',
    C=1.0,
    solver='liblinear'  # –∞–±–æ 'saga'
)

log_reg_lasso.fit(X_train, y_train)
```

### Elastic Net

```python
log_reg_elastic = LogisticRegression(
    penalty='elasticnet',
    C=1.0,
    l1_ratio=0.5,
    solver='saga'
)

log_reg_elastic.fit(X_train, y_train)
```

---

## –í–∞–∂–ª–∏–≤—ñ—Å—Ç—å –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—ó –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—ó

### –ß–æ–º—É –ø–æ—Ç—Ä—ñ–±–Ω–∞?

**–†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—è –∫–∞—Ä–∞—î –≤–µ–ª–∏–∫—ñ –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç–∏:**
- –Ø–∫—â–æ –æ–∑–Ω–∞–∫–∏ –≤ —Ä—ñ–∑–Ω–∏—Ö —à–∫–∞–ª–∞—Ö ‚Üí –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç–∏ —Ç–µ–∂ —Ä—ñ–∑–Ω—ñ
- –û–∑–Ω–∞–∫–∞ –∑ –≤–µ–ª–∏–∫–∏–º–∏ –∑–Ω–∞—á–µ–Ω–Ω—è–º–∏ ‚Üí –º–∞–ª–∏–π –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç
- –û–∑–Ω–∞–∫–∞ –∑ –º–∞–ª–∏–º–∏ –∑–Ω–∞—á–µ–Ω–Ω—è–º–∏ ‚Üí –≤–µ–ª–∏–∫–∏–π –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç
- –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—è –Ω–µ–ø—Ä–æ–ø–æ—Ä—Ü—ñ–π–Ω–æ –∫–∞—Ä–∞—î –¥—Ä—É–≥—É –æ–∑–Ω–∞–∫—É!

### –ü—Ä–∏–∫–ª–∞–¥

```python
# –ë–ï–ó –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—ó (–ü–û–ì–ê–ù–û!)
X = [[1000, 1],      # –û–∑–Ω–∞–∫–∞ 1: —Ç–∏—Å—è—á—ñ, –û–∑–Ω–∞–∫–∞ 2: –æ–¥–∏–Ω–∏—Ü—ñ
     [2000, 2],
     [3000, 3]]

# Ridge –±–µ–∑ –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—ó
ridge = Ridge(alpha=1.0)
ridge.fit(X, y)
# Œ≤‚ÇÅ ‚âà 0.001 (–º–∞–ª–∞, –±–æ x‚ÇÅ –≤–µ–ª–∏–∫–∞)
# Œ≤‚ÇÇ ‚âà 10 (–≤–µ–ª–∏–∫–∞, –±–æ x‚ÇÇ –º–∞–ª–∞)
# –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—è –±—ñ–ª—å—à–µ –∫–∞—Ä–∞—î Œ≤‚ÇÇ!

# –ó –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—î—é (–î–û–ë–†–ï!)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

ridge.fit(X_scaled, y)
# Œ≤‚ÇÅ, Œ≤‚ÇÇ –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—ñ –∑–∞ –≤–µ–ª–∏—á–∏–Ω–æ—é
# –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—è —Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤–∞!
```

### –ü—Ä–∞–≤–∏–ª–æ

> **–ó–ê–í–ñ–î–ò –Ω–æ—Ä–º–∞–ª—ñ–∑—É–π –¥–∞–Ω—ñ –ø–µ—Ä–µ–¥ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—î—é!**

```python
# ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

ridge.fit(X_train_scaled, y_train)

# ‚ùå –ù–ï–ü–†–ê–í–ò–õ–¨–ù–û
ridge.fit(X_train, y_train)  # –ë–µ–∑ –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—ó
```

---

## Regularization Path

### –©–æ —Ü–µ?

–ì—Ä–∞—Ñ—ñ–∫ –∑–º—ñ–Ω–∏ –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç—ñ–≤ –∑–∞–ª–µ–∂–Ω–æ –≤—ñ–¥ Œª.

```python
from sklearn.linear_model import lasso_path, ridge_path

# Lasso path
alphas_lasso, coefs_lasso, _ = lasso_path(
    X_train_scaled, y_train, alphas=alphas
)

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
plt.figure(figsize=(12, 6))

for i in range(coefs_lasso.shape[0]):
    plt.plot(alphas_lasso, coefs_lasso[i], label=f'Feature {i}')

plt.xscale('log')
plt.xlabel('Alpha (Œª)', fontsize=12)
plt.ylabel('Coefficients', fontsize=12)
plt.title('Lasso Regularization Path', fontsize=14, fontweight='bold')
plt.axhline(y=0, color='black', linestyle='--', linewidth=1)
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

**–°–ø–æ—Å—Ç–µ—Ä–µ–∂–µ–Ω–Ω—è:**
- –ü—Ä–∏ –º–∞–ª–∏—Ö Œª: –≤—Å—ñ –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç–∏ –Ω–µ–Ω—É–ª—å–æ–≤—ñ
- –ü—Ä–∏ –∑–±—ñ–ª—å—à–µ–Ω–Ω—ñ Œª: –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç–∏ –ø–æ —á–µ—Ä–∑—ñ –∑–∞–Ω—É–ª—è—é—Ç—å—Å—è
- –ü–æ—Ä—è–¥–æ–∫ –∑–∞–Ω—É–ª—è–Ω–Ω—è ‚Üí –≤–∞–∂–ª–∏–≤—ñ—Å—Ç—å –æ–∑–Ω–∞–∫ (–æ—Å—Ç–∞–Ω–Ω—ñ –≤–∞–∂–ª–∏–≤—ñ—à—ñ)

---

## –ü–µ—Ä–µ–≤–∞–≥–∏ —Ç–∞ –Ω–µ–¥–æ–ª—ñ–∫–∏

### Ridge (L2)

**–ü–µ—Ä–µ–≤–∞–≥–∏ ‚úì**
- ‚úÖ –î–æ–±—Ä–µ –ø—Ä–∏ –º—É–ª—å—Ç–∏–∫–æ–ª—ñ–Ω–µ–∞—Ä–Ω–æ—Å—Ç—ñ
- ‚úÖ –°—Ç–∞–±—ñ–ª—å–Ω—ñ –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç–∏
- ‚úÖ –®–≤–∏–¥–∫—ñ –æ–±—á–∏—Å–ª–µ–Ω–Ω—è
- ‚úÖ –ü—Ä–∞—Ü—é—î –ø—Ä–∏ $p > n$

**–ù–µ–¥–æ–ª—ñ–∫–∏ ‚úó**
- ‚ùå –ù–µ —Ä–æ–±–∏—Ç—å feature selection
- ‚ùå –í—Å—ñ –æ–∑–Ω–∞–∫–∏ –∑–∞–ª–∏—à–∞—é—Ç—å—Å—è –≤ –º–æ–¥–µ–ª—ñ
- ‚ùå –ú–µ–Ω—à–∞ —ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å

### Lasso (L1)

**–ü–µ—Ä–µ–≤–∞–≥–∏ ‚úì**
- ‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∞ feature selection
- ‚úÖ Sparse –º–æ–¥–µ–ª—ñ (–º–∞–ª–æ –æ–∑–Ω–∞–∫)
- ‚úÖ –í–∏—Å–æ–∫–∞ —ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å
- ‚úÖ –ü—Ä–∞—Ü—é—î —è–∫ regularization + feature engineering

**–ù–µ–¥–æ–ª—ñ–∫–∏ ‚úó**
- ‚ùå –ù–µ—Å—Ç–∞–±—ñ–ª—å–Ω–∏–π –ø—Ä–∏ –∫–æ—Ä–µ–ª—å–æ–≤–∞–Ω–∏—Ö –æ–∑–Ω–∞–∫–∞—Ö
- ‚ùå –í–∏–±–∏—Ä–∞—î —Ç—ñ–ª—å–∫–∏ –æ–¥–Ω—É –∑ –≥—Ä—É–ø–∏ —Å—Ö–æ–∂–∏—Ö –æ–∑–Ω–∞–∫
- ‚ùå –ü–æ–≤—ñ–ª—å–Ω—ñ—à—ñ –æ–±—á–∏—Å–ª–µ–Ω–Ω—è

### Elastic Net

**–ü–µ—Ä–µ–≤–∞–≥–∏ ‚úì**
- ‚úÖ –ö–æ–º–±—ñ–Ω—É—î –ø–µ—Ä–µ–≤–∞–≥–∏ Ridge —Ç–∞ Lasso
- ‚úÖ Feature selection + —Å—Ç–∞–±—ñ–ª—å–Ω—ñ—Å—Ç—å
- ‚úÖ –î–æ–±—Ä–µ –ø—Ä–∏ –∫–æ—Ä–µ–ª—å–æ–≤–∞–Ω–∏—Ö –æ–∑–Ω–∞–∫–∞—Ö

**–ù–µ–¥–æ–ª—ñ–∫–∏ ‚úó**
- ‚ùå –î–≤–∞ –≥—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –¥–ª—è –ø—ñ–¥–±–æ—Ä—É
- ‚ùå –°–∫–ª–∞–¥–Ω—ñ—à–∞ —ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü—ñ—è

---

## –ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ —è–∫–∏–π –º–µ—Ç–æ–¥?

### Decision Tree üå≥

```
                –ö–æ—Ä–µ–ª—å–æ–≤–∞–Ω—ñ –æ–∑–Ω–∞–∫–∏?
                /                  \
             –¢–∞–∫                    –ù—ñ
              |                      |
       Feature selection?       Feature selection?
         /           \              /           \
       –¢–∞–∫           –ù—ñ           –¢–∞–∫           –ù—ñ
        |             |            |             |
  Elastic Net     Ridge        Lasso         Ridge
                               –∞–±–æ
                          Elastic Net
```

### –ü—Ä–∞–∫—Ç–∏—á–Ω—ñ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó

| –°–∏—Ç—É–∞—Ü—ñ—è | –í–∏–±—ñ—Ä |
|----------|-------|
| –ú—É–ª—å—Ç–∏–∫–æ–ª—ñ–Ω–µ–∞—Ä–Ω—ñ—Å—Ç—å + –≤—Å—ñ –æ–∑–Ω–∞–∫–∏ –≤–∞–∂–ª–∏–≤—ñ | **Ridge** |
| –ë–∞–≥–∞—Ç–æ –Ω–µ–ø–æ—Ç—Ä—ñ–±–Ω–∏—Ö –æ–∑–Ω–∞–∫ | **Lasso** |
| –ö–æ—Ä–µ–ª—å–æ–≤–∞–Ω—ñ –æ–∑–Ω–∞–∫–∏ + feature selection | **Elastic Net** |
| $p > n$ (–±—ñ–ª—å—à–µ –æ–∑–Ω–∞–∫, –Ω—ñ–∂ –¥–∞–Ω–∏—Ö) | **Ridge** –∞–±–æ **Elastic Net** |
| –ü–æ—Ç—Ä—ñ–±–Ω–∞ —ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω—ñ—Å—Ç—å | **Lasso** |
| –ù–µ–≤–ø–µ–≤–Ω–µ–Ω—ñ—Å—Ç—å | **Elastic Net** (—É–Ω—ñ–≤–µ—Ä—Å–∞–ª—å–Ω–∏–π) |

---

## –ü—Ä–∞–∫—Ç–∏—á–Ω—ñ –ø–æ—Ä–∞–¥–∏ üí°

1. **–ó–ê–í–ñ–î–ò –Ω–æ—Ä–º–∞–ª—ñ–∑—É–π** –¥–∞–Ω—ñ –ø–µ—Ä–µ–¥ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—î—é (StandardScaler)
2. **Cross-validation** –¥–ª—è –ø—ñ–¥–±–æ—Ä—É Œª ‚Äî –Ω–µ –≤–≥–∞–¥—É–π!
3. **–ü–æ—á–Ω–∏ –∑ Ridge** ‚Äî –ø—Ä–æ—Å—Ç—ñ—à–∏–π, —à–≤–∏–¥—à–∏–π baseline
4. **Lasso –¥–ª—è feature selection** ‚Äî –∫–æ–ª–∏ –±–∞–≥–∞—Ç–æ –æ–∑–Ω–∞–∫
5. **Elastic Net –ø—Ä–∏ —Å—É–º–Ω—ñ–≤–∞—Ö** ‚Äî —É–Ω—ñ–≤–µ—Ä—Å–∞–ª—å–Ω–∏–π –≤–∏–±—ñ—Ä
6. **Regularization path** ‚Äî –ø–æ–¥–∏–≤–∏—Å—å, —è–∫ –∑–Ω–∏–∫–∞—é—Ç—å –æ–∑–Ω–∞–∫–∏
7. **–ù–µ —Ä–µ–≥—É–ª—è—Ä–∏–∑—É–π intercept** ‚Äî —Ç—ñ–ª—å–∫–∏ –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç–∏ –æ–∑–Ω–∞–∫
8. **Grid Search** –¥–ª—è Elastic Net ‚Äî –ø—ñ–¥–±–∏—Ä–∞–π Œ± —Ç–∞ Œª —Ä–∞–∑–æ–º
9. **–ü–æ—Ä—ñ–≤–Ω—é–π train vs test** ‚Äî —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—è –∑–º–µ–Ω—à—É—î overfitting
10. **Feature importance** –∑ Lasso ‚Äî —è–∫—ñ –æ–∑–Ω–∞–∫–∏ –∑–∞–ª–∏—à–∏–ª–∏—Å—å?

---

## –ü–æ—à–∏—Ä–µ–Ω—ñ –ø–æ–º–∏–ª–∫–∏ ‚ùå

### 1. –ó–∞–±—É—Ç–∏ –Ω–æ—Ä–º–∞–ª—ñ–∑—É–≤–∞—Ç–∏

```python
# ‚ùå –ù–ï–ü–†–ê–í–ò–õ–¨–ù–û
ridge = Ridge(alpha=1.0)
ridge.fit(X_train, y_train)

# ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
ridge.fit(X_train_scaled, y_train)
```

### 2. –ü—ñ–¥–±–∏—Ä–∞—Ç–∏ Œª –Ω–∞ train –±–µ–∑ CV

```python
# ‚ùå –ù–ï–ü–†–ê–í–ò–õ–¨–ù–û
best_alpha = None
best_score = -np.inf
for alpha in alphas:
    ridge = Ridge(alpha=alpha)
    ridge.fit(X_train, y_train)
    score = ridge.score(X_train, y_train)  # Overfitting!
    if score > best_score:
        best_score = score
        best_alpha = alpha

# ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û
ridge_cv = RidgeCV(alphas=alphas, cv=5)
ridge_cv.fit(X_train, y_train)
best_alpha = ridge_cv.alpha_
```

### 3. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –∑–∞–Ω–∞–¥—Ç–æ –≤–µ–ª–∏–∫–∏–π Œª

```python
# –Ø–∫—â–æ Œª ‚Üí ‚àû, –≤—Å—ñ –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç–∏ ‚Üí 0
# –ú–æ–¥–µ–ª—å —Å—Ç–∞—î –∫–æ–Ω—Å—Ç–∞–Ω—Ç–æ—é (underfitting)
ridge = Ridge(alpha=1e10)  # ‚ùå –ó–∞–Ω–∞–¥—Ç–æ –±–∞–≥–∞—Ç–æ!
```

### 4. –ù–µ –≤—Ä–∞—Ö—É–≤–∞—Ç–∏ —Ä—ñ–∑–Ω–∏—Ü—é –º—ñ–∂ Œ± (sklearn) —Ç–∞ Œª

–£ scikit-learn –¥–ª—è Logistic Regression:
- **C = 1/Œª**
- –ë—ñ–ª—å—à–µ C ‚Üí –º–µ–Ω—à–∞ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—è
- –ú–µ–Ω—à–µ C ‚Üí –±—ñ–ª—å—à–∞ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—è

```python
# Ridge Regression
Ridge(alpha=1.0)  # alpha = Œª

# Logistic Regression
LogisticRegression(C=1.0)  # C = 1/Œª
```

---

## –ú–∞—Ç–µ–º–∞—Ç–∏—á–Ω–µ –ø–æ—è—Å–Ω–µ–Ω–Ω—è

### –ß–æ–º—É L1 –∑–∞–Ω—É–ª—è—î –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç–∏?

**–ì—Ä–∞–¥—ñ—î–Ω—Ç L1:**
$$\frac{\partial}{\partial \beta_j} |\beta_j| = \text{sign}(\beta_j) = \begin{cases}
+1 & \text{—è–∫—â–æ } \beta_j > 0 \\
-1 & \text{—è–∫—â–æ } \beta_j < 0 \\
\text{undefined} & \text{—è–∫—â–æ } \beta_j = 0
\end{cases}$$

**–ì—Ä–∞–¥—ñ—î–Ω—Ç L2:**
$$\frac{\partial}{\partial \beta_j} \beta_j^2 = 2\beta_j$$

**–†—ñ–∑–Ω–∏—Ü—è:**
- L1: –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω–∏–π –≥—Ä–∞–¥—ñ—î–Ω—Ç ‚Üí –º–æ–∂–µ –¥–æ—Å—è–≥—Ç–∏ 0
- L2: –≥—Ä–∞–¥—ñ—î–Ω—Ç –ø—Ä–æ–ø–æ—Ä—Ü—ñ–π–Ω–∏–π Œ≤ ‚Üí –Ω—ñ–∫–æ–ª–∏ –Ω–µ –¥–æ—Å—è–≥–∞—î 0

### –ì–µ–æ–º–µ—Ç—Ä–∏—á–Ω–∞ —ñ–Ω—Ç—É—ó—Ü—ñ—è

**L1 constraint region:** —Ä–æ–º–± (–≥–æ—Å—Ç—Ä—ñ –∫—É—Ç–∏ –Ω–∞ –æ—Å—è—Ö)
**L2 constraint region:** –∫–æ–ª–æ (–≥–ª–∞–¥–∫–µ)

–û–ø—Ç–∏–º—É–º —á–∞—Å—Ç–æ –ø–æ–ø–∞–¥–∞—î –Ω–∞ –∫—É—Ç —Ä–æ–º–±–∞ = –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞ –¥–æ—Ä—ñ–≤–Ω—é—î 0.

---

## –ü–æ–≤'—è–∑–∞–Ω—ñ —Ç–µ–º–∏

- [[01_Linear_Regression]] ‚Äî –±–∞–∑–æ–≤–∞ —Ä–µ–≥—Ä–µ—Å—ñ—è
- [[02_Logistic_Regression]] ‚Äî –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è –∑ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—î—é
- [[Cross_Validation]] ‚Äî –ø—ñ–¥–±—ñ—Ä –≥—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤
- [[Feature_Selection]] ‚Äî –≤—ñ–¥–±—ñ—Ä –æ–∑–Ω–∞–∫
- [[Bias_Variance_Tradeoff]] ‚Äî —Ç–µ–æ—Ä—ñ—è overfitting

## –†–µ—Å—É—Ä—Å–∏

- [Scikit-learn: Linear Models](https://scikit-learn.org/stable/modules/linear_model.html)
- [StatQuest: Regularization](https://www.youtube.com/watch?v=Q81RR3yKn30)
- [Andrew Ng: Regularization](https://www.coursera.org/learn/machine-learning)
- [Elements of Statistical Learning](https://hastie.su.domains/ElemStatLearn/)

---

## –ö–ª—é—á–æ–≤—ñ –≤–∏—Å–Ω–æ–≤–∫–∏

> –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—è –¥–æ–¥–∞—î —à—Ç—Ä–∞—Ñ –¥–æ —Ñ—É–Ω–∫—Ü—ñ—ó –≤—Ç—Ä–∞—Ç –¥–ª—è –∑–∞–ø–æ–±—ñ–≥–∞–Ω–Ω—è overfitting —Ç–∞ –∫–æ–Ω—Ç—Ä–æ–ª—é —Å–∫–ª–∞–¥–Ω–æ—Å—Ç—ñ –º–æ–¥–µ–ª—ñ.

**–û—Å–Ω–æ–≤–Ω—ñ —Ç–∏–ø–∏:**
- **Ridge (L2):** $\text{MSE} + \lambda \sum \beta_j^2$ ‚Äî –∑–º–µ–Ω—à—É—î –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç–∏
- **Lasso (L1):** $\text{MSE} + \lambda \sum |\beta_j|$ ‚Äî –∑–∞–Ω—É–ª—è—î –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç–∏
- **Elastic Net:** –∫–æ–º–±—ñ–Ω–∞—Ü—ñ—è L1 + L2

**–ö–ª—é—á–æ–≤—ñ –ø—Ä–∏–Ω—Ü–∏–ø–∏:**
- –ó–∞–≤–∂–¥–∏ –Ω–æ—Ä–º–∞–ª—ñ–∑—É–π –¥–∞–Ω—ñ
- –ü—ñ–¥–±–∏—Ä–∞–π Œª —á–µ—Ä–µ–∑ cross-validation
- Ridge –¥–ª—è —Å—Ç–∞–±—ñ–ª—å–Ω–æ—Å—Ç—ñ, Lasso –¥–ª—è feature selection
- Elastic Net ‚Äî —É–Ω—ñ–≤–µ—Ä—Å–∞–ª—å–Ω–∏–π –≤–∏–±—ñ—Ä

**–ö–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏:**
- Overfitting + –±–∞–≥–∞—Ç–æ –æ–∑–Ω–∞–∫ + –º—É–ª—å—Ç–∏–∫–æ–ª—ñ–Ω–µ–∞—Ä–Ω—ñ—Å—Ç—å = Regularization ‚úì

---

#ml #supervised-learning #regularization #ridge #lasso #elastic-net #overfitting
